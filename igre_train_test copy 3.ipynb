{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from igre_dqn_v2 import IgreDQNTrainer\n",
    "from igre_v1 import Igre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'causal_attention_cuda'\n",
      "Successfully imported the causal dot product kernel! \n",
      "Successfully imported the FLA triton kernels! \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from based.models.gpt import GPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPTLMHeadModel.from_pretrained_hf(\"hazyresearch/based-360m\")\n",
    "\n",
    "based_model = model.transformer\n",
    "lm_head = model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sim4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sim_4_training_data_unpickled = pickle.load(open(\"sim4_data2.pkl\", \"rb\"))\n",
    "len(sim_4_training_data_unpickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<simulation.sim4.actors.player_actor.HumanPlayerActor at 0x7f4e89933250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_4_training_data_unpickled[1].actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Igre(\n",
       "  (encoder): IEncoderBased(\n",
       "    (based_model): GPTModel(\n",
       "      (embeddings): GPT2Embeddings(\n",
       "        (word_embeddings): Embedding(50264, 1024)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (mixer): LinearAttention(\n",
       "            (feature_map): TaylorExp()\n",
       "            (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (mixer): SlidingAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (3-5): 3 x Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (mixer): LinearAttention(\n",
       "            (feature_map): TaylorExp()\n",
       "            (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (mixer): SlidingAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (8-10): 3 x Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (mixer): LinearAttention(\n",
       "            (feature_map): TaylorExp()\n",
       "            (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (12): Block(\n",
       "          (mixer): SlidingAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (13-15): 3 x Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (16): Block(\n",
       "          (mixer): LinearAttention(\n",
       "            (feature_map): TaylorExp()\n",
       "            (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (17): Block(\n",
       "          (mixer): SlidingAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (18-20): 3 x Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (21): Block(\n",
       "          (mixer): LinearAttention(\n",
       "            (feature_map): TaylorExp()\n",
       "            (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "            (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (22): Block(\n",
       "          (mixer): SlidingAttention(\n",
       "            (rotary_emb): RotaryEmbedding()\n",
       "            (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "            (inner_attn): FlashSelfAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (inner_cross_attn): FlashCrossAttention(\n",
       "              (drop): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "        (23-26): 4 x Block(\n",
       "          (mixer): BaseConv(\n",
       "            (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (conv): ShortConvolution(\n",
       "              (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "            (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (drop_f): Dropout(p=0, inplace=False)\n",
       "      (ln_f): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (sys1): iSys1(\n",
       "    (option_gen): iOptionGen(\n",
       "      (c_embedding): Embedding(5, 1024)\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (actor): iActor(\n",
       "      (decoder): iDecoderBased(\n",
       "        (based_model): GPTModel(\n",
       "          (embeddings): GPT2Embeddings(\n",
       "            (word_embeddings): Embedding(50264, 1024)\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0): Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (1): Block(\n",
       "              (mixer): LinearAttention(\n",
       "                (feature_map): TaylorExp()\n",
       "                (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (2): Block(\n",
       "              (mixer): SlidingAttention(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (3-5): 3 x Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (6): Block(\n",
       "              (mixer): LinearAttention(\n",
       "                (feature_map): TaylorExp()\n",
       "                (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (7): Block(\n",
       "              (mixer): SlidingAttention(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (8-10): 3 x Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (11): Block(\n",
       "              (mixer): LinearAttention(\n",
       "                (feature_map): TaylorExp()\n",
       "                (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (12): Block(\n",
       "              (mixer): SlidingAttention(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (13-15): 3 x Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (16): Block(\n",
       "              (mixer): LinearAttention(\n",
       "                (feature_map): TaylorExp()\n",
       "                (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (17): Block(\n",
       "              (mixer): SlidingAttention(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (18-20): 3 x Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (21): Block(\n",
       "              (mixer): LinearAttention(\n",
       "                (feature_map): TaylorExp()\n",
       "                (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "                (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (22): Block(\n",
       "              (mixer): SlidingAttention(\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "                (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "                (inner_attn): FlashSelfAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (inner_cross_attn): FlashCrossAttention(\n",
       "                  (drop): Dropout(p=0, inplace=False)\n",
       "                )\n",
       "                (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "            (23-26): 4 x Block(\n",
       "              (mixer): BaseConv(\n",
       "                (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (conv): ShortConvolution(\n",
       "                  (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0, inplace=False)\n",
       "              (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm1): RMSNorm()\n",
       "              (mlp): GatedMlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              )\n",
       "              (dropout2): Dropout(p=0, inplace=False)\n",
       "              (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "              (norm2): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (drop_f): Dropout(p=0, inplace=False)\n",
       "          (ln_f): RMSNorm()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (critic): iCritic(\n",
       "      (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "    (appraiserfast): iOptionAppraiserFast(\n",
       "      (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sys2): iSys2(\n",
       "    (pos_embedding): Embedding(9, 1024)\n",
       "    (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Igre(based_model, lm_head, tokenizer)\n",
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_NAN = True\n",
    "def check_activation_for_nan(name):\n",
    "    def hook(model, input, output):\n",
    "        if type(output) is tuple:\n",
    "            for i, out in enumerate(output):\n",
    "                if type(out) is torch.Tensor and torch.isnan(out).any():\n",
    "                    print(f\"Found NaN in {name}[{i}]\")\n",
    "        elif type(output) is torch.Tensor:\n",
    "            if torch.isnan(output).any():\n",
    "                print(f\"Found NaN in {name}\")\n",
    "    return hook\n",
    "if DEBUG_NAN:\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(check_activation_for_nan(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_trainer = IgreDQNTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "teacher_forcing_count = 0\n",
    "\n",
    "def get_completion(prompt):\n",
    "    global teacher_forcing_count\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    response_ids, z = dqn_trainer.get_response(prompt_ids)\n",
    "    metadata = {\n",
    "        \"z\": z,\n",
    "        \"response_ids\": response_ids.detach().cpu()\n",
    "    }\n",
    "    # use_teacher_forcing = random.random() < 0.5\n",
    "    # if use_teacher_forcing and teacher_forcing_count < 100:\n",
    "    #     teacher_forcing_count += 1\n",
    "    #     print(\"use teacher forcing\")\n",
    "    #     finish_X_re = re.compile(\" finish ([ABCD])\")\n",
    "    #     match = finish_X_re.search(prompt)\n",
    "    #     if match:\n",
    "    #         finish_X = match.group(0).strip()\n",
    "    #         return finish_X, metadata\n",
    "    print(\"completion response ids\")\n",
    "    print(response_ids)\n",
    "    response = tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=True)\n",
    "    # if '\"' in response or \"'\" in response:\n",
    "    #     # Remove quotes\n",
    "    #     response = response.replace('\"', \"\")\n",
    "    #     response = response.replace(\"'\", \"\")\n",
    "    #     response_ids = tokenizer.encode(response, return_tensors=\"pt\").cuda()\n",
    "    return tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=True), metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288]], device='cuda:0')\n",
      "test\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([9.2419], device='cuda:0')\n",
      "max logit tensor(9.2419, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532]], device='cuda:0')\n",
      "test -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.3626], device='cuda:0')\n",
      "max logit tensor(10.3626, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532]], device='cuda:0')\n",
      "test - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([11.3673], device='cuda:0')\n",
      "max logit tensor(11.3673, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532]], device='cuda:0')\n",
      "test - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([12.4585], device='cuda:0')\n",
      "max logit tensor(12.4585, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532,  532]], device='cuda:0')\n",
      "test - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([11.5816], device='cuda:0')\n",
      "max logit tensor(11.5816, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532,  532,  532]], device='cuda:0')\n",
      "test - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([11.0527], device='cuda:0')\n",
      "max logit tensor(11.0527, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532,  532,  532,  532]], device='cuda:0')\n",
      "test - - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.9504], device='cuda:0')\n",
      "max logit tensor(10.9504, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532,  532,  532,  532,  532]], device='cuda:0')\n",
      "test - - - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.8635], device='cuda:0')\n",
      "max logit tensor(10.8635, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[9288,  532,  532,  532,  532,  532,  532,  532,  532]],\n",
      "       device='cuda:0')\n",
      "test - - - - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.7140], device='cuda:0')\n",
      "max logit tensor(10.7140, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[9288,  532,  532,  532,  532,  532,  532,  532,  532,  532]],\n",
      "       device='cuda:0')\n",
      "test - - - - - - - - -\n",
      "completion response ids\n",
      "tensor([[532, 532, 532, 532, 532, 532, 532, 532, 532]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' - - - - - - - - -',\n",
       " {'z': tensor([[-0.6576,  0.0202,  0.7125,  ..., -0.1541, -0.1631, -0.3022]],\n",
       "         device='cuda:0'),\n",
       "  'response_ids': tensor([[532, 532, 532, 532, 532, 532, 532, 532, 532]])})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules\n",
      "['/home/mocuto/code/mamba-chat/simulation/sim4/__init__.py', '/home/mocuto/code/mamba-chat/simulation/sim3/__init__.py', '/home/mocuto/code/mamba-chat/simulation/sim0/__init__.py']\n",
      "y\n",
      "simulation.sim4\n",
      "y\n",
      "simulation.sim3\n",
      "y\n",
      "simulation.sim0\n"
     ]
    }
   ],
   "source": [
    "from simulation.sim_runner import SimRunner\n",
    "from simulation.common.base_actor import AIActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transition(transition, finetune=False, finetune_state_to_prompt_fn=None):\n",
    "    assert finetune == False or (finetune == True and finetune_state_to_prompt_fn is not None)\n",
    "    def inner(prompt, response, next_state_prompt):\n",
    "        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        response_ids = tokenizer.encode(\n",
    "            response, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).long()\n",
    "        # Add eos token to response_ids\n",
    "        response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        next_state_prompt_ids = tokenizer.encode(\n",
    "            next_state_prompt, return_tensors=\"pt\", add_special_tokens=True\n",
    "        ).long()\n",
    "\n",
    "        act_metadata = transition.action.metadata\n",
    "        z = torch.zeros(1)\n",
    "        if act_metadata is not None and \"z\" in act_metadata:\n",
    "            z = act_metadata[\"z\"]\n",
    "        if act_metadata is not None and \"response_ids\" in act_metadata:\n",
    "            response_ids = act_metadata[\"response_ids\"]\n",
    "            response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        reward = (transition.next_state.score - transition.state.score) / 100\n",
    "        reward = torch.tensor([reward]).float().detach()\n",
    "\n",
    "        next_state_is_terminal = torch.tensor([1. if transition.next_state.is_terminal else 0.])\n",
    "\n",
    "        print(f\"prompt: {prompt}\")\n",
    "        print(f\"response: {response}\")\n",
    "        dqn_trainer.add_transition(\n",
    "            prompt_ids.squeeze(0),\n",
    "            response_ids.squeeze(0),\n",
    "            z,\n",
    "            reward,\n",
    "            next_state_prompt_ids.squeeze(0),\n",
    "            next_state_is_terminal,\n",
    "            add_to_finetune_memory=finetune,\n",
    "        )\n",
    "    if finetune:\n",
    "        # Check the classname for \"player\"\n",
    "        if \"player\" in transition.actor.__class__.__name__.lower():\n",
    "            assert finetune_state_to_prompt_fn is not None\n",
    "            prompt = finetune_state_to_prompt_fn(transition.state)\n",
    "            response = transition.action.content\n",
    "            next_state_prompt = finetune_state_to_prompt_fn(transition.next_state)\n",
    "            inner(prompt, response, next_state_prompt)\n",
    "    else:\n",
    "        if isinstance(transition.actor, AIActor) and transition.actor.name == \"player\":\n",
    "            prompt = transition.action.completion_prompt\n",
    "            response = transition.action.completion_response\n",
    "\n",
    "            print(\"## DEBUG prompt\")\n",
    "            print(prompt)\n",
    "            prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            print(prompt_ids)\n",
    "\n",
    "            next_state = transition.next_state\n",
    "            next_state_prompt = transition.actor.state_to_prompt(next_state)\n",
    "            inner(prompt, response, next_state_prompt)\n",
    "\n",
    "        # prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        # response_ids = tokenizer.encode(\n",
    "        #     response, return_tensors=\"pt\", add_special_tokens=False\n",
    "        # ).long()\n",
    "        # # Add eos token to response_ids\n",
    "        # response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        # next_state_prompt_ids = tokenizer.encode(\n",
    "        #     next_state_prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "        # ).long()\n",
    "\n",
    "        # z = transition.action.metadata[\"z\"]\n",
    "        # reward = transition.next_state.score - transition.state.score\n",
    "        # reward = torch.tensor([reward])\n",
    "        # print(f\"prompt: {prompt}\")\n",
    "        # print(f\"response: {response}\")\n",
    "        # dqn_trainer.add_transition(\n",
    "        #     prompt_ids.squeeze(0),\n",
    "        #     response_ids.squeeze(0),\n",
    "        #     z,\n",
    "        #     reward,\n",
    "        #     next_state_prompt_ids.squeeze(0),\n",
    "        #     transition.next_state.is_terminal,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_name = \"sim4\"\n",
    "sim_runner = SimRunner(\n",
    "    sim_name=sim_name,\n",
    "    seed=1,\n",
    "    model_forward_func_for_ai={\n",
    "        \"player\": get_completion,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process transitions using the player actor in the sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<simulation.sim4.actors.player_actor.AIPlayerActor at 0x7f4e89a87130>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_actor = None\n",
    "for actor in sim_runner.sim.actors:\n",
    "    if isinstance(actor, AIActor):\n",
    "        ai_actor = actor\n",
    "        break\n",
    "ai_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 13 + 22\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.1108,  7.6410,  2.5764,  6.9331,  3.1296,  7.2354]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 19.455163955688477\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 13 + 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/code/mamba-chat/igre_dqn_v2.py:286: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  F.mse_loss(tmp.detach().cpu(), reward.view(-1, 1)).detach().cpu().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.1108,  7.6410,  2.5764,  6.9331,  3.1296,  7.2354]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 19.455163955688477\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 13 + 22\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.1108,  7.6410,  2.5764,  6.9331,  3.1296,  7.2354]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 19.455163955688477\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 13 + 22\n",
      "judge: 13 + 22 = 35\n",
      "\n",
      "player:\n",
      "response: finish x = 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4116, 10.4455,  2.7391,  9.8091,  1.5058,  7.8234]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 28.08022689819336\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 13 + 22\n",
      "judge: 13 + 22 = 35\n",
      "\n",
      "player:\n",
      "response: finish x = 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4116, 10.4455,  2.7391,  9.8091,  1.5058,  7.8234]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 28.08022689819336\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 13 + 22\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 13 + 22\n",
      "judge: 13 + 22 = 35\n",
      "\n",
      "player:\n",
      "response: finish x = 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4116, 10.4455,  2.7391,  9.8091,  1.5058,  7.8234]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 28.08022689819336\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 5 + 74\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0798, 3.4797, 4.5618, 9.1240, 1.6407, 7.2900]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 11.374847412109375\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 5 + 74\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0798, 3.4797, 4.5618, 9.1240, 1.6407, 7.2900]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 11.374847412109375\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 5 + 74\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0798, 3.4797, 4.5618, 9.1240, 1.6407, 7.2900]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 11.374847412109375\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 38 + 51\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5207,  7.4743, -1.5031,  7.0108,  3.4200,  7.0177]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.394373893737793\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 38 + 51\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5207,  7.4743, -1.5031,  7.0108,  3.4200,  7.0177]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.394373893737793\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 38 + 51\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5207,  7.4743, -1.5031,  7.0108,  3.4200,  7.0177]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.394373893737793\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 38 + 51\n",
      "judge: 38 + 51 = 89\n",
      "\n",
      "player:\n",
      "response: finish x = 89\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3090, 11.1696,  2.6174, 10.0499,  3.3996,  8.1975]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 33.64576721191406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 38 + 51\n",
      "judge: 38 + 51 = 89\n",
      "\n",
      "player:\n",
      "response: finish x = 89\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3090, 11.1696,  2.6174, 10.0499,  3.3996,  8.1975]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 33.64576721191406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 38 + 51\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 38 + 51\n",
      "judge: 38 + 51 = 89\n",
      "\n",
      "player:\n",
      "response: finish x = 89\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3090, 11.1696,  2.6174, 10.0499,  3.3996,  8.1975]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 33.64576721191406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 68 + 18\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.6072,  7.6054,  0.7896,  6.8661,  3.4744,  6.9070]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 16.127273559570312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 68 + 18\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.6072,  7.6054,  0.7896,  6.8661,  3.4744,  6.9070]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 16.127273559570312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 68 + 18\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.6072,  7.6054,  0.7896,  6.8661,  3.4744,  6.9070]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 16.127273559570312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 68 + 18\n",
      "judge: 68 + 18 = 86\n",
      "\n",
      "player:\n",
      "response: finish x = 86\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  4.8670, 10.6153,  2.2876,  9.7948,  3.9311,  8.0447]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.361093521118164\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 68 + 18\n",
      "judge: 68 + 18 = 86\n",
      "\n",
      "player:\n",
      "response: finish x = 86\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  4.8670, 10.6153,  2.2876,  9.7948,  3.9311,  8.0447]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.361093521118164\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 68 + 18\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 68 + 18\n",
      "judge: 68 + 18 = 86\n",
      "\n",
      "player:\n",
      "response: finish x = 86\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  4.8670, 10.6153,  2.2876,  9.7948,  3.9311,  8.0447]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.361093521118164\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 59 + 21\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3529,  7.5999, -0.4744,  7.0240,  2.9345,  7.2958]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 14.805645942687988\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 59 + 21\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3529,  7.5999, -0.4744,  7.0240,  2.9345,  7.2958]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 14.805645942687988\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 59 + 21\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3529,  7.5999, -0.4744,  7.0240,  2.9345,  7.2958]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 14.805645942687988\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "\n",
      "player:\n",
      "response: calc x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  1.2923,  9.1166,  3.3322, 11.2082,  4.0675,  7.4347]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 38.25733184814453\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "\n",
      "player:\n",
      "response: calc x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  1.2923,  9.1166,  3.3322, 11.2082,  4.0675,  7.4347]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 38.25733184814453\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "\n",
      "player:\n",
      "response: calc x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  1.2923,  9.1166,  3.3322, 11.2082,  4.0675,  7.4347]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 38.25733184814453\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "player: calc x = 80\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0835,\n",
      "          9.8691,  3.0658, 11.3281,  4.2224,  8.4043]], device='cuda:0')\n",
      "## DEBUG priority: 34.09250259399414\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "player: calc x = 80\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0835,\n",
      "          9.8691,  3.0658, 11.3281,  4.2224,  8.4043]], device='cuda:0')\n",
      "## DEBUG priority: 34.09250259399414\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 59 + 21\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 59 + 21\n",
      "judge: 59 + 21 = 80\n",
      "player: calc x = 80\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 80\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0835,\n",
      "          9.8691,  3.0658, 11.3281,  4.2224,  8.4043]], device='cuda:0')\n",
      "## DEBUG priority: 34.09250259399414\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 65 + 88\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4723,  7.4948,  2.0929,  6.7050,  3.1894,  7.6765]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.413162231445312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 65 + 88\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4723,  7.4948,  2.0929,  6.7050,  3.1894,  7.6765]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.413162231445312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 65 + 88\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4723,  7.4948,  2.0929,  6.7050,  3.1894,  7.6765]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.413162231445312\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 65 + 88\n",
      "judge: 65 + 88 = 153\n",
      "\n",
      "player:\n",
      "response: finish x = 153\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3772, 11.0711,  2.5352, 10.6155,  3.5857,  8.3309]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 35.1562385559082\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 65 + 88\n",
      "judge: 65 + 88 = 153\n",
      "\n",
      "player:\n",
      "response: finish x = 153\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3772, 11.0711,  2.5352, 10.6155,  3.5857,  8.3309]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 35.1562385559082\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 65 + 88\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 65 + 88\n",
      "judge: 65 + 88 = 153\n",
      "\n",
      "player:\n",
      "response: finish x = 153\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3772, 11.0711,  2.5352, 10.6155,  3.5857,  8.3309]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 35.1562385559082\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 78\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0957, 3.5624, 4.5632, 9.2897, 0.1914, 7.1557]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 9.941512107849121\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 78\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0957, 3.5624, 4.5632, 9.2897, 0.1914, 7.1557]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 9.941512107849121\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 78\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0957, 3.5624, 4.5632, 9.2897, 0.1914, 7.1557]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 9.941512107849121\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 76 + 90\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4656,  7.5367,  1.6968,  6.7682,  3.5818,  7.2946]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.023574829101562\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 76 + 90\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4656,  7.5367,  1.6968,  6.7682,  3.5818,  7.2946]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.023574829101562\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 76 + 90\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.4656,  7.5367,  1.6968,  6.7682,  3.5818,  7.2946]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.023574829101562\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 76 + 90\n",
      "judge: 76 + 90 = 166\n",
      "\n",
      "player:\n",
      "response: finish x = 166\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0017, 11.3351,  2.3642, 10.3411,  2.6861,  8.5399]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.733863830566406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 76 + 90\n",
      "judge: 76 + 90 = 166\n",
      "\n",
      "player:\n",
      "response: finish x = 166\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0017, 11.3351,  2.3642, 10.3411,  2.6861,  8.5399]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.733863830566406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 76 + 90\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 76 + 90\n",
      "judge: 76 + 90 = 166\n",
      "\n",
      "player:\n",
      "response: finish x = 166\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0017, 11.3351,  2.3642, 10.3411,  2.6861,  8.5399]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.733863830566406\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 69 + 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3661,  7.5116, -0.3823,  6.7183,  2.2474,  7.0376]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.233261108398438\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 69 + 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3661,  7.5116, -0.3823,  6.7183,  2.2474,  7.0376]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.233261108398438\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 69 + 35\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.3661,  7.5116, -0.3823,  6.7183,  2.2474,  7.0376]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.233261108398438\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 69 + 35\n",
      "judge: 69 + 35 = 104\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.1327, 10.5490,  2.3749, 10.3710,  3.6509,  8.1336]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.62708282470703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 69 + 35\n",
      "judge: 69 + 35 = 104\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.1327, 10.5490,  2.3749, 10.3710,  3.6509,  8.1336]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.62708282470703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 69 + 35\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 69 + 35\n",
      "judge: 69 + 35 = 104\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.1327, 10.5490,  2.3749, 10.3710,  3.6509,  8.1336]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.62708282470703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 6 + 3\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x  = 9\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0144, 3.5639, 4.5661, 6.8285, 6.3392, 1.5277, 6.4633]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.215521812438965\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 6 + 3\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x  = 9\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0144, 3.5639, 4.5661, 6.8285, 6.3392, 1.5277, 6.4633]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.215521812438965\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 6 + 3\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x  = 9\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0144, 3.5639, 4.5661, 6.8285, 6.3392, 1.5277, 6.4633]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.215521812438965\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 36 + 68\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7909,  7.4940,  0.4007,  6.8316,  3.3273,  7.3390]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 15.55201244354248\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 36 + 68\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7909,  7.4940,  0.4007,  6.8316,  3.3273,  7.3390]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 15.55201244354248\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 36 + 68\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7909,  7.4940,  0.4007,  6.8316,  3.3273,  7.3390]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 15.55201244354248\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "\n",
      "player:\n",
      "response: calc x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.4289,  9.4590,  3.2716, 10.5750,  3.7904,  7.2741]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 34.9260139465332\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "\n",
      "player:\n",
      "response: calc x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.4289,  9.4590,  3.2716, 10.5750,  3.7904,  7.2741]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 34.9260139465332\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "\n",
      "player:\n",
      "response: calc x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.4289,  9.4590,  3.2716, 10.5750,  3.7904,  7.2741]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 34.9260139465332\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "player: calc x = 104\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3813,\n",
      "         10.5089,  3.8206, 11.1131,  4.3003,  8.5581]], device='cuda:0')\n",
      "## DEBUG priority: 37.4998664855957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "player: calc x = 104\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3813,\n",
      "         10.5089,  3.8206, 11.1131,  4.3003,  8.5581]], device='cuda:0')\n",
      "## DEBUG priority: 37.4998664855957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 68\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 36 + 68\n",
      "judge: 36 + 68 = 104\n",
      "player: calc x = 104\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response: finish x = 104\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3813,\n",
      "         10.5089,  3.8206, 11.1131,  4.3003,  8.5581]], device='cuda:0')\n",
      "## DEBUG priority: 37.4998664855957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 42 + 15\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 57\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0704, 3.6346, 4.7646, 9.4012, 0.5986, 7.0357]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.633072853088379\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 42 + 15\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 57\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0704, 3.6346, 4.7646, 9.4012, 0.5986, 7.0357]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.633072853088379\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 42 + 15\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish x = 57\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000,\n",
      "         0.0000, 0.0704, 3.6346, 4.7646, 9.4012, 0.5986, 7.0357]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 10.633072853088379\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.2034,  3.8578,  6.5916, -1.0216,  6.0989]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 17.272645950317383\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.2034,  3.8578,  6.5916, -1.0216,  6.0989]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 17.272645950317383\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: finish = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.2034,  3.8578,  6.5916, -1.0216,  6.0989]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 17.272645950317383\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: finish = 70\n",
      "judge: Invalid format. Must be in the form 'finish <letter> = <number>'\n",
      "\n",
      "player:\n",
      "response: finish x = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000,\n",
      "         -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 4.8325, 8.2901, 2.0486, 9.3779, 1.7123, 6.5938]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 20.12303352355957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: finish = 70\n",
      "judge: Invalid format. Must be in the form 'finish <letter> = <number>'\n",
      "\n",
      "player:\n",
      "response: finish x = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000,\n",
      "         -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 4.8325, 8.2901, 2.0486, 9.3779, 1.7123, 6.5938]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 20.12303352355957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 36 + 34\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: finish = 70\n",
      "judge: Invalid format. Must be in the form 'finish <letter> = <number>'\n",
      "\n",
      "player:\n",
      "response: finish x = 70\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, 0.0000,\n",
      "         -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 4.8325, 8.2901, 2.0486, 9.3779, 1.7123, 6.5938]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 20.12303352355957\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 80 + 87\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.2343,  7.5151,  2.7875,  6.8596,  2.5087,  7.5133]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.792179107666016\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 80 + 87\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.2343,  7.5151,  2.7875,  6.8596,  2.5087,  7.5133]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.792179107666016\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 80 + 87\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.2343,  7.5151,  2.7875,  6.8596,  2.5087,  7.5133]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 18.792179107666016\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 80 + 87\n",
      "judge: 80 + 87 = 167\n",
      "\n",
      "player:\n",
      "response: finish x = 167\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3083, 11.1196,  2.6896, 10.5703,  1.4515,  8.3846]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.330093383789062\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 80 + 87\n",
      "judge: 80 + 87 = 167\n",
      "\n",
      "player:\n",
      "response: finish x = 167\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3083, 11.1196,  2.6896, 10.5703,  1.4515,  8.3846]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.330093383789062\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 80 + 87\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 80 + 87\n",
      "judge: 80 + 87 = 167\n",
      "\n",
      "player:\n",
      "response: finish x = 167\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.3083, 11.1196,  2.6896, 10.5703,  1.4515,  8.3846]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 31.330093383789062\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 33 + 27\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7771,  7.4719, -0.7937,  6.8258,  3.8489,  6.6124]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.749622344970703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 33 + 27\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7771,  7.4719, -0.7937,  6.8258,  3.8489,  6.6124]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.749622344970703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 33 + 27\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.7771,  7.4719, -0.7937,  6.8258,  3.8489,  6.6124]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 13.749622344970703\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 33 + 27\n",
      "judge: 33 + 27 = 60\n",
      "\n",
      "player:\n",
      "response: finish x = 60\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0238, 10.6087,  3.4040, 10.1160,  2.9425,  7.8669]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.15244674682617\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 33 + 27\n",
      "judge: 33 + 27 = 60\n",
      "\n",
      "player:\n",
      "response: finish x = 60\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0238, 10.6087,  3.4040, 10.1160,  2.9425,  7.8669]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.15244674682617\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 33 + 27\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 33 + 27\n",
      "judge: 33 + 27 = 60\n",
      "\n",
      "player:\n",
      "response: finish x = 60\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.0238, 10.6087,  3.4040, 10.1160,  2.9425,  7.8669]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 32.15244674682617\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 23 + 93\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5168,  7.4961, -1.3766,  7.1213,  2.2444,  7.3614]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 12.708887100219727\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 23 + 93\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5168,  7.4961, -1.3766,  7.1213,  2.2444,  7.3614]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 12.708887100219727\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 23 + 93\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000, -1.5168,  7.4961, -1.3766,  7.1213,  2.2444,  7.3614]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 12.708887100219727\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 23 + 93\n",
      "judge: 23 + 93 = 116\n",
      "\n",
      "player:\n",
      "response: finish x = 116\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4439, 11.7868,  2.8242, 10.2485,  4.1376,  8.0882]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 37.18806457519531\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 23 + 93\n",
      "judge: 23 + 93 = 116\n",
      "\n",
      "player:\n",
      "response: finish x = 116\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4439, 11.7868,  2.8242, 10.2485,  4.1376,  8.0882]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 37.18806457519531\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 23 + 93\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 23 + 93\n",
      "judge: 23 + 93 = 116\n",
      "\n",
      "player:\n",
      "response: finish x = 116\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  5.4439, 11.7868,  2.8242, 10.2485,  4.1376,  8.0882]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 37.18806457519531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_trainer.reset_memory(finetune=True)\n",
    "for x in sim_4_training_data_unpickled:\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "len(dqn_trainer.finetune_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## DEBUG sim run len\n",
      "0\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([366], device='cuda:0')\n",
      "logit at token tensor([7.7793], device='cuda:0')\n",
      "max logit tensor(7.7793, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([7.0782], device='cuda:0')\n",
      "max logit tensor(7.0782, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([7.8261], device='cuda:0')\n",
      "max logit tensor(7.8261, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([8.6844], device='cuda:0')\n",
      "max logit tensor(8.6844, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([9.5057], device='cuda:0')\n",
      "max logit tensor(9.5057, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59,   438]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.3964], device='cuda:0')\n",
      "max logit tensor(11.3964, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59,   438,\n",
      "            59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([10.4403], device='cuda:0')\n",
      "max logit tensor(10.4403, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59,   438,\n",
      "            59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([12.1173], device='cuda:0')\n",
      "max logit tensor(12.1173, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59,   438,\n",
      "            59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([10.5462], device='cuda:0')\n",
      "max logit tensor(10.5462, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,   366,    59,   438,    59,   438,\n",
      "            59,   438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "completion response ids\n",
      "tensor([[366,  59, 438,  59, 438,  59, 438,  59, 438]], device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([366], device='cuda:0')\n",
      "logit at token tensor([8.9675], device='cuda:0')\n",
      "max logit tensor(8.9675, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([9.5860], device='cuda:0')\n",
      "max logit tensor(9.5860, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.9702], device='cuda:0')\n",
      "max logit tensor(11.9702, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.3576], device='cuda:0')\n",
      "max logit tensor(11.3576, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([12.2538], device='cuda:0')\n",
      "max logit tensor(12.2538, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.9920], device='cuda:0')\n",
      "max logit tensor(11.9920, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59,   438,    59]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([12.8322], device='cuda:0')\n",
      "max logit tensor(12.8322, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59,   438,    59,   438]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.9417], device='cuda:0')\n",
      "max logit tensor(11.9417, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59,   438,    59,   438,    59]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.9234], device='cuda:0')\n",
      "max logit tensor(11.9234, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25,   366,    59,   438,    59,   438,    59,   438,    59,\n",
      "           438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "completion response ids\n",
      "tensor([[366,  59, 438,  59, 438,  59, 438,  59, 438]], device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response:  \"\\--\\--\\--\\--\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  7.8747,  7.1300,  7.8783,  8.7477,  9.5836, 11.4542,\n",
      "         10.5030, 12.1668, 10.5940,  4.7669]], device='cuda:0')\n",
      "## DEBUG priority: 84.27093505859375\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([366], device='cuda:0')\n",
      "logit at token tensor([9.2974], device='cuda:0')\n",
      "max logit tensor(9.2974, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([9.3691], device='cuda:0')\n",
      "max logit tensor(9.3691, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.7208], device='cuda:0')\n",
      "max logit tensor(11.7208, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.1435], device='cuda:0')\n",
      "max logit tensor(11.1435, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.9850], device='cuda:0')\n",
      "max logit tensor(11.9850, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.6742], device='cuda:0')\n",
      "max logit tensor(11.6742, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([12.4405], device='cuda:0')\n",
      "max logit tensor(12.4405, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59,   438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.4662], device='cuda:0')\n",
      "max logit tensor(11.4662, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59,   438,    59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.9176], device='cuda:0')\n",
      "max logit tensor(11.9176, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,   366,\n",
      "            59,   438,    59,   438,    59,   438,    59,   438]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "completion response ids\n",
      "tensor([[366,  59, 438,  59, 438,  59, 438,  59, 438]], device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,   198,\n",
      "          7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  \"\\--\\--\\--\\--\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  9.0047,  9.6101, 11.9784, 11.3864, 12.2726,\n",
      "         12.0327, 12.8474, 11.9775, 11.9363,  6.3238]], device='cuda:0')\n",
      "## DEBUG priority: 122.03599548339844\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([366], device='cuda:0')\n",
      "logit at token tensor([9.3134], device='cuda:0')\n",
      "max logit tensor(9.3134, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([8.9351], device='cuda:0')\n",
      "max logit tensor(8.9351, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.2732], device='cuda:0')\n",
      "max logit tensor(11.2732, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([10.4340], device='cuda:0')\n",
      "max logit tensor(10.4340, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.4697], device='cuda:0')\n",
      "max logit tensor(11.4697, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59,\n",
      "           438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.1853], device='cuda:0')\n",
      "max logit tensor(11.1853, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59,\n",
      "           438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.9698], device='cuda:0')\n",
      "max logit tensor(11.9698, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59,\n",
      "           438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([59], device='cuda:0')\n",
      "logit at token tensor([11.0151], device='cuda:0')\n",
      "max logit tensor(11.0151, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59,\n",
      "           438,    59,   438,    59]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([438], device='cuda:0')\n",
      "logit at token tensor([11.7010], device='cuda:0')\n",
      "max logit tensor(11.7010, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,   366,    59,   438,    59,\n",
      "           438,    59,   438,    59,   438]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "completion response ids\n",
      "tensor([[366,  59, 438,  59, 438,  59, 438,  59, 438]], device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  \"\\--\\--\\--\\--\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  9.3273,  9.3881, 11.7209, 11.1704,\n",
      "         11.9993, 11.7134, 12.4418, 11.4923, 11.9175,  6.5632]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 118.4488296508789\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([366], device='cuda:0')\n",
      "logit at token tensor([9.2459], device='cuda:0')\n",
      "max logit tensor(9.2459, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \"\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([8.8135], device='cuda:0')\n",
      "max logit tensor(8.8135, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([8.4273], device='cuda:0')\n",
      "max logit tensor(8.4273, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([9.2426], device='cuda:0')\n",
      "max logit tensor(9.2426, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([9.8603], device='cuda:0')\n",
      "max logit tensor(9.8603, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532,   532]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.4363], device='cuda:0')\n",
      "max logit tensor(10.4363, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532,   532,   532]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.6658], device='cuda:0')\n",
      "max logit tensor(10.6658, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532,   532,   532,   532]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.7343], device='cuda:0')\n",
      "max logit tensor(10.7343, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532,   532,   532,   532,\n",
      "           532]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - - - - - -\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([532], device='cuda:0')\n",
      "logit at token tensor([10.9195], device='cuda:0')\n",
      "max logit tensor(10.9195, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,   366,   532,   532,   532,   532,   532,   532,\n",
      "           532,   532]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: \" - - - - - - - -\n",
      "completion response ids\n",
      "tensor([[366, 532, 532, 532, 532, 532, 532, 532, 532]], device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  \"\\--\\--\\--\\--\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.3391,  8.9466, 11.2648,\n",
      "         10.4537, 11.4742, 11.2125, 11.9715, 11.0373, 11.6952,  6.4414]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 110.11611938476562\n",
      "done\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         38703,  1343, 14956,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 37082,   438,    59,   438,    59,   438,\n",
      "            59,   438,   198, 10456,   469,    25, 26003, 42790,   198,  7829,\n",
      "            25, 37082,   438,    59,   438,    59,   438,    59,   438,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25, 37082,   438,\n",
      "            59,   438,    59,   438,    59,   438,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25, 37082,   438,    59,   438,    59,\n",
      "           438,    59,   438,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 277 + 240\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "player: \"\\--\\--\\--\\--\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  \" - - - - - - - -\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.2468,  8.8087,\n",
      "          8.4213,  9.2397,  9.8571, 10.4309, 10.6567, 10.7261, 10.9096,  6.4387]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 111.80033111572266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def run():\n",
    "    sim_runner.reset(seed=1)\n",
    "    print(\"## DEBUG sim run len\")\n",
    "    print(len(sim_runner.history))\n",
    "    while not sim_runner.get_state().is_terminal:\n",
    "        transitions = sim_runner.step()\n",
    "        for x in transitions:\n",
    "            if isinstance(x.actor, AIActor) and x.actor.name == \"player\":\n",
    "                process_transition(x)\n",
    "    print(\"done\")\n",
    "    final_transitions = sim_runner.final()\n",
    "    did_win = False\n",
    "    for x in final_transitions:\n",
    "        process_transition(x)\n",
    "        if x.next_state.score > 0:\n",
    "            did_win = True\n",
    "    return did_win\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def pltsin(ax, fig, x, y, hdisplay):\n",
    "    if ax.lines:\n",
    "        for line in ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            line.set_ydata(y)\n",
    "            # update scale\n",
    "            ax.relim()\n",
    "            ax.autoscale_view()\n",
    "\n",
    "    else:\n",
    "        ax.plot(x, y)\n",
    "    hdisplay.update(fig)\n",
    "    time.sleep(1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "hdisplay = display.display(\"\", display_id=True)\n",
    "ax.set_label(\"game_count\")\n",
    "ax.set_label(\"win_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15643, 680, 2124, 796, 352]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"finish x = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15643, 680, 2124, 796, 352]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"finish x = 1\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7760,  0.9900,  0.9900,  0.9900,  0.9900, -0.9150, -1.2082],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4907e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.9399e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1337, -0.1350, -0.1364,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1851, -0.1870, -0.1889,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1668, -0.1685, -0.1702,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7569,  0.9656,  0.9608,  0.9656,  0.9656, -0.8924, -1.1553],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1387, -3.9267, -1.0589,  ..., -4.7587, -4.7219, -4.7107],\n",
      "        [-2.8931, -3.9632, -1.0040,  ..., -4.5777, -4.5649, -4.5429],\n",
      "        [-3.0899, -3.9583, -1.0388,  ..., -5.3588, -5.3882, -5.3705],\n",
      "        ...,\n",
      "        [-3.0995, -3.9599, -1.0456,  ..., -5.4195, -5.4544, -5.4338],\n",
      "        [-2.8576, -3.9839, -1.0254,  ..., -4.2830, -4.2639, -4.2592],\n",
      "        [-2.8950, -3.9368, -1.0085,  ..., -4.2915, -4.3073, -4.3226]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.5719, -0.6207,  0.8977,  0.7037,  0.5782,  0.9033, -0.9018, -1.1186],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8655, -0.9179,  0.9900,  0.9900, -0.9061, -0.9159],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3036e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1751, -0.1769, -0.1786,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1833, -0.1852, -0.1870,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1853, -0.1872, -0.1891,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8442, -0.8953,  0.9656,  0.9656, -0.8837, -0.8933],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0955, -3.9585, -1.0335,  ..., -5.3311, -5.3398, -5.3287],\n",
      "        [-3.1172, -3.9539, -1.0390,  ..., -5.3252, -5.3434, -5.3211],\n",
      "        [-2.8717, -3.9776, -1.0230,  ..., -4.3380, -4.3244, -4.3199],\n",
      "        ...,\n",
      "        [-3.0828, -3.9480, -1.0209,  ..., -5.2804, -5.2762, -5.2502],\n",
      "        [-2.8710, -3.9735, -1.0195,  ..., -4.3306, -4.3131, -4.3054],\n",
      "        [-2.8671, -3.9759, -1.0192,  ..., -4.2737, -4.2556, -4.2490]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9113,  0.8830, -0.9240, -0.8907,  0.9334,  0.8461, -0.8825, -0.8490],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8725,  0.9900,  0.9900, -0.7801,  0.9900, -1.7596,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0552e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1765, -0.1783, -0.1801,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3596, -0.3632, -0.3669,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8510,  0.9656,  0.9656, -0.7609,  0.9656, -1.7247,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1294, -3.9530, -1.0310,  ..., -5.3723, -5.3999, -5.3835],\n",
      "        [-2.8927, -3.9761, -1.0191,  ..., -4.3366, -4.3216, -4.3175],\n",
      "        [-3.1386, -3.9478, -1.0320,  ..., -5.3783, -5.4008, -5.3789],\n",
      "        ...,\n",
      "        [-3.1360, -3.9488, -1.0268,  ..., -5.3710, -5.4008, -5.3893],\n",
      "        [-2.8112, -3.9811, -0.9869,  ..., -4.2548, -4.2568, -4.2572],\n",
      "        [-3.1094, -3.9486, -1.0340,  ..., -5.2230, -5.2384, -5.2275]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9430, -0.8425,  0.9930,  0.6512, -0.6074,  0.9436, -1.0837,  0.8968],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8977, -0.8677,  0.9900,  0.9900,  0.9900, -1.2731],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6795e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.8180e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1816, -0.1834, -0.1853,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2085, -0.2107, -0.2128,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8755, -0.8462,  0.9656,  0.9656,  0.9656, -1.2173],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1416, -3.9443, -1.0288,  ..., -5.3707, -5.4070, -5.3820],\n",
      "        [-3.2043, -3.9157, -1.0439,  ..., -4.7503, -4.7159, -4.7040],\n",
      "        [-2.8995, -3.9760, -1.0147,  ..., -4.3228, -4.3058, -4.3010],\n",
      "        ...,\n",
      "        [-3.2018, -3.9148, -1.0424,  ..., -4.7789, -4.7439, -4.7300],\n",
      "        [-3.1470, -3.9469, -1.0255,  ..., -5.3982, -5.4244, -5.4008],\n",
      "        [-2.8896, -3.9423, -0.9955,  ..., -4.5737, -4.5752, -4.5804]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8602,  0.6669, -0.8293, -0.8188,  0.9579,  0.6370,  0.9791, -1.1127],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000, -0.9034,  0.9900, -0.9385,  0.9900, -0.7923,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5792e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.2477e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1828, -0.1846, -0.1865,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1365, -0.1378, -0.1392,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518, -0.8812,  0.9656, -0.9153,  0.9656, -0.7727,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9920, -3.9162, -1.0091,  ..., -1.0164, -1.0412, -1.2637],\n",
      "        [-2.9160, -3.9745, -1.0043,  ..., -4.3123, -4.2949, -4.2907],\n",
      "        [-3.1628, -3.9507, -1.0248,  ..., -5.3391, -5.3588, -5.3336],\n",
      "        ...,\n",
      "        [-2.9491, -3.9530, -0.9930,  ..., -4.6651, -4.6495, -4.6304],\n",
      "        [-3.1408, -3.9556, -1.0215,  ..., -5.3436, -5.3520, -5.3445],\n",
      "        [-3.1647, -3.9479, -1.0221,  ..., -5.3861, -5.4117, -5.3878]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0985, -0.8637,  0.9283, -0.8449,  0.9803, -0.6359,  0.9536,  1.0163],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8856, -0.9374,  0.9900,  0.9900,  0.9900, -0.8959],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0078e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1792, -0.1810, -0.1828,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1812, -0.1831, -0.1849,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8638, -0.9143,  0.9656,  0.9656,  0.9656, -0.8738],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1706, -3.9512, -1.0214,  ..., -5.4092, -5.4349, -5.4118],\n",
      "        [-3.1379, -3.9488, -1.0075,  ..., -5.2928, -5.2891, -5.2692],\n",
      "        [-2.9314, -3.9819, -1.0015,  ..., -4.3034, -4.2889, -4.2854],\n",
      "        ...,\n",
      "        [-3.1379, -3.9488, -1.0075,  ..., -5.2928, -5.2891, -5.2692],\n",
      "        [-3.1689, -3.9561, -1.0216,  ..., -5.3868, -5.4154, -5.3970],\n",
      "        [-2.9289, -3.9754, -1.0039,  ..., -4.2767, -4.2675, -4.2643]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9909,  0.8743, -0.8848, -0.8430,  0.9186,  0.8743,  0.9688, -0.8221],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8999, -0.8659], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(1.1407e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1820, -0.1839, -0.1857, -0.1876, -0.1895, -0.1914, -0.1934, -0.1953,\n",
      "         -0.1973, -0.1993, -0.2013, -0.2033, -0.2054, -0.2075, -0.2095, -0.2117,\n",
      "         -0.2138, -0.2160, -0.2181, -0.2203, -0.2226, -0.2248, -0.2271, -0.2294,\n",
      "         -0.2317, -0.2340, -0.2364, -0.2388, -0.2412, -0.2436, -0.2461, -0.2486,\n",
      "         -0.2511, -0.2536, -0.2562, -0.2588, -0.2614, -0.2640, -0.2667, -0.2694,\n",
      "         -0.2721, -0.2749, -0.2776, -0.2805, -0.2833, -0.2861, -0.2890, -0.2920,\n",
      "         -0.2949, -0.2979, -0.3009, -0.3039, -0.3070, -0.3101, -0.3132, -0.3164,\n",
      "         -0.3196, -0.3228, -0.3261, -0.3294, -0.3327, -0.3361, -0.3395, -0.3429,\n",
      "         -0.3464, -0.3499, -0.3534, -0.3570, -0.3606, -0.3642, -0.3679, -0.3716,\n",
      "         -0.3754, -0.3791, -0.3830, -0.3868, -0.3908, -0.3947, -0.3987, -0.4027,\n",
      "         -0.4068, -0.4109, -0.4150, -0.4192, -0.4235, -0.4277, -0.4321, -0.4364,\n",
      "         -0.4408, -0.4453, -0.4498, -0.4543, -0.4589, -0.4636, -0.4682, -0.4730,\n",
      "         -0.4777, -0.4826, -0.4874, -0.4924, -0.4973, -0.5024, -0.5074, -0.5126,\n",
      "         -0.5177, -0.5230, -0.5283, -0.5336, -0.5390, -0.5444, -0.5499, -0.5555,\n",
      "         -0.5611, -0.5668, -0.5725, -0.5783, -0.5841, -0.5900, -0.5960, -0.6020,\n",
      "         -0.6081, -0.6142, -0.6204, -0.6267, -0.6330, -0.6394, -0.6459, -0.6524,\n",
      "         -0.6590, -0.6656, -0.6724, -0.6791, -0.6860, -0.6929, -0.6999, -0.7070,\n",
      "         -0.7141, -0.7214, -0.7286, -0.7360, -0.7434, -0.7510, -0.7585, -0.7662,\n",
      "         -0.7739, -0.7818, -0.7897, -0.7976, -0.8057, -0.8138, -0.8220, -0.8303,\n",
      "         -0.8387, -0.8472, -0.8558, -0.8644, -0.8731, -0.8820, -0.8909, -0.8999,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1752, -0.1769, -0.1787, -0.1805, -0.1824, -0.1842, -0.1861, -0.1879,\n",
      "         -0.1898, -0.1918, -0.1937, -0.1957, -0.1976, -0.1996, -0.2016, -0.2037,\n",
      "         -0.2057, -0.2078, -0.2099, -0.2120, -0.2142, -0.2163, -0.2185, -0.2207,\n",
      "         -0.2230, -0.2252, -0.2275, -0.2298, -0.2321, -0.2345, -0.2368, -0.2392,\n",
      "         -0.2416, -0.2441, -0.2465, -0.2490, -0.2515, -0.2541, -0.2567, -0.2592,\n",
      "         -0.2619, -0.2645, -0.2672, -0.2699, -0.2726, -0.2754, -0.2781, -0.2810,\n",
      "         -0.2838, -0.2867, -0.2896, -0.2925, -0.2954, -0.2984, -0.3014, -0.3045,\n",
      "         -0.3075, -0.3107, -0.3138, -0.3170, -0.3202, -0.3234, -0.3267, -0.3300,\n",
      "         -0.3333, -0.3367, -0.3401, -0.3435, -0.3470, -0.3505, -0.3540, -0.3576,\n",
      "         -0.3612, -0.3649, -0.3685, -0.3723, -0.3760, -0.3798, -0.3837, -0.3875,\n",
      "         -0.3914, -0.3954, -0.3994, -0.4034, -0.4075, -0.4116, -0.4158, -0.4200,\n",
      "         -0.4242, -0.4285, -0.4328, -0.4372, -0.4416, -0.4461, -0.4506, -0.4551,\n",
      "         -0.4597, -0.4644, -0.4691, -0.4738, -0.4786, -0.4834, -0.4883, -0.4932,\n",
      "         -0.4982, -0.5033, -0.5083, -0.5135, -0.5187, -0.5239, -0.5292, -0.5345,\n",
      "         -0.5399, -0.5454, -0.5509, -0.5565, -0.5621, -0.5678, -0.5735, -0.5793,\n",
      "         -0.5851, -0.5911, -0.5970, -0.6031, -0.6091, -0.6153, -0.6215, -0.6278,\n",
      "         -0.6341, -0.6405, -0.6470, -0.6535, -0.6601, -0.6668, -0.6735, -0.6804,\n",
      "         -0.6872, -0.6942, -0.7012, -0.7083, -0.7154, -0.7226, -0.7299, -0.7373,\n",
      "         -0.7448, -0.7523, -0.7599, -0.7676, -0.7753, -0.7831, -0.7911, -0.7990,\n",
      "         -0.8071, -0.8153, -0.8235, -0.8318, -0.8402, -0.8487, -0.8573, -0.8659,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8777, -0.8446], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9305, -3.9758, -0.9997, -2.2146, -0.1194, -2.3461, -4.4676, -0.6602,\n",
      "         -4.2324, -2.7267, -3.9636,  0.2728, -0.4522, -3.7725, -3.1652, -2.6154,\n",
      "         -3.4118, -1.9394, -2.1822, -1.0057, -1.3915, -1.5249, -0.9825,  0.2262,\n",
      "         -2.0321, -3.5741, -3.2107, -2.7945, -3.9955,  0.8700, -1.9186, -0.2606,\n",
      "         -0.8757, -3.6514, -1.8740, -1.9425, -0.8840, -3.3649, -1.8470, -0.2816,\n",
      "         -0.9297, -0.6015, -0.5655, -3.9973, -1.7297, -3.6434, -3.6254, -3.3706,\n",
      "          0.4023,  0.3692, -3.9706, -1.6207, -6.8901, -3.1163, -3.2997, -1.1784,\n",
      "         -2.5158, -3.1273, -2.5711, -1.7868, -3.7367, -1.3710, -5.7668, -2.6557,\n",
      "         -3.7772,  0.9471, -1.5846, -0.1655, -0.3967, -3.2989, -4.2465, -5.3327,\n",
      "         -2.9924, -0.2856, -3.9171, -3.9967, -2.7041, -4.0931, -2.6513, -3.6613,\n",
      "         -4.7290, -2.7426, -0.6194, -1.3335, -1.7514, -3.0761, -2.0324, -0.7396,\n",
      "         -3.5885, -1.1315, -3.3352, -3.8910, -2.6905, -0.8449, -0.9766, -2.8824,\n",
      "         -2.8894, -3.9498, -3.2907, -3.2398, -0.3806, -1.0326, -3.6655, -2.1796,\n",
      "         -0.5900, -2.1141, -3.8227, -3.7438,  0.3619, -1.2543, -1.5963, -0.8295,\n",
      "         -4.5898, -4.6233,  0.0803, -1.0207, -2.0856, -1.4180, -0.0533, -1.2885,\n",
      "         -3.1883, -4.4745, -2.8659, -1.0439, -1.1970, -2.8153, -2.5575, -0.8995,\n",
      "         -0.6768, -4.3323,  0.0211,  0.4398, -3.5517, -4.0090, -1.4231, -1.9911,\n",
      "         -3.1819, -2.4074, -1.9399, -3.1830, -2.9107, -3.6132, -5.5617, -1.6149,\n",
      "         -0.7966, -0.8547, -0.8263, -0.8661, -0.9030, -0.9309, -4.9998, -5.0114,\n",
      "         -5.3733, -4.9932, -4.8734, -4.5698, -4.4632, -4.6049, -4.7233, -4.7139,\n",
      "         -4.7757, -4.8140, -4.8258, -4.8207, -4.7816, -4.7245, -4.6997, -4.7025,\n",
      "         -4.7019, -4.6819, -4.6608, -4.6501, -4.6458, -4.6409, -4.6696, -4.7062,\n",
      "         -4.7176, -4.7092, -4.6981, -4.6807, -4.6632, -4.6796, -4.6831, -4.6851,\n",
      "         -4.6864, -4.6860, -4.7040, -4.7206, -4.7336, -4.7406, -4.7446, -4.7658,\n",
      "         -4.7926, -4.8234, -4.8360, -4.8405, -4.8427, -4.8339, -4.8281, -4.8222,\n",
      "         -4.8210, -4.8150, -4.8004, -4.7786, -4.7608, -4.7647, -4.7759, -4.7985,\n",
      "         -4.7931, -4.7723, -4.7747, -4.7287, -4.6728, -4.6116, -4.5635, -4.4639,\n",
      "         -4.3845, -4.3863, -4.3635, -4.3310, -4.3145, -4.3063],\n",
      "        [-2.9355, -3.9803, -1.0050, -2.2122, -0.1216, -2.3444, -4.4651, -0.6620,\n",
      "         -4.2384, -2.7265, -3.9587,  0.2718, -0.4460, -3.7749, -3.1602, -2.6165,\n",
      "         -3.4108, -1.9424, -2.1834, -1.0065, -1.3881, -1.5238, -0.9801,  0.2315,\n",
      "         -2.0296, -3.5767, -3.2017, -2.7958, -3.9982,  0.8680, -1.9212, -0.2603,\n",
      "         -0.8729, -3.6466, -1.8662, -1.9410, -0.8857, -3.3630, -1.8516, -0.2769,\n",
      "         -0.9355, -0.6021, -0.5642, -3.9923, -1.7289, -3.6442, -3.6219, -3.3647,\n",
      "          0.3990,  0.3696, -3.9716, -1.6164, -6.8920, -3.1165, -3.3011, -1.1808,\n",
      "         -2.5194, -3.1311, -2.5661, -1.7864, -3.7375, -1.3674, -5.7650, -2.6560,\n",
      "         -3.7786,  0.9436, -1.5891, -0.1665, -0.3937, -3.2935, -4.2447, -5.3215,\n",
      "         -2.9938, -0.2871, -3.9174, -3.9908, -2.7053, -4.0902, -2.6545, -3.6609,\n",
      "         -4.7165, -2.7437, -0.6173, -1.3322, -1.7446, -3.0768, -2.0334, -0.7395,\n",
      "         -3.5865, -1.1366, -3.3377, -3.8826, -2.6907, -0.8440, -0.9758, -2.8794,\n",
      "         -2.8880, -3.9504, -3.2926, -3.2397, -0.3789, -1.0312, -3.6623, -2.1806,\n",
      "         -0.5868, -2.1167, -3.8220, -3.7395,  0.3593, -1.1079, -1.4540, -0.8545,\n",
      "         -4.9450, -4.7054,  0.0319, -0.9873, -2.0832, -1.3752, -0.0315, -1.2865,\n",
      "         -3.2040, -4.4932, -2.8677, -1.0589, -1.1586, -2.7938, -2.5191, -0.8981,\n",
      "         -0.6214, -4.2796,  0.0282,  0.4599, -3.5658, -3.9928, -1.4132, -1.9948,\n",
      "         -3.1831, -2.3881, -1.9315, -3.1654, -2.8957, -3.6187, -5.5851, -1.6418,\n",
      "         -0.8075, -0.8292, -0.8197, -0.8044, -0.8444, -0.8518, -5.0140, -4.9834,\n",
      "         -5.2750, -4.9671, -4.9002, -4.6242, -4.5442, -4.6024, -4.7127, -4.6899,\n",
      "         -4.7171, -4.7658, -4.8021, -4.8025, -4.7586, -4.6904, -4.6504, -4.6370,\n",
      "         -4.6314, -4.6237, -4.6206, -4.6355, -4.6524, -4.6560, -4.6843, -4.7167,\n",
      "         -4.7216, -4.7142, -4.7240, -4.7192, -4.7059, -4.7148, -4.7160, -4.7066,\n",
      "         -4.7023, -4.7020, -4.7214, -4.7467, -4.7695, -4.7793, -4.7862, -4.8075,\n",
      "         -4.8354, -4.8515, -4.8566, -4.8581, -4.8613, -4.8552, -4.8507, -4.8396,\n",
      "         -4.8318, -4.8267, -4.8163, -4.7990, -4.7934, -4.7932, -4.7979, -4.8155,\n",
      "         -4.8089, -4.7866, -4.7949, -4.7457, -4.6935, -4.6234, -4.5635, -4.4676,\n",
      "         -4.3937, -4.3985, -4.3729, -4.3395, -4.3258, -4.3214]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8629, -0.8262], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9351, -0.9007, -0.7749, -1.2035, -0.7749,  0.9900, -0.8649,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.0845e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.9292e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1892, -0.1911, -0.1930,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1822, -0.1840, -0.1859,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1335, -0.1348, -0.1362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1750, -0.1767, -0.1785,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9120, -0.8785, -0.7558, -1.1508, -0.7558,  0.9656, -0.8435,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9357, -3.9816, -1.0011,  ..., -4.2893, -4.2754, -4.2708],\n",
      "        [-2.9384, -3.9804, -0.9976,  ..., -4.3165, -4.2995, -4.2950],\n",
      "        [-2.9738, -3.9640, -0.9801,  ..., -4.5995, -4.5857, -4.5658],\n",
      "        ...,\n",
      "        [-3.2447, -3.9258, -1.0317,  ..., -4.7761, -4.7435, -4.7355],\n",
      "        [-2.9415, -3.9821, -1.0026,  ..., -4.3408, -4.3269, -4.3215],\n",
      "        [-3.2447, -3.9258, -1.0317,  ..., -4.7761, -4.7435, -4.7355]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8430, -0.8630, -0.6101, -1.1557, -0.6101,  0.7006, -0.8279,  0.7006],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9154, -0.8870, -0.9389,  0.9900, -0.9035, -0.7926, -0.8638,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.4113e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1852, -0.1871, -0.1890,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1794, -0.1813, -0.1831,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1899, -0.1919, -0.1938,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1365, -0.1379, -0.1393,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1748, -0.1765, -0.1783,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8928, -0.8651, -0.9157,  0.9656, -0.8812, -0.7730, -0.8425,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9526, -3.9891, -1.0072,  ..., -4.2814, -4.2669, -4.2615],\n",
      "        [-2.9553, -3.9919, -0.9992,  ..., -4.3070, -4.2933, -4.2896],\n",
      "        [-2.9468, -3.9864, -1.0066,  ..., -4.2896, -4.2759, -4.2723],\n",
      "        ...,\n",
      "        [-2.9836, -3.9637, -0.9912,  ..., -4.6722, -4.6600, -4.6391],\n",
      "        [-2.9496, -3.9858, -1.0057,  ..., -4.3382, -4.3247, -4.3239],\n",
      "        [-3.1965, -3.9598, -1.0198,  ..., -5.4191, -5.4449, -5.4171]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8656, -0.8931, -0.8528,  0.9791, -0.8756, -0.6570, -0.9127,  0.9791],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -1.7559,  0.9900,  0.9900, -0.9155],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6315e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1852, -0.1871, -0.1890,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -1.7211,  0.9656,  0.9656, -0.8930],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2024, -3.9687, -1.0233,  ..., -5.4001, -5.4269, -5.4111],\n",
      "        [-3.2120, -3.9711, -1.0312,  ..., -5.4575, -5.4888, -5.4691],\n",
      "        [-3.2024, -3.9687, -1.0233,  ..., -5.4001, -5.4269, -5.4111],\n",
      "        ...,\n",
      "        [-3.2024, -3.9687, -1.0233,  ..., -5.4001, -5.4269, -5.4111],\n",
      "        [-3.2101, -3.9670, -1.0276,  ..., -5.3590, -5.3755, -5.3534],\n",
      "        [-2.9621, -3.9940, -1.0104,  ..., -4.2806, -4.2674, -4.2620]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9605,  0.9594,  0.9605,  0.6594, -1.0721,  0.9605,  0.9011, -0.8741],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8699, -0.9072,  0.9900,  0.9900,  0.9900, -0.8725],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7969e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1760, -0.1778, -0.1796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1765, -0.1783, -0.1801,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8485, -0.8848,  0.9656,  0.9656,  0.9656, -0.8510],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2146, -3.9748, -1.0334,  ..., -5.4587, -5.4928, -5.4715],\n",
      "        [-3.2102, -3.9687, -1.0210,  ..., -5.4064, -5.4360, -5.4230],\n",
      "        [-2.9619, -3.9926, -1.0122,  ..., -4.3338, -4.3177, -4.3126],\n",
      "        ...,\n",
      "        [-3.2612, -3.9414, -1.0432,  ..., -4.8327, -4.8014, -4.7905],\n",
      "        [-3.1829, -3.9679, -1.0285,  ..., -5.2693, -5.2850, -5.2733],\n",
      "        [-2.9657, -3.9960, -1.0109,  ..., -4.3440, -4.3288, -4.3263]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9465,  0.9550, -0.8312, -0.8963,  0.8507,  0.6280,  0.8925, -0.8528],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.9186,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.9192e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1858, -0.1877, -0.1896,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656,  0.9656,  0.9656,  0.9656, -0.8959,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2663, -3.9409, -1.0437,  ..., -4.8844, -4.7808, -4.7369],\n",
      "        [-3.1918, -3.9781, -1.0239,  ..., -5.3793, -5.3897, -5.3810],\n",
      "        [-3.2066, -3.9738, -1.0281,  ..., -5.3757, -5.3965, -5.3798],\n",
      "        ...,\n",
      "        [-2.9603, -3.9950, -1.0089,  ..., -4.2762, -4.2603, -4.2542],\n",
      "        [-3.2150, -3.9703, -1.0256,  ..., -5.4145, -5.4389, -5.4149],\n",
      "        [-3.2734, -3.9396, -1.0415,  ..., -4.8310, -4.7970, -4.7858]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7256,  0.9268,  0.9523,  0.8782,  0.8402, -0.8512,  0.9910,  0.6478],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8964, -1.1000,  0.9900,  0.9900,  0.9900, -0.8986, -1.2723],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3498e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1813, -0.1832, -0.1850,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1818, -0.1836, -0.1855,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2084, -0.2105, -0.2126,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8743, -1.0518,  0.9656,  0.9656,  0.9656, -0.8765, -1.2166],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2209, -3.9714, -1.0232,  ..., -5.4186, -5.4428, -5.4221],\n",
      "        [-2.9684, -4.0017, -1.0125,  ..., -4.3320, -4.3180, -4.3146],\n",
      "        [-3.0464, -3.9380, -1.0091,  ..., -1.0407, -1.0669, -1.2736],\n",
      "        ...,\n",
      "        [-3.1864, -3.9697, -1.0098,  ..., -5.3196, -5.3193, -5.2971],\n",
      "        [-2.9761, -3.9983, -1.0053,  ..., -4.2863, -4.2766, -4.2775],\n",
      "        [-2.9581, -3.9683, -0.9928,  ..., -4.5900, -4.5907, -4.5964]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9884, -0.8643, -1.1163,  0.9236,  0.8370,  0.8370, -0.8465, -1.1706],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.6975e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.7546e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2169e+00, -3.9763e+00, -1.0229e+00, -2.3389e+00, -1.7563e-01,\n",
      "         -2.3902e+00, -4.4815e+00, -8.5202e-01, -4.4944e+00, -2.8932e+00,\n",
      "         -3.9842e+00,  2.2046e-01, -5.7650e-01, -3.8300e+00, -3.1085e+00,\n",
      "         -2.7447e+00, -3.4837e+00, -2.0038e+00, -2.1198e+00, -1.1229e+00,\n",
      "         -1.4014e+00, -1.6679e+00, -1.0794e+00,  2.9651e-01, -2.2112e+00,\n",
      "         -3.6312e+00, -3.1607e+00, -2.8981e+00, -3.9784e+00,  8.8951e-01,\n",
      "         -2.0294e+00, -3.9635e-01, -8.5020e-01, -3.6786e+00, -1.8965e+00,\n",
      "         -1.8682e+00, -8.5402e-01, -3.4122e+00, -1.9358e+00, -3.9007e-01,\n",
      "         -9.8599e-01, -5.7660e-01, -5.9247e-01, -3.9553e+00, -1.8840e+00,\n",
      "         -3.8774e+00, -3.8415e+00, -3.3966e+00,  6.2978e-01,  5.1391e-01,\n",
      "         -4.0092e+00, -1.6940e+00, -6.9392e+00, -3.1911e+00, -3.3384e+00,\n",
      "         -1.2728e+00, -2.4842e+00, -3.1593e+00, -2.6032e+00, -1.9348e+00,\n",
      "         -3.7879e+00, -1.4335e+00, -5.8227e+00, -2.6977e+00, -3.7658e+00,\n",
      "          9.7855e-01, -1.6840e+00, -3.0445e-01, -3.5356e-01, -3.3287e+00,\n",
      "         -4.3117e+00, -5.3530e+00, -3.0570e+00, -2.5631e-01, -3.9518e+00,\n",
      "         -3.9856e+00, -2.7661e+00, -4.1268e+00, -2.7257e+00, -3.6889e+00,\n",
      "         -4.7394e+00, -2.7964e+00, -6.5739e-01, -1.4593e+00, -1.7966e+00,\n",
      "         -3.1664e+00, -2.1390e+00, -8.6893e-01, -3.6278e+00, -1.2135e+00,\n",
      "         -3.3698e+00, -3.8809e+00, -2.7403e+00, -8.6991e-01, -1.1067e+00,\n",
      "         -2.8130e+00, -3.0322e+00, -4.1579e+00, -3.3567e+00, -3.3037e+00,\n",
      "         -4.3374e-01, -1.1522e+00, -3.6639e+00, -2.2462e+00, -5.2765e-01,\n",
      "         -2.2719e+00, -4.0151e+00, -3.8274e+00,  3.9109e-01, -1.1005e+00,\n",
      "         -1.3698e+00, -8.7689e-01, -5.1654e+00, -4.7308e+00, -4.3555e-03,\n",
      "         -1.1095e+00, -2.0902e+00, -1.4891e+00, -8.0927e-02, -1.3617e+00,\n",
      "         -3.3834e+00, -4.4884e+00, -2.8959e+00, -1.0404e+00, -1.2525e+00,\n",
      "         -2.8460e+00, -2.6548e+00, -9.5614e-01, -7.5362e-01, -4.3129e+00,\n",
      "          2.7550e-01,  6.2110e-01, -3.6056e+00, -3.9963e+00, -1.4842e+00,\n",
      "         -1.9620e+00, -3.1940e+00, -2.4217e+00, -2.0392e+00, -3.1758e+00,\n",
      "         -3.3045e+00, -5.6950e+00, -1.9903e+00, -2.2725e+00, -1.2432e+00,\n",
      "         -1.0021e+00, -1.0438e+00, -4.5347e+00, -3.9923e+00, -2.2256e-01,\n",
      "         -2.2121e+00, -4.4261e-01, -1.0334e+00, -4.5266e-01, -2.4453e-01,\n",
      "         -5.5099e-01, -4.4527e+00, -3.6098e+00, -4.8443e+00, -1.6028e+00,\n",
      "          9.6632e-01,  9.9856e-01,  9.9959e-01,  8.9904e-01,  7.4628e-01,\n",
      "          1.0733e+00, -4.8049e+00, -4.9530e+00, -5.2704e+00, -5.0356e+00,\n",
      "         -5.2363e+00, -5.3064e+00, -5.4236e+00, -5.4263e+00, -5.3605e+00,\n",
      "         -5.3464e+00, -5.3336e+00, -5.2692e+00, -5.1477e+00, -5.0544e+00,\n",
      "         -5.0048e+00, -4.9924e+00, -5.0080e+00, -5.0284e+00, -5.0137e+00,\n",
      "         -4.9920e+00, -4.9896e+00, -5.0233e+00, -5.0847e+00, -5.1591e+00,\n",
      "         -5.2092e+00, -5.2209e+00, -5.2202e+00, -5.2468e+00, -5.3124e+00,\n",
      "         -5.3845e+00, -5.4180e+00, -5.4010e+00, -5.3584e+00, -5.2990e+00,\n",
      "         -5.2644e+00, -5.2352e+00, -5.2328e+00, -5.2474e+00, -5.2752e+00,\n",
      "         -5.3262e+00, -5.3737e+00, -5.3904e+00, -5.3771e+00, -5.3332e+00,\n",
      "         -5.3233e+00, -5.3178e+00, -5.3331e+00, -5.3700e+00, -5.3898e+00,\n",
      "         -5.3964e+00, -5.3867e+00, -5.3706e+00, -5.3678e+00, -5.3764e+00,\n",
      "         -5.3979e+00, -5.3798e+00],\n",
      "        [-3.1961e+00, -3.9721e+00, -1.0228e+00, -2.3190e+00, -1.6652e-01,\n",
      "         -2.3969e+00, -4.4873e+00, -8.1310e-01, -4.4744e+00, -2.8630e+00,\n",
      "         -3.9807e+00,  2.2077e-01, -5.6504e-01, -3.8047e+00, -3.1198e+00,\n",
      "         -2.7312e+00, -3.4960e+00, -1.9933e+00, -2.1437e+00, -1.1006e+00,\n",
      "         -1.3932e+00, -1.6488e+00, -1.0630e+00,  2.7761e-01, -2.1975e+00,\n",
      "         -3.6027e+00, -3.1675e+00, -2.8918e+00, -3.9889e+00,  8.8367e-01,\n",
      "         -2.0093e+00, -3.6920e-01, -8.4850e-01, -3.6786e+00, -1.8992e+00,\n",
      "         -1.8813e+00, -8.5157e-01, -3.4310e+00, -1.9200e+00, -3.7807e-01,\n",
      "         -9.7778e-01, -5.7677e-01, -5.8887e-01, -3.9673e+00, -1.8728e+00,\n",
      "         -3.8613e+00, -3.8070e+00, -3.3907e+00,  5.7097e-01,  4.7681e-01,\n",
      "         -3.9875e+00, -1.6860e+00, -6.9314e+00, -3.1865e+00, -3.3525e+00,\n",
      "         -1.2606e+00, -2.4851e+00, -3.1477e+00, -2.6067e+00, -1.9240e+00,\n",
      "         -3.7600e+00, -1.4241e+00, -5.8091e+00, -2.6961e+00, -3.7726e+00,\n",
      "          9.6979e-01, -1.6668e+00, -2.7847e-01, -3.5595e-01, -3.3272e+00,\n",
      "         -4.2937e+00, -5.3472e+00, -3.0553e+00, -2.7989e-01, -3.9269e+00,\n",
      "         -3.9890e+00, -2.7638e+00, -4.1452e+00, -2.7121e+00, -3.6630e+00,\n",
      "         -4.7357e+00, -2.7942e+00, -6.5156e-01, -1.4307e+00, -1.7920e+00,\n",
      "         -3.1634e+00, -2.1211e+00, -8.4270e-01, -3.6161e+00, -1.2099e+00,\n",
      "         -3.3438e+00, -3.8816e+00, -2.7398e+00, -8.6577e-01, -1.0794e+00,\n",
      "         -2.8134e+00, -3.0241e+00, -4.1408e+00, -3.3415e+00, -3.2994e+00,\n",
      "         -4.3174e-01, -1.1272e+00, -3.6713e+00, -2.2356e+00, -5.4514e-01,\n",
      "         -2.2506e+00, -3.9987e+00, -3.8188e+00,  3.6863e-01, -1.4326e+00,\n",
      "         -1.5176e+00, -9.0469e-01, -4.8419e+00, -4.6862e+00,  7.0208e-02,\n",
      "         -1.0955e+00, -2.0763e+00, -1.4561e+00, -2.8957e-02, -1.2990e+00,\n",
      "         -3.2732e+00, -4.5275e+00, -2.8850e+00, -1.0255e+00, -1.1571e+00,\n",
      "         -2.7737e+00, -2.6634e+00, -9.6714e-01, -7.3950e-01, -4.3008e+00,\n",
      "          1.6763e-01,  5.5052e-01, -3.5889e+00, -4.0181e+00, -1.4965e+00,\n",
      "         -1.9664e+00, -3.2045e+00, -2.4163e+00, -2.0489e+00, -3.2025e+00,\n",
      "         -3.3167e+00, -5.6992e+00, -1.9744e+00, -2.3016e+00, -1.3636e+00,\n",
      "         -1.0611e+00, -7.5752e-01, -4.4606e+00, -3.9026e+00, -2.6097e-01,\n",
      "         -2.3217e+00, -6.0494e-01, -1.2502e+00, -4.7385e-01, -2.5651e-01,\n",
      "         -8.8421e-01, -4.4165e+00, -5.0584e+00, -1.7693e+00, -2.2760e+00,\n",
      "         -7.7817e-02,  1.8881e-01,  8.5659e-03, -4.2431e+00, -3.9838e+00,\n",
      "         -2.0773e-01, -1.4522e+00, -4.3158e+00, -6.0994e+00, -5.0682e+00,\n",
      "         -3.2454e+00, -4.7620e+00, -1.5109e+00,  9.6207e-01,  9.8764e-01,\n",
      "          9.7720e-01,  9.7872e-01,  4.1629e-01,  9.3704e-01, -4.7382e+00,\n",
      "         -4.9527e+00, -5.2596e+00, -5.0127e+00, -5.1490e+00, -5.0705e+00,\n",
      "         -5.1075e+00, -5.1712e+00, -5.2285e+00, -5.2329e+00, -5.1931e+00,\n",
      "         -5.1213e+00, -5.0756e+00, -5.0490e+00, -5.0522e+00, -5.0679e+00,\n",
      "         -5.0829e+00, -5.0822e+00, -5.0839e+00, -5.0884e+00, -5.1038e+00,\n",
      "         -5.1443e+00, -5.1979e+00, -5.2555e+00, -5.3035e+00, -5.3243e+00,\n",
      "         -5.3361e+00, -5.3440e+00, -5.3693e+00, -5.3897e+00, -5.3864e+00,\n",
      "         -5.3682e+00, -5.3295e+00, -5.2684e+00, -5.2229e+00, -5.1801e+00,\n",
      "         -5.1702e+00, -5.1884e+00, -5.2124e+00, -5.2528e+00, -5.2724e+00,\n",
      "         -5.2880e+00, -5.2779e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9472, 0.8765], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8994,  0.9900,  0.9900,  0.9900, -0.8496,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9918e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1819, -0.1838, -0.1856,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1719, -0.1736, -0.1754,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8772,  0.9656,  0.9656,  0.9656, -0.8286,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2880, -3.9493, -1.0410,  ..., -4.8029, -4.7746, -4.7650],\n",
      "        [-2.9739, -4.0033, -1.0101,  ..., -4.2806, -4.2661, -4.2578],\n",
      "        [-3.2785, -3.9531, -1.0431,  ..., -4.8368, -4.8055, -4.7950],\n",
      "        ...,\n",
      "        [-2.9817, -4.0066, -1.0116,  ..., -4.3497, -4.3349, -4.3323],\n",
      "        [-3.2191, -3.9816, -1.0277,  ..., -5.3753, -5.3958, -5.3771],\n",
      "        [-3.2781, -3.9487, -1.0440,  ..., -4.8832, -4.7782, -4.7364]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6931, -0.8667,  0.6113,  0.6249,  0.9247, -0.8732,  0.9293,  0.7177],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8431,  0.9900,  0.9900,  0.9900, -0.8411,  0.9900, -0.8649],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6134e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1706, -0.1723, -0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1702, -0.1719, -0.1736,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1750, -0.1767, -0.1785,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8223,  0.9656,  0.9656,  0.9656, -0.8203,  0.9656, -0.8435],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2307, -3.9902, -1.0368,  ..., -5.4694, -5.5011, -5.4778],\n",
      "        [-2.9871, -4.0092, -1.0151,  ..., -4.3511, -4.3381, -4.3349],\n",
      "        [-3.2869, -3.9522, -1.0444,  ..., -4.8248, -4.7942, -4.7836],\n",
      "        ...,\n",
      "        [-2.9855, -4.0074, -1.0167,  ..., -4.3417, -4.3257, -4.3224],\n",
      "        [-3.1948, -3.9804, -1.0145,  ..., -5.3136, -5.3097, -5.2872],\n",
      "        [-2.9914, -4.0149, -1.0093,  ..., -4.3182, -4.3052, -4.3019]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9123, -0.8803,  0.6140,  0.8413,  0.8178, -0.8590,  0.8178, -0.9286],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8633, -0.8867,  0.9900,  0.9900, -1.1000,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9516e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1747, -0.1764, -0.1782,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1794, -0.1812, -0.1830,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8420, -0.8648,  0.9656,  0.9656, -1.0518,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2259, -3.9893, -1.0292,  ..., -5.4101, -5.4368, -5.4197],\n",
      "        [-2.9841, -4.0126, -1.0199,  ..., -4.3389, -4.3258, -4.3196],\n",
      "        [-2.9943, -4.0127, -1.0170,  ..., -4.2936, -4.2759, -4.2720],\n",
      "        ...,\n",
      "        [-3.0586, -3.9492, -1.0149,  ..., -1.0391, -1.0636, -1.2925],\n",
      "        [-3.2259, -3.9893, -1.0292,  ..., -5.4101, -5.4368, -5.4197],\n",
      "        [-3.2103, -3.9944, -1.0286,  ..., -5.3664, -5.3737, -5.3626]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9254, -0.8894, -0.9045,  0.9142,  0.8857, -1.1143,  0.9254,  0.9009],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8683,  0.9900, -0.7516, -1.7259, -0.7666,  0.9900, -0.8835,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9172e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1757, -0.1774, -0.1792,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1295, -0.1308, -0.1321,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1787, -0.1805, -0.1824,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8469,  0.9656, -0.7330, -1.6917, -0.7477,  0.9656, -0.8617,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9902, -4.0063, -1.0140,  ..., -4.3445, -4.3295, -4.3243],\n",
      "        [-3.2326, -3.9879, -1.0317,  ..., -5.3731, -5.3912, -5.3693],\n",
      "        [-3.0238, -3.9926, -0.9939,  ..., -4.6270, -4.6174, -4.5945],\n",
      "        ...,\n",
      "        [-3.2052, -3.9860, -1.0327,  ..., -5.2614, -5.2772, -5.2640],\n",
      "        [-2.9976, -4.0139, -1.0176,  ..., -4.2948, -4.2791, -4.2760],\n",
      "        [-3.2250, -3.9831, -1.0334,  ..., -5.4058, -5.4386, -5.4139]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9279,  0.8419, -0.6561, -1.0926, -0.7025,  0.8561, -0.9025,  0.8307],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7516,  0.9900,  0.9900, -0.8670, -0.8660,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9380e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1295, -0.1308, -0.1321,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1754, -0.1772, -0.1789,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1752, -0.1770, -0.1788,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7331,  0.9656,  0.9656, -0.8456, -0.8446,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2279, -3.9845, -1.0260,  ..., -5.4151, -5.4407, -5.4181],\n",
      "        [-3.2245, -3.9843, -1.0248,  ..., -5.4400, -5.4684, -5.4414],\n",
      "        [-3.0175, -3.9927, -0.9901,  ..., -4.6309, -4.6211, -4.5972],\n",
      "        ...,\n",
      "        [-2.9851, -4.0061, -1.0102,  ..., -4.3463, -4.3311, -4.3258],\n",
      "        [-2.9879, -4.0096, -1.0083,  ..., -4.3277, -4.3145, -4.3116],\n",
      "        [-3.1941, -3.9811, -1.0110,  ..., -5.3068, -5.3056, -5.2802]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9695,  0.8816, -0.6515,  0.9030,  0.8535, -0.9237, -0.9171,  0.8329],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1902,  0.9900,  0.9900, -0.9086, -0.8205, -0.8633, -0.9086],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7271e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1643, -0.1660, -0.1677,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1660, -0.1677, -0.1694,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1747, -0.1764, -0.1782,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1838, -0.1857, -0.1876,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1380,  0.9656,  0.9656, -0.8862, -0.8002, -0.8420, -0.8862],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2825, -3.9516, -1.0353,  ..., -4.8168, -4.7892, -4.7844],\n",
      "        [-3.0090, -3.9633, -0.9899,  ..., -4.3047, -4.3215, -4.3359],\n",
      "        [-3.2159, -3.9871, -1.0200,  ..., -5.4070, -5.4333, -5.4172],\n",
      "        ...,\n",
      "        [-2.9851, -4.0086, -1.0098,  ..., -4.3568, -4.3461, -4.3451],\n",
      "        [-2.9873, -4.0073, -1.0041,  ..., -4.2992, -4.2903, -4.2888],\n",
      "        [-2.9823, -4.0093, -1.0067,  ..., -4.3073, -4.2936, -4.2901]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7217, -1.2208,  0.9271,  0.9684, -0.8976, -0.9509, -0.8617, -0.8976],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2533], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4772e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2053, -0.2074, -0.2095, -0.2116, -0.2137, -0.2159, -0.2181, -0.2203,\n",
      "         -0.2225, -0.2247, -0.2270, -0.2293, -0.2316, -0.2340, -0.2363, -0.2387,\n",
      "         -0.2411, -0.2435, -0.2460, -0.2485, -0.2510, -0.2535, -0.2561, -0.2587,\n",
      "         -0.2613, -0.2639, -0.2666, -0.2693, -0.2720, -0.2748, -0.2775, -0.2803,\n",
      "         -0.2832, -0.2860, -0.2889, -0.2918, -0.2948, -0.2978, -0.3008, -0.3038,\n",
      "         -0.3069, -0.3100, -0.3131, -0.3163, -0.3195, -0.3227, -0.3260, -0.3293,\n",
      "         -0.3326, -0.3359, -0.3393, -0.3428, -0.3462, -0.3497, -0.3532, -0.3568,\n",
      "         -0.3604, -0.3641, -0.3677, -0.3715, -0.3752, -0.3790, -0.3828, -0.3867,\n",
      "         -0.3906, -0.3945, -0.3985, -0.4026, -0.4066, -0.4107, -0.4149, -0.4191,\n",
      "         -0.4233, -0.4276, -0.4319, -0.4363, -0.4407, -0.4451, -0.4496, -0.4542,\n",
      "         -0.4587, -0.4634, -0.4681, -0.4728, -0.4776, -0.4824, -0.4873, -0.4922,\n",
      "         -0.4971, -0.5022, -0.5072, -0.5124, -0.5175, -0.5228, -0.5280, -0.5334,\n",
      "         -0.5388, -0.5442, -0.5497, -0.5553, -0.5609, -0.5665, -0.5723, -0.5780,\n",
      "         -0.5839, -0.5898, -0.5957, -0.6017, -0.6078, -0.6140, -0.6202, -0.6264,\n",
      "         -0.6328, -0.6392, -0.6456, -0.6521, -0.6587, -0.6654, -0.6721, -0.6789,\n",
      "         -0.6857, -0.6927, -0.6997, -0.7067, -0.7139, -0.7211, -0.7284, -0.7357,\n",
      "         -0.7431, -0.7507, -0.7582, -0.7659, -0.7736, -0.7814, -0.7893, -0.7973,\n",
      "         -0.8054, -0.8135, -0.8217, -0.8300, -0.8384, -0.8469, -0.8554, -0.8641,\n",
      "         -0.8728, -0.8816, -0.8905, -0.8995, -0.9086, -0.9178, -0.9270, -0.9364,\n",
      "         -0.9459, -0.9554, -0.9651, -0.9748, -0.9847, -0.9946, -1.0047, -1.0148,\n",
      "         -1.0251, -1.0354, -1.0459, -1.0564, -1.0671, -1.0779, -1.0888, -1.0998,\n",
      "         -1.1109, -1.1221, -1.1334, -1.1449, -1.1565, -1.1681, -1.1799, -1.1919,\n",
      "         -1.2039, -1.2160, -1.2283, -1.2407, -1.2533,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1984], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2100, -3.9820, -1.0189, -2.3541, -0.1703, -2.3841, -4.5125, -0.8609,\n",
      "         -4.4907, -2.8918, -3.9753,  0.2003, -0.5840, -3.8207, -3.1332, -2.7551,\n",
      "         -3.4958, -1.9912, -2.1449, -1.1168, -1.3705, -1.6572, -1.0675,  0.3076,\n",
      "         -2.2167, -3.6271, -3.1818, -2.9097, -3.9583,  0.8869, -2.0202, -0.3966,\n",
      "         -0.8457, -3.6762, -1.8873, -1.8785, -0.8511, -3.4225, -1.9328, -0.3638,\n",
      "         -0.9782, -0.5719, -0.5914, -3.9738, -1.8948, -3.8781, -3.8373, -3.3911,\n",
      "          0.6120,  0.4729, -4.0004, -1.7042, -6.9568, -3.1996, -3.3455, -1.2592,\n",
      "         -2.5003, -3.1454, -2.6087, -1.9384, -3.7841, -1.4394, -5.8291, -2.7049,\n",
      "         -3.7418,  0.9801, -1.6730, -0.3011, -0.3495, -3.3259, -4.2958, -5.3568,\n",
      "         -3.0620, -0.2207, -3.9394, -4.0202, -2.7724, -4.1378, -2.7260, -3.6754,\n",
      "         -4.7404, -2.8008, -0.6370, -1.4545, -1.8031, -3.1690, -2.1226, -0.8607,\n",
      "         -3.6208, -1.2054, -3.3561, -3.9175, -2.7483, -0.8556, -1.1010, -2.8149,\n",
      "         -3.0352, -4.1535, -3.3711, -3.3203, -0.4198, -1.1450, -3.6829, -2.2711,\n",
      "         -0.5155, -2.2622, -4.0133, -3.8220,  0.4279, -1.1560, -1.3740, -0.9083,\n",
      "         -5.1645, -4.7433,  0.0170, -1.1060, -2.0615, -1.4766, -0.0730, -1.3505,\n",
      "         -3.3724, -4.4971, -2.8897, -1.0312, -1.2422, -2.8491, -2.6382, -0.9542,\n",
      "         -0.7475, -4.3090,  0.2649,  0.5807, -3.6230, -3.9881, -1.4703, -1.9788,\n",
      "         -3.1810, -2.4252, -2.0336, -3.1709, -3.2927, -5.7109, -1.9861, -2.3050,\n",
      "         -1.2949, -0.9987, -1.0702, -4.5146, -4.0005, -0.2452, -2.1860, -0.5021,\n",
      "         -1.0322, -0.4791, -0.1962, -0.5797, -4.4573, -3.6095, -4.8566, -1.5891,\n",
      "          0.9121,  0.9333,  0.9983,  0.9448,  0.7054,  1.0694, -4.8058, -4.9512,\n",
      "         -5.2702, -5.0435, -5.2379, -5.3026, -5.4242, -5.4296, -5.3632, -5.3499,\n",
      "         -5.3322, -5.2600, -5.1423, -5.0525, -5.0094, -5.0026, -5.0109, -5.0274,\n",
      "         -5.0109, -4.9887, -4.9877, -5.0198, -5.0820, -5.1555, -5.2023, -5.2129,\n",
      "         -5.2082, -5.2295, -5.2911, -5.3664, -5.3944, -5.3768, -5.3363, -5.2744,\n",
      "         -5.2402, -5.2147, -5.2126, -5.2254, -5.2535, -5.3042, -5.3467, -5.3594,\n",
      "         -5.3415, -5.2995, -5.2889, -5.2901, -5.3112, -5.3510, -5.3696, -5.3762,\n",
      "         -5.3703, -5.3553, -5.3545, -5.3623, -5.3830, -5.3661],\n",
      "        [-2.9560, -3.9756, -0.9849, -2.2142, -0.1002, -2.3327, -4.4817, -0.6676,\n",
      "         -4.2582, -2.7137, -3.9545,  0.1968, -0.5017, -3.7501, -3.1800, -2.6497,\n",
      "         -3.4353, -1.9185, -2.1930, -0.9754, -1.2922, -1.4963, -0.9393,  0.2468,\n",
      "         -2.0719, -3.5426, -3.2010, -2.8263, -3.9871,  0.8546, -1.9183, -0.2454,\n",
      "         -0.8482, -3.6549, -1.8262, -1.9905, -0.9197, -3.3784, -1.8173, -0.2027,\n",
      "         -0.9239, -0.6268, -0.5766, -4.0158, -1.7653, -3.6622, -3.6309, -3.3664,\n",
      "          0.2908,  0.2736, -3.9415, -1.6431, -6.9350, -3.1421, -3.3144, -1.1585,\n",
      "         -2.5012, -3.1348, -2.5537, -1.8071, -3.7092, -1.3804, -5.7839, -2.6744,\n",
      "         -3.7571,  0.9360, -1.5793, -0.1473, -0.3715, -3.2975, -4.1901, -5.3000,\n",
      "         -3.0150, -0.2861, -3.8770, -4.0299, -2.7230, -4.1094, -2.6256, -3.6185,\n",
      "         -4.6845, -2.7592, -0.5959, -1.3035, -1.7079, -3.0534, -2.0145, -0.7093,\n",
      "         -3.5396, -1.1244, -3.2901, -3.9211, -2.7063, -0.8297, -0.9421, -2.8744,\n",
      "         -2.9055, -3.9601, -3.2981, -3.2481, -0.3627, -0.9952, -3.7111, -2.2259,\n",
      "         -0.5642, -2.0928, -3.8359, -3.7366,  0.3724, -5.0945, -1.7250, -2.7694,\n",
      "         -4.5784, -4.5533,  0.1672, -0.9533, -2.0068, -1.3554, -0.0253, -1.1747,\n",
      "         -3.0896, -4.5805, -2.8491, -1.0378, -1.1023, -2.7402, -2.5017, -0.9019,\n",
      "         -0.7369, -4.3130, -0.0170,  0.3619, -3.5451, -3.9829, -1.4150, -1.9894,\n",
      "         -3.2201, -2.3711, -1.9098, -3.2154, -3.2010, -5.6894, -1.7874, -6.0952,\n",
      "         -1.7350, -1.5410, -1.4837, -1.2459, -1.4348, -1.2477, -1.3662, -4.3369,\n",
      "         -4.7965, -0.3675, -2.1011, -4.3499, -5.6281, -4.9536, -3.4868, -4.9753,\n",
      "         -1.9772, -1.2396, -1.3092, -1.0909, -1.2145, -1.2132, -1.2181, -1.2570,\n",
      "         -1.0959, -1.2615, -1.1028, -4.8306, -4.8891, -5.1279, -4.8933, -5.0883,\n",
      "         -5.0205, -4.9274, -4.8351, -4.7072, -4.5592, -4.6706, -4.7495, -4.7303,\n",
      "         -4.6860, -4.6382, -4.5889, -4.5384, -4.4622, -4.3643, -4.2724, -4.2181,\n",
      "         -4.2036, -4.2096, -4.2203, -4.2427, -4.2636, -4.2730, -4.2581, -4.2635,\n",
      "         -4.2962, -4.3335, -4.3718, -4.4054, -4.4254, -4.4369, -4.4463, -4.4577,\n",
      "         -4.4718, -4.4872, -4.4889, -4.4975, -4.5109, -4.5257, -4.5364, -4.5451,\n",
      "         -4.5553, -4.5685, -4.5890, -4.6026, -4.6032, -4.6096]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9272, -1.2003], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8266,  0.9900,  0.9900,  0.9900,  0.9900, -0.7472, -0.8234],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8674e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1672, -0.1689, -0.1706,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1287, -0.1300, -0.1313,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1666, -0.1683, -0.1700,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608, -0.8062,  0.9656,  0.9656,  0.9656,  0.9656, -0.7287, -0.8031],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2704, -3.9482, -1.0288,  ..., -4.9014, -4.7927, -4.7524],\n",
      "        [-2.9818, -4.0056, -0.9977,  ..., -4.3590, -4.3449, -4.3420],\n",
      "        [-3.2165, -3.9831, -1.0139,  ..., -5.3566, -5.3759, -5.3512],\n",
      "        ...,\n",
      "        [-3.2778, -3.9466, -1.0247,  ..., -4.8071, -4.7812, -4.7713],\n",
      "        [-3.0078, -3.9877, -0.9747,  ..., -4.6173, -4.6058, -4.5826],\n",
      "        [-2.9788, -4.0024, -0.9969,  ..., -4.3494, -4.3358, -4.3334]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7567, -0.8544,  0.8354,  0.9149,  0.9708,  0.7317, -0.6376, -0.8355],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.2414,  0.9900,  0.9900,  0.9900, -0.8085, -0.8986, -0.8176,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4150e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2033, -0.2054, -0.2075,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1818, -0.1836, -0.1855,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1654, -0.1671, -0.1688,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1870,  0.9656,  0.9656,  0.9656, -0.7885, -0.8764, -0.7974,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9575, -3.9722, -0.9758,  ..., -4.5939, -4.5960, -4.6015],\n",
      "        [-3.2178, -3.9791, -1.0065,  ..., -5.4206, -5.4445, -5.4189],\n",
      "        [-3.2193, -3.9786, -1.0076,  ..., -5.3949, -5.4167, -5.3965],\n",
      "        ...,\n",
      "        [-2.9770, -4.0020, -0.9927,  ..., -4.3077, -4.2956, -4.2912],\n",
      "        [-2.9834, -4.0045, -0.9932,  ..., -4.3597, -4.3446, -4.3434],\n",
      "        [-3.1915, -3.9786, -1.0100,  ..., -5.2331, -5.2448, -5.2293]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1888,  0.8832,  0.9752,  0.9359, -0.9225, -0.8746, -0.8446,  0.8618],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8607, -0.8584,  0.9900,  0.9900, -0.8423,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8170e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1741, -0.1759, -0.1777,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1737, -0.1754, -0.1772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8395, -0.8372,  0.9656,  0.9656, -0.8215,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9884, -4.0056, -0.9883,  ..., -4.2975, -4.2812, -4.2772],\n",
      "        [-2.9740, -3.9995, -0.9869,  ..., -4.2863, -4.2716, -4.2638],\n",
      "        [-3.2805, -3.9434, -1.0155,  ..., -4.7977, -4.7674, -4.7640],\n",
      "        ...,\n",
      "        [-3.2150, -3.9797, -1.0042,  ..., -5.3485, -5.3670, -5.3463],\n",
      "        [-3.2230, -3.9807, -1.0043,  ..., -5.3474, -5.3646, -5.3426],\n",
      "        [-3.1879, -3.9741, -0.9854,  ..., -5.2833, -5.2827, -5.2532]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8555, -0.8335,  0.7527,  0.9236, -0.8270,  0.9427,  0.8459,  0.8593],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7388,  0.9900,  0.9900,  0.9900, -1.1688, -0.8361,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8626e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.4316e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1273, -0.1285, -0.1298,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1691, -0.1708, -0.1726,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7206,  0.9656,  0.9656,  0.9656, -1.1176, -0.8155,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0122, -3.9781, -0.9674,  ..., -4.6926, -4.6810, -4.6589],\n",
      "        [-3.2045, -3.9845, -0.9945,  ..., -5.3279, -5.3261, -5.3147],\n",
      "        [-3.2767, -3.9441, -1.0123,  ..., -4.8331, -4.8056, -4.7986],\n",
      "        ...,\n",
      "        [-2.9816, -3.9952, -0.9800,  ..., -4.3496, -4.3321, -4.3276],\n",
      "        [-3.2210, -3.9783, -0.9951,  ..., -5.3803, -5.4045, -5.3882],\n",
      "        [-3.1912, -3.9716, -0.9810,  ..., -5.2812, -5.2761, -5.2489]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6586,  0.9189,  0.6720,  0.8971, -1.1934, -0.8597,  0.9492,  0.8707],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.6902,  0.9900, -0.8323,  0.9900, -0.8513, -0.8332, -0.7222,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8260e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.1185e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3454, -0.3489, -0.3524,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1684, -0.1701, -0.1718,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1686, -0.1703, -0.1720,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1244, -0.1257, -0.1269,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6567,  0.9656, -0.8118,  0.9656, -0.8303, -0.8127, -0.7044,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8942, -4.0028, -0.9445,  ..., -4.2785, -4.2815, -4.2815],\n",
      "        [-3.2007, -3.9724, -0.9960,  ..., -5.2207, -5.2322, -5.2163],\n",
      "        [-2.9899, -3.9954, -0.9752,  ..., -4.3307, -4.3136, -4.3131],\n",
      "        ...,\n",
      "        [-2.9880, -3.9925, -0.9763,  ..., -4.3477, -4.3335, -4.3283],\n",
      "        [-3.0188, -3.9799, -0.9572,  ..., -4.6182, -4.6096, -4.5856],\n",
      "        [-3.1947, -3.9688, -0.9772,  ..., -5.2824, -5.2763, -5.2499]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0361,  0.8867, -0.8506,  0.6096, -0.8353, -0.8496, -0.6203,  0.8816],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8768, -0.8183,  0.9900, -1.1000, -0.8197],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4173e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0116e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1658, -0.1675, -0.1692,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8552, -0.7981,  0.9656, -1.0518, -0.7994],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2161, -3.9667, -0.9791,  ..., -5.3793, -5.4124, -5.3964],\n",
      "        [-3.2100, -3.9717, -0.9817,  ..., -5.3791, -5.4061, -5.3871],\n",
      "        [-3.2064, -3.9661, -0.9876,  ..., -5.3702, -5.4032, -5.3799],\n",
      "        ...,\n",
      "        [-3.2100, -3.9717, -0.9817,  ..., -5.3791, -5.4061, -5.3871],\n",
      "        [-3.0446, -3.9337, -0.9690,  ..., -0.9565, -0.9823, -1.2608],\n",
      "        [-2.9733, -3.9965, -0.9725,  ..., -4.3445, -4.3305, -4.3270]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9359,  0.9461,  0.8520, -0.8355, -0.8466,  0.9461, -1.0373, -0.7905],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(3.9980e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170,\n",
      "         0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376,\n",
      "         0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601,\n",
      "         0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847,\n",
      "         0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117,\n",
      "         0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412,\n",
      "         0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735,\n",
      "         0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088,\n",
      "         0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475,\n",
      "         0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899,\n",
      "         0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363,\n",
      "         0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870,\n",
      "         0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426,\n",
      "         0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034,\n",
      "         0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700,\n",
      "         0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429,\n",
      "         0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227,\n",
      "         0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1783, -3.9769, -0.9768, -2.2932, -0.1267, -2.3631, -4.5248, -0.7890,\n",
      "         -4.4604, -2.8353, -3.9471,  0.2497, -0.5299, -3.7600, -3.1396, -2.7129,\n",
      "         -3.4885, -1.9575, -2.1203, -1.0724, -1.3437, -1.6142, -1.0259,  0.3080,\n",
      "         -2.1734, -3.5557, -3.1865, -2.8692, -3.9521,  0.9187, -1.9771, -0.3427,\n",
      "         -0.8307, -3.6399, -1.8879, -1.8621, -0.8235, -3.4246, -1.8848, -0.3332,\n",
      "         -0.9500, -0.5448, -0.5552, -3.9626, -1.8559, -3.8542, -3.7816, -3.3522,\n",
      "          0.4957,  0.4465, -3.9516, -1.6760, -6.9537, -3.1717, -3.3439, -1.2098,\n",
      "         -2.4876, -3.0955, -2.5906, -1.8907, -3.7188, -1.4069, -5.8072, -2.6702,\n",
      "         -3.7271,  1.0174, -1.6190, -0.2384, -0.3367, -3.2831, -4.2493, -5.3808,\n",
      "         -3.0290, -0.2689, -3.8738, -4.0431, -2.7335, -4.1373, -2.6904, -3.6083,\n",
      "         -4.7604, -2.7621, -0.6095, -1.4015, -1.7906, -3.1399, -2.0773, -0.8038,\n",
      "         -3.6030, -1.1830, -3.2825, -3.9335, -2.7095, -0.8314, -1.0469, -2.7987,\n",
      "         -3.0005, -4.1320, -3.3685, -3.2849, -0.3872, -1.0933, -3.6737, -2.2403,\n",
      "         -0.5155, -2.2172, -3.9861, -3.8058,  0.3849, -1.1129, -1.5571, -0.6200,\n",
      "         -4.9716, -4.6147,  0.0571, -1.0947, -2.0581, -1.4669, -0.0381, -1.3002,\n",
      "         -3.2416, -4.5428, -2.8260, -0.9988, -1.1960, -2.8320, -2.6349, -0.9317,\n",
      "         -0.7321, -4.3023,  0.0823,  0.5315, -3.6209, -4.0203, -1.4353, -1.9612,\n",
      "         -3.1489, -2.4067, -1.9979, -3.1460, -3.2417, -5.7446, -1.9351, -2.2902,\n",
      "         -1.0804, -1.1521, -0.5641, -4.4345, -3.9672, -0.2685, -2.2441, -0.2084,\n",
      "         -1.3066, -0.4569,  0.1277, -0.6407, -4.5519, -5.0031, -1.8621, -2.2105,\n",
      "         -0.0232, -0.0669,  0.2899, -4.1603, -4.0080, -0.3964, -1.5031, -4.2755,\n",
      "         -5.9977, -4.9559, -3.2657, -4.8367, -1.5456,  0.7802,  0.9043,  1.0280,\n",
      "          0.9549,  0.7778,  0.9861, -4.8335, -4.9713, -5.2691, -5.0666, -5.2589,\n",
      "         -5.1313, -5.1620, -5.1699, -5.1639, -5.1642, -5.1535, -5.1012, -5.0777,\n",
      "         -5.0639, -5.0778, -5.0975, -5.1165, -5.1126, -5.1084, -5.1174, -5.1386,\n",
      "         -5.1873, -5.2376, -5.2905, -5.3393, -5.3582, -5.3673, -5.3689, -5.3794,\n",
      "         -5.4051, -5.4050, -5.3937, -5.3575, -5.3048, -5.2709, -5.2389, -5.2305,\n",
      "         -5.2494, -5.2718, -5.3008, -5.3140, -5.3150, -5.2997],\n",
      "        [-3.2597, -3.9315, -0.9922, -2.3378, -0.1128, -2.3530, -4.5956, -0.9025,\n",
      "         -4.5597, -2.9225, -3.9537,  0.2435, -0.5224, -3.8623, -3.1226, -2.8131,\n",
      "         -3.4799, -1.9738, -2.0927, -1.0990, -1.3532, -1.6486, -1.0514,  0.3561,\n",
      "         -2.2090, -3.6534, -3.1778, -2.9481, -3.9287,  0.9466, -1.9952, -0.3893,\n",
      "         -0.8223, -3.6458, -1.8623, -1.8334, -0.7809, -3.3948, -1.9371, -0.3595,\n",
      "         -0.9675, -0.5023, -0.5438, -3.9228, -1.8880, -3.9296, -3.8642, -3.3560,\n",
      "          0.6654,  0.5782, -4.0282, -1.6738, -6.9674, -3.2341, -3.3183, -1.2434,\n",
      "         -2.5066, -3.0834, -2.6061, -1.9115, -3.8036, -1.4123, -5.8266, -2.7279,\n",
      "         -3.7049,  1.0531, -1.6378, -0.2905, -0.3187, -3.2913, -4.3414, -5.4315,\n",
      "         -3.0951, -0.2356, -3.9531, -4.0158, -2.7950, -4.1022, -2.7286, -3.6866,\n",
      "         -4.8055, -2.8275, -0.6229, -1.4551, -1.8268, -3.1718, -2.1130, -0.8497,\n",
      "         -3.6320, -1.2018, -3.3733, -3.9119, -2.7693, -0.8363, -1.1024, -2.7760,\n",
      "         -3.0214, -4.1985, -3.3696, -3.2959, -0.4012, -1.1510, -3.6358, -2.2450,\n",
      "         -0.4689, -2.2719, -4.0656, -3.8531,  0.4114, -2.0083, -1.5554, -2.4942,\n",
      "         -5.0902, -4.6766,  0.0808, -1.1629, -2.0234, -1.5016, -0.1041, -1.2915,\n",
      "         -3.2867, -4.5236, -2.8274, -0.9759, -1.1633, -2.7950, -2.6597, -0.9574,\n",
      "         -0.8148, -4.2948,  0.4061,  0.6778, -3.5775, -4.0555, -1.4424, -1.9775,\n",
      "         -3.1265, -2.4354, -2.0044, -3.1512, -2.8669, -3.8330, -5.6360, -1.8178,\n",
      "          0.5672,  0.5183,  0.9852,  0.6746, -0.3554,  1.1476, -4.8538, -4.9918,\n",
      "         -5.2617, -5.0103, -5.1197, -5.0081, -5.2789, -5.3313, -5.3192, -5.2538,\n",
      "         -5.2213, -5.1994, -5.1757, -5.1432, -5.1071, -5.0968, -5.1093, -5.1319,\n",
      "         -5.1480, -5.1633, -5.1855, -5.2132, -5.2669, -5.3184, -5.3709, -5.3836,\n",
      "         -5.4000, -5.4314, -5.4892, -5.5125, -5.5014, -5.4768, -5.4522, -5.4464,\n",
      "         -5.4582, -5.4835, -5.5103, -5.5353, -5.5554, -5.5711, -5.5793, -5.5788,\n",
      "         -5.5798, -5.5834, -5.5812, -5.6028, -5.6286, -5.6427, -5.6590, -5.6638,\n",
      "         -5.6629, -5.6627, -5.6675, -5.6677, -5.6613, -5.6355, -5.6202, -5.6369,\n",
      "         -5.6707, -5.6959, -5.7024, -5.6732, -5.6264, -5.4877, -5.3900, -4.9958,\n",
      "         -4.9101, -5.0097, -4.9386, -4.8445, -4.8206, -4.8144]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9052, 0.5896], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8433,  0.9900,  0.9900, -0.8244,  0.9900, -1.2256, -0.8712, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7553e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1706, -0.1723, -0.1741,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2008, -0.2028, -0.2048,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1762, -0.1780, -0.1798,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8225,  0.9656,  0.9656, -0.8041,  0.9656, -1.1719, -0.8497, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9644, -3.9917, -0.9608,  ..., -4.2946, -4.2796, -4.2754],\n",
      "        [-3.1756, -3.9730, -0.9718,  ..., -5.3095, -5.3098, -5.2950],\n",
      "        [-3.1959, -3.9665, -0.9768,  ..., -5.3299, -5.3501, -5.3252],\n",
      "        ...,\n",
      "        [-2.9356, -3.9568, -0.9417,  ..., -4.6097, -4.6122, -4.6165],\n",
      "        [-2.9592, -3.9884, -0.9587,  ..., -4.3036, -4.2910, -4.2859],\n",
      "        [-3.0286, -3.9297, -0.9587,  ..., -0.9379, -0.9633, -1.2638]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8116,  0.9112,  0.8502, -0.8203,  0.9241, -1.1310, -0.8199, -1.0208],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1653,  0.9900,  0.9900,  0.9900,  0.9900, -0.7757,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0694e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(9.2883e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1609, -0.1625, -0.1642,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1569, -0.1585, -0.1601,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1143,  0.9656,  0.9656,  0.9656,  0.9656, -0.7565,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9818, -3.9405, -0.9374,  ..., -4.2917, -4.3094, -4.3257],\n",
      "        [-3.1730, -3.9689, -0.9687,  ..., -5.3067, -5.3079, -5.2919],\n",
      "        [-3.2561, -3.9245, -0.9837,  ..., -4.8590, -4.8316, -4.8272],\n",
      "        ...,\n",
      "        [-2.9565, -3.9857, -0.9555,  ..., -4.3537, -4.3407, -4.3410],\n",
      "        [-3.2561, -3.9245, -0.9837,  ..., -4.8590, -4.8316, -4.8272],\n",
      "        [-3.1929, -3.9629, -0.9719,  ..., -5.3220, -5.3429, -5.3189]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1421,  0.9197,  0.6162,  0.8864,  0.7659, -0.8461,  0.6162,  0.8587],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.6790, -0.8415,  0.9900,  0.9900, -0.8697, -0.7833,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5182e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.4766e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3431, -0.3466, -0.3501,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1702, -0.1719, -0.1737,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1759, -0.1777, -0.1795,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1585, -0.1601, -0.1617,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.6457, -0.8207,  0.9656,  0.9656, -0.8483, -0.7640,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2502, -3.9261, -0.9837,  ..., -4.8691, -4.8395, -4.8383],\n",
      "        [-2.8575, -3.9906, -0.9178,  ..., -4.2703, -4.2742, -4.2771],\n",
      "        [-2.9618, -3.9862, -0.9543,  ..., -4.2953, -4.2774, -4.2739],\n",
      "        ...,\n",
      "        [-2.9540, -3.9836, -0.9533,  ..., -4.3024, -4.2886, -4.2832],\n",
      "        [-2.9601, -3.9842, -0.9545,  ..., -4.3553, -4.3405, -4.3382],\n",
      "        [-3.1606, -3.9552, -0.9511,  ..., -5.2593, -5.2511, -5.2182]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7060, -1.0183, -0.7919,  1.0033,  1.0033, -0.7998, -0.7644,  0.9085],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8209,  0.9900, -0.8219,  0.9900,  0.9900, -0.7146,  0.9900, -0.7827],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3849e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1661, -0.1678, -0.1694,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1663, -0.1680, -0.1697,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1231, -0.1243, -0.1256,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1583, -0.1599, -0.1616,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8007,  0.9656, -0.8017,  0.9656,  0.9656, -0.6970,  0.9656, -0.7634],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9524, -3.9878, -0.9671,  ..., -4.2939, -4.2841, -4.2827],\n",
      "        [-3.1539, -3.9619, -0.9707,  ..., -5.2663, -5.2586, -5.2260],\n",
      "        [-2.9451, -3.9844, -0.9675,  ..., -4.3441, -4.3271, -4.3218],\n",
      "        ...,\n",
      "        [-2.9773, -3.9726, -0.9460,  ..., -4.6090, -4.5980, -4.5774],\n",
      "        [-3.1781, -3.9694, -0.9859,  ..., -5.3592, -5.3877, -5.3716],\n",
      "        [-2.9460, -3.9861, -0.9713,  ..., -4.3466, -4.3309, -4.3244]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7864,  0.8954, -0.8134,  0.9240,  0.8497, -0.6095,  0.9339, -0.7675],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8196,  0.9900, -0.8088, -0.7294,  0.9900, -0.7132],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4244e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1658, -0.1675, -0.1692,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1256, -0.1269, -0.1282,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1229, -0.1241, -0.1254,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7994,  0.9656, -0.7888, -0.7114,  0.9656, -0.6957],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1791, -3.9704, -0.9988,  ..., -5.3662, -5.3986, -5.3846],\n",
      "        [-3.1732, -3.9733, -1.0086,  ..., -5.3293, -5.3491, -5.3268],\n",
      "        [-2.9394, -3.9922, -0.9816,  ..., -4.3288, -4.3128, -4.3079],\n",
      "        ...,\n",
      "        [-2.9671, -3.9723, -0.9691,  ..., -4.6851, -4.6728, -4.6511],\n",
      "        [-3.1452, -3.9669, -0.9871,  ..., -5.2729, -5.2680, -5.2364],\n",
      "        [-2.9693, -3.9779, -0.9610,  ..., -4.6056, -4.5962, -4.5725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9149,  0.9196, -0.8421,  0.8693, -0.8413, -0.6409,  0.8829, -0.6259],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7780,  0.9900,  0.9900, -0.8334,  0.9900,  0.9900, -0.8065,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8003e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(8.3516e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1574, -0.1590, -0.1606,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1632, -0.1648, -0.1665,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7588,  0.9656,  0.9656, -0.8129,  0.9608,  0.9656, -0.7866,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9380, -3.9977, -0.9978,  ..., -4.3555, -4.3415, -4.3386],\n",
      "        [-3.1684, -3.9748, -1.0175,  ..., -5.4025, -5.4323, -5.4029],\n",
      "        [-3.1684, -3.9748, -1.0175,  ..., -5.4025, -5.4323, -5.4029],\n",
      "        ...,\n",
      "        [-3.2360, -3.9375, -1.0296,  ..., -4.8279, -4.8026, -4.7961],\n",
      "        [-2.9290, -3.9986, -0.9997,  ..., -4.3405, -4.3249, -4.3218],\n",
      "        [-3.1674, -3.9786, -1.0158,  ..., -5.3754, -5.4049, -5.3840]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8099,  0.8692,  0.8692, -0.8136,  0.8020,  0.7116, -0.7995,  0.9021],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.5626e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1600, -3.9780, -1.0371, -2.3589, -0.1290, -2.4040, -4.5631, -0.8538,\n",
      "         -4.4345, -2.7885, -3.9608,  0.2567, -0.4822, -3.8342, -3.1532, -2.7839,\n",
      "         -3.4834, -1.9420, -2.1882, -1.1027, -1.4048, -1.6395, -1.0578,  0.3040,\n",
      "         -2.1715, -3.6430, -3.1969, -2.9384, -3.9704,  0.8702, -1.9883, -0.3817,\n",
      "         -0.8371, -3.6661, -1.8578, -1.8501, -0.8586, -3.4162, -1.9396, -0.3406,\n",
      "         -0.9195, -0.5736, -0.5759, -4.0151, -1.8568, -3.8242, -3.7341, -3.3750,\n",
      "          0.5497,  0.5486, -4.0059, -1.6676, -6.9427, -3.2254, -3.3331, -1.2159,\n",
      "         -2.5731, -3.1253, -2.6302, -1.8998, -3.7963, -1.4022, -5.8180, -2.7379,\n",
      "         -3.7530,  0.9655, -1.6435, -0.2959, -0.3373, -3.3180, -4.2751, -5.3522,\n",
      "         -3.0913, -0.2496, -3.9509, -4.0503, -2.8050, -4.1281, -2.7263, -3.6911,\n",
      "         -4.7360, -2.8296, -0.6087, -1.4391, -1.8103, -3.1782, -2.0877, -0.8475,\n",
      "         -3.6277, -1.1482, -3.3727, -3.9475, -2.7818, -0.8271, -1.0891, -2.8258,\n",
      "         -2.9944, -4.1106, -3.3664, -3.3549, -0.3889, -1.1337, -3.6819, -2.2399,\n",
      "         -0.5094, -2.2410, -3.9639, -3.8150,  0.3940, -1.2265, -1.3420, -0.9777,\n",
      "         -5.1073, -4.7482,  0.0507, -1.0939, -2.0951, -1.4678, -0.1148, -1.3377,\n",
      "         -3.3402, -4.5042, -2.8847, -1.0371, -1.2301, -2.8593, -2.5629, -0.9438,\n",
      "         -0.7345, -4.2993,  0.2086,  0.6624, -3.6238, -4.0242, -1.4289, -2.0490,\n",
      "         -3.1585, -2.4403, -1.9947, -3.1534, -3.2421, -5.7033, -1.9689, -2.3278,\n",
      "         -1.3636, -0.9603, -1.1437, -4.4572, -4.0336, -0.2205, -2.1395, -0.5999,\n",
      "         -1.0070, -0.5547, -0.2399, -0.7146, -4.4098, -3.5674, -4.8496, -1.5611,\n",
      "          0.9129,  1.0113,  0.9405,  0.8660,  0.5678,  1.0324, -4.7877, -4.9351,\n",
      "         -5.2528, -5.0361, -5.2343, -5.3106, -5.4360, -5.4258, -5.3592, -5.3501,\n",
      "         -5.3233, -5.2370, -5.1150, -5.0218, -4.9837, -4.9872, -5.0166, -5.0415,\n",
      "         -5.0300, -5.0113, -5.0081, -5.0353, -5.0969, -5.1732, -5.2246, -5.2431,\n",
      "         -5.2432, -5.2707, -5.3295, -5.4011, -5.4307, -5.4146, -5.3710, -5.3118,\n",
      "         -5.2759, -5.2500, -5.2464, -5.2564, -5.2806, -5.3238, -5.3578, -5.3648,\n",
      "         -5.3436, -5.3004, -5.2920, -5.2949, -5.3111, -5.3445, -5.3643, -5.3666,\n",
      "         -5.3574, -5.3436, -5.3446, -5.3473, -5.3688, -5.3482],\n",
      "        [-3.1625, -3.9803, -1.0320, -2.3532, -0.1316, -2.4022, -4.5587, -0.8579,\n",
      "         -4.4398, -2.7874, -3.9599,  0.2559, -0.4809, -3.8315, -3.1535, -2.7855,\n",
      "         -3.4765, -1.9414, -2.1921, -1.1020, -1.4040, -1.6411, -1.0595,  0.3045,\n",
      "         -2.1672, -3.6417, -3.1963, -2.9434, -3.9700,  0.8717, -1.9844, -0.3839,\n",
      "         -0.8343, -3.6654, -1.8599, -1.8467, -0.8561, -3.4100, -1.9359, -0.3439,\n",
      "         -0.9155, -0.5732, -0.5749, -4.0134, -1.8534, -3.8272, -3.7323, -3.3747,\n",
      "          0.5510,  0.5479, -4.0035, -1.6664, -6.9409, -3.2283, -3.3270, -1.2156,\n",
      "         -2.5669, -3.1248, -2.6335, -1.8973, -3.7956, -1.4020, -5.8135, -2.7428,\n",
      "         -3.7530,  0.9670, -1.6394, -0.2963, -0.3353, -3.3182, -4.2761, -5.3528,\n",
      "         -3.0930, -0.2464, -3.9476, -4.0491, -2.8071, -4.1219, -2.7206, -3.6900,\n",
      "         -4.7368, -2.8319, -0.6089, -1.4387, -1.8071, -3.1741, -2.0835, -0.8472,\n",
      "         -3.6222, -1.1433, -3.3702, -3.9468, -2.7843, -0.8277, -1.0899, -2.8254,\n",
      "         -2.9909, -4.1126, -3.3676, -3.3515, -0.3894, -1.1358, -3.6793, -2.2394,\n",
      "         -0.5078, -2.2410, -3.9681, -3.8123,  0.3944, -1.0243, -1.4184, -0.7642,\n",
      "         -4.8992, -4.6559,  0.0830, -1.0830, -2.0980, -1.4976, -0.0933, -1.2882,\n",
      "         -3.2637, -4.4854, -2.9029, -1.0372, -1.2350, -2.8554, -2.5874, -0.9486,\n",
      "         -0.7658, -4.3266,  0.2326,  0.6418, -3.5939, -4.0347, -1.4314, -2.0516,\n",
      "         -3.1727, -2.4490, -2.0097, -3.1680, -3.2542, -5.7269, -1.9732, -2.3405,\n",
      "         -1.3650, -1.0081, -1.1276, -4.3295, -3.8993, -0.2345, -2.1228, -0.8691,\n",
      "         -1.0377, -0.7322, -0.2774, -0.5359, -4.4919, -3.5037, -4.7706, -1.6521,\n",
      "          0.9515,  1.0064,  0.8896,  0.8835,  0.6042,  0.9751, -4.8212, -4.9617,\n",
      "         -5.2425, -5.0069, -5.2720, -5.2704, -5.3769, -5.3708, -5.2774, -5.2597,\n",
      "         -5.2441, -5.1693, -5.0724, -5.0106, -4.9990, -5.0278, -5.0618, -5.0857,\n",
      "         -5.0779, -5.0652, -5.0731, -5.1177, -5.1806, -5.2539, -5.2973, -5.3056,\n",
      "         -5.2972, -5.3103, -5.3607, -5.4264, -5.4533, -5.4366, -5.3992, -5.3445,\n",
      "         -5.3057, -5.2735, -5.2586, -5.2660, -5.2865, -5.3248, -5.3563, -5.3639,\n",
      "         -5.3490, -5.3161, -5.3117, -5.3152, -5.3366, -5.3713, -5.3820, -5.3843,\n",
      "         -5.3796, -5.3668, -5.3771, -5.3897, -5.4191, -5.3979]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.8885, 0.8851], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8339, -0.8630,  0.9900,  0.9900, -0.8630,  0.9900, -1.2147, -0.7770],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7235e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1687, -0.1704, -0.1721,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1746, -0.1764, -0.1781,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1990, -0.2010, -0.2030,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1572, -0.1588, -0.1604,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8133, -0.8417,  0.9656,  0.9656, -0.8417,  0.9656, -1.1615, -0.7579],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9342, -4.0006, -1.0260,  ..., -4.3025, -4.2848, -4.2793],\n",
      "        [-2.9263, -3.9976, -1.0241,  ..., -4.3104, -4.2953, -4.2919],\n",
      "        [-3.1396, -3.9765, -1.0473,  ..., -5.2505, -5.2600, -5.2441],\n",
      "        ...,\n",
      "        [-3.1665, -3.9757, -1.0388,  ..., -5.3996, -5.4285, -5.4138],\n",
      "        [-2.9021, -3.9671, -1.0048,  ..., -4.6163, -4.6164, -4.6214],\n",
      "        [-2.9266, -3.9956, -1.0262,  ..., -4.3514, -4.3351, -4.3311]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8713, -0.8783,  0.8288,  0.8505, -0.8783,  0.8695, -1.0980, -0.8315],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8244, -0.8252,  0.9900, -0.7198, -1.1663, -0.7198,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7563e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1668, -0.1685, -0.1702,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1669, -0.1686, -0.1703,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1240, -0.1252, -0.1265,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8041, -0.8048,  0.9656, -0.7020, -1.1152, -0.7020,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9227, -3.9935, -1.0297,  ..., -4.3545, -4.3380, -4.3319],\n",
      "        [-2.9250, -3.9956, -1.0275,  ..., -4.3419, -4.3252, -4.3225],\n",
      "        [-3.1646, -3.9782, -1.0570,  ..., -5.3686, -5.3874, -5.3623],\n",
      "        ...,\n",
      "        [-2.9556, -3.9805, -1.0070,  ..., -4.6170, -4.6106, -4.5878],\n",
      "        [-3.1562, -3.9728, -1.0577,  ..., -5.4053, -5.4379, -5.4087],\n",
      "        [-3.1602, -3.9803, -1.0516,  ..., -5.4181, -5.4451, -5.4251]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8658, -0.8872,  0.8067, -0.6930, -1.1146, -0.6930,  0.7999,  0.8677],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.8260, -0.8280, -0.8534,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7353e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1675, -0.1692, -0.1709,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1726, -0.1744, -0.1761,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656,  0.9656,  0.9656, -0.8056, -0.8076, -0.8323,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2215, -3.9373, -1.0780,  ..., -4.9621, -4.8553, -4.8181],\n",
      "        [-3.2281, -3.9345, -1.0725,  ..., -4.8754, -4.8482, -4.8424],\n",
      "        [-3.1655, -3.9726, -1.0610,  ..., -5.4197, -5.4424, -5.4200],\n",
      "        ...,\n",
      "        [-2.9172, -3.9976, -1.0417,  ..., -4.3548, -4.3410, -4.3374],\n",
      "        [-2.9154, -3.9932, -1.0359,  ..., -4.2989, -4.2817, -4.2742],\n",
      "        [-3.1334, -3.9726, -1.0626,  ..., -5.2697, -5.2796, -5.2670]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7712,  0.6344,  0.9070,  0.8341, -0.8872, -0.8339, -0.8482,  0.8198],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7975,  0.9900,  0.9900,  0.9900,  0.9900, -0.8394],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1739e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1613, -0.1630, -0.1646,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1698, -0.1715, -0.1733,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7779,  0.9656,  0.9656,  0.9656,  0.9656, -0.8187],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1563, -3.9748, -1.0624,  ..., -5.4324, -5.4600, -5.4395],\n",
      "        [-3.1563, -3.9748, -1.0624,  ..., -5.4324, -5.4600, -5.4395],\n",
      "        [-2.9202, -3.9924, -1.0416,  ..., -4.3680, -4.3559, -4.3524],\n",
      "        ...,\n",
      "        [-3.1390, -3.9793, -1.0612,  ..., -5.3710, -5.3744, -5.3625],\n",
      "        [-3.2197, -3.9355, -1.0783,  ..., -4.9190, -4.8905, -4.8847],\n",
      "        [-2.9155, -3.9885, -1.0365,  ..., -4.3608, -4.3454, -4.3379]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8705,  0.8705, -0.9102,  0.8326,  0.8122,  0.8173,  0.6301, -0.8661],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8040,  0.9900,  0.9900,  0.9900, -0.8415, -1.1000, -0.8040],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.1779e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1627, -0.1643, -0.1660,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1702, -0.1720, -0.1737,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1627, -0.1643, -0.1660,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7842,  0.9656,  0.9656,  0.9656, -0.8208, -1.0518, -0.7842],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1569, -3.9696, -1.0668,  ..., -5.3961, -5.4140, -5.3908],\n",
      "        [-2.9221, -3.9894, -1.0410,  ..., -4.3775, -4.3633, -4.3574],\n",
      "        [-3.2283, -3.9294, -1.0782,  ..., -4.8820, -4.8510, -4.8438],\n",
      "        ...,\n",
      "        [-2.9220, -3.9876, -1.0367,  ..., -4.3153, -4.3026, -4.3005],\n",
      "        [-2.9887, -3.9312, -1.0383,  ..., -0.9972, -1.0223, -1.2282],\n",
      "        [-2.9221, -3.9894, -1.0410,  ..., -4.3775, -4.3633, -4.3574]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8893, -0.8368,  0.6360,  0.8500,  0.8253, -0.8638, -1.0637, -0.8368],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7415, -1.6958,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.3665e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1277, -0.1290, -0.1303,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7232, -1.6623,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2287, -3.9260, -1.0749,  ..., -4.9026, -4.8726, -4.8643],\n",
      "        [-3.1331, -3.9595, -1.0441,  ..., -5.3358, -5.3322, -5.3039],\n",
      "        [-2.9512, -3.9646, -1.0204,  ..., -4.6916, -4.6794, -4.6575],\n",
      "        ...,\n",
      "        [-3.1590, -3.9663, -1.0635,  ..., -5.3992, -5.4167, -5.3934],\n",
      "        [-3.1331, -3.9595, -1.0441,  ..., -5.3358, -5.3322, -5.3039],\n",
      "        [-3.2287, -3.9260, -1.0749,  ..., -4.9026, -4.8726, -4.8643]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6105,  0.8667, -0.6866, -1.0147,  0.9335,  0.9055,  0.8667,  0.6105],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8510,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.3622e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1722, -0.1739, -0.1756, -0.1774, -0.1792, -0.1810, -0.1829, -0.1847,\n",
      "         -0.1866, -0.1885, -0.1904, -0.1923, -0.1942, -0.1962, -0.1982, -0.2002,\n",
      "         -0.2022, -0.2042, -0.2063, -0.2084, -0.2105, -0.2126, -0.2148, -0.2169,\n",
      "         -0.2191, -0.2213, -0.2236, -0.2258, -0.2281, -0.2304, -0.2327, -0.2351,\n",
      "         -0.2375, -0.2399, -0.2423, -0.2447, -0.2472, -0.2497, -0.2522, -0.2548,\n",
      "         -0.2573, -0.2599, -0.2626, -0.2652, -0.2679, -0.2706, -0.2733, -0.2761,\n",
      "         -0.2789, -0.2817, -0.2845, -0.2874, -0.2903, -0.2933, -0.2962, -0.2992,\n",
      "         -0.3022, -0.3053, -0.3084, -0.3115, -0.3146, -0.3178, -0.3210, -0.3243,\n",
      "         -0.3275, -0.3308, -0.3342, -0.3376, -0.3410, -0.3444, -0.3479, -0.3514,\n",
      "         -0.3550, -0.3585, -0.3622, -0.3658, -0.3695, -0.3733, -0.3770, -0.3808,\n",
      "         -0.3847, -0.3886, -0.3925, -0.3965, -0.4005, -0.4045, -0.4086, -0.4127,\n",
      "         -0.4169, -0.4211, -0.4254, -0.4296, -0.4340, -0.4384, -0.4428, -0.4473,\n",
      "         -0.4518, -0.4564, -0.4610, -0.4656, -0.4703, -0.4751, -0.4799, -0.4847,\n",
      "         -0.4896, -0.4946, -0.4996, -0.5046, -0.5097, -0.5148, -0.5200, -0.5253,\n",
      "         -0.5306, -0.5360, -0.5414, -0.5469, -0.5524, -0.5580, -0.5636, -0.5693,\n",
      "         -0.5750, -0.5808, -0.5867, -0.5926, -0.5986, -0.6047, -0.6108, -0.6169,\n",
      "         -0.6232, -0.6295, -0.6358, -0.6423, -0.6487, -0.6553, -0.6619, -0.6686,\n",
      "         -0.6754, -0.6822, -0.6891, -0.6960, -0.7031, -0.7102, -0.7173, -0.7246,\n",
      "         -0.7319, -0.7393, -0.7468, -0.7543, -0.7619, -0.7696, -0.7774, -0.7852,\n",
      "         -0.7932, -0.8012, -0.8093, -0.8174, -0.8257, -0.8340, -0.8425, -0.8510,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8300,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9224, -3.9850, -1.0321, -2.2201, -0.1094, -2.4029, -4.5445, -0.6953,\n",
      "         -4.2067, -2.6710, -3.9591,  0.2207, -0.4207, -3.8080, -3.1885, -2.6721,\n",
      "         -3.4420, -1.9085, -2.2754, -1.0320, -1.4298, -1.5518, -0.9851,  0.2273,\n",
      "         -2.0612, -3.6212, -3.2171, -2.8573, -4.0162,  0.8232, -1.9204, -0.2889,\n",
      "         -0.8891, -3.6595, -1.8304, -1.9541, -0.9057, -3.4001, -1.8755, -0.2376,\n",
      "         -0.9153, -0.6108, -0.5700, -4.0698, -1.7702, -3.6240, -3.5792, -3.3692,\n",
      "          0.3478,  0.3455, -4.0020, -1.6148, -6.9000, -3.1672, -3.3180, -1.1605,\n",
      "         -2.6224, -3.1213, -2.6120, -1.8279, -3.7818, -1.3598, -5.7682, -2.7184,\n",
      "         -3.7921,  0.9028, -1.5910, -0.2104, -0.4038, -3.3126, -4.2157, -5.3164,\n",
      "         -3.0500, -0.3835, -3.9522, -4.0451, -2.7641, -4.1212, -2.6646, -3.7034,\n",
      "         -4.7097, -2.7982, -0.6012, -1.3564, -1.7665, -3.1069, -2.0250, -0.7689,\n",
      "         -3.6101, -1.1270, -3.3854, -3.9445, -2.7546, -0.8312, -1.0042, -2.9028,\n",
      "         -2.9184, -3.9391, -3.3009, -3.3108, -0.3741, -1.0624, -3.6809, -2.2333,\n",
      "         -0.5777, -2.1227, -3.8079, -3.7358,  0.2530, -1.4391, -1.6050, -0.7844,\n",
      "         -4.6592, -4.5992,  0.0956, -1.0137, -2.1460, -1.4128, -0.0209, -1.2458,\n",
      "         -3.0674, -4.6089, -2.8351, -1.0475, -1.1099, -2.7580, -2.5178, -0.9133,\n",
      "         -0.7063, -4.3149, -0.0176,  0.4290, -3.6093, -4.0155, -1.4146, -2.1102,\n",
      "         -3.1825, -2.4023, -1.9420, -3.1061, -2.8470, -3.5748, -5.6105, -1.6494,\n",
      "         -0.7652, -0.7808, -0.9451, -0.8494, -0.9949, -0.8504, -4.9946, -5.0069,\n",
      "         -5.3332, -5.0121, -4.9820, -4.6695, -4.5866, -4.6774, -4.7396, -4.7193,\n",
      "         -4.7644, -4.8016, -4.8252, -4.8206, -4.7700, -4.6991, -4.6749, -4.6760,\n",
      "         -4.6788, -4.6705, -4.6588, -4.6653, -4.6787, -4.6866, -4.7047, -4.7309,\n",
      "         -4.7294, -4.7261, -4.7453, -4.7594, -4.7516, -4.7584, -4.7542, -4.7439,\n",
      "         -4.7431, -4.7478, -4.7734, -4.7955, -4.8118, -4.8264, -4.8331, -4.8506,\n",
      "         -4.8675, -4.8778, -4.8790, -4.8789, -4.8825, -4.8814, -4.8716, -4.8511,\n",
      "         -4.8357, -4.8311, -4.8266, -4.8113, -4.8011, -4.8027, -4.8061, -4.8218,\n",
      "         -4.8066, -4.7820, -4.7909, -4.7442, -4.7058, -4.6248, -4.5772, -4.4698,\n",
      "         -4.3730, -4.3802, -4.3574, -4.3170, -4.2994, -4.2935],\n",
      "        [-3.1307, -3.9560, -1.0374, -2.3358, -0.1397, -2.4514, -4.5734, -0.8489,\n",
      "         -4.3969, -2.7936, -4.0053,  0.2063, -0.5142, -3.8257, -3.1401, -2.7734,\n",
      "         -3.5203, -1.9433, -2.2187, -1.1280, -1.4227, -1.6641, -1.0648,  0.2853,\n",
      "         -2.1972, -3.6366, -3.1767, -2.9412, -4.0108,  0.8476, -1.9924, -0.3996,\n",
      "         -0.8506, -3.7111, -1.8472, -1.8616, -0.8638, -3.4588, -1.9268, -0.3317,\n",
      "         -0.9440, -0.5781, -0.5824, -4.0204, -1.8801, -3.7899, -3.7316, -3.4152,\n",
      "          0.5650,  0.5057, -3.9965, -1.6672, -6.9143, -3.2230, -3.3637, -1.2227,\n",
      "         -2.6046, -3.1235, -2.6468, -1.9322, -3.7903, -1.4019, -5.7921, -2.7496,\n",
      "         -3.7912,  0.9362, -1.6536, -0.3266, -0.3539, -3.3654, -4.2594, -5.3349,\n",
      "         -3.1003, -0.3258, -3.9455, -4.0513, -2.8134, -4.1657, -2.7065, -3.6900,\n",
      "         -4.7240, -2.8391, -0.6115, -1.4532, -1.8050, -3.1874, -2.1000, -0.8733,\n",
      "         -3.6258, -1.1760, -3.3777, -3.9519, -2.7935, -0.8331, -1.1101, -2.8079,\n",
      "         -3.0251, -4.0770, -3.3405, -3.3657, -0.4005, -1.1586, -3.6636, -2.2638,\n",
      "         -0.5259, -2.2463, -3.9295, -3.7930,  0.3145, -1.3189, -1.5589, -2.5270,\n",
      "         -4.7966, -4.6402,  0.0619, -1.1258, -2.1547, -1.4995, -0.0263, -1.3403,\n",
      "         -3.1425, -4.6398, -2.8333, -1.0188, -1.1749, -2.8261, -2.6147, -0.9628,\n",
      "         -0.7575, -4.3641,  0.1807,  0.6014, -3.6490, -4.0598, -1.4519, -2.0864,\n",
      "         -3.1753, -2.4370, -2.0373, -3.0961, -3.2058, -5.7001, -1.9706, -3.7889,\n",
      "         -1.1057, -1.4250, -4.5387, -4.3700, -0.2889, -1.8779, -4.5451, -2.9593,\n",
      "         -3.0265, -4.2936,  0.2328, -2.4493, -1.1878, -0.5050, -3.6659, -0.4035,\n",
      "          0.3524, -4.5012, -5.3210, -3.8794, -0.0749, -4.5268, -4.3211, -3.3716,\n",
      "         -3.9130, -4.1646, -3.0876, -4.8880, -1.5982,  1.0340,  1.0408,  0.8593,\n",
      "          0.8200,  0.4961,  1.0267, -4.8335, -4.9882, -5.3547, -5.2073, -5.3280,\n",
      "         -5.3485, -5.4414, -5.2971, -5.4168, -5.5074, -5.4477, -5.3581, -5.2742,\n",
      "         -5.2185, -5.2331, -5.2908, -5.3490, -5.3718, -5.3655, -5.3395, -5.3153,\n",
      "         -5.3310, -5.3713, -5.4170, -5.4373, -5.4302, -5.4256, -5.4472, -5.4927,\n",
      "         -5.5216, -5.5081, -5.4810, -5.4207, -5.3658, -5.3216, -5.3026, -5.3113,\n",
      "         -5.3173, -5.3247, -5.3312, -5.3376, -5.3331, -5.3069]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8643,  0.8795], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1627, -0.8333,  0.9900, -0.7990,  0.9900, -0.8513,  0.9900, -1.2173],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8167e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1605, -0.1622, -0.1638,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.1703, -0.1720,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1722, -0.1739, -0.1757,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1994, -0.2014, -0.2035,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1117, -0.8127,  0.9656, -0.7793,  0.9656, -0.8303,  0.9656, -1.1640],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9460, -3.9350, -1.0076,  ..., -4.3194, -4.3345, -4.3509],\n",
      "        [-2.9104, -3.9747, -1.0215,  ..., -4.3674, -4.3489, -4.3409],\n",
      "        [-3.1660, -3.9575, -1.0405,  ..., -5.4343, -5.4607, -5.4452],\n",
      "        ...,\n",
      "        [-2.9052, -3.9762, -1.0232,  ..., -4.3071, -4.2875, -4.2802],\n",
      "        [-3.1563, -3.9557, -1.0517,  ..., -5.4219, -5.4510, -5.4236],\n",
      "        [-2.8940, -3.9482, -1.0058,  ..., -4.6130, -4.6130, -4.6195]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1176, -0.8250,  0.8964, -0.8072,  0.9323, -0.8112,  0.8549, -1.1044],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7201,  0.9900,  0.9900,  0.9900,  0.9900, -0.8248, -0.7361, -0.8805],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5439e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1240, -0.1253, -0.1266,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1669, -0.1685, -0.1702,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1268, -0.1281, -0.1294,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1781, -0.1799, -0.1817,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7023,  0.9656,  0.9608,  0.9656,  0.9656, -0.8044, -0.7180, -0.8588],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9484, -3.9586, -0.9952,  ..., -4.6114, -4.6016, -4.5780],\n",
      "        [-3.1673, -3.9608, -1.0468,  ..., -5.4935, -5.5199, -5.4969],\n",
      "        [-3.2189, -3.9203, -1.0583,  ..., -4.9442, -4.8402, -4.7983],\n",
      "        ...,\n",
      "        [-2.9199, -3.9797, -1.0123,  ..., -4.3422, -4.3260, -4.3200],\n",
      "        [-2.9443, -3.9553, -1.0013,  ..., -4.6897, -4.6741, -4.6512],\n",
      "        [-2.9102, -3.9734, -1.0163,  ..., -4.3316, -4.3169, -4.3117]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6836,  0.8820,  0.8208,  0.9632,  0.8853, -0.8450, -0.6576, -0.8464],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8427,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.1000,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.3269e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1705, -0.1722, -0.1739,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8219,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -1.0518,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9142, -3.9705, -1.0079,  ..., -4.3233, -4.3105, -4.3062],\n",
      "        [-3.1654, -3.9532, -1.0340,  ..., -5.4352, -5.4553, -5.4309],\n",
      "        [-3.2297, -3.9145, -1.0483,  ..., -4.8688, -4.8391, -4.8275],\n",
      "        ...,\n",
      "        [-3.1604, -3.9568, -1.0343,  ..., -5.4418, -5.4644, -5.4451],\n",
      "        [-2.9864, -3.9151, -1.0107,  ..., -0.9512, -0.9776, -1.2257],\n",
      "        [-3.1305, -3.9473, -1.0156,  ..., -5.3431, -5.3352, -5.3104]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8153,  0.9727,  0.6734,  0.8947,  0.6734,  0.9418, -1.0249,  0.9228],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.7014, -0.8480,  0.9900, -0.8525, -0.8419, -0.8130],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8968e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3477, -0.3512, -0.3547,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1725, -0.1742, -0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1703, -0.1720, -0.1738,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1645, -0.1661, -0.1678,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.6677, -0.8271,  0.9656, -0.8315, -0.8212, -0.7929],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2263, -3.9145, -1.0439,  ..., -4.9189, -4.8845, -4.8735],\n",
      "        [-3.1641, -3.9531, -1.0274,  ..., -5.4416, -5.4664, -5.4479],\n",
      "        [-2.8240, -3.9763, -0.9692,  ..., -4.2940, -4.2957, -4.2937],\n",
      "        ...,\n",
      "        [-2.9126, -3.9677, -0.9986,  ..., -4.3624, -4.3455, -4.3382],\n",
      "        [-2.9111, -3.9709, -1.0044,  ..., -4.3700, -4.3542, -4.3473],\n",
      "        [-2.9178, -3.9691, -1.0032,  ..., -4.3901, -4.3723, -4.3654]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7187,  0.9547, -0.9773, -0.7899,  0.9397, -0.8229, -0.7571, -0.7772],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.8942, -0.7341,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.4202e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.5948e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1264, -0.1277, -0.1290,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.8721, -0.7160,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2307, -3.9077, -1.0308,  ..., -4.9183, -4.8848, -4.8705],\n",
      "        [-3.1387, -3.9456, -1.0192,  ..., -5.2766, -5.2864, -5.2712],\n",
      "        [-3.1629, -3.9508, -1.0193,  ..., -5.4465, -5.4715, -5.4532],\n",
      "        ...,\n",
      "        [-2.9510, -3.9493, -0.9726,  ..., -4.6248, -4.6156, -4.5881],\n",
      "        [-3.1604, -3.9477, -1.0217,  ..., -5.4079, -5.4256, -5.4016],\n",
      "        [-3.1314, -3.9410, -1.0020,  ..., -5.3505, -5.3458, -5.3179]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6843,  0.9168,  0.9605,  0.6843, -0.8165, -0.6642,  0.9779,  0.9488],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8144,  0.9900, -0.8688, -0.8089,  0.9900,  0.9900,  0.9900, -0.8688],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3634e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1647, -0.1664, -0.1681,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1758, -0.1775, -0.1793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1758, -0.1775, -0.1793,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7943,  0.9656, -0.8473, -0.7889,  0.9656,  0.9656,  0.9656, -0.8473],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9133, -3.9650, -0.9938,  ..., -4.3786, -4.3596, -4.3509],\n",
      "        [-3.1461, -3.9552, -1.0139,  ..., -5.3965, -5.4002, -5.3877],\n",
      "        [-2.9193, -3.9696, -0.9927,  ..., -4.3300, -4.3125, -4.3035],\n",
      "        ...,\n",
      "        [-3.1461, -3.9552, -1.0139,  ..., -5.3965, -5.4002, -5.3877],\n",
      "        [-3.1731, -3.9533, -1.0234,  ..., -5.5053, -5.5352, -5.5126],\n",
      "        [-2.9193, -3.9696, -0.9927,  ..., -4.3300, -4.3125, -4.3035]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7751,  0.8891, -0.8083, -0.8211,  0.9312,  0.8891,  0.9044, -0.8083],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.7958e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.0401e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1728e+00, -3.9502e+00, -1.0170e+00, -2.3154e+00, -1.2619e-01,\n",
      "         -2.4352e+00, -4.5741e+00, -8.4083e-01, -4.4537e+00, -2.8718e+00,\n",
      "         -3.9764e+00,  1.6244e-01, -5.6713e-01, -3.8033e+00, -3.1426e+00,\n",
      "         -2.7176e+00, -3.4962e+00, -1.9574e+00, -2.1675e+00, -1.1522e+00,\n",
      "         -1.4588e+00, -1.6914e+00, -1.0799e+00,  2.9497e-01, -2.2156e+00,\n",
      "         -3.6131e+00, -3.1772e+00, -2.8687e+00, -3.9918e+00,  8.7998e-01,\n",
      "         -2.0248e+00, -4.2084e-01, -8.5412e-01, -3.6619e+00, -1.8797e+00,\n",
      "         -1.7579e+00, -8.6026e-01, -3.4285e+00, -1.9190e+00, -3.5651e-01,\n",
      "         -9.6141e-01, -5.6373e-01, -5.5038e-01, -3.9931e+00, -1.8963e+00,\n",
      "         -3.8526e+00, -3.8430e+00, -3.3762e+00,  5.3397e-01,  4.9495e-01,\n",
      "         -3.9714e+00, -1.6818e+00, -6.9351e+00, -3.1503e+00, -3.3305e+00,\n",
      "         -1.2278e+00, -2.5590e+00, -3.1066e+00, -2.6341e+00, -1.9384e+00,\n",
      "         -3.7609e+00, -1.4142e+00, -5.8000e+00, -2.6588e+00, -3.7687e+00,\n",
      "          9.8026e-01, -1.6745e+00, -3.3444e-01, -3.4284e-01, -3.3137e+00,\n",
      "         -4.2506e+00, -5.4144e+00, -3.0157e+00, -1.7224e-01, -3.9123e+00,\n",
      "         -4.0358e+00, -2.7271e+00, -4.1316e+00, -2.7000e+00, -3.6513e+00,\n",
      "         -4.7931e+00, -2.7483e+00, -6.1248e-01, -1.4846e+00, -1.8239e+00,\n",
      "         -3.1850e+00, -2.1253e+00, -8.9304e-01, -3.6500e+00, -1.1912e+00,\n",
      "         -3.3280e+00, -3.9334e+00, -2.6998e+00, -8.2685e-01, -1.1313e+00,\n",
      "         -2.8106e+00, -3.0378e+00, -4.1470e+00, -3.3525e+00, -3.3288e+00,\n",
      "         -3.9729e-01, -1.1826e+00, -3.6603e+00, -2.2943e+00, -5.2267e-01,\n",
      "         -2.2429e+00, -3.9944e+00, -3.8203e+00,  4.6897e-01, -1.1921e+00,\n",
      "         -1.4570e+00, -1.0060e+00, -4.5314e+00, -4.5966e+00,  1.2228e-01,\n",
      "         -1.2101e+00, -2.1352e+00, -1.5618e+00, -3.2067e-02, -1.3376e+00,\n",
      "         -3.3232e+00, -4.4424e+00, -2.8839e+00, -1.0428e+00, -1.2462e+00,\n",
      "         -2.8358e+00, -2.6441e+00, -9.5281e-01, -7.9031e-01, -4.3141e+00,\n",
      "          1.6972e-01,  5.9815e-01, -3.5913e+00, -4.0510e+00, -1.4706e+00,\n",
      "         -2.0391e+00, -3.1419e+00, -2.4299e+00, -2.0375e+00, -3.1705e+00,\n",
      "         -3.2673e+00, -5.6915e+00, -1.9400e+00, -2.3268e+00, -1.2032e+00,\n",
      "         -9.6825e-01, -6.3525e-01, -4.1311e+00, -3.9393e+00, -2.2016e-01,\n",
      "         -2.1878e+00, -9.9105e-02, -9.8673e-01, -5.1636e-01,  1.6266e-01,\n",
      "         -4.1599e-01, -4.4069e+00, -4.0603e+00, -4.8296e+00, -1.5848e+00,\n",
      "          8.5778e-01,  9.3009e-01,  1.0556e+00,  1.0494e+00,  8.2884e-01,\n",
      "          8.1396e-01, -4.8468e+00, -4.9717e+00, -5.2810e+00, -5.0586e+00,\n",
      "         -5.2403e+00, -5.2512e+00, -5.4279e+00, -5.4120e+00, -5.3206e+00,\n",
      "         -5.3369e+00, -5.3237e+00, -5.2267e+00, -5.1092e+00, -5.0225e+00,\n",
      "         -4.9885e+00, -4.9953e+00, -5.0242e+00, -5.0561e+00, -5.0506e+00,\n",
      "         -5.0357e+00, -5.0319e+00, -5.0576e+00, -5.1098e+00, -5.1762e+00,\n",
      "         -5.2220e+00, -5.2324e+00, -5.2280e+00, -5.2521e+00, -5.3244e+00,\n",
      "         -5.4197e+00, -5.4714e+00, -5.4650e+00, -5.4251e+00, -5.3591e+00,\n",
      "         -5.3187e+00, -5.2881e+00, -5.2857e+00, -5.2989e+00, -5.3256e+00,\n",
      "         -5.3679e+00, -5.4010e+00, -5.4067e+00, -5.3919e+00, -5.3504e+00,\n",
      "         -5.3402e+00, -5.3429e+00, -5.3558e+00, -5.3922e+00, -5.4034e+00,\n",
      "         -5.4065e+00, -5.3993e+00, -5.3919e+00, -5.3931e+00, -5.4049e+00,\n",
      "         -5.4201e+00, -5.3984e+00],\n",
      "        [-3.1732e+00, -3.9481e+00, -1.0151e+00, -2.3204e+00, -1.2825e-01,\n",
      "         -2.4329e+00, -4.5722e+00, -8.3753e-01, -4.4509e+00, -2.8724e+00,\n",
      "         -3.9715e+00,  1.6937e-01, -5.6481e-01, -3.8037e+00, -3.1382e+00,\n",
      "         -2.7147e+00, -3.4970e+00, -1.9600e+00, -2.1642e+00, -1.1552e+00,\n",
      "         -1.4600e+00, -1.6948e+00, -1.0812e+00,  3.0066e-01, -2.2159e+00,\n",
      "         -3.6132e+00, -3.1706e+00, -2.8644e+00, -3.9921e+00,  8.7600e-01,\n",
      "         -2.0238e+00, -4.2458e-01, -8.5387e-01, -3.6560e+00, -1.8795e+00,\n",
      "         -1.7538e+00, -8.5451e-01, -3.4283e+00, -1.9160e+00, -3.5407e-01,\n",
      "         -9.6543e-01, -5.5785e-01, -5.4476e-01, -3.9972e+00, -1.8959e+00,\n",
      "         -3.8509e+00, -3.8400e+00, -3.3716e+00,  5.2900e-01,  4.9013e-01,\n",
      "         -3.9708e+00, -1.6813e+00, -6.9418e+00, -3.1484e+00, -3.3303e+00,\n",
      "         -1.2287e+00, -2.5571e+00, -3.1002e+00, -2.6320e+00, -1.9386e+00,\n",
      "         -3.7615e+00, -1.4149e+00, -5.8035e+00, -2.6589e+00, -3.7679e+00,\n",
      "          9.7471e-01, -1.6756e+00, -3.3942e-01, -3.4254e-01, -3.3092e+00,\n",
      "         -4.2494e+00, -5.4143e+00, -3.0137e+00, -1.7075e-01, -3.9147e+00,\n",
      "         -4.0335e+00, -2.7251e+00, -4.1298e+00, -2.6959e+00, -3.6533e+00,\n",
      "         -4.7933e+00, -2.7467e+00, -6.0788e-01, -1.4870e+00, -1.8212e+00,\n",
      "         -3.1815e+00, -2.1283e+00, -8.9713e-01, -3.6474e+00, -1.1961e+00,\n",
      "         -3.3293e+00, -3.9308e+00, -2.6993e+00, -8.2436e-01, -1.1346e+00,\n",
      "         -2.8101e+00, -3.0396e+00, -4.1458e+00, -3.3504e+00, -3.3280e+00,\n",
      "         -3.9558e-01, -1.1881e+00, -3.6520e+00, -2.2948e+00, -5.1697e-01,\n",
      "         -2.2416e+00, -3.9952e+00, -3.8248e+00,  4.6935e-01, -1.2296e+00,\n",
      "         -1.4049e+00, -4.9521e-01, -4.8766e+00, -4.6161e+00,  9.8947e-02,\n",
      "         -1.1288e+00, -2.1549e+00, -1.5155e+00,  2.7756e-03, -1.2640e+00,\n",
      "         -3.2310e+00, -4.5935e+00, -2.8104e+00, -9.9624e-01, -1.1839e+00,\n",
      "         -2.7980e+00, -2.6558e+00, -9.4746e-01, -8.3472e-01, -4.3075e+00,\n",
      "          1.6779e-01,  5.9367e-01, -3.6231e+00, -4.0411e+00, -1.4603e+00,\n",
      "         -2.0361e+00, -3.1471e+00, -2.4158e+00, -2.0267e+00, -3.0870e+00,\n",
      "         -3.2625e+00, -5.7416e+00, -1.9392e+00, -2.3335e+00, -1.1092e+00,\n",
      "         -9.0779e-01, -4.8486e-01, -4.3380e+00, -4.0344e+00, -2.1082e-01,\n",
      "         -2.1237e+00, -4.1337e-01, -1.0334e+00, -2.4667e-01, -1.5643e-01,\n",
      "         -5.7030e-01, -4.7675e+00, -3.6150e+00, -4.8621e+00, -1.5044e+00,\n",
      "          8.9219e-01,  9.1286e-01,  1.0044e+00,  1.0986e+00,  6.9122e-01,\n",
      "          1.0014e+00, -4.8079e+00, -4.9679e+00, -5.2662e+00, -5.0653e+00,\n",
      "         -5.2544e+00, -5.2649e+00, -5.5046e+00, -5.4438e+00, -5.3672e+00,\n",
      "         -5.3780e+00, -5.3691e+00, -5.2885e+00, -5.1737e+00, -5.0967e+00,\n",
      "         -5.0703e+00, -5.0834e+00, -5.1042e+00, -5.1200e+00, -5.1070e+00,\n",
      "         -5.0987e+00, -5.1013e+00, -5.1380e+00, -5.2066e+00, -5.2887e+00,\n",
      "         -5.3403e+00, -5.3602e+00, -5.3566e+00, -5.3739e+00, -5.4310e+00,\n",
      "         -5.5104e+00, -5.5564e+00, -5.5474e+00, -5.5068e+00, -5.4407e+00,\n",
      "         -5.3980e+00, -5.3642e+00, -5.3590e+00, -5.3720e+00, -5.3955e+00,\n",
      "         -5.4400e+00, -5.4777e+00, -5.4905e+00, -5.4781e+00, -5.4379e+00,\n",
      "         -5.4291e+00, -5.4290e+00, -5.4390e+00, -5.4763e+00, -5.4912e+00,\n",
      "         -5.4910e+00, -5.4820e+00, -5.4676e+00, -5.4706e+00, -5.4820e+00,\n",
      "         -5.5076e+00, -5.4785e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9226, 0.9335], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8400, -0.7961,  0.9900, -0.8563, -1.2200, -1.1641,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7503e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1699, -0.1717, -0.1734,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1611, -0.1627, -0.1643,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1607, -0.1624, -0.1640,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8193, -0.7765,  0.9656, -0.8351, -1.1666, -1.1130,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9077, -3.9698, -0.9982,  ..., -4.3799, -4.3591, -4.3495],\n",
      "        [-2.9096, -3.9736, -1.0014,  ..., -4.3836, -4.3675, -4.3605],\n",
      "        [-3.1535, -3.9510, -1.0286,  ..., -5.4376, -5.4656, -5.4408],\n",
      "        ...,\n",
      "        [-2.9418, -3.9311, -0.9838,  ..., -4.3228, -4.3399, -4.3571],\n",
      "        [-3.1642, -3.9561, -1.0271,  ..., -5.3988, -5.4150, -5.3916],\n",
      "        [-3.1397, -3.9603, -1.0212,  ..., -5.3867, -5.3899, -5.3786]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7931, -0.8326,  0.8860, -0.7771, -1.1150, -1.1296,  0.9072,  0.8777],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.6850,  0.9900,  0.9900,  0.9900, -0.8758,  0.9900,  0.9900, -0.8333],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6748e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.3137e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3443, -0.3478, -0.3513,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.1703, -0.1720,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6517,  0.9656,  0.9656,  0.9656, -0.8542,  0.9656,  0.9656, -0.8127],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8050, -3.9846, -0.9757,  ..., -4.2829, -4.2848, -4.2837],\n",
      "        [-3.2220, -3.9192, -1.0480,  ..., -4.8807, -4.8450, -4.8347],\n",
      "        [-3.2220, -3.9192, -1.0480,  ..., -4.8807, -4.8450, -4.8347],\n",
      "        ...,\n",
      "        [-3.1253, -3.9569, -1.0355,  ..., -5.2682, -5.2761, -5.2601],\n",
      "        [-3.2149, -3.9237, -1.0499,  ..., -4.9215, -4.8853, -4.8743],\n",
      "        [-2.9062, -3.9755, -1.0077,  ..., -4.3246, -4.3118, -4.3075]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9881,  0.6632,  0.6632,  0.9714, -0.8267,  0.8942,  0.6689, -0.8097],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8255,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.7942],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.4969e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1670, -0.1687, -0.1704,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1607, -0.1623, -0.1639,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8051,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.7746],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1471, -3.9612, -1.0421,  ..., -5.4390, -5.4595, -5.4359],\n",
      "        [-2.8866, -3.9832, -1.0212,  ..., -4.3629, -4.3458, -4.3395],\n",
      "        [-3.1475, -3.9634, -1.0436,  ..., -5.3923, -5.4085, -5.3821],\n",
      "        ...,\n",
      "        [-3.1205, -3.9696, -1.0392,  ..., -5.3717, -5.3723, -5.3649],\n",
      "        [-3.1404, -3.9642, -1.0413,  ..., -5.4392, -5.4652, -5.4446],\n",
      "        [-2.8901, -3.9813, -1.0196,  ..., -4.3797, -4.3613, -4.3560]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9619, -0.7864,  0.8794,  0.8808,  0.8971,  0.8571,  0.9371, -0.7939],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.8520,  0.9900, -0.8349],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.3148e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1724, -0.1741, -0.1759,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1689, -0.1706, -0.1723,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.8310,  0.9656, -0.8143],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1384, -3.9644, -1.0424,  ..., -5.4459, -5.4708, -5.4534],\n",
      "        [-3.1366, -3.9652, -1.0482,  ..., -5.4660, -5.4895, -5.4592],\n",
      "        [-3.1432, -3.9708, -1.0557,  ..., -5.4976, -5.5266, -5.5032],\n",
      "        ...,\n",
      "        [-2.8863, -3.9877, -1.0271,  ..., -4.3149, -4.2967, -4.2877],\n",
      "        [-3.2037, -3.9276, -1.0615,  ..., -4.9017, -4.8615, -4.8480],\n",
      "        [-2.8786, -3.9811, -1.0228,  ..., -4.3675, -4.3480, -4.3390]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8910,  0.8976,  0.8668,  0.9070,  0.9306, -0.8391,  0.6405, -0.8303],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8066, -1.1000,  0.9900,  0.9900,  0.9900, -0.8601,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9336e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1632, -0.1648, -0.1665,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1740, -0.1758, -0.1775,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7867, -1.0518,  0.9608,  0.9656,  0.9656, -0.8389,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0966, -3.9609, -1.0365,  ..., -5.3357, -5.3294, -5.3060],\n",
      "        [-2.8787, -3.9891, -1.0305,  ..., -4.3757, -4.3572, -4.3505],\n",
      "        [-2.9506, -3.9304, -1.0284,  ..., -0.9886, -1.0137, -1.2456],\n",
      "        ...,\n",
      "        [-3.1300, -3.9673, -1.0533,  ..., -5.4676, -5.4906, -5.4621],\n",
      "        [-2.8786, -3.9892, -1.0296,  ..., -4.3135, -4.2954, -4.2865],\n",
      "        [-3.1271, -3.9703, -1.0519,  ..., -5.4450, -5.4711, -5.4509]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9022, -0.8033, -1.0543,  0.8082,  0.8665,  0.8971, -0.8421,  0.9303],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7359,  0.9900, -0.7359, -0.8379, -0.7512,  0.9900, -0.8930, -0.8556],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.1825e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1268, -0.1280, -0.1293,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1268, -0.1280, -0.1293,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1807, -0.1825, -0.1843,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1731, -0.1748, -0.1766,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7177,  0.9656, -0.7177, -0.8172, -0.7327,  0.9656, -0.8710, -0.8345],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9053, -3.9714, -1.0095,  ..., -4.6076, -4.5956, -4.5731],\n",
      "        [-3.1924, -3.9299, -1.0687,  ..., -4.9189, -4.8798, -4.8630],\n",
      "        [-2.9053, -3.9714, -1.0095,  ..., -4.6076, -4.5956, -4.5731],\n",
      "        ...,\n",
      "        [-3.0916, -3.9613, -1.0405,  ..., -5.3341, -5.3310, -5.3077],\n",
      "        [-2.8665, -3.9871, -1.0314,  ..., -4.3243, -4.3077, -4.3001],\n",
      "        [-2.8686, -3.9863, -1.0264,  ..., -4.3496, -4.3335, -4.3246]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6866,  0.6424, -0.6866, -0.8407, -0.6685,  0.9024, -0.8408, -0.8576],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8174], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.2191e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1654, -0.1670, -0.1687, -0.1704, -0.1721, -0.1739, -0.1756, -0.1774,\n",
      "         -0.1792, -0.1810, -0.1828, -0.1847, -0.1865, -0.1884, -0.1903, -0.1923,\n",
      "         -0.1942, -0.1962, -0.1981, -0.2001, -0.2022, -0.2042, -0.2063, -0.2084,\n",
      "         -0.2105, -0.2126, -0.2147, -0.2169, -0.2191, -0.2213, -0.2235, -0.2258,\n",
      "         -0.2281, -0.2304, -0.2327, -0.2351, -0.2374, -0.2398, -0.2423, -0.2447,\n",
      "         -0.2472, -0.2497, -0.2522, -0.2547, -0.2573, -0.2599, -0.2625, -0.2652,\n",
      "         -0.2679, -0.2706, -0.2733, -0.2761, -0.2789, -0.2817, -0.2845, -0.2874,\n",
      "         -0.2903, -0.2932, -0.2962, -0.2992, -0.3022, -0.3053, -0.3083, -0.3115,\n",
      "         -0.3146, -0.3178, -0.3210, -0.3242, -0.3275, -0.3308, -0.3342, -0.3375,\n",
      "         -0.3409, -0.3444, -0.3479, -0.3514, -0.3549, -0.3585, -0.3621, -0.3658,\n",
      "         -0.3695, -0.3732, -0.3770, -0.3808, -0.3846, -0.3885, -0.3925, -0.3964,\n",
      "         -0.4004, -0.4045, -0.4086, -0.4127, -0.4168, -0.4211, -0.4253, -0.4296,\n",
      "         -0.4339, -0.4383, -0.4428, -0.4472, -0.4517, -0.4563, -0.4609, -0.4656,\n",
      "         -0.4703, -0.4750, -0.4798, -0.4847, -0.4896, -0.4945, -0.4995, -0.5046,\n",
      "         -0.5096, -0.5148, -0.5200, -0.5252, -0.5306, -0.5359, -0.5413, -0.5468,\n",
      "         -0.5523, -0.5579, -0.5635, -0.5692, -0.5750, -0.5808, -0.5866, -0.5926,\n",
      "         -0.5986, -0.6046, -0.6107, -0.6169, -0.6231, -0.6294, -0.6358, -0.6422,\n",
      "         -0.6487, -0.6552, -0.6618, -0.6685, -0.6753, -0.6821, -0.6890, -0.6960,\n",
      "         -0.7030, -0.7101, -0.7173, -0.7245, -0.7318, -0.7392, -0.7467, -0.7542,\n",
      "         -0.7618, -0.7695, -0.7773, -0.7852, -0.7931, -0.8011, -0.8092, -0.8174,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7972], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1178, -3.9667, -1.0630, -2.4016, -0.1375, -2.4309, -4.5482, -0.8415,\n",
      "         -4.4025, -2.8235, -3.9824,  0.1869, -0.5455, -3.8482, -3.1745, -2.7956,\n",
      "         -3.4969, -1.9602, -2.2120, -1.1464, -1.4755, -1.6822, -1.0830,  0.2546,\n",
      "         -2.2150, -3.6573, -3.2063, -2.9526, -3.9924,  0.8296, -2.0122, -0.4153,\n",
      "         -0.8661, -3.6830, -1.8854, -1.8333, -0.9117, -3.4301, -1.9603, -0.3677,\n",
      "         -0.9382, -0.6174, -0.6006, -4.0434, -1.8992, -3.8071, -3.7855, -3.3982,\n",
      "          0.5335,  0.5052, -4.0172, -1.6846, -6.9533, -3.2351, -3.3334, -1.2381,\n",
      "         -2.5953, -3.1324, -2.6274, -1.9464, -3.8103, -1.4183, -5.8266, -2.7508,\n",
      "         -3.7703,  0.9251, -1.6688, -0.3342, -0.3563, -3.3368, -4.2938, -5.3846,\n",
      "         -3.1063, -0.2297, -3.9670, -4.0665, -2.8200, -4.1341, -2.7337, -3.7088,\n",
      "         -4.7666, -2.8408, -0.6362, -1.4777, -1.8377, -3.2026, -2.1153, -0.8909,\n",
      "         -3.6290, -1.1647, -3.3893, -3.9646, -2.7939, -0.8479, -1.1262, -2.8476,\n",
      "         -3.0330, -4.1001, -3.3377, -3.3631, -0.4127, -1.1790, -3.6782, -2.2964,\n",
      "         -0.5580, -2.2386, -3.9526, -3.8453,  0.4091, -1.1603, -1.3337, -0.8918,\n",
      "         -5.0854, -4.7201,  0.0289, -1.1314, -2.1558, -1.5114, -0.0982, -1.3664,\n",
      "         -3.3400, -4.4631, -2.8963, -1.0652, -1.2624, -2.8573, -2.6112, -0.9622,\n",
      "         -0.7751, -4.3169,  0.1982,  0.6184, -3.6255, -4.0155, -1.4510, -2.0696,\n",
      "         -3.1631, -2.4379, -2.0367, -3.1548, -3.2251, -5.7047, -1.9589, -2.3418,\n",
      "         -1.2913, -0.9388, -1.0445, -4.4497, -3.9901, -0.2160, -2.1382, -0.5297,\n",
      "         -0.9914, -0.4744, -0.2290, -1.0839, -4.3757, -3.5243, -4.8467, -1.5709,\n",
      "          0.9599,  0.9463,  0.9797,  0.9472,  0.2027,  1.0543, -4.8058, -4.9455,\n",
      "         -5.2656, -5.0492, -5.2482, -5.3213, -5.4402, -5.4231, -5.3550, -5.3464,\n",
      "         -5.3310, -5.2516, -5.1344, -5.0461, -5.0101, -5.0116, -5.0345, -5.0585,\n",
      "         -5.0457, -5.0262, -5.0263, -5.0575, -5.1237, -5.2035, -5.2621, -5.2750,\n",
      "         -5.2716, -5.2949, -5.3629, -5.4507, -5.4964, -5.4852, -5.4422, -5.3820,\n",
      "         -5.3405, -5.3128, -5.3071, -5.3176, -5.3379, -5.3798, -5.4121, -5.4248,\n",
      "         -5.4124, -5.3763, -5.3678, -5.3658, -5.3810, -5.4169, -5.4340, -5.4394,\n",
      "         -5.4292, -5.4117, -5.4092, -5.4122, -5.4297, -5.4088],\n",
      "        [-2.8652, -3.9850, -1.0353, -2.2504, -0.0913, -2.3914, -4.5018, -0.6402,\n",
      "         -4.1673, -2.6768, -3.9459,  0.1876, -0.4530, -3.7959, -3.2223, -2.6666,\n",
      "         -3.4292, -1.9073, -2.2721, -1.0281, -1.4318, -1.5441, -0.9785,  0.1974,\n",
      "         -2.0539, -3.6010, -3.2423, -2.8490, -4.0043,  0.8070, -1.9121, -0.2786,\n",
      "         -0.9017, -3.6434, -1.8546, -1.9431, -0.9461, -3.3863, -1.8647, -0.2419,\n",
      "         -0.9072, -0.6462, -0.5780, -4.0846, -1.7641, -3.5957, -3.5942, -3.3574,\n",
      "          0.2345,  0.3032, -3.9904, -1.6168, -6.9294, -3.1646, -3.3020, -1.1520,\n",
      "         -2.6170, -3.1175, -2.5812, -1.8137, -3.7648, -1.3579, -5.7871, -2.7121,\n",
      "         -3.7767,  0.8896, -1.5783, -0.1927, -0.4102, -3.2907, -4.2194, -5.3610,\n",
      "         -3.0465, -0.3055, -3.9371, -4.0726, -2.7589, -4.1066, -2.6547, -3.6869,\n",
      "         -4.7533, -2.7910, -0.6037, -1.3499, -1.7779, -3.1040, -2.0157, -0.7600,\n",
      "         -3.5853, -1.1133, -3.3608, -3.9672, -2.7446, -0.8306, -0.9944, -2.9178,\n",
      "         -2.9079, -3.9152, -3.2855, -3.2859, -0.3677, -1.0554, -3.6886, -2.2592,\n",
      "         -0.6080, -2.0869, -3.7837, -3.7685,  0.3351, -1.3458, -1.5892, -0.6577,\n",
      "         -4.7174, -4.5819,  0.1377, -1.0097, -2.1457, -1.4077, -0.0503, -1.2327,\n",
      "         -3.1283, -4.4703, -2.8691, -1.0787, -1.1416, -2.7444, -2.5026, -0.9053,\n",
      "         -0.6945, -4.2892, -0.1358,  0.3856, -3.5769, -3.9912, -1.4092, -2.0895,\n",
      "         -3.1600, -2.3951, -1.9338, -3.1476, -2.8782, -3.5469, -5.6024, -1.6148,\n",
      "         -0.8101, -0.8366, -0.7818, -0.7845, -0.8267, -0.8329, -4.9806, -4.9821,\n",
      "         -5.3423, -5.0216, -4.9675, -4.6338, -4.5838, -4.7170, -4.7832, -4.7366,\n",
      "         -4.7952, -4.8365, -4.8503, -4.8423, -4.7933, -4.7361, -4.7127, -4.7101,\n",
      "         -4.7066, -4.6907, -4.6743, -4.6831, -4.6971, -4.7043, -4.7167, -4.7361,\n",
      "         -4.7297, -4.7272, -4.7403, -4.7439, -4.7325, -4.7452, -4.7480, -4.7407,\n",
      "         -4.7356, -4.7394, -4.7623, -4.7880, -4.8103, -4.8260, -4.8364, -4.8563,\n",
      "         -4.8765, -4.8905, -4.8936, -4.8903, -4.8979, -4.8893, -4.8799, -4.8668,\n",
      "         -4.8536, -4.8492, -4.8385, -4.8207, -4.8111, -4.8116, -4.8181, -4.8266,\n",
      "         -4.8172, -4.7844, -4.7875, -4.7347, -4.7079, -4.6476, -4.5999, -4.5074,\n",
      "         -4.4179, -4.4202, -4.3963, -4.3605, -4.3415, -4.3347]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8484, -0.8121], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8106, -0.8700, -1.1000,  0.9900,  0.9900,  0.9900, -0.7379],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4041e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1640, -0.1656, -0.1673,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1760, -0.1778, -0.1796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1271, -0.1284, -0.1297,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7906, -0.8486, -1.0518,  0.9656,  0.9656,  0.9656, -0.7197],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1155, -3.9623, -1.0639,  ..., -5.4367, -5.4655, -5.4383],\n",
      "        [-2.8683, -3.9865, -1.0356,  ..., -4.3678, -4.3531, -4.3488],\n",
      "        [-2.8582, -3.9853, -1.0320,  ..., -4.3002, -4.2793, -4.2700],\n",
      "        ...,\n",
      "        [-3.0956, -3.9655, -1.0606,  ..., -5.2473, -5.2579, -5.2478],\n",
      "        [-3.1218, -3.9706, -1.0585,  ..., -5.4478, -5.4736, -5.4558],\n",
      "        [-2.9063, -3.9708, -1.0133,  ..., -4.6070, -4.5937, -4.5720]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8729, -0.8697, -0.8054, -1.0678,  0.9097,  0.8803,  0.9457, -0.6836],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8505,  0.9900, -0.7311, -0.8872,  0.9900, -0.8070],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2694e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1721, -0.1738, -0.1755,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1795, -0.1813, -0.1831,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1633, -0.1649, -0.1666,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8295,  0.9656, -0.7131, -0.8653,  0.9656, -0.7871],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1274, -3.9666, -1.0619,  ..., -5.3938, -5.4082, -5.3828],\n",
      "        [-3.1971, -3.9286, -1.0711,  ..., -4.8853, -4.8462, -4.8334],\n",
      "        [-2.8671, -3.9836, -1.0305,  ..., -4.3471, -4.3284, -4.3221],\n",
      "        ...,\n",
      "        [-2.8661, -3.9850, -1.0362,  ..., -4.3186, -4.3047, -4.2966],\n",
      "        [-3.1267, -3.9641, -1.0580,  ..., -5.4655, -5.4885, -5.4609],\n",
      "        [-2.8685, -3.9818, -1.0364,  ..., -4.3569, -4.3386, -4.3304]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8824,  0.6726, -0.8479,  0.8824, -0.6805, -0.8285,  0.9199, -0.8030],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8427, -0.8414, -0.8575,  0.9900, -0.8575],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2444e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1735, -0.1752, -0.1770,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1735, -0.1752, -0.1770,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8219, -0.8206, -0.8363,  0.9656, -0.8363],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1339, -3.9681, -1.0627,  ..., -5.4960, -5.5240, -5.5009],\n",
      "        [-3.1339, -3.9681, -1.0627,  ..., -5.4960, -5.5240, -5.5009],\n",
      "        [-3.1955, -3.9259, -1.0686,  ..., -4.9074, -4.8681, -4.8485],\n",
      "        ...,\n",
      "        [-2.8743, -3.9858, -1.0349,  ..., -4.3049, -4.2869, -4.2792],\n",
      "        [-3.0945, -3.9572, -1.0386,  ..., -5.3275, -5.3237, -5.3009],\n",
      "        [-2.8743, -3.9858, -1.0349,  ..., -4.3049, -4.2869, -4.2792]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8976,  0.8976,  0.6765, -0.8080, -0.8292, -0.8250,  0.9225, -0.8250],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.8060, -0.8060, -0.8845],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8478e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1631, -0.1647, -0.1664,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1631, -0.1647, -0.1664,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1789, -0.1807, -0.1826,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.7862, -0.7862, -0.8626],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2008, -3.9270, -1.0664,  ..., -4.8736, -4.8356, -4.8207],\n",
      "        [-3.1015, -3.9620, -1.0550,  ..., -5.2308, -5.2424, -5.2309],\n",
      "        [-3.0964, -3.9566, -1.0358,  ..., -5.3259, -5.3204, -5.2964],\n",
      "        ...,\n",
      "        [-2.8741, -3.9842, -1.0302,  ..., -4.3660, -4.3483, -4.3432],\n",
      "        [-2.8741, -3.9842, -1.0302,  ..., -4.3660, -4.3483, -4.3432],\n",
      "        [-2.8673, -3.9828, -1.0305,  ..., -4.3171, -4.3022, -4.2940]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6994,  0.9056,  0.9324,  0.9303,  0.8635, -0.7812, -0.7812, -0.8124],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8413, -1.2220, -0.8472,  0.9900,  0.9900,  0.9900, -0.7409],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6517e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1702, -0.1719, -0.1737,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2002, -0.2022, -0.2042,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1276, -0.1289, -0.1302,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8205, -1.1685, -0.8263,  0.9656,  0.9656,  0.9656, -0.7226],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1366, -3.9597, -1.0490,  ..., -5.4294, -5.4503, -5.4241],\n",
      "        [-2.8692, -3.9831, -1.0290,  ..., -4.3471, -4.3291, -4.3225],\n",
      "        [-2.8573, -3.9531, -1.0089,  ..., -4.5763, -4.5752, -4.5837],\n",
      "        ...,\n",
      "        [-3.1307, -3.9636, -1.0471,  ..., -5.4281, -5.4525, -5.4317],\n",
      "        [-3.1343, -3.9604, -1.0483,  ..., -5.4463, -5.4695, -5.4411],\n",
      "        [-2.9083, -3.9619, -1.0116,  ..., -4.6644, -4.6521, -4.6305]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0015, -0.7790, -1.1521, -0.8148,  0.9839,  0.9839,  0.9520, -0.6509],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.6980,  0.9900, -0.8366, -1.1641,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8889e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3470, -0.3505, -0.3540,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1607, -0.1624, -0.1640,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.6644,  0.9656, -0.8159, -1.1131,  0.9608,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2071, -3.9237, -1.0573,  ..., -4.9065, -4.8635, -4.8439],\n",
      "        [-3.2012, -3.9282, -1.0603,  ..., -4.9162, -4.8768, -4.8568],\n",
      "        [-2.7859, -3.9898, -0.9868,  ..., -4.2756, -4.2765, -4.2733],\n",
      "        ...,\n",
      "        [-2.9140, -3.9392, -1.0038,  ..., -4.2922, -4.3075, -4.3226],\n",
      "        [-3.1979, -3.9272, -1.0613,  ..., -4.9533, -4.8486, -4.8004],\n",
      "        [-3.1151, -3.9677, -1.0420,  ..., -5.3479, -5.3473, -5.3387]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7150,  0.6586, -0.9663,  0.9100, -0.8023, -1.1665,  0.8606,  0.9100],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.8572e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(2.1407e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1429, -3.9588, -1.0391, -2.4004, -0.1430, -2.4198, -4.5311, -0.8402,\n",
      "         -4.4414, -2.8501, -3.9906,  0.2310, -0.5145, -3.8551, -3.1504, -2.7924,\n",
      "         -3.4821, -1.9815, -2.1615, -1.1690, -1.4885, -1.6958, -1.0925,  0.2609,\n",
      "         -2.2187, -3.6619, -3.1849, -2.9450, -3.9884,  0.8552, -2.0208, -0.4314,\n",
      "         -0.8561, -3.6814, -1.8956, -1.8477, -0.9011, -3.4124, -1.9568, -0.3884,\n",
      "         -0.9519, -0.6044, -0.5894, -4.0078, -1.9022, -3.8468, -3.8226, -3.3983,\n",
      "          0.6065,  0.5799, -4.0246, -1.6725, -6.9387, -3.2297, -3.3125, -1.2526,\n",
      "         -2.5571, -3.1181, -2.6141, -1.9444, -3.8157, -1.4032, -5.8009, -2.7388,\n",
      "         -3.7626,  0.9546, -1.6708, -0.3419, -0.3416, -3.3291, -4.3170, -5.3967,\n",
      "         -3.0990, -0.2433, -3.9693, -4.0440, -2.8095, -4.1164, -2.7355, -3.7108,\n",
      "         -4.7751, -2.8281, -0.6462, -1.4975, -1.8387, -3.2006, -2.1284, -0.9067,\n",
      "         -3.6230, -1.1764, -3.3877, -3.9362, -2.7777, -0.8586, -1.1464, -2.8388,\n",
      "         -3.0438, -4.1401, -3.3482, -3.3347, -0.4219, -1.2006, -3.6505, -2.2682,\n",
      "         -0.5554, -2.2504, -3.9892, -3.8654,  0.4034, -1.1586, -1.2697, -1.0180,\n",
      "         -4.8267, -4.6011,  0.0623, -1.2163, -2.1768, -1.5645, -0.1676, -1.2943,\n",
      "         -3.3373, -4.4559, -2.8870, -1.0441, -1.2708, -2.7985, -2.6599, -0.9660,\n",
      "         -0.8983, -4.3245,  0.2481,  0.6834, -3.5994, -4.0113, -1.4780, -2.0259,\n",
      "         -3.1584, -2.4080, -2.0434, -3.1430, -3.2733, -5.7025, -1.9629, -2.3148,\n",
      "         -1.1547, -1.0125, -0.6879, -4.4336, -3.8382, -0.1867, -2.1810, -0.2237,\n",
      "         -0.8980, -0.3776,  0.0564, -0.3043, -4.2273, -3.5889, -4.7791, -1.6189,\n",
      "          0.9652,  1.0114,  0.9575,  1.1205,  0.9266,  1.0791, -4.8294, -4.9553,\n",
      "         -5.2625, -5.0702, -5.2686, -5.2567, -5.4246, -5.4521, -5.3704, -5.3301,\n",
      "         -5.2787, -5.2025, -5.1143, -5.0482, -5.0317, -5.0551, -5.0864, -5.1090,\n",
      "         -5.1012, -5.0893, -5.0920, -5.1238, -5.1838, -5.2572, -5.3031, -5.3194,\n",
      "         -5.3122, -5.3231, -5.3787, -5.4611, -5.5056, -5.4973, -5.4572, -5.3968,\n",
      "         -5.3485, -5.3100, -5.2977, -5.3079, -5.3315, -5.3702, -5.4031, -5.4136,\n",
      "         -5.4043, -5.3736, -5.3654, -5.3620, -5.3703, -5.4055, -5.4223, -5.4251,\n",
      "         -5.4206, -5.4141, -5.4163, -5.4225, -5.4434, -5.4194],\n",
      "        [-3.1351, -3.9596, -1.0404, -2.3970, -0.1405, -2.4175, -4.5299, -0.8266,\n",
      "         -4.4326, -2.8436, -3.9885,  0.2351, -0.5148, -3.8453, -3.1465, -2.7790,\n",
      "         -3.4788, -1.9771, -2.1558, -1.1634, -1.4802, -1.6905, -1.0872,  0.2638,\n",
      "         -2.2167, -3.6527, -3.1804, -2.9333, -3.9831,  0.8548, -2.0202, -0.4242,\n",
      "         -0.8575, -3.6785, -1.8942, -1.8464, -0.9072, -3.4091, -1.9534, -0.3826,\n",
      "         -0.9525, -0.6086, -0.5921, -4.0099, -1.9013, -3.8361, -3.8152, -3.3964,\n",
      "          0.5975,  0.5726, -4.0164, -1.6707, -6.9400, -3.2217, -3.3112, -1.2464,\n",
      "         -2.5580, -3.1196, -2.6073, -1.9416, -3.8060, -1.4033, -5.8032, -2.7307,\n",
      "         -3.7585,  0.9528, -1.6708, -0.3367, -0.3453, -3.3276, -4.3052, -5.3904,\n",
      "         -3.0896, -0.2410, -3.9639, -4.0475, -2.8004, -4.1137, -2.7332, -3.7038,\n",
      "         -4.7695, -2.8189, -0.6419, -1.4920, -1.8351, -3.1955, -2.1256, -0.8988,\n",
      "         -3.6231, -1.1778, -3.3817, -3.9396, -2.7708, -0.8563, -1.1394, -2.8388,\n",
      "         -3.0387, -4.1303, -3.3490, -3.3347, -0.4166, -1.1939, -3.6553, -2.2676,\n",
      "         -0.5531, -2.2404, -3.9807, -3.8604,  0.4040, -1.0794, -1.3515, -0.8119,\n",
      "         -5.1226, -4.7154,  0.0256, -1.1430, -2.1560, -1.5177, -0.1057, -1.3452,\n",
      "         -3.3538, -4.4688, -2.8798, -1.0497, -1.2627, -2.8382, -2.6305, -0.9518,\n",
      "         -0.7810, -4.3115,  0.2589,  0.6883, -3.6016, -4.0097, -1.4599, -2.0255,\n",
      "         -3.1499, -2.4123, -2.0323, -3.1619, -3.2480, -5.6878, -1.9627, -2.2998,\n",
      "         -1.2060, -0.9517, -0.9603, -4.4913, -3.9554, -0.1896, -2.1460, -0.4342,\n",
      "         -1.0035, -0.3831, -0.2316, -1.1098, -4.4055, -3.5581, -4.8232, -1.5659,\n",
      "          1.0275,  1.0267,  0.9752,  0.9496,  0.2162,  1.0726, -4.8074, -4.9437,\n",
      "         -5.2632, -5.0453, -5.2484, -5.3241, -5.4385, -5.4279, -5.3529, -5.3417,\n",
      "         -5.3295, -5.2553, -5.1346, -5.0374, -4.9929, -4.9888, -5.0089, -5.0304,\n",
      "         -5.0220, -5.0028, -5.0043, -5.0359, -5.1037, -5.1804, -5.2362, -5.2536,\n",
      "         -5.2528, -5.2788, -5.3406, -5.4333, -5.4795, -5.4732, -5.4320, -5.3676,\n",
      "         -5.3261, -5.2967, -5.2900, -5.2985, -5.3187, -5.3604, -5.3940, -5.4035,\n",
      "         -5.3876, -5.3532, -5.3433, -5.3427, -5.3561, -5.3931, -5.4100, -5.4141,\n",
      "         -5.4040, -5.3909, -5.3854, -5.3872, -5.4051, -5.3832]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([1.0101, 0.8780], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.2264,  0.9900, -0.8546,  0.9900,  0.9900,  0.9900, -0.7332, -0.8167],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4639e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2009, -0.2029, -0.2050,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1729, -0.1746, -0.1764,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1263, -0.1276, -0.1289,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1652, -0.1669, -0.1686,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1726,  0.9656, -0.8335,  0.9656,  0.9656,  0.9656, -0.7151, -0.7965],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8672, -3.9513, -0.9888,  ..., -4.5777, -4.5800, -4.5861],\n",
      "        [-3.1466, -3.9597, -1.0310,  ..., -5.4222, -5.4426, -5.4181],\n",
      "        [-2.8745, -3.9767, -1.0025,  ..., -4.3608, -4.3406, -4.3315],\n",
      "        ...,\n",
      "        [-3.1417, -3.9626, -1.0297,  ..., -5.4181, -5.4429, -5.4270],\n",
      "        [-2.9220, -3.9634, -0.9862,  ..., -4.5891, -4.5770, -4.5543],\n",
      "        [-2.8816, -3.9811, -1.0060,  ..., -4.3700, -4.3509, -4.3448]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1494,  1.0090, -0.7972,  0.9257,  0.7268,  0.9916, -0.6496, -0.7571],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8095, -0.8685,  0.9900,  0.9900, -0.8151, -0.8531],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2566e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0591e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1638, -0.1654, -0.1671,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1649, -0.1666, -0.1682,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1726, -0.1743, -0.1761,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7896, -0.8471,  0.9656,  0.9656, -0.7950, -0.8321],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2224, -3.9236, -1.0369,  ..., -4.9260, -4.8853, -4.8638],\n",
      "        [-3.2227, -3.9227, -1.0374,  ..., -4.8982, -4.8591, -4.8428],\n",
      "        [-2.8852, -3.9813, -0.9989,  ..., -4.3682, -4.3515, -4.3443],\n",
      "        ...,\n",
      "        [-3.1221, -3.9578, -1.0262,  ..., -5.2262, -5.2362, -5.2266],\n",
      "        [-2.8860, -3.9811, -0.9995,  ..., -4.3728, -4.3548, -4.3457],\n",
      "        [-2.8872, -3.9804, -0.9975,  ..., -4.3115, -4.2946, -4.2891]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7326,  0.7387, -0.8133, -0.7881,  1.0088,  0.9265, -0.7514, -0.7653],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.6920,  0.9900,  0.9900, -0.8860, -0.8056,  0.9900,  0.9900, -0.8376],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6047e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.0803e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3457, -0.3492, -0.3528,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1695, -0.1712, -0.1729,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6585,  0.9656,  0.9656, -0.8642, -0.7858,  0.9656,  0.9608, -0.8170],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7941, -3.9926, -0.9649,  ..., -4.2814, -4.2821, -4.2799],\n",
      "        [-3.1567, -3.9700, -1.0312,  ..., -5.4731, -5.5028, -5.4827],\n",
      "        [-3.1442, -3.9603, -1.0273,  ..., -5.4035, -5.4332, -5.4101],\n",
      "        ...,\n",
      "        [-3.1466, -3.9640, -1.0265,  ..., -5.3888, -5.4033, -5.3829],\n",
      "        [-3.2126, -3.9298, -1.0391,  ..., -4.9757, -4.8700, -4.8234],\n",
      "        [-2.8842, -3.9857, -1.0032,  ..., -4.3521, -4.3353, -4.3246]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9793,  0.9244,  0.9143, -0.7813, -0.7548,  0.8783,  0.8685, -0.7518],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7207,  0.9900,  0.9900, -0.8349, -0.8371,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8278e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.3113e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.0091e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1241, -0.1254, -0.1267,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1689, -0.1706, -0.1723,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1693, -0.1711, -0.1728,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7029,  0.9656,  0.9656, -0.8143, -0.8165,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1395, -3.9720, -1.0358,  ..., -5.3891, -5.4048, -5.3865],\n",
      "        [-3.2122, -3.9389, -1.0507,  ..., -4.9432, -4.9044, -4.8874],\n",
      "        [-2.9226, -3.9763, -0.9879,  ..., -4.6028, -4.5919, -4.5672],\n",
      "        ...,\n",
      "        [-2.8800, -3.9882, -1.0073,  ..., -4.3612, -4.3390, -4.3314],\n",
      "        [-2.8822, -3.9910, -1.0036,  ..., -4.3467, -4.3266, -4.3195],\n",
      "        [-3.1419, -3.9754, -1.0329,  ..., -5.4205, -5.4439, -5.4275]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8543,  0.6426, -0.6583,  0.8932,  0.9013, -0.8128, -0.8230,  0.9533],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8431,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.8653e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.2854e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1706, -0.1723, -0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8223,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1009, -3.9750, -1.0287,  ..., -5.3264, -5.3206, -5.2964],\n",
      "        [-2.8797, -4.0052, -1.0213,  ..., -4.3030, -4.2861, -4.2773],\n",
      "        [-3.1422, -3.9863, -1.0543,  ..., -5.4721, -5.5027, -5.4836],\n",
      "        ...,\n",
      "        [-3.1123, -3.9886, -1.0431,  ..., -5.3510, -5.3534, -5.3443],\n",
      "        [-3.2095, -3.9444, -1.0589,  ..., -4.8979, -4.8622, -4.8488],\n",
      "        [-3.1009, -3.9750, -1.0287,  ..., -5.3264, -5.3206, -5.2964]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9048, -0.8325,  0.8764,  0.8960,  0.8628,  0.8628,  0.6902,  0.9048],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7269,  0.9900,  0.9900, -1.1519, -1.1000,  0.9900,  0.9900, -0.8062],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3784e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1252, -0.1265, -0.1277,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1631, -0.1647, -0.1664,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7090,  0.9656,  0.9656, -1.1014, -1.0518,  0.9656,  0.9656, -0.7863],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8942, -3.9863, -1.0151,  ..., -4.6752, -4.6618, -4.6389],\n",
      "        [-3.1195, -3.9849, -1.0564,  ..., -5.4400, -5.4650, -5.4372],\n",
      "        [-3.0859, -3.9800, -1.0387,  ..., -5.3204, -5.3149, -5.2926],\n",
      "        ...,\n",
      "        [-3.1145, -3.9884, -1.0549,  ..., -5.4167, -5.4411, -5.4204],\n",
      "        [-3.1195, -3.9849, -1.0564,  ..., -5.4400, -5.4650, -5.4372],\n",
      "        [-2.8684, -4.0134, -1.0279,  ..., -4.3150, -4.2993, -4.2950]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6697,  0.8997,  0.8896, -1.1561, -1.0878,  0.9074,  0.8997, -0.8446],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8335, -0.8586], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(1.9047e-09, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1686, -0.1703, -0.1720, -0.1738, -0.1755, -0.1773, -0.1791, -0.1809,\n",
      "         -0.1827, -0.1846, -0.1864, -0.1883, -0.1902, -0.1921, -0.1941, -0.1960,\n",
      "         -0.1980, -0.2000, -0.2020, -0.2041, -0.2062, -0.2082, -0.2103, -0.2125,\n",
      "         -0.2146, -0.2168, -0.2190, -0.2212, -0.2234, -0.2257, -0.2279, -0.2303,\n",
      "         -0.2326, -0.2349, -0.2373, -0.2397, -0.2421, -0.2446, -0.2470, -0.2495,\n",
      "         -0.2520, -0.2546, -0.2572, -0.2598, -0.2624, -0.2650, -0.2677, -0.2704,\n",
      "         -0.2732, -0.2759, -0.2787, -0.2815, -0.2844, -0.2872, -0.2901, -0.2931,\n",
      "         -0.2960, -0.2990, -0.3020, -0.3051, -0.3082, -0.3113, -0.3144, -0.3176,\n",
      "         -0.3208, -0.3240, -0.3273, -0.3306, -0.3340, -0.3373, -0.3407, -0.3442,\n",
      "         -0.3477, -0.3512, -0.3547, -0.3583, -0.3619, -0.3656, -0.3693, -0.3730,\n",
      "         -0.3768, -0.3806, -0.3844, -0.3883, -0.3922, -0.3962, -0.4002, -0.4042,\n",
      "         -0.4083, -0.4124, -0.4166, -0.4208, -0.4251, -0.4294, -0.4337, -0.4381,\n",
      "         -0.4425, -0.4470, -0.4515, -0.4560, -0.4607, -0.4653, -0.4700, -0.4748,\n",
      "         -0.4795, -0.4844, -0.4893, -0.4942, -0.4992, -0.5043, -0.5094, -0.5145,\n",
      "         -0.5197, -0.5249, -0.5302, -0.5356, -0.5410, -0.5465, -0.5520, -0.5576,\n",
      "         -0.5632, -0.5689, -0.5746, -0.5804, -0.5863, -0.5922, -0.5982, -0.6043,\n",
      "         -0.6104, -0.6165, -0.6228, -0.6290, -0.6354, -0.6418, -0.6483, -0.6548,\n",
      "         -0.6615, -0.6681, -0.6749, -0.6817, -0.6886, -0.6956, -0.7026, -0.7097,\n",
      "         -0.7168, -0.7241, -0.7314, -0.7388, -0.7462, -0.7538, -0.7614, -0.7691,\n",
      "         -0.7769, -0.7847, -0.7926, -0.8006, -0.8087, -0.8169, -0.8251, -0.8335,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1737, -0.1754, -0.1772, -0.1790, -0.1808, -0.1826, -0.1845, -0.1863,\n",
      "         -0.1882, -0.1901, -0.1920, -0.1940, -0.1959, -0.1979, -0.1999, -0.2019,\n",
      "         -0.2040, -0.2060, -0.2081, -0.2102, -0.2124, -0.2145, -0.2167, -0.2189,\n",
      "         -0.2211, -0.2233, -0.2256, -0.2278, -0.2301, -0.2325, -0.2348, -0.2372,\n",
      "         -0.2396, -0.2420, -0.2444, -0.2469, -0.2494, -0.2519, -0.2545, -0.2570,\n",
      "         -0.2596, -0.2623, -0.2649, -0.2676, -0.2703, -0.2730, -0.2758, -0.2786,\n",
      "         -0.2814, -0.2842, -0.2871, -0.2900, -0.2929, -0.2959, -0.2989, -0.3019,\n",
      "         -0.3049, -0.3080, -0.3111, -0.3143, -0.3174, -0.3206, -0.3239, -0.3271,\n",
      "         -0.3305, -0.3338, -0.3372, -0.3406, -0.3440, -0.3475, -0.3510, -0.3545,\n",
      "         -0.3581, -0.3617, -0.3654, -0.3691, -0.3728, -0.3766, -0.3804, -0.3842,\n",
      "         -0.3881, -0.3920, -0.3960, -0.4000, -0.4040, -0.4081, -0.4122, -0.4164,\n",
      "         -0.4206, -0.4248, -0.4291, -0.4335, -0.4379, -0.4423, -0.4467, -0.4513,\n",
      "         -0.4558, -0.4604, -0.4651, -0.4698, -0.4745, -0.4793, -0.4841, -0.4890,\n",
      "         -0.4940, -0.4990, -0.5040, -0.5091, -0.5142, -0.5194, -0.5247, -0.5300,\n",
      "         -0.5353, -0.5407, -0.5462, -0.5517, -0.5573, -0.5629, -0.5686, -0.5743,\n",
      "         -0.5802, -0.5860, -0.5919, -0.5979, -0.6039, -0.6100, -0.6162, -0.6224,\n",
      "         -0.6287, -0.6351, -0.6415, -0.6480, -0.6545, -0.6611, -0.6678, -0.6745,\n",
      "         -0.6814, -0.6882, -0.6952, -0.7022, -0.7093, -0.7165, -0.7237, -0.7310,\n",
      "         -0.7384, -0.7459, -0.7534, -0.7610, -0.7687, -0.7765, -0.7843, -0.7922,\n",
      "         -0.8002, -0.8083, -0.8165, -0.8247, -0.8331, -0.8415, -0.8500, -0.8586,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8129, -0.8374], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8365e+00, -4.0090e+00, -1.0379e+00, -2.2987e+00, -7.3383e-02,\n",
      "         -2.3784e+00, -4.4900e+00, -6.3042e-01, -4.1348e+00, -2.6518e+00,\n",
      "         -3.9503e+00,  1.9514e-01, -4.2500e-01, -3.7774e+00, -3.2386e+00,\n",
      "         -2.6528e+00, -3.4138e+00, -1.8888e+00, -2.2843e+00, -1.0210e+00,\n",
      "         -1.4210e+00, -1.5243e+00, -9.6454e-01,  2.0436e-01, -2.0388e+00,\n",
      "         -3.5919e+00, -3.2545e+00, -2.8409e+00, -3.9812e+00,  8.0738e-01,\n",
      "         -1.8941e+00, -2.6826e-01, -9.0320e-01, -3.6467e+00, -1.8628e+00,\n",
      "         -1.9740e+00, -9.4685e-01, -3.3758e+00, -1.8631e+00, -2.1707e-01,\n",
      "         -8.7673e-01, -6.4207e-01, -5.8100e-01, -4.1064e+00, -1.7536e+00,\n",
      "         -3.5629e+00, -3.5671e+00, -3.3554e+00,  2.1437e-01,  3.0369e-01,\n",
      "         -3.9854e+00, -1.6051e+00, -6.9517e+00, -3.1603e+00, -3.2840e+00,\n",
      "         -1.1268e+00, -2.6408e+00, -3.1248e+00, -2.5844e+00, -1.8047e+00,\n",
      "         -3.7564e+00, -1.3440e+00, -5.7975e+00, -2.7080e+00, -3.7508e+00,\n",
      "          8.9167e-01, -1.5550e+00, -1.7694e-01, -4.0989e-01, -3.2932e+00,\n",
      "         -4.2134e+00, -5.3606e+00, -3.0402e+00, -2.9494e-01, -3.9290e+00,\n",
      "         -4.1039e+00, -2.7521e+00, -4.0919e+00, -2.6514e+00, -3.6767e+00,\n",
      "         -4.7454e+00, -2.7823e+00, -5.8237e-01, -1.3438e+00, -1.7828e+00,\n",
      "         -3.0930e+00, -1.9984e+00, -7.5167e-01, -3.5662e+00, -1.0824e+00,\n",
      "         -3.3540e+00, -3.9958e+00, -2.7385e+00, -8.1137e-01, -9.8520e-01,\n",
      "         -2.9121e+00, -2.8905e+00, -3.8856e+00, -3.3033e+00, -3.3079e+00,\n",
      "         -3.4646e-01, -1.0486e+00, -3.7106e+00, -2.2444e+00, -6.0654e-01,\n",
      "         -2.0724e+00, -3.7513e+00, -3.7614e+00,  3.4555e-01, -1.2649e+00,\n",
      "         -1.4093e+00, -1.1827e+00, -4.5643e+00, -4.6058e+00,  1.1554e-01,\n",
      "         -1.0717e+00, -2.1318e+00, -1.4253e+00, -9.3437e-02, -1.2323e+00,\n",
      "         -3.1102e+00, -4.4790e+00, -2.8852e+00, -1.0807e+00, -1.1626e+00,\n",
      "         -2.7521e+00, -2.4899e+00, -8.9870e-01, -7.3583e-01, -4.2874e+00,\n",
      "         -1.6040e-01,  3.8282e-01, -3.5767e+00, -3.9794e+00, -1.3778e+00,\n",
      "         -2.1084e+00, -3.1821e+00, -2.3854e+00, -1.9319e+00, -3.1230e+00,\n",
      "         -2.8520e+00, -3.5404e+00, -5.6371e+00, -1.5940e+00, -8.1489e-01,\n",
      "         -7.9406e-01, -8.8464e-01, -7.8777e-01, -9.3702e-01, -8.2718e-01,\n",
      "         -4.9587e+00, -4.9826e+00, -5.3139e+00, -5.0012e+00, -4.9480e+00,\n",
      "         -4.6764e+00, -4.5822e+00, -4.6754e+00, -4.7272e+00, -4.7184e+00,\n",
      "         -4.7746e+00, -4.7882e+00, -4.7963e+00, -4.7830e+00, -4.7264e+00,\n",
      "         -4.6536e+00, -4.6172e+00, -4.5986e+00, -4.5868e+00, -4.5736e+00,\n",
      "         -4.5668e+00, -4.5809e+00, -4.5919e+00, -4.5998e+00, -4.6284e+00,\n",
      "         -4.6611e+00, -4.6702e+00, -4.6711e+00, -4.6850e+00, -4.6793e+00,\n",
      "         -4.6629e+00, -4.6699e+00, -4.6687e+00, -4.6636e+00, -4.6601e+00,\n",
      "         -4.6616e+00, -4.6830e+00, -4.7002e+00, -4.7131e+00, -4.7145e+00,\n",
      "         -4.7153e+00, -4.7303e+00, -4.7520e+00, -4.7733e+00, -4.7851e+00,\n",
      "         -4.7923e+00, -4.8018e+00, -4.8070e+00, -4.7960e+00, -4.7865e+00,\n",
      "         -4.7783e+00, -4.7836e+00, -4.7803e+00, -4.7616e+00, -4.7422e+00,\n",
      "         -4.7360e+00, -4.7542e+00, -4.7820e+00, -4.7902e+00, -4.7600e+00,\n",
      "         -4.7628e+00, -4.7227e+00, -4.6766e+00, -4.5827e+00, -4.5282e+00,\n",
      "         -4.4306e+00, -4.3505e+00, -4.3412e+00, -4.3135e+00, -4.2767e+00,\n",
      "         -4.2574e+00, -4.2475e+00],\n",
      "        [-2.8435e+00, -4.0105e+00, -1.0408e+00, -2.2957e+00, -7.2895e-02,\n",
      "         -2.3773e+00, -4.4906e+00, -6.2721e-01, -4.1377e+00, -2.6469e+00,\n",
      "         -3.9448e+00,  1.9426e-01, -4.2049e-01, -3.7755e+00, -3.2385e+00,\n",
      "         -2.6545e+00, -3.4077e+00, -1.8868e+00, -2.2873e+00, -1.0154e+00,\n",
      "         -1.4186e+00, -1.5176e+00, -9.6135e-01,  2.0737e-01, -2.0285e+00,\n",
      "         -3.5899e+00, -3.2513e+00, -2.8438e+00, -3.9777e+00,  8.0746e-01,\n",
      "         -1.8906e+00, -2.6215e-01, -8.9945e-01, -3.6422e+00, -1.8586e+00,\n",
      "         -1.9754e+00, -9.4667e-01, -3.3685e+00, -1.8604e+00, -2.1583e-01,\n",
      "         -8.7355e-01, -6.4192e-01, -5.7900e-01, -4.1020e+00, -1.7445e+00,\n",
      "         -3.5656e+00, -3.5589e+00, -3.3492e+00,  2.1177e-01,  3.0998e-01,\n",
      "         -3.9843e+00, -1.5992e+00, -6.9500e+00, -3.1617e+00, -3.2787e+00,\n",
      "         -1.1259e+00, -2.6420e+00, -3.1247e+00, -2.5832e+00, -1.7948e+00,\n",
      "         -3.7518e+00, -1.3381e+00, -5.7951e+00, -2.7137e+00, -3.7470e+00,\n",
      "          8.9123e-01, -1.5521e+00, -1.7244e-01, -4.0661e-01, -3.2882e+00,\n",
      "         -4.2146e+00, -5.3567e+00, -3.0418e+00, -2.9381e-01, -3.9252e+00,\n",
      "         -4.1034e+00, -2.7551e+00, -4.0854e+00, -2.6496e+00, -3.6748e+00,\n",
      "         -4.7400e+00, -2.7861e+00, -5.7519e-01, -1.3382e+00, -1.7785e+00,\n",
      "         -3.0884e+00, -1.9947e+00, -7.4480e-01, -3.5657e+00, -1.0792e+00,\n",
      "         -3.3510e+00, -3.9967e+00, -2.7405e+00, -8.0641e-01, -9.7952e-01,\n",
      "         -2.9123e+00, -2.8826e+00, -3.8861e+00, -3.3021e+00, -3.3039e+00,\n",
      "         -3.4015e-01, -1.0435e+00, -3.7130e+00, -2.2376e+00, -6.0161e-01,\n",
      "         -2.0704e+00, -3.7546e+00, -3.7608e+00,  3.4835e-01, -1.5620e+00,\n",
      "         -1.6213e+00, -1.0074e+00, -4.5823e+00, -4.6790e+00,  1.4671e-01,\n",
      "         -1.0131e+00, -2.1067e+00, -1.3586e+00, -4.9072e-03, -1.2239e+00,\n",
      "         -3.0453e+00, -4.5341e+00, -2.8699e+00, -1.0637e+00, -1.0791e+00,\n",
      "         -2.7241e+00, -2.4846e+00, -9.0189e-01, -6.2955e-01, -4.2623e+00,\n",
      "         -1.8098e-01,  3.7231e-01, -3.5446e+00, -3.9972e+00, -1.3810e+00,\n",
      "         -2.1148e+00, -3.1966e+00, -2.3880e+00, -1.9347e+00, -3.1556e+00,\n",
      "         -2.8657e+00, -3.5572e+00, -5.6447e+00, -1.6263e+00, -8.2283e-01,\n",
      "         -7.8459e-01, -9.1108e-01, -8.5761e-01, -8.9243e-01, -8.8398e-01,\n",
      "         -5.0033e+00, -4.9914e+00, -5.3158e+00, -5.0171e+00, -4.9468e+00,\n",
      "         -4.6427e+00, -4.5720e+00, -4.6485e+00, -4.7341e+00, -4.7149e+00,\n",
      "         -4.7389e+00, -4.7735e+00, -4.8109e+00, -4.8131e+00, -4.7652e+00,\n",
      "         -4.6905e+00, -4.6572e+00, -4.6527e+00, -4.6508e+00, -4.6448e+00,\n",
      "         -4.6393e+00, -4.6500e+00, -4.6617e+00, -4.6711e+00, -4.6858e+00,\n",
      "         -4.7102e+00, -4.7061e+00, -4.7023e+00, -4.7082e+00, -4.7050e+00,\n",
      "         -4.6951e+00, -4.7019e+00, -4.6984e+00, -4.6859e+00, -4.6788e+00,\n",
      "         -4.6770e+00, -4.6969e+00, -4.7182e+00, -4.7371e+00, -4.7481e+00,\n",
      "         -4.7578e+00, -4.7818e+00, -4.8023e+00, -4.8194e+00, -4.8224e+00,\n",
      "         -4.8179e+00, -4.8187e+00, -4.8137e+00, -4.8110e+00, -4.8039e+00,\n",
      "         -4.7959e+00, -4.7890e+00, -4.7839e+00, -4.7738e+00, -4.7705e+00,\n",
      "         -4.7773e+00, -4.7786e+00, -4.7874e+00, -4.7782e+00, -4.7427e+00,\n",
      "         -4.7478e+00, -4.7030e+00, -4.6744e+00, -4.6041e+00, -4.5393e+00,\n",
      "         -4.4401e+00, -4.3611e+00, -4.3616e+00, -4.3350e+00, -4.2986e+00,\n",
      "         -4.2805e+00, -4.2729e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8409, -0.8588], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7345, -0.8328,  0.9900, -1.1000, -0.8211, -0.8468,  0.9900, -0.7916],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.0485e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1265, -0.1278, -0.1291,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1685, -0.1702, -0.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1713, -0.1730, -0.1748,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1601, -0.1618, -0.1634,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7164, -0.8122,  0.9656, -1.0518, -0.8009, -0.8259,  0.9656, -0.7721],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8629, -3.9939, -1.0291,  ..., -4.6545, -4.6408, -4.6186],\n",
      "        [-2.8262, -4.0129, -1.0396,  ..., -4.3200, -4.3034, -4.2960],\n",
      "        [-3.0499, -3.9886, -1.0536,  ..., -5.3108, -5.3072, -5.2825],\n",
      "        ...,\n",
      "        [-2.8167, -4.0127, -1.0430,  ..., -4.2709, -4.2534, -4.2421],\n",
      "        [-3.1586, -3.9577, -1.0886,  ..., -4.8770, -4.8450, -4.8296],\n",
      "        [-2.8256, -4.0116, -1.0465,  ..., -4.3302, -4.3127, -4.3040]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6801, -0.8902,  0.8715, -1.0827, -0.8482, -0.8539,  0.6551, -0.8472],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7979,  0.9900,  0.9900,  0.9900,  0.9900, -0.8814, -0.8385],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6982e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1614, -0.1630, -0.1647,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1783, -0.1801, -0.1819,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1696, -0.1713, -0.1731,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7782,  0.9656,  0.9656,  0.9656,  0.9656, -0.8597, -0.8178],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0660, -3.9977, -1.0763,  ..., -5.3884, -5.4116, -5.3935],\n",
      "        [-2.8114, -4.0166, -1.0490,  ..., -4.3344, -4.3171, -4.3130],\n",
      "        [-3.0464, -4.0024, -1.0715,  ..., -5.3280, -5.3310, -5.3244],\n",
      "        ...,\n",
      "        [-3.0733, -3.9974, -1.0792,  ..., -5.3428, -5.3596, -5.3342],\n",
      "        [-2.8067, -4.0151, -1.0473,  ..., -4.2871, -4.2730, -4.2651],\n",
      "        [-2.8074, -4.0124, -1.0440,  ..., -4.3285, -4.3104, -4.3002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8719, -0.9169,  0.8207,  0.8341,  0.8778,  0.7727, -0.8755, -0.8942],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7310, -1.2229,  0.9900,  0.9900,  0.9900, -0.8091],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4374e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1259, -0.1272, -0.1285,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1637, -0.1653, -0.1670,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7129, -1.1693,  0.9656,  0.9656,  0.9656, -0.7891],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0612, -3.9964, -1.0811,  ..., -5.3312, -5.3479, -5.3224],\n",
      "        [-3.1306, -3.9595, -1.0956,  ..., -4.8867, -4.8486, -4.8299],\n",
      "        [-2.8382, -3.9997, -1.0300,  ..., -4.5563, -4.5457, -4.5228],\n",
      "        ...,\n",
      "        [-3.0521, -3.9943, -1.0815,  ..., -5.3448, -5.3602, -5.3390],\n",
      "        [-3.1241, -3.9629, -1.0982,  ..., -4.8963, -4.8580, -4.8429],\n",
      "        [-2.7969, -4.0163, -1.0503,  ..., -4.3313, -4.3154, -4.3088]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7683,  0.5848, -0.6836, -1.1258,  0.8368,  0.7856,  0.5738, -0.8465],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.6968, -0.8473,  0.9900,  0.9900,  0.9900, -0.8640,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9580e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3467, -0.3502, -0.3538,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1714, -0.1731, -0.1749,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1748, -0.1765, -0.1783,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.6632, -0.8264,  0.9656,  0.9656,  0.9608, -0.8427,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0549, -3.9919, -1.0726,  ..., -5.3678, -5.3963, -5.3821],\n",
      "        [-2.6996, -4.0220, -1.0145,  ..., -4.2430, -4.2442, -4.2428],\n",
      "        [-2.7859, -4.0093, -1.0458,  ..., -4.3177, -4.2984, -4.2892],\n",
      "        ...,\n",
      "        [-3.1162, -3.9596, -1.0995,  ..., -4.9194, -4.8197, -4.7751],\n",
      "        [-2.7923, -4.0152, -1.0476,  ..., -4.2682, -4.2493, -4.2417],\n",
      "        [-3.0494, -3.9957, -1.0776,  ..., -5.3732, -5.3971, -5.3786]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8719, -1.0311, -0.8907,  0.8361,  0.8504,  0.7872, -0.8788,  0.8837],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.8885, -0.8642],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.2781e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1798, -0.1816, -0.1834,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1748, -0.1766, -0.1784,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.8666, -0.8428],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1179, -3.9547, -1.0925,  ..., -4.8423, -4.8055, -4.7894],\n",
      "        [-3.0473, -3.9899, -1.0757,  ..., -5.3616, -5.3802, -5.3560],\n",
      "        [-3.0102, -3.9857, -1.0564,  ..., -5.2994, -5.2962, -5.2700],\n",
      "        ...,\n",
      "        [-3.0414, -3.9933, -1.0733,  ..., -5.3706, -5.3942, -5.3767],\n",
      "        [-2.7759, -4.0105, -1.0443,  ..., -4.2776, -4.2622, -4.2557],\n",
      "        [-2.7838, -4.0132, -1.0446,  ..., -4.2651, -4.2464, -4.2405]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6830,  0.8992,  0.8948,  0.8078,  0.9038,  0.8895, -0.8662, -0.8727],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.1623,  0.9900,  0.9900, -0.8089,  0.9900, -0.8469],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7265e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1605, -0.1621, -0.1637,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1637, -0.1653, -0.1670,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1713, -0.1731, -0.1748,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.1113,  0.9656,  0.9656, -0.7890,  0.9656, -0.8260],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0453, -3.9868, -1.0700,  ..., -5.3601, -5.3802, -5.3538],\n",
      "        [-3.0085, -3.9823, -1.0524,  ..., -5.2969, -5.2941, -5.2681],\n",
      "        [-2.8215, -3.9647, -1.0268,  ..., -4.2523, -4.2680, -4.2853],\n",
      "        ...,\n",
      "        [-2.7807, -4.0082, -1.0402,  ..., -4.3231, -4.3053, -4.2977],\n",
      "        [-3.0143, -3.9870, -1.0695,  ..., -5.1986, -5.2082, -5.1951],\n",
      "        [-2.7824, -4.0077, -1.0372,  ..., -4.2614, -4.2462, -4.2421]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9088,  0.9080, -1.1319,  0.8696,  0.6031, -0.8287,  0.8696, -0.8454],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8233, -0.7241], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.5950e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1665, -0.1682, -0.1699, -0.1716, -0.1734, -0.1751, -0.1769, -0.1787,\n",
      "         -0.1805, -0.1823, -0.1842, -0.1860, -0.1879, -0.1898, -0.1917, -0.1936,\n",
      "         -0.1956, -0.1976, -0.1996, -0.2016, -0.2036, -0.2057, -0.2078, -0.2099,\n",
      "         -0.2120, -0.2141, -0.2163, -0.2185, -0.2207, -0.2229, -0.2252, -0.2274,\n",
      "         -0.2297, -0.2320, -0.2344, -0.2368, -0.2392, -0.2416, -0.2440, -0.2465,\n",
      "         -0.2490, -0.2515, -0.2540, -0.2566, -0.2592, -0.2618, -0.2644, -0.2671,\n",
      "         -0.2698, -0.2725, -0.2753, -0.2781, -0.2809, -0.2837, -0.2866, -0.2895,\n",
      "         -0.2924, -0.2953, -0.2983, -0.3013, -0.3044, -0.3075, -0.3106, -0.3137,\n",
      "         -0.3169, -0.3201, -0.3233, -0.3266, -0.3299, -0.3332, -0.3366, -0.3400,\n",
      "         -0.3434, -0.3469, -0.3504, -0.3539, -0.3575, -0.3611, -0.3648, -0.3684,\n",
      "         -0.3722, -0.3759, -0.3797, -0.3835, -0.3874, -0.3913, -0.3953, -0.3993,\n",
      "         -0.4033, -0.4074, -0.4115, -0.4157, -0.4199, -0.4241, -0.4284, -0.4327,\n",
      "         -0.4371, -0.4415, -0.4460, -0.4505, -0.4550, -0.4596, -0.4643, -0.4689,\n",
      "         -0.4737, -0.4785, -0.4833, -0.4882, -0.4931, -0.4981, -0.5031, -0.5082,\n",
      "         -0.5133, -0.5185, -0.5238, -0.5290, -0.5344, -0.5398, -0.5452, -0.5507,\n",
      "         -0.5563, -0.5619, -0.5676, -0.5733, -0.5791, -0.5850, -0.5909, -0.5969,\n",
      "         -0.6029, -0.6090, -0.6151, -0.6213, -0.6276, -0.6340, -0.6404, -0.6468,\n",
      "         -0.6534, -0.6600, -0.6666, -0.6734, -0.6802, -0.6870, -0.6940, -0.7010,\n",
      "         -0.7081, -0.7152, -0.7224, -0.7297, -0.7371, -0.7446, -0.7521, -0.7597,\n",
      "         -0.7673, -0.7751, -0.7829, -0.7908, -0.7988, -0.8069, -0.8150, -0.8233,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1247, -0.1260, -0.1273, -0.1285, -0.1298, -0.1312, -0.1325, -0.1338,\n",
      "         -0.1352, -0.1365, -0.1379, -0.1393, -0.1407, -0.1421, -0.1436, -0.1450,\n",
      "         -0.1465, -0.1480, -0.1495, -0.1510, -0.1525, -0.1540, -0.1556, -0.1572,\n",
      "         -0.1587, -0.1604, -0.1620, -0.1636, -0.1653, -0.1669, -0.1686, -0.1703,\n",
      "         -0.1720, -0.1738, -0.1755, -0.1773, -0.1791, -0.1809, -0.1827, -0.1846,\n",
      "         -0.1864, -0.1883, -0.1902, -0.1922, -0.1941, -0.1961, -0.1980, -0.2000,\n",
      "         -0.2021, -0.2041, -0.2062, -0.2082, -0.2103, -0.2125, -0.2146, -0.2168,\n",
      "         -0.2190, -0.2212, -0.2234, -0.2257, -0.2280, -0.2303, -0.2326, -0.2349,\n",
      "         -0.2373, -0.2397, -0.2421, -0.2446, -0.2470, -0.2495, -0.2521, -0.2546,\n",
      "         -0.2572, -0.2598, -0.2624, -0.2650, -0.2677, -0.2704, -0.2732, -0.2759,\n",
      "         -0.2787, -0.2815, -0.2844, -0.2872, -0.2901, -0.2931, -0.2960, -0.2990,\n",
      "         -0.3020, -0.3051, -0.3082, -0.3113, -0.3144, -0.3176, -0.3208, -0.3240,\n",
      "         -0.3273, -0.3306, -0.3340, -0.3373, -0.3407, -0.3442, -0.3477, -0.3512,\n",
      "         -0.3547, -0.3583, -0.3619, -0.3656, -0.3693, -0.3730, -0.3768, -0.3806,\n",
      "         -0.3844, -0.3883, -0.3922, -0.3962, -0.4002, -0.4042, -0.4083, -0.4124,\n",
      "         -0.4166, -0.4208, -0.4251, -0.4294, -0.4337, -0.4381, -0.4425, -0.4470,\n",
      "         -0.4515, -0.4561, -0.4607, -0.4653, -0.4700, -0.4748, -0.4796, -0.4844,\n",
      "         -0.4893, -0.4942, -0.4992, -0.5043, -0.5094, -0.5145, -0.5197, -0.5250,\n",
      "         -0.5303, -0.5356, -0.5410, -0.5465, -0.5520, -0.5576, -0.5632, -0.5689,\n",
      "         -0.5747, -0.5805, -0.5863, -0.5922, -0.5982, -0.6043, -0.6104, -0.6165,\n",
      "         -0.6228, -0.6291, -0.6354, -0.6418, -0.6483, -0.6549, -0.6615, -0.6682,\n",
      "         -0.6749, -0.6817, -0.6886, -0.6956, -0.7026, -0.7097, -0.7169, -0.7241,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8030, -0.7062], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7884, -4.0115, -1.0295, -2.3258, -0.0565, -2.3718, -4.5158, -0.6223,\n",
      "         -4.0791, -2.6232, -3.9411,  0.1868, -0.4392, -3.7425, -3.2424, -2.6299,\n",
      "         -3.4018, -1.8616, -2.2961, -1.0039, -1.3992, -1.4996, -0.9509,  0.2147,\n",
      "         -2.0121, -3.5647, -3.2538, -2.8231, -3.9590,  0.8136, -1.8779, -0.2543,\n",
      "         -0.8970, -3.6405, -1.8580, -1.9489, -0.9432, -3.3588, -1.8641, -0.1792,\n",
      "         -0.8559, -0.6360, -0.5776, -4.1320, -1.7295, -3.4952, -3.5294, -3.3440,\n",
      "          0.2310,  0.3076, -3.9537, -1.5992, -6.9528, -3.1394, -3.2681, -1.1007,\n",
      "         -2.6771, -3.1309, -2.5959, -1.7805, -3.7270, -1.3362, -5.7973, -2.6899,\n",
      "         -3.7278,  0.8969, -1.5399, -0.1649, -0.4069, -3.2904, -4.1902, -5.3365,\n",
      "         -3.0152, -0.2319, -3.8955, -4.1271, -2.7300, -4.0738, -2.6518, -3.6473,\n",
      "         -4.7181, -2.7602, -0.5443, -1.3285, -1.7759, -3.0836, -1.9768, -0.7365,\n",
      "         -3.5641, -1.0623, -3.3292, -4.0237, -2.7181, -0.7800, -0.9683, -2.9064,\n",
      "         -2.8635, -3.8156, -3.2931, -3.3271, -0.3120, -1.0310, -3.7605, -2.2402,\n",
      "         -0.6010, -2.0503, -3.6863, -3.7298,  0.4091, -1.5375, -1.5188, -0.8387,\n",
      "         -4.6285, -4.7165,  0.1645, -0.9515, -2.1059, -1.3453,  0.0404, -1.2221,\n",
      "         -2.8775, -4.6539, -2.8116, -1.0582, -1.0496, -2.7402, -2.4471, -0.8836,\n",
      "         -0.6305, -4.2691, -0.0652,  0.4078, -3.5658, -4.0088, -1.3232, -2.1444,\n",
      "         -3.1927, -2.4052, -1.9016, -3.0380, -2.7784, -3.4770, -5.6789, -1.6002,\n",
      "         -0.7993, -0.8043, -1.0552, -0.7696, -0.8669, -0.7802, -4.9663, -4.9557,\n",
      "         -5.3211, -4.9637, -4.9018, -4.6251, -4.5941, -4.6904, -4.7670, -4.7249,\n",
      "         -4.7181, -4.7341, -4.7638, -4.7559, -4.6875, -4.5990, -4.5514, -4.5414,\n",
      "         -4.5385, -4.5430, -4.5514, -4.5748, -4.6046, -4.6231, -4.6599, -4.7007,\n",
      "         -4.6964, -4.6823, -4.6951, -4.7049, -4.6917, -4.6954, -4.6856, -4.6674,\n",
      "         -4.6472, -4.6462, -4.6672, -4.6843, -4.6973, -4.7030, -4.7076, -4.7249,\n",
      "         -4.7459, -4.7639, -4.7610, -4.7568, -4.7580, -4.7561, -4.7491, -4.7356,\n",
      "         -4.7203, -4.7171, -4.7056, -4.6957, -4.6863, -4.6875, -4.6876, -4.6899,\n",
      "         -4.6749, -4.6484, -4.6565, -4.6203, -4.5773, -4.5165, -4.4715, -4.3914,\n",
      "         -4.3253, -4.3273, -4.3074, -4.2783, -4.2629, -4.2563],\n",
      "        [-2.8255, -3.9912, -1.0174, -2.3659, -0.0191, -2.3634, -4.5244, -0.6366,\n",
      "         -4.1005, -2.6052, -3.9140,  0.1806, -0.4861, -3.7390, -3.2643, -2.6473,\n",
      "         -3.4129, -1.8220, -2.3085, -1.0045, -1.3979, -1.4962, -0.9591,  0.2058,\n",
      "         -2.0250, -3.5484, -3.2576, -2.8340, -3.9748,  0.8015, -1.8926, -0.2607,\n",
      "         -0.9100, -3.6264, -1.8793, -1.9334, -0.9954, -3.3699, -1.8599, -0.2065,\n",
      "         -0.8218, -0.6884, -0.5812, -4.1628, -1.7405, -3.5128, -3.5334, -3.3328,\n",
      "          0.2219,  0.3259, -3.9363, -1.5981, -6.9599, -3.1394, -3.2707, -1.0782,\n",
      "         -2.6874, -3.1555, -2.5905, -1.7884, -3.7071, -1.3254, -5.8010, -2.6803,\n",
      "         -3.7401,  0.8905, -1.5510, -0.1690, -0.4130, -3.2779, -4.1666, -5.3079,\n",
      "         -3.0079, -0.2134, -3.8732, -4.1493, -2.7191, -4.0737, -2.6415, -3.6190,\n",
      "         -4.6872, -2.7471, -0.5401, -1.3264, -1.7432, -3.0851, -1.9789, -0.7363,\n",
      "         -3.5285, -1.0362, -3.2956, -4.0417, -2.7031, -0.7711, -0.9614, -2.9101,\n",
      "         -2.8590, -3.8228, -3.2804, -3.3380, -0.3030, -1.0243, -3.7807, -2.2405,\n",
      "         -0.6038, -2.0460, -3.6910, -3.7212,  0.4325, -1.2944, -1.6273, -0.8478,\n",
      "         -4.6432, -4.5705,  0.1221, -1.0298, -2.1205, -1.3849,  0.0125, -1.2637,\n",
      "         -2.9860, -4.5053, -2.8705, -1.0498, -1.1675, -2.8060, -2.4474, -0.9021,\n",
      "         -0.6581, -4.2936, -0.1553,  0.4127, -3.5646, -3.9983, -1.3130, -2.1563,\n",
      "         -3.2116, -2.3999, -1.9018, -3.0492, -3.0040, -5.7616, -1.7410, -2.4718,\n",
      "         -1.2857, -1.2239, -0.7695, -4.1091, -4.0026, -0.2179, -2.0735, -0.4339,\n",
      "         -1.3911, -0.6862,  0.1061, -0.9697, -4.2400, -3.1579, -4.8529, -1.4490,\n",
      "         -0.7250, -0.7092, -0.6383, -0.6353, -0.5043, -0.6968, -4.9903, -4.9540,\n",
      "         -5.2341, -4.8935, -5.1819, -5.0932, -5.0505, -5.1145, -5.1052, -5.1195,\n",
      "         -5.1958, -5.2816, -5.1925, -4.9917, -4.8653, -4.8024, -4.6761, -4.5571,\n",
      "         -4.4385, -4.3590, -4.3310, -4.3493, -4.3856, -4.4194, -4.4261, -4.4197,\n",
      "         -4.4296, -4.4740, -4.5364, -4.5955, -4.6197, -4.6065, -4.5765, -4.5570,\n",
      "         -4.5757, -4.6134, -4.6421, -4.6589, -4.6658, -4.6546, -4.6393, -4.6071,\n",
      "         -4.5635, -4.5282, -4.5201, -4.5263, -4.5392, -4.5630, -4.5934, -4.6012,\n",
      "         -4.5956, -4.5922, -4.5728, -4.5556, -4.5451, -4.5195]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8459, -0.6515], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7820,  0.9900,  0.9900, -0.8195, -0.8372, -0.8235,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2142e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1582, -0.1598, -0.1614,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1666, -0.1683, -0.1700,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7627,  0.9656,  0.9656, -0.7993, -0.8165, -0.8032,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7940, -4.0033, -1.0338,  ..., -4.3185, -4.3025, -4.2949],\n",
      "        [-3.0422, -3.9831, -1.0621,  ..., -5.3481, -5.3622, -5.3395],\n",
      "        [-3.1207, -3.9470, -1.0771,  ..., -4.8575, -4.8214, -4.8051],\n",
      "        ...,\n",
      "        [-2.7889, -4.0025, -1.0284,  ..., -4.2969, -4.2791, -4.2725],\n",
      "        [-3.0423, -3.9789, -1.0649,  ..., -5.3679, -5.3958, -5.3667],\n",
      "        [-3.1125, -3.9508, -1.0799,  ..., -4.9045, -4.8042, -4.7624]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8101,  0.8524,  0.6235, -0.8548, -0.8431, -0.8553,  0.8972,  0.8180],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8497, -0.7916,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6080e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1719, -0.1736, -0.1754,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8287, -0.7721,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0545, -3.9807, -1.0536,  ..., -5.3878, -5.4085, -5.3888],\n",
      "        [-3.0545, -3.9807, -1.0536,  ..., -5.3878, -5.4085, -5.3888],\n",
      "        [-2.7982, -3.9999, -1.0294,  ..., -4.2653, -4.2506, -4.2449],\n",
      "        ...,\n",
      "        [-3.0545, -3.9807, -1.0536,  ..., -5.3878, -5.4085, -5.3888],\n",
      "        [-3.0639, -3.9830, -1.0608,  ..., -5.4381, -5.4627, -5.4350],\n",
      "        [-3.0581, -3.9780, -1.0489,  ..., -5.3818, -5.4062, -5.3921]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9355,  0.9355, -0.8245, -0.8216,  0.9149,  0.9355,  0.9149,  0.9339],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.6615],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(4.0866e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(6.2218e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3395, -0.3429, -0.3464,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -1.6286],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0698, -3.9768, -1.0527,  ..., -5.3461, -5.3605, -5.3345],\n",
      "        [-3.0640, -3.9750, -1.0494,  ..., -5.4137, -5.4371, -5.4046],\n",
      "        [-3.0292, -3.9693, -1.0297,  ..., -5.2775, -5.2733, -5.2519],\n",
      "        ...,\n",
      "        [-3.1323, -3.9433, -1.0675,  ..., -4.8833, -4.8485, -4.8385],\n",
      "        [-3.0696, -3.9743, -1.0501,  ..., -5.3887, -5.4087, -5.3793],\n",
      "        [-2.7096, -4.0081, -0.9857,  ..., -4.2195, -4.2200, -4.2208]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8330,  0.9737,  0.9658,  0.7583,  0.9130,  0.6669,  0.9574, -0.9700],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7169, -0.6999, -0.7574,  0.9900,  0.9900, -0.8435, -0.7981],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8355e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.1735e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1235, -0.1247, -0.1260,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1206, -0.1218, -0.1230,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1706, -0.1724, -0.1741,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1615, -0.1631, -0.1647,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6992, -0.6826, -0.7388,  0.9656,  0.9656, -0.8227, -0.7784],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1456, -3.9368, -1.0539,  ..., -4.8908, -4.8564, -4.8444],\n",
      "        [-2.8476, -3.9742, -0.9998,  ..., -4.6840, -4.6711, -4.6458],\n",
      "        [-2.8525, -3.9806, -0.9927,  ..., -4.6191, -4.6128, -4.5861],\n",
      "        ...,\n",
      "        [-3.0402, -3.9718, -1.0418,  ..., -5.1954, -5.2082, -5.1986],\n",
      "        [-2.8192, -3.9939, -1.0164,  ..., -4.2691, -4.2548, -4.2488],\n",
      "        [-2.8274, -3.9943, -1.0144,  ..., -4.2652, -4.2530, -4.2493]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6586, -0.6279, -0.6294, -0.7788,  0.9732,  0.9324, -0.7993, -0.7862],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8244, -0.8284,  0.9900, -0.7661,  0.9900, -0.7601,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5238e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1668, -0.1685, -0.1702,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1676, -0.1693, -0.1710,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1538, -0.1553, -0.1569,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8041, -0.8079,  0.9656, -0.7472,  0.9656, -0.7413,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8508, -3.9959, -1.0119,  ..., -4.2641, -4.2467, -4.2411],\n",
      "        [-2.8364, -3.9915, -1.0071,  ..., -4.2490, -4.2289, -4.2236],\n",
      "        [-3.1002, -3.9670, -1.0339,  ..., -5.4089, -5.4304, -5.4045],\n",
      "        ...,\n",
      "        [-2.8449, -3.9913, -1.0134,  ..., -4.3198, -4.3080, -4.3032],\n",
      "        [-3.0964, -3.9675, -1.0327,  ..., -5.4347, -5.4581, -5.4272],\n",
      "        [-3.0607, -3.9627, -1.0158,  ..., -5.2846, -5.2820, -5.2606]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7934, -0.7780,  0.9609, -0.7647,  0.9322, -0.8290,  0.9751,  0.9710],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2169, -1.1000, -0.8205, -1.1577,  0.9900,  0.9900, -0.8113],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.6705e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.4086e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1993, -0.2014, -0.2034,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1641, -0.1658, -0.1675,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1636, -1.0518, -0.8003, -1.1070,  0.9656,  0.9656, -0.7913],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1953, -3.9296, -1.0403,  ..., -4.8717, -4.8392, -4.8267],\n",
      "        [-2.8346, -3.9587, -0.9814,  ..., -4.5909, -4.5933, -4.6004],\n",
      "        [-2.9307, -3.9295, -0.9990,  ..., -0.9632, -0.9882, -1.2127],\n",
      "        ...,\n",
      "        [-3.1213, -3.9647, -1.0313,  ..., -5.3648, -5.3760, -5.3541],\n",
      "        [-3.0906, -3.9733, -1.0239,  ..., -5.3106, -5.3168, -5.3096],\n",
      "        [-2.8579, -3.9901, -1.0084,  ..., -4.3055, -4.2913, -4.2840]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7642, -1.1105, -1.0270, -0.7991, -1.1231,  0.8374,  0.9183, -0.7580],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7252], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.5518e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1249, -0.1262, -0.1275, -0.1287, -0.1300, -0.1314, -0.1327, -0.1340,\n",
      "         -0.1354, -0.1367, -0.1381, -0.1395, -0.1409, -0.1424, -0.1438, -0.1452,\n",
      "         -0.1467, -0.1482, -0.1497, -0.1512, -0.1527, -0.1543, -0.1558, -0.1574,\n",
      "         -0.1590, -0.1606, -0.1622, -0.1639, -0.1655, -0.1672, -0.1689, -0.1706,\n",
      "         -0.1723, -0.1740, -0.1758, -0.1776, -0.1794, -0.1812, -0.1830, -0.1849,\n",
      "         -0.1867, -0.1886, -0.1905, -0.1924, -0.1944, -0.1964, -0.1983, -0.2003,\n",
      "         -0.2024, -0.2044, -0.2065, -0.2086, -0.2107, -0.2128, -0.2149, -0.2171,\n",
      "         -0.2193, -0.2215, -0.2238, -0.2260, -0.2283, -0.2306, -0.2329, -0.2353,\n",
      "         -0.2377, -0.2401, -0.2425, -0.2449, -0.2474, -0.2499, -0.2524, -0.2550,\n",
      "         -0.2576, -0.2602, -0.2628, -0.2655, -0.2681, -0.2708, -0.2736, -0.2763,\n",
      "         -0.2791, -0.2820, -0.2848, -0.2877, -0.2906, -0.2935, -0.2965, -0.2995,\n",
      "         -0.3025, -0.3056, -0.3086, -0.3118, -0.3149, -0.3181, -0.3213, -0.3246,\n",
      "         -0.3278, -0.3311, -0.3345, -0.3379, -0.3413, -0.3447, -0.3482, -0.3517,\n",
      "         -0.3553, -0.3589, -0.3625, -0.3662, -0.3698, -0.3736, -0.3774, -0.3812,\n",
      "         -0.3850, -0.3889, -0.3928, -0.3968, -0.4008, -0.4049, -0.4090, -0.4131,\n",
      "         -0.4173, -0.4215, -0.4257, -0.4300, -0.4344, -0.4388, -0.4432, -0.4477,\n",
      "         -0.4522, -0.4568, -0.4614, -0.4660, -0.4707, -0.4755, -0.4803, -0.4851,\n",
      "         -0.4900, -0.4950, -0.5000, -0.5050, -0.5102, -0.5153, -0.5205, -0.5258,\n",
      "         -0.5311, -0.5364, -0.5419, -0.5473, -0.5529, -0.5584, -0.5641, -0.5698,\n",
      "         -0.5755, -0.5814, -0.5872, -0.5932, -0.5991, -0.6052, -0.6113, -0.6175,\n",
      "         -0.6237, -0.6300, -0.6364, -0.6428, -0.6493, -0.6559, -0.6625, -0.6692,\n",
      "         -0.6759, -0.6828, -0.6897, -0.6966, -0.7037, -0.7108, -0.7180, -0.7252,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7073], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1345, -3.9637, -1.0283, -2.4217, -0.1170, -2.3935, -4.5474, -0.8195,\n",
      "         -4.4258, -2.7997, -3.9837,  0.2524, -0.5170, -3.7803, -3.1176, -2.7058,\n",
      "         -3.4603, -1.9501, -2.1459, -1.1614, -1.4316, -1.6621, -1.0742,  0.3062,\n",
      "         -2.1999, -3.6051, -3.1453, -2.8741, -3.9282,  0.8692, -2.0052, -0.4225,\n",
      "         -0.8612, -3.6720, -1.9129, -1.8198, -0.8950, -3.3957, -1.9423, -0.3352,\n",
      "         -0.9235, -0.5920, -0.5862, -4.0496, -1.8876, -3.8085, -3.7688, -3.3804,\n",
      "          0.6079,  0.5502, -3.9712, -1.6615, -6.9496, -3.1617, -3.2855, -1.2048,\n",
      "         -2.6127, -3.1225, -2.6275, -1.9262, -3.7601, -1.3876, -5.7846, -2.6694,\n",
      "         -3.7020,  0.9699, -1.6456, -0.3268, -0.3462, -3.3195, -4.2524, -5.3653,\n",
      "         -3.0223, -0.1726, -3.9094, -4.0773, -2.7326, -4.0847, -2.7205, -3.6483,\n",
      "         -4.7300, -2.7500, -0.5924, -1.4928, -1.8144, -3.1706, -2.1067, -0.8960,\n",
      "         -3.6078, -1.1452, -3.3320, -3.9772, -2.7076, -0.8186, -1.1331, -2.8079,\n",
      "         -3.0151, -4.0982, -3.3625, -3.3656, -0.3733, -1.1880, -3.7231, -2.2619,\n",
      "         -0.5258, -2.2137, -3.9558, -3.8108,  0.4757, -1.0669, -1.3051, -0.8253,\n",
      "         -5.1042, -4.7459,  0.0754, -1.1453, -2.1296, -1.4766, -0.0314, -1.3242,\n",
      "         -3.2856, -4.4911, -2.8755, -1.0413, -1.2636, -2.8647, -2.5714, -0.9272,\n",
      "         -0.7724, -4.3095,  0.2572,  0.6662, -3.5972, -4.0170, -1.4141, -2.0786,\n",
      "         -3.1571, -2.4324, -2.0100, -3.1289, -3.2316, -5.7514, -1.9267, -2.3058,\n",
      "         -1.1846, -0.9048, -0.9600, -4.4914, -4.0167, -0.1630, -2.1180, -0.4178,\n",
      "         -0.9707, -0.3675, -0.1696, -0.9244, -4.4002, -3.6178, -4.9041, -1.5208,\n",
      "          0.9845,  0.9606,  1.0159,  0.9621,  0.4431,  1.0248, -4.7612, -4.9042,\n",
      "         -5.2266, -4.9973, -5.2069, -5.2594, -5.3845, -5.3711, -5.3070, -5.2881,\n",
      "         -5.2802, -5.2081, -5.0833, -4.9893, -4.9507, -4.9505, -4.9665, -4.9880,\n",
      "         -4.9729, -4.9569, -4.9583, -4.9917, -5.0550, -5.1322, -5.1781, -5.1894,\n",
      "         -5.1784, -5.2018, -5.2708, -5.3674, -5.4211, -5.4157, -5.3746, -5.3117,\n",
      "         -5.2683, -5.2434, -5.2385, -5.2517, -5.2755, -5.3175, -5.3554, -5.3677,\n",
      "         -5.3602, -5.3247, -5.3186, -5.3198, -5.3378, -5.3771, -5.3964, -5.4050,\n",
      "         -5.3972, -5.3836, -5.3806, -5.3872, -5.4033, -5.3815],\n",
      "        [-2.9149, -3.9699, -0.9783, -2.3090, -0.0282, -2.3436, -4.5012, -0.6283,\n",
      "         -4.2023, -2.6377, -3.9142,  0.2418, -0.4652, -3.7233, -3.2019, -2.5908,\n",
      "         -3.3999, -1.8527, -2.2209, -1.0395, -1.3841, -1.5109, -0.9704,  0.2339,\n",
      "         -2.0476, -3.5351, -3.1952, -2.7812, -3.9573,  0.8381, -1.9154, -0.2859,\n",
      "         -0.9051, -3.6145, -1.8990, -1.9220, -0.9860, -3.3592, -1.8401, -0.2283,\n",
      "         -0.8564, -0.6715, -0.5595, -4.1221, -1.7591, -3.6111, -3.5817, -3.3233,\n",
      "          0.2767,  0.3507, -3.9291, -1.5834, -6.9368, -3.0911, -3.2560, -1.0926,\n",
      "         -2.6484, -3.1408, -2.5660, -1.8002, -3.6954, -1.3089, -5.7542, -2.6220,\n",
      "         -3.7188,  0.9303, -1.5636, -0.1831, -0.4030, -3.2604, -4.1572, -5.3150,\n",
      "         -2.9546, -0.2374, -3.8554, -4.1102, -2.6598, -4.0562, -2.6294, -3.6004,\n",
      "         -4.6838, -2.6874, -0.5528, -1.3585, -1.7183, -3.0657, -2.0038, -0.7591,\n",
      "         -3.5225, -1.0667, -3.2739, -4.0022, -2.6438, -0.7897, -0.9876, -2.8828,\n",
      "         -2.8831, -3.9154, -3.3017, -3.3039, -0.3157, -1.0525, -3.7564, -2.2173,\n",
      "         -0.5840, -2.0548, -3.7857, -3.7183,  0.4144, -1.1262, -1.6131, -0.6586,\n",
      "         -4.7475, -4.5728,  0.1167, -1.0530, -2.1098, -1.3956,  0.0212, -1.2350,\n",
      "         -3.0532, -4.5182, -2.8440, -1.0267, -1.1730, -2.7861, -2.4696, -0.8832,\n",
      "         -0.6722, -4.2858, -0.1213,  0.4350, -3.5556, -3.9968, -1.3294, -2.1099,\n",
      "         -3.1939, -2.3721, -1.9130, -3.1085, -3.0880, -5.7489, -1.7427, -2.4192,\n",
      "         -1.1045, -1.2029, -0.5446, -4.2248, -3.9966, -0.1889, -2.0851, -0.2394,\n",
      "         -1.3775, -0.4735,  0.1022, -0.7590, -4.3467, -3.2660, -4.8292, -1.4531,\n",
      "         -0.6817, -0.6925, -0.6437, -0.6663, -0.2939, -0.7526, -4.9965, -4.9606,\n",
      "         -5.2429, -4.8881, -5.1948, -5.0991, -5.0460, -5.1098, -5.0952, -5.1113,\n",
      "         -5.1852, -5.2716, -5.1844, -4.9851, -4.8718, -4.8327, -4.7270, -4.6103,\n",
      "         -4.4854, -4.3962, -4.3663, -4.3873, -4.4278, -4.4625, -4.4709, -4.4616,\n",
      "         -4.4718, -4.5176, -4.5852, -4.6444, -4.6704, -4.6554, -4.6246, -4.6053,\n",
      "         -4.6264, -4.6679, -4.7005, -4.7176, -4.7266, -4.7176, -4.7021, -4.6671,\n",
      "         -4.6209, -4.5845, -4.5710, -4.5805, -4.5977, -4.6233, -4.6603, -4.6677,\n",
      "         -4.6643, -4.6550, -4.6359, -4.6186, -4.6092, -4.5812]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8985, -0.6218], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8392,  0.9900,  0.9900,  0.9900, -1.6891,  0.9900,  0.9900, -0.7957],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0796e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.2997e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1698, -0.1715, -0.1732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1610, -0.1626, -0.1642,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8185,  0.9656,  0.9656,  0.9656, -1.6556,  0.9656,  0.9656, -0.7761],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8917, -3.9802, -0.9947,  ..., -4.3140, -4.2964, -4.2888],\n",
      "        [-3.2416, -3.9274, -1.0373,  ..., -4.8568, -4.8218, -4.8058],\n",
      "        [-3.1637, -3.9607, -1.0160,  ..., -5.4095, -5.4357, -5.4205],\n",
      "        ...,\n",
      "        [-3.1570, -3.9624, -1.0245,  ..., -5.3841, -5.3986, -5.3774],\n",
      "        [-3.1556, -3.9577, -1.0258,  ..., -5.4086, -5.4363, -5.4077],\n",
      "        [-2.8955, -3.9837, -0.9989,  ..., -4.3188, -4.3038, -4.3018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7905,  0.7609,  0.9437,  0.9595, -0.9734,  0.9020,  0.9318, -0.8081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8812, -0.8575,  0.9900, -0.7196,  0.9900,  0.9900, -0.8034],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4250e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1783, -0.1801, -0.1819,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1735, -0.1752, -0.1770,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1625, -0.1642, -0.1658,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8595, -0.8363,  0.9656, -0.7019,  0.9656,  0.9656, -0.7836],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1520, -3.9739, -1.0187,  ..., -5.3275, -5.3287, -5.3222],\n",
      "        [-2.9023, -3.9848, -0.9950,  ..., -4.2833, -4.2649, -4.2598],\n",
      "        [-2.8960, -3.9840, -0.9938,  ..., -4.2508, -4.2306, -4.2224],\n",
      "        ...,\n",
      "        [-3.1855, -3.9700, -1.0294,  ..., -5.4675, -5.4913, -5.4655],\n",
      "        [-3.1475, -3.9652, -1.0229,  ..., -5.2056, -5.2166, -5.2063],\n",
      "        [-2.9096, -3.9863, -0.9978,  ..., -4.3257, -4.3087, -4.3033]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9028, -0.7784, -0.7590,  0.9270, -0.6290,  0.9148,  0.9061, -0.7516],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8805, -0.8002,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.2109],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7556e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1781, -0.1799, -0.1817,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1619, -0.1635, -0.1652,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1984, -0.2004, -0.2024,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8588, -0.7804,  0.9656,  0.9608,  0.9656,  0.9656,  0.9656, -1.1579],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9099, -3.9880, -0.9955,  ..., -4.2837, -4.2672, -4.2614],\n",
      "        [-2.9129, -3.9853, -0.9996,  ..., -4.3109, -4.2945, -4.2875],\n",
      "        [-3.2614, -3.9364, -1.0436,  ..., -4.8583, -4.8229, -4.8048],\n",
      "        ...,\n",
      "        [-3.1955, -3.9659, -1.0229,  ..., -5.3976, -5.4167, -5.3827],\n",
      "        [-3.1538, -3.9620, -1.0056,  ..., -5.3148, -5.3131, -5.2902],\n",
      "        [-2.9020, -3.9584, -0.9784,  ..., -4.5414, -4.5431, -4.5533]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7826, -0.7571,  0.6506,  0.7225,  0.8899,  0.9393,  0.9205, -1.1360],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8350, -0.8040,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1965e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1689, -0.1706, -0.1724,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8144, -0.7842,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2025, -3.9712, -1.0217,  ..., -5.4077, -5.4266, -5.4040],\n",
      "        [-3.1659, -3.9643, -1.0068,  ..., -5.3178, -5.3127, -5.2927],\n",
      "        [-2.9197, -3.9919, -1.0000,  ..., -4.3011, -4.2844, -4.2781],\n",
      "        ...,\n",
      "        [-3.2810, -3.9348, -1.0414,  ..., -4.8111, -4.7729, -4.7549],\n",
      "        [-3.2075, -3.9702, -1.0262,  ..., -5.3532, -5.3665, -5.3349],\n",
      "        [-3.2121, -3.9739, -1.0309,  ..., -5.4677, -5.4882, -5.4607]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9336,  0.9116, -0.7681, -0.7573,  0.6300,  0.7420,  0.8152,  0.9032],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8619, -0.8619, -1.1528,  0.9900,  0.9900, -0.7175, -0.8451],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8490e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1744, -0.1761, -0.1779,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1744, -0.1761, -0.1779,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1236, -0.1248, -0.1261,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1710, -0.1727, -0.1744,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8406, -0.8406, -1.1023,  0.9656,  0.9656, -0.6998, -0.8243],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2190, -3.9688, -1.0219,  ..., -5.4375, -5.4566, -5.4225],\n",
      "        [-2.9373, -3.9915, -0.9963,  ..., -4.2654, -4.2482, -4.2411],\n",
      "        [-2.9373, -3.9915, -0.9963,  ..., -4.2654, -4.2482, -4.2411],\n",
      "        ...,\n",
      "        [-3.2145, -3.9719, -1.0222,  ..., -5.4176, -5.4397, -5.4132],\n",
      "        [-2.9781, -3.9756, -0.9779,  ..., -4.5861, -4.5709, -4.5459],\n",
      "        [-2.9325, -3.9845, -0.9924,  ..., -4.3154, -4.2974, -4.2879]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8974, -0.7799, -0.7799, -1.1600,  0.9392,  0.9392, -0.6373, -0.8045],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8600,  0.9900, -1.1000, -0.8365,  0.9900, -0.7416,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2184e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(7.4258e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1740, -0.1757, -0.1775,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1277, -0.1290, -0.1303,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8388,  0.9656, -1.0518, -0.8159,  0.9656, -0.7233,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2196, -3.9688, -1.0241,  ..., -5.4007, -5.4151, -5.3894],\n",
      "        [-2.9398, -3.9870, -0.9899,  ..., -4.3021, -4.2835, -4.2787],\n",
      "        [-3.3021, -3.9335, -1.0382,  ..., -4.8643, -4.8224, -4.8013],\n",
      "        ...,\n",
      "        [-3.2277, -3.9699, -1.0238,  ..., -5.3757, -5.3873, -5.3617],\n",
      "        [-2.9818, -3.9693, -0.9834,  ..., -4.6571, -4.6422, -4.6175],\n",
      "        [-3.2294, -3.9670, -1.0229,  ..., -5.4242, -5.4418, -5.4138]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9044, -0.7958,  0.6472, -1.0661, -0.7656,  0.8287, -0.6677,  0.9488],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8563,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.3372e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.7113e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1732, -0.1750, -0.1768, -0.1785, -0.1803, -0.1822, -0.1840, -0.1859,\n",
      "         -0.1877, -0.1896, -0.1915, -0.1935, -0.1954, -0.1974, -0.1994, -0.2014,\n",
      "         -0.2035, -0.2055, -0.2076, -0.2097, -0.2118, -0.2139, -0.2161, -0.2183,\n",
      "         -0.2205, -0.2227, -0.2250, -0.2272, -0.2295, -0.2319, -0.2342, -0.2366,\n",
      "         -0.2390, -0.2414, -0.2438, -0.2463, -0.2488, -0.2513, -0.2538, -0.2564,\n",
      "         -0.2590, -0.2616, -0.2642, -0.2669, -0.2696, -0.2723, -0.2751, -0.2778,\n",
      "         -0.2806, -0.2835, -0.2863, -0.2892, -0.2921, -0.2951, -0.2981, -0.3011,\n",
      "         -0.3041, -0.3072, -0.3103, -0.3134, -0.3166, -0.3198, -0.3230, -0.3263,\n",
      "         -0.3296, -0.3329, -0.3363, -0.3397, -0.3431, -0.3466, -0.3501, -0.3536,\n",
      "         -0.3572, -0.3608, -0.3644, -0.3681, -0.3718, -0.3756, -0.3794, -0.3832,\n",
      "         -0.3871, -0.3910, -0.3950, -0.3989, -0.4030, -0.4070, -0.4112, -0.4153,\n",
      "         -0.4195, -0.4237, -0.4280, -0.4323, -0.4367, -0.4411, -0.4456, -0.4501,\n",
      "         -0.4546, -0.4592, -0.4639, -0.4685, -0.4733, -0.4781, -0.4829, -0.4878,\n",
      "         -0.4927, -0.4977, -0.5027, -0.5078, -0.5129, -0.5181, -0.5233, -0.5286,\n",
      "         -0.5339, -0.5393, -0.5448, -0.5503, -0.5558, -0.5615, -0.5671, -0.5729,\n",
      "         -0.5786, -0.5845, -0.5904, -0.5964, -0.6024, -0.6085, -0.6146, -0.6208,\n",
      "         -0.6271, -0.6334, -0.6398, -0.6463, -0.6528, -0.6594, -0.6661, -0.6728,\n",
      "         -0.6796, -0.6865, -0.6934, -0.7004, -0.7075, -0.7146, -0.7218, -0.7291,\n",
      "         -0.7365, -0.7439, -0.7514, -0.7590, -0.7667, -0.7744, -0.7823, -0.7902,\n",
      "         -0.7981, -0.8062, -0.8144, -0.8226, -0.8309, -0.8393, -0.8478, -0.8563,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8352,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9503, -3.9889, -0.9922, -2.2405, -0.1020, -2.3778, -4.4488, -0.6291,\n",
      "         -4.2478, -2.6807, -3.9649,  0.2454, -0.4057, -3.7373, -3.0948, -2.5496,\n",
      "         -3.3915, -1.9295, -2.1723, -1.0776, -1.3594, -1.5473, -0.9901,  0.2775,\n",
      "         -2.0899, -3.5710, -3.0997, -2.7456, -3.9331,  0.8422, -1.9335, -0.3171,\n",
      "         -0.9256, -3.6523, -1.8814, -1.9461, -0.9582, -3.3480, -1.8345, -0.2319,\n",
      "         -0.9384, -0.6456, -0.5655, -4.0649, -1.7986, -3.6569, -3.6218, -3.3585,\n",
      "          0.2413,  0.3130, -3.9644, -1.5858, -6.9203, -3.0677, -3.2520, -1.1533,\n",
      "         -2.6171, -3.1309, -2.5650, -1.8449, -3.7362, -1.3198, -5.7347, -2.6101,\n",
      "         -3.6996,  0.9265, -1.5861, -0.2203, -0.4269, -3.2971, -4.1669, -5.3210,\n",
      "         -2.9422, -0.2744, -3.9000, -4.0333, -2.6533, -4.0583, -2.6283, -3.6489,\n",
      "         -4.6911, -2.6828, -0.5909, -1.4017, -1.7403, -3.0725, -2.0348, -0.8000,\n",
      "         -3.5733, -1.1415, -3.3272, -3.9356, -2.6411, -0.8362, -1.0365, -2.8825,\n",
      "         -2.9432, -3.9781, -3.3221, -3.2874, -0.3672, -1.1004, -3.7168, -2.2470,\n",
      "         -0.5496, -2.0813, -3.8496, -3.7224,  0.3766, -1.1878, -1.6378, -0.9965,\n",
      "         -4.3898, -4.5751,  0.1370, -1.1353, -2.0753, -1.4492, -0.0170, -1.2641,\n",
      "         -3.1613, -4.4392, -2.8990, -1.0745, -1.1948, -2.7546, -2.5053, -0.8945,\n",
      "         -0.6844, -4.3139, -0.1564,  0.3866, -3.5276, -3.9578, -1.4149, -2.0897,\n",
      "         -3.1849, -2.3656, -1.9565, -3.2100, -2.8885, -3.6611, -5.5616, -1.5708,\n",
      "         -0.8101, -0.7774, -0.7103, -0.7536, -0.7335, -0.7987, -4.9403, -4.9476,\n",
      "         -5.2945, -4.9168, -4.8388, -4.5013, -4.4051, -4.5196, -4.6330, -4.6365,\n",
      "         -4.6952, -4.7460, -4.7768, -4.7732, -4.7177, -4.6421, -4.6046, -4.6013,\n",
      "         -4.6154, -4.6165, -4.6184, -4.6391, -4.6655, -4.6843, -4.7154, -4.7551,\n",
      "         -4.7656, -4.7671, -4.7865, -4.7829, -4.7685, -4.7718, -4.7612, -4.7456,\n",
      "         -4.7338, -4.7269, -4.7434, -4.7600, -4.7679, -4.7717, -4.7727, -4.7827,\n",
      "         -4.7914, -4.8028, -4.7953, -4.7938, -4.7917, -4.7780, -4.7661, -4.7453,\n",
      "         -4.7336, -4.7254, -4.7128, -4.7009, -4.6940, -4.6981, -4.6961, -4.6999,\n",
      "         -4.6785, -4.6559, -4.6632, -4.6182, -4.5937, -4.5226, -4.4756, -4.3828,\n",
      "         -4.3199, -4.3178, -4.2938, -4.2654, -4.2536, -4.2491],\n",
      "        [-3.1981, -3.9780, -1.0181, -2.3684, -0.1426, -2.4261, -4.4964, -0.8018,\n",
      "         -4.4768, -2.8093, -4.0011,  0.2363, -0.5128, -3.7623, -3.0582, -2.6674,\n",
      "         -3.4815, -1.9731, -2.1270, -1.1798, -1.4010, -1.6756, -1.0851,  0.3172,\n",
      "         -2.2557, -3.5935, -3.0719, -2.8421, -3.9339,  0.8533, -2.0240, -0.4327,\n",
      "         -0.8971, -3.6937, -1.9263, -1.8398, -0.9146, -3.4215, -1.9139, -0.3561,\n",
      "         -0.9638, -0.6167, -0.5906, -4.0292, -1.9433, -3.8681, -3.7906, -3.3975,\n",
      "          0.4938,  0.4886, -3.9668, -1.6577, -6.9400, -3.1333, -3.3102, -1.2254,\n",
      "         -2.5908, -3.1379, -2.6211, -1.9866, -3.7519, -1.3800, -5.7630, -2.6430,\n",
      "         -3.7053,  0.9502, -1.6694, -0.3443, -0.3825, -3.3420, -4.2274, -5.3393,\n",
      "         -2.9991, -0.2143, -3.9010, -4.0327, -2.7052, -4.1153, -2.6942, -3.6431,\n",
      "         -4.7060, -2.7263, -0.6220, -1.5095, -1.7954, -3.1737, -2.1300, -0.9149,\n",
      "         -3.6109, -1.1982, -3.3274, -3.9356, -2.6837, -0.8526, -1.1509, -2.8147,\n",
      "         -3.0776, -4.1565, -3.3664, -3.3553, -0.4125, -1.2089, -3.7042, -2.2863,\n",
      "         -0.5164, -2.2230, -4.0102, -3.7987,  0.4386, -1.1125, -1.5298, -0.6179,\n",
      "         -4.9847, -4.6067,  0.0503, -1.2030, -2.1358, -1.5320, -0.0434, -1.3428,\n",
      "         -3.2690, -4.4987, -2.8776, -1.0509, -1.2870, -2.8582, -2.6501, -0.9716,\n",
      "         -0.8294, -4.3670,  0.0724,  0.5743, -3.6181, -3.9935, -1.4546, -2.0588,\n",
      "         -3.1888, -2.4252, -2.0718, -3.1685, -3.2677, -5.7310, -1.9202, -2.3349,\n",
      "         -1.0677, -1.0995, -0.4872, -4.4490, -3.9115, -0.1870, -2.2207, -0.1905,\n",
      "         -1.2742, -0.4321,  0.1357, -0.7649, -4.5721, -5.0222, -1.8354, -2.2708,\n",
      "         -0.0307, -0.0701,  0.1878, -4.1977, -3.9561, -0.3078, -1.4917, -4.2722,\n",
      "         -6.1008, -4.9822, -3.3166, -4.8346, -1.5610,  0.8803,  0.9072,  0.9910,\n",
      "          0.9043,  0.7894,  0.9756, -4.8207, -4.9560, -5.2543, -5.0497, -5.2562,\n",
      "         -5.1200, -5.1470, -5.1352, -5.1336, -5.1501, -5.1497, -5.1106, -5.0933,\n",
      "         -5.0811, -5.0884, -5.1070, -5.1194, -5.1077, -5.1020, -5.1082, -5.1337,\n",
      "         -5.1796, -5.2250, -5.2768, -5.3246, -5.3450, -5.3657, -5.3775, -5.3982,\n",
      "         -5.4239, -5.4262, -5.4107, -5.3740, -5.3260, -5.2918, -5.2649, -5.2607,\n",
      "         -5.2839, -5.3066, -5.3303, -5.3482, -5.3527, -5.3449]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7639,  0.9080], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8199,  0.9900, -0.8581, -0.8421, -0.7382],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2764e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0741e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1736, -0.1753, -0.1771,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1703, -0.1721, -0.1738,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1271, -0.1284, -0.1297,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.7997,  0.9656, -0.8369, -0.8213, -0.7200],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2310, -3.9711, -1.0237,  ..., -5.4664, -5.4852, -5.4577],\n",
      "        [-3.2020, -3.9789, -1.0185,  ..., -5.3455, -5.3539, -5.3468],\n",
      "        [-3.3076, -3.9353, -1.0367,  ..., -4.8987, -4.8562, -4.8345],\n",
      "        ...,\n",
      "        [-2.9490, -3.9876, -0.9925,  ..., -4.3119, -4.2961, -4.2862],\n",
      "        [-2.9576, -3.9970, -0.9914,  ..., -4.2819, -4.2683, -4.2635],\n",
      "        [-2.9920, -3.9769, -0.9760,  ..., -4.6206, -4.6076, -4.5801]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8981,  0.9144,  0.6687, -0.7612,  0.9251, -0.8091, -0.7685, -0.6428],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8671,  0.9900,  0.9900, -0.8913,  0.9900, -0.8671, -0.8115],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3737e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.2320e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1754, -0.1772, -0.1790,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1754, -0.1772, -0.1790,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1642, -0.1658, -0.1675,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8457,  0.9656,  0.9656, -0.8693,  0.9608, -0.8457, -0.7915],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2382, -3.9706, -1.0203,  ..., -5.4531, -5.4761, -5.4626],\n",
      "        [-2.9632, -3.9949, -1.0000,  ..., -4.2654, -4.2493, -4.2420],\n",
      "        [-3.2358, -3.9704, -1.0253,  ..., -5.4720, -5.4929, -5.4650],\n",
      "        ...,\n",
      "        [-3.3045, -3.9371, -1.0420,  ..., -4.9674, -4.8696, -4.8219],\n",
      "        [-2.9632, -3.9949, -1.0000,  ..., -4.2654, -4.2493, -4.2420],\n",
      "        [-2.9590, -3.9894, -1.0015,  ..., -4.3158, -4.2995, -4.2913]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9392, -0.7888,  0.8989,  0.9633, -0.7958,  0.7281, -0.7888, -0.7669],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7258, -0.8402, -0.8028,  0.9900, -0.8411,  0.9900, -1.2216,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8824e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1250, -0.1263, -0.1276,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1700, -0.1717, -0.1734,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1624, -0.1641, -0.1657,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2001, -0.2021, -0.2042,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7079, -0.8195, -0.7830,  0.9656, -0.8204,  0.9656, -1.1681,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9924, -3.9812, -0.9770,  ..., -4.6338, -4.6227, -4.5944],\n",
      "        [-2.9527, -3.9927, -0.9945,  ..., -4.3140, -4.2952, -4.2878],\n",
      "        [-2.9600, -3.9987, -0.9996,  ..., -4.3272, -4.3119, -4.3057],\n",
      "        ...,\n",
      "        [-3.2258, -3.9796, -1.0227,  ..., -5.4624, -5.4869, -5.4679],\n",
      "        [-2.9369, -3.9680, -0.9778,  ..., -4.5889, -4.5889, -4.5956],\n",
      "        [-3.2223, -3.9716, -1.0287,  ..., -5.4443, -5.4726, -5.4456]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6524, -0.8221, -0.7751,  0.9164, -0.7837,  0.9569, -1.1512,  0.9181],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8723,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.7158e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1765, -0.1782, -0.1800,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8507,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1858, -3.9752, -1.0051,  ..., -5.3372, -5.3372, -5.3173],\n",
      "        [-3.1939, -3.9805, -1.0232,  ..., -5.2338, -5.2484, -5.2383],\n",
      "        [-2.9470, -3.9999, -0.9963,  ..., -4.2794, -4.2668, -4.2593],\n",
      "        ...,\n",
      "        [-3.2214, -3.9835, -1.0206,  ..., -5.4578, -5.4847, -5.4629],\n",
      "        [-3.2290, -3.9826, -1.0253,  ..., -5.3995, -5.4143, -5.3892],\n",
      "        [-3.2997, -3.9480, -1.0389,  ..., -4.9150, -4.8728, -4.8535]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9123,  0.8870, -0.8132,  0.6851,  0.7874,  0.9541,  0.8503,  0.6903],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1470, -0.8420, -0.8164,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8870e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1584, -0.1600, -0.1616,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1703, -0.1721, -0.1738,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1652, -0.1668, -0.1685,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0968, -0.8213, -0.7963,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9849, -3.9600, -0.9744,  ..., -4.2777, -4.2980, -4.3142],\n",
      "        [-2.9420, -4.0009, -0.9910,  ..., -4.2520, -4.2319, -4.2253],\n",
      "        [-2.9491, -4.0058, -0.9969,  ..., -4.3054, -4.2892, -4.2829],\n",
      "        ...,\n",
      "        [-3.2240, -3.9860, -1.0174,  ..., -5.4460, -5.4722, -5.4531],\n",
      "        [-3.3068, -3.9470, -1.0340,  ..., -4.8611, -4.8234, -4.8020],\n",
      "        [-3.2308, -3.9826, -1.0196,  ..., -5.4349, -5.4552, -5.4281]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1635, -0.7763, -0.7994,  0.8582,  0.8927,  0.9583,  0.7945,  0.9653],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.6704, -0.8228,  0.9900,  0.9900,  0.9900, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1574e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3413, -0.3448, -0.3483,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.6373, -0.8025,  0.9656,  0.9656,  0.9656, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2006, -3.9936, -1.0095,  ..., -5.3507, -5.3591, -5.3506],\n",
      "        [-3.2338, -3.9914, -1.0211,  ..., -5.4880, -5.5150, -5.4870],\n",
      "        [-2.8494, -4.0171, -0.9511,  ..., -4.2439, -4.2448, -4.2422],\n",
      "        ...,\n",
      "        [-3.1872, -3.9804, -0.9973,  ..., -5.3355, -5.3315, -5.3124],\n",
      "        [-3.2233, -3.9857, -1.0168,  ..., -5.3997, -5.4188, -5.3956],\n",
      "        [-3.0303, -3.9477, -0.9885,  ..., -0.9926, -1.0193, -1.2279]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9340,  0.9394, -1.0315, -0.8272,  0.9356,  0.9194,  0.9356, -1.0508],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7168, -0.7789], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.5738e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.5059e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1235, -0.1247, -0.1260, -0.1272, -0.1285, -0.1298, -0.1311, -0.1325,\n",
      "         -0.1338, -0.1352, -0.1365, -0.1379, -0.1393, -0.1407, -0.1421, -0.1436,\n",
      "         -0.1450, -0.1465, -0.1480, -0.1494, -0.1510, -0.1525, -0.1540, -0.1556,\n",
      "         -0.1571, -0.1587, -0.1603, -0.1620, -0.1636, -0.1652, -0.1669, -0.1686,\n",
      "         -0.1703, -0.1720, -0.1738, -0.1755, -0.1773, -0.1791, -0.1809, -0.1827,\n",
      "         -0.1846, -0.1864, -0.1883, -0.1902, -0.1921, -0.1941, -0.1960, -0.1980,\n",
      "         -0.2000, -0.2020, -0.2041, -0.2061, -0.2082, -0.2103, -0.2124, -0.2146,\n",
      "         -0.2168, -0.2190, -0.2212, -0.2234, -0.2257, -0.2279, -0.2302, -0.2326,\n",
      "         -0.2349, -0.2373, -0.2397, -0.2421, -0.2445, -0.2470, -0.2495, -0.2520,\n",
      "         -0.2546, -0.2571, -0.2597, -0.2624, -0.2650, -0.2677, -0.2704, -0.2731,\n",
      "         -0.2759, -0.2787, -0.2815, -0.2843, -0.2872, -0.2901, -0.2930, -0.2960,\n",
      "         -0.2990, -0.3020, -0.3051, -0.3081, -0.3113, -0.3144, -0.3176, -0.3208,\n",
      "         -0.3240, -0.3273, -0.3306, -0.3339, -0.3373, -0.3407, -0.3442, -0.3476,\n",
      "         -0.3512, -0.3547, -0.3583, -0.3619, -0.3656, -0.3692, -0.3730, -0.3767,\n",
      "         -0.3806, -0.3844, -0.3883, -0.3922, -0.3962, -0.4002, -0.4042, -0.4083,\n",
      "         -0.4124, -0.4166, -0.4208, -0.4250, -0.4293, -0.4337, -0.4380, -0.4425,\n",
      "         -0.4469, -0.4515, -0.4560, -0.4606, -0.4653, -0.4700, -0.4747, -0.4795,\n",
      "         -0.4844, -0.4893, -0.4942, -0.4992, -0.5042, -0.5093, -0.5145, -0.5197,\n",
      "         -0.5249, -0.5302, -0.5356, -0.5410, -0.5464, -0.5520, -0.5575, -0.5632,\n",
      "         -0.5689, -0.5746, -0.5804, -0.5863, -0.5922, -0.5982, -0.6042, -0.6103,\n",
      "         -0.6165, -0.6227, -0.6290, -0.6354, -0.6418, -0.6483, -0.6548, -0.6614,\n",
      "         -0.6681, -0.6748, -0.6817, -0.6885, -0.6955, -0.7025, -0.7096, -0.7168,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1576, -0.1592, -0.1608, -0.1624, -0.1640, -0.1657, -0.1674, -0.1691,\n",
      "         -0.1708, -0.1725, -0.1742, -0.1760, -0.1778, -0.1796, -0.1814, -0.1832,\n",
      "         -0.1851, -0.1869, -0.1888, -0.1907, -0.1927, -0.1946, -0.1966, -0.1986,\n",
      "         -0.2006, -0.2026, -0.2046, -0.2067, -0.2088, -0.2109, -0.2130, -0.2152,\n",
      "         -0.2174, -0.2196, -0.2218, -0.2240, -0.2263, -0.2286, -0.2309, -0.2332,\n",
      "         -0.2356, -0.2379, -0.2403, -0.2428, -0.2452, -0.2477, -0.2502, -0.2527,\n",
      "         -0.2553, -0.2579, -0.2605, -0.2631, -0.2658, -0.2684, -0.2711, -0.2739,\n",
      "         -0.2767, -0.2794, -0.2823, -0.2851, -0.2880, -0.2909, -0.2938, -0.2968,\n",
      "         -0.2998, -0.3028, -0.3059, -0.3090, -0.3121, -0.3153, -0.3184, -0.3217,\n",
      "         -0.3249, -0.3282, -0.3315, -0.3349, -0.3382, -0.3417, -0.3451, -0.3486,\n",
      "         -0.3521, -0.3557, -0.3593, -0.3629, -0.3666, -0.3703, -0.3740, -0.3778,\n",
      "         -0.3816, -0.3855, -0.3893, -0.3933, -0.3973, -0.4013, -0.4053, -0.4094,\n",
      "         -0.4135, -0.4177, -0.4219, -0.4262, -0.4305, -0.4349, -0.4393, -0.4437,\n",
      "         -0.4482, -0.4527, -0.4573, -0.4619, -0.4666, -0.4713, -0.4760, -0.4808,\n",
      "         -0.4857, -0.4906, -0.4956, -0.5006, -0.5056, -0.5107, -0.5159, -0.5211,\n",
      "         -0.5264, -0.5317, -0.5370, -0.5425, -0.5479, -0.5535, -0.5591, -0.5647,\n",
      "         -0.5704, -0.5762, -0.5820, -0.5879, -0.5938, -0.5998, -0.6059, -0.6120,\n",
      "         -0.6182, -0.6244, -0.6307, -0.6371, -0.6435, -0.6500, -0.6566, -0.6632,\n",
      "         -0.6699, -0.6767, -0.6835, -0.6904, -0.6974, -0.7045, -0.7116, -0.7188,\n",
      "         -0.7260, -0.7334, -0.7408, -0.7483, -0.7558, -0.7634, -0.7712, -0.7789,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6991, -0.7597], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9852, -3.9886, -0.9688, -2.2658, -0.0509, -2.3779, -4.4574, -0.6200,\n",
      "         -4.2631, -2.6680, -3.9349,  0.1953, -0.4724, -3.7383, -3.1655, -2.5622,\n",
      "         -3.3972, -1.8738, -2.2126, -1.0468, -1.3621, -1.5369, -0.9744,  0.2615,\n",
      "         -2.0860, -3.5569, -3.1408, -2.7442, -3.9508,  0.8501, -1.9352, -0.2881,\n",
      "         -0.9383, -3.6245, -1.8767, -1.9643, -0.9897, -3.3470, -1.8255, -0.2167,\n",
      "         -0.8860, -0.6767, -0.5346, -4.1133, -1.7850, -3.6672, -3.6254, -3.3345,\n",
      "          0.2368,  0.3703, -3.9513, -1.5926, -6.9731, -3.0561, -3.2441, -1.1033,\n",
      "         -2.6257, -3.1592, -2.5681, -1.8335, -3.7161, -1.3132, -5.7731, -2.5907,\n",
      "         -3.7106,  0.9435, -1.5836, -0.1834, -0.4337, -3.2671, -4.1487, -5.3195,\n",
      "         -2.9218, -0.2490, -3.8759, -4.0860, -2.6268, -4.0495, -2.6145, -3.6206,\n",
      "         -4.6857, -2.6561, -0.5571, -1.3578, -1.6993, -3.0678, -2.0209, -0.7640,\n",
      "         -3.5571, -1.0959, -3.2935, -3.9816, -2.6136, -0.7997, -0.9869, -2.9260,\n",
      "         -2.9134, -3.9818, -3.3079, -3.2927, -0.3337, -1.0532, -3.7392, -2.2104,\n",
      "         -0.5584, -2.0589, -3.8402, -3.7133,  0.4096, -1.4794, -1.6011, -0.9337,\n",
      "         -4.6757, -4.6727,  0.1818, -1.0174, -2.0649, -1.3683,  0.0472, -1.1654,\n",
      "         -3.1109, -4.5174, -2.8469, -1.0064, -1.0942, -2.7080, -2.4742, -0.9164,\n",
      "         -0.6329, -4.2536, -0.1578,  0.4390, -3.5154, -3.9631, -1.3721, -2.0934,\n",
      "         -3.2101, -2.3631, -1.9377, -3.2020, -3.1919, -5.7093, -1.7642, -2.4694,\n",
      "         -1.4046, -1.1220, -0.7344, -4.3455, -3.9303, -0.2376, -2.0973, -0.6526,\n",
      "         -1.3376, -0.4657, -0.2632, -0.9400, -4.3016, -3.5479, -4.9147, -1.3437,\n",
      "         -0.7516, -0.7059, -0.6969, -0.6787, -0.6575, -0.7294, -4.9803, -4.9847,\n",
      "         -5.2593, -4.8367, -5.0783, -4.9950, -4.8810, -4.9622, -5.0008, -5.1149,\n",
      "         -5.2589, -5.3739, -5.2818, -5.0892, -4.9839, -4.9453, -4.8991, -4.8629,\n",
      "         -4.7679, -4.6470, -4.5811, -4.5828, -4.6171, -4.6483, -4.6487, -4.6338,\n",
      "         -4.6406, -4.6826, -4.7400, -4.7857, -4.8012, -4.7848, -4.7555, -4.7368,\n",
      "         -4.7494, -4.7794, -4.8027, -4.8158, -4.8171, -4.7996, -4.7816, -4.7439,\n",
      "         -4.7025, -4.6645, -4.6616, -4.6703, -4.6770, -4.6961, -4.7284, -4.7322,\n",
      "         -4.7274, -4.7260, -4.7072, -4.6773, -4.6621, -4.6383],\n",
      "        [-2.9475, -4.0070, -0.9837, -2.2267, -0.0860, -2.3891, -4.4546, -0.6154,\n",
      "         -4.2411, -2.6915, -3.9613,  0.2048, -0.4321, -3.7342, -3.1366, -2.5486,\n",
      "         -3.3824, -1.9121, -2.1903, -1.0542, -1.3676, -1.5488, -0.9756,  0.2784,\n",
      "         -2.0830, -3.5661, -3.1322, -2.7373, -3.9321,  0.8637, -1.9173, -0.2897,\n",
      "         -0.9284, -3.6404, -1.8656, -1.9599, -0.9332, -3.3333, -1.8202, -0.2081,\n",
      "         -0.9142, -0.6250, -0.5322, -4.0805, -1.7800, -3.6503, -3.6254, -3.3481,\n",
      "          0.2520,  0.3642, -3.9655, -1.5994, -6.9632, -3.0602, -3.2380, -1.1278,\n",
      "         -2.6121, -3.1316, -2.5825, -1.8326, -3.7334, -1.3314, -5.7664, -2.6022,\n",
      "         -3.6972,  0.9506, -1.5669, -0.1886, -0.4277, -3.2826, -4.1809, -5.3555,\n",
      "         -2.9341, -0.2566, -3.8954, -4.0706, -2.6427, -4.0470, -2.6161, -3.6451,\n",
      "         -4.7265, -2.6753, -0.5630, -1.3686, -1.7385, -3.0694, -2.0173, -0.7730,\n",
      "         -3.6015, -1.1184, -3.3222, -3.9706, -2.6336, -0.8083, -1.0034, -2.9197,\n",
      "         -2.9245, -3.9763, -3.3243, -3.2801, -0.3466, -1.0697, -3.7184, -2.2182,\n",
      "         -0.5466, -2.0698, -3.8430, -3.7296,  0.4004, -0.9395, -1.5427, -0.5468,\n",
      "         -4.7406, -4.6324,  0.1356, -1.0139, -2.0879, -1.4275,  0.0166, -1.2073,\n",
      "         -3.1176, -4.4600, -2.8648, -1.0578, -1.1804, -2.7952, -2.5057, -0.9058,\n",
      "         -0.6702, -4.3091, -0.1022,  0.4292, -3.5190, -3.9530, -1.3824, -2.0836,\n",
      "         -3.1842, -2.3910, -1.9514, -3.1990, -2.9040, -3.5991, -5.6354, -1.6091,\n",
      "         -0.7954, -0.8131, -0.8098, -0.8785, -0.8928, -0.8403, -4.9671, -5.0008,\n",
      "         -5.3667, -4.9703, -4.8674, -4.5691, -4.4631, -4.5884, -4.6521, -4.6189,\n",
      "         -4.7196, -4.8129, -4.8658, -4.8736, -4.8245, -4.7488, -4.6983, -4.6722,\n",
      "         -4.6526, -4.6306, -4.6107, -4.6227, -4.6445, -4.6540, -4.6789, -4.7143,\n",
      "         -4.7197, -4.7223, -4.7345, -4.7183, -4.6956, -4.7096, -4.7128, -4.7040,\n",
      "         -4.6937, -4.6915, -4.7103, -4.7305, -4.7456, -4.7524, -4.7585, -4.7824,\n",
      "         -4.8077, -4.8323, -4.8393, -4.8432, -4.8524, -4.8527, -4.8395, -4.8300,\n",
      "         -4.8250, -4.8207, -4.8092, -4.7907, -4.7743, -4.7720, -4.7751, -4.7817,\n",
      "         -4.7711, -4.7429, -4.7399, -4.6830, -4.6433, -4.5855, -4.5431, -4.4324,\n",
      "         -4.3603, -4.3632, -4.3473, -4.3198, -4.3046, -4.2997]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7033, -0.8383], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8349, -0.8190,  0.9900,  0.9900,  0.9900, -0.7807,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9064e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1689, -0.1706, -0.1723,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1657, -0.1674, -0.1691,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1579, -0.1595, -0.1611,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8143, -0.7988,  0.9656,  0.9656,  0.9656, -0.7614,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2159, -3.9915, -1.0065,  ..., -5.3868, -5.4062, -5.3833],\n",
      "        [-2.9477, -4.0110, -0.9762,  ..., -4.2673, -4.2491, -4.2417],\n",
      "        [-2.9477, -4.0071, -0.9744,  ..., -4.2669, -4.2539, -4.2490],\n",
      "        ...,\n",
      "        [-3.1946, -3.9980, -1.0000,  ..., -5.3545, -5.3625, -5.3535],\n",
      "        [-2.9436, -4.0062, -0.9786,  ..., -4.3148, -4.2979, -4.2874],\n",
      "        [-3.3008, -3.9537, -1.0198,  ..., -4.8626, -4.8202, -4.8024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9350, -0.8153, -0.7973,  0.9583,  0.8673,  0.9312, -0.7885,  0.8010],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.8196, -0.7877,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5462e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1593, -0.1610, -0.1626,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.7994, -0.7682,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2236, -3.9929, -1.0018,  ..., -5.3733, -5.3899, -5.3640],\n",
      "        [-3.1816, -3.9851, -0.9800,  ..., -5.3383, -5.3369, -5.3143],\n",
      "        [-3.1906, -3.9905, -0.9985,  ..., -5.2341, -5.2469, -5.2354],\n",
      "        ...,\n",
      "        [-2.9445, -4.0096, -0.9709,  ..., -4.3308, -4.3129, -4.3087],\n",
      "        [-3.1906, -3.9905, -0.9985,  ..., -5.2341, -5.2469, -5.2354],\n",
      "        [-3.2280, -3.9977, -1.0051,  ..., -5.4766, -5.5062, -5.4817]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8720,  0.9162,  0.8988,  0.9592, -0.7880, -0.7810,  0.8988,  0.9381],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8378,  0.9900, -0.7948,  0.9900, -1.1000, -1.1501,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4221e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1695, -0.1712, -0.1729,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1588, -0.1604, -0.1620,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8171,  0.9656, -0.7752,  0.9656, -1.0518, -1.0997,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2232, -3.9904, -0.9865,  ..., -5.4169, -5.4453, -5.4314],\n",
      "        [-2.9374, -4.0075, -0.9597,  ..., -4.3080, -4.2889, -4.2823],\n",
      "        [-3.2253, -3.9971, -1.0005,  ..., -5.4747, -5.5041, -5.4820],\n",
      "        ...,\n",
      "        [-3.0254, -3.9515, -0.9658,  ..., -0.9748, -1.0002, -1.2309],\n",
      "        [-2.9807, -3.9673, -0.9484,  ..., -4.2799, -4.2991, -4.3166],\n",
      "        [-3.2903, -3.9553, -1.0125,  ..., -4.9614, -4.8633, -4.8146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9354, -0.8168,  0.9414, -0.7760,  0.9712, -1.0325, -1.1470,  0.7680],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8548,  0.9900, -1.2131,  0.9900, -0.8379,  0.9900, -1.6865,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5845e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1729, -0.1747, -0.1764,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1987, -0.2007, -0.2027,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3446, -0.3481, -0.3516,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8338,  0.9656, -1.1599,  0.9656, -0.8173,  0.9656, -1.6531,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9422, -4.0110, -0.9591,  ..., -4.2724, -4.2534, -4.2460],\n",
      "        [-3.2099, -3.9882, -0.9908,  ..., -5.4015, -5.4367, -5.4084],\n",
      "        [-2.9244, -3.9785, -0.9407,  ..., -4.5808, -4.5825, -4.5898],\n",
      "        ...,\n",
      "        [-3.2130, -3.9932, -0.9905,  ..., -5.3833, -5.4006, -5.3825],\n",
      "        [-2.8399, -4.0195, -0.9227,  ..., -4.2657, -4.2658, -4.2648],\n",
      "        [-3.2156, -3.9951, -0.9871,  ..., -5.4265, -5.4551, -5.4373]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8019,  0.9002, -1.1288,  0.9233, -0.8064,  0.9488, -1.0374,  0.9651],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.7238, -0.7238, -0.8814, -0.8598,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.1933e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1783, -0.1801, -0.1819,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1739, -0.1757, -0.1775,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.7059, -0.7059, -0.8597, -0.8386,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2961, -3.9535, -0.9965,  ..., -4.9298, -4.8861, -4.8617],\n",
      "        [-3.2961, -3.9535, -0.9965,  ..., -4.9298, -4.8861, -4.8617],\n",
      "        [-3.2992, -3.9531, -0.9962,  ..., -4.9012, -4.8575, -4.8367],\n",
      "        ...,\n",
      "        [-2.9277, -4.0082, -0.9510,  ..., -4.2899, -4.2748, -4.2667],\n",
      "        [-2.9228, -4.0072, -0.9484,  ..., -4.2630, -4.2406, -4.2311],\n",
      "        [-3.2198, -3.9919, -0.9818,  ..., -5.4157, -5.4392, -5.4151]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7375,  0.7375,  0.8086, -0.6527, -0.6527, -0.8010, -0.7474,  0.9721],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8857, -0.8306,  0.9900,  0.9900,  0.9900, -0.8468,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8611e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0593e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1792, -0.1810, -0.1828,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1680, -0.1697, -0.1714,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1713, -0.1730, -0.1748,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8639, -0.8101,  0.9656,  0.9656,  0.9656, -0.8259,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1773, -3.9859, -0.9592,  ..., -5.3490, -5.3466, -5.3227],\n",
      "        [-2.9251, -4.0095, -0.9462,  ..., -4.2913, -4.2761, -4.2671],\n",
      "        [-2.9366, -4.0151, -0.9414,  ..., -4.2903, -4.2758, -4.2686],\n",
      "        ...,\n",
      "        [-3.2155, -3.9929, -0.9765,  ..., -5.4442, -5.4700, -5.4431],\n",
      "        [-2.9271, -4.0058, -0.9426,  ..., -4.3260, -4.3059, -4.2950],\n",
      "        [-3.2155, -3.9929, -0.9765,  ..., -5.4442, -5.4700, -5.4431]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9232, -0.7972, -0.7706,  0.7398,  0.9423,  0.8900, -0.7944,  0.8900],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7396, -0.8043], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.7239e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1274, -0.1287, -0.1300, -0.1313, -0.1326, -0.1340, -0.1353, -0.1367,\n",
      "         -0.1381, -0.1395, -0.1409, -0.1423, -0.1437, -0.1452, -0.1467, -0.1481,\n",
      "         -0.1496, -0.1511, -0.1527, -0.1542, -0.1558, -0.1573, -0.1589, -0.1605,\n",
      "         -0.1622, -0.1638, -0.1654, -0.1671, -0.1688, -0.1705, -0.1722, -0.1740,\n",
      "         -0.1757, -0.1775, -0.1793, -0.1811, -0.1829, -0.1848, -0.1867, -0.1885,\n",
      "         -0.1904, -0.1924, -0.1943, -0.1963, -0.1983, -0.2003, -0.2023, -0.2043,\n",
      "         -0.2064, -0.2085, -0.2106, -0.2127, -0.2149, -0.2170, -0.2192, -0.2214,\n",
      "         -0.2237, -0.2259, -0.2282, -0.2305, -0.2328, -0.2352, -0.2376, -0.2400,\n",
      "         -0.2424, -0.2448, -0.2473, -0.2498, -0.2523, -0.2549, -0.2575, -0.2601,\n",
      "         -0.2627, -0.2653, -0.2680, -0.2707, -0.2735, -0.2762, -0.2790, -0.2818,\n",
      "         -0.2847, -0.2876, -0.2905, -0.2934, -0.2964, -0.2994, -0.3024, -0.3054,\n",
      "         -0.3085, -0.3116, -0.3148, -0.3180, -0.3212, -0.3244, -0.3277, -0.3310,\n",
      "         -0.3343, -0.3377, -0.3411, -0.3446, -0.3481, -0.3516, -0.3551, -0.3587,\n",
      "         -0.3623, -0.3660, -0.3697, -0.3734, -0.3772, -0.3810, -0.3849, -0.3888,\n",
      "         -0.3927, -0.3966, -0.4007, -0.4047, -0.4088, -0.4129, -0.4171, -0.4213,\n",
      "         -0.4256, -0.4299, -0.4342, -0.4386, -0.4430, -0.4475, -0.4520, -0.4566,\n",
      "         -0.4612, -0.4658, -0.4705, -0.4753, -0.4801, -0.4850, -0.4898, -0.4948,\n",
      "         -0.4998, -0.5048, -0.5099, -0.5151, -0.5203, -0.5256, -0.5309, -0.5362,\n",
      "         -0.5416, -0.5471, -0.5526, -0.5582, -0.5639, -0.5696, -0.5753, -0.5811,\n",
      "         -0.5870, -0.5929, -0.5989, -0.6050, -0.6111, -0.6172, -0.6235, -0.6298,\n",
      "         -0.6361, -0.6426, -0.6490, -0.6556, -0.6622, -0.6689, -0.6757, -0.6825,\n",
      "         -0.6894, -0.6964, -0.7034, -0.7105, -0.7177, -0.7249, -0.7322, -0.7396,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1627, -0.1644, -0.1660, -0.1677, -0.1694, -0.1711, -0.1728, -0.1746,\n",
      "         -0.1763, -0.1781, -0.1799, -0.1817, -0.1836, -0.1854, -0.1873, -0.1892,\n",
      "         -0.1911, -0.1930, -0.1950, -0.1970, -0.1989, -0.2010, -0.2030, -0.2050,\n",
      "         -0.2071, -0.2092, -0.2113, -0.2134, -0.2156, -0.2178, -0.2200, -0.2222,\n",
      "         -0.2244, -0.2267, -0.2290, -0.2313, -0.2337, -0.2360, -0.2384, -0.2408,\n",
      "         -0.2432, -0.2457, -0.2482, -0.2507, -0.2532, -0.2558, -0.2584, -0.2610,\n",
      "         -0.2636, -0.2663, -0.2690, -0.2717, -0.2744, -0.2772, -0.2800, -0.2828,\n",
      "         -0.2857, -0.2886, -0.2915, -0.2944, -0.2974, -0.3004, -0.3034, -0.3065,\n",
      "         -0.3096, -0.3127, -0.3159, -0.3191, -0.3223, -0.3255, -0.3288, -0.3322,\n",
      "         -0.3355, -0.3389, -0.3423, -0.3458, -0.3493, -0.3528, -0.3564, -0.3600,\n",
      "         -0.3636, -0.3673, -0.3710, -0.3747, -0.3785, -0.3823, -0.3862, -0.3901,\n",
      "         -0.3940, -0.3980, -0.4020, -0.4061, -0.4102, -0.4143, -0.4185, -0.4228,\n",
      "         -0.4270, -0.4313, -0.4357, -0.4401, -0.4445, -0.4490, -0.4536, -0.4582,\n",
      "         -0.4628, -0.4675, -0.4722, -0.4769, -0.4818, -0.4866, -0.4915, -0.4965,\n",
      "         -0.5015, -0.5066, -0.5117, -0.5169, -0.5221, -0.5274, -0.5327, -0.5381,\n",
      "         -0.5435, -0.5490, -0.5546, -0.5602, -0.5658, -0.5715, -0.5773, -0.5831,\n",
      "         -0.5890, -0.5950, -0.6010, -0.6071, -0.6132, -0.6194, -0.6256, -0.6320,\n",
      "         -0.6383, -0.6448, -0.6513, -0.6579, -0.6645, -0.6712, -0.6780, -0.6849,\n",
      "         -0.6918, -0.6988, -0.7058, -0.7130, -0.7202, -0.7274, -0.7348, -0.7422,\n",
      "         -0.7497, -0.7573, -0.7649, -0.7726, -0.7805, -0.7883, -0.7963, -0.8043,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7214, -0.7845], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9748, -3.9945, -0.9359, -2.2231, -0.0352, -2.3749, -4.4604, -0.5918,\n",
      "         -4.2481, -2.6708, -3.9314,  0.2394, -0.4515, -3.7206, -3.1741, -2.5333,\n",
      "         -3.3792, -1.8625, -2.1865, -1.0245, -1.3742, -1.5322, -0.9567,  0.2663,\n",
      "         -2.0649, -3.5326, -3.1496, -2.7114, -3.9537,  0.8841, -1.9205, -0.2580,\n",
      "         -0.9335, -3.6070, -1.8644, -1.9561, -0.9623, -3.3233, -1.8043, -0.1940,\n",
      "         -0.8760, -0.6450, -0.5002, -4.1119, -1.7607, -3.6468, -3.6311, -3.3213,\n",
      "          0.2766,  0.2943, -3.9317, -1.5871, -6.9908, -3.0275, -3.2228, -1.0835,\n",
      "         -2.6022, -3.1474, -2.5579, -1.8054, -3.6931, -1.3059, -5.7718, -2.5566,\n",
      "         -3.7095,  0.9788, -1.5636, -0.1477, -0.4260, -3.2447, -4.1388, -5.3537,\n",
      "         -2.8916, -0.2748, -3.8501, -4.1017, -2.5955, -4.0294, -2.5997, -3.5971,\n",
      "         -4.7186, -2.6237, -0.5378, -1.3278, -1.6931, -3.0559, -2.0048, -0.7333,\n",
      "         -3.5678, -1.0835, -3.2639, -3.9930, -2.5794, -0.7835, -0.9541, -2.9307,\n",
      "         -2.8920, -3.9669, -3.3093, -3.2629, -0.3145, -1.0231, -3.7226, -2.1788,\n",
      "         -0.5548, -2.0442, -3.8246, -3.7108,  0.3911, -1.4125, -1.5961, -0.9134,\n",
      "         -4.6653, -4.6849,  0.2066, -0.9827, -2.0728, -1.3594,  0.0684, -1.1386,\n",
      "         -3.0794, -4.5369, -2.8204, -0.9850, -1.0704, -2.6965, -2.4683, -0.9077,\n",
      "         -0.5970, -4.2350, -0.1230,  0.3562, -3.4894, -3.9754, -1.3653, -2.0687,\n",
      "         -3.1929, -2.3489, -1.9140, -3.1921, -3.1709, -5.7204, -1.7532, -2.4418,\n",
      "         -1.3447, -1.1225, -0.7189, -4.3484, -3.9356, -0.2568, -2.0887, -0.5789,\n",
      "         -1.3328, -0.4431, -0.2799, -0.8410, -4.2971, -3.5538, -4.9261, -1.3280,\n",
      "         -0.7507, -0.6872, -0.6823, -0.7000, -0.5656, -0.7336, -4.9895, -4.9877,\n",
      "         -5.2653, -4.8337, -5.0836, -5.0100, -4.8933, -4.9676, -5.0039, -5.1186,\n",
      "         -5.2620, -5.3764, -5.2868, -5.0930, -5.0013, -4.9716, -4.9397, -4.9114,\n",
      "         -4.8091, -4.6818, -4.6134, -4.6135, -4.6496, -4.6816, -4.6814, -4.6682,\n",
      "         -4.6699, -4.7095, -4.7621, -4.8108, -4.8240, -4.8071, -4.7775, -4.7563,\n",
      "         -4.7653, -4.7913, -4.8120, -4.8245, -4.8253, -4.8060, -4.7860, -4.7541,\n",
      "         -4.7108, -4.6759, -4.6715, -4.6777, -4.6824, -4.7006, -4.7304, -4.7339,\n",
      "         -4.7309, -4.7299, -4.7122, -4.6838, -4.6705, -4.6412],\n",
      "        [-2.9259, -4.0144, -0.9487, -2.1827, -0.0653, -2.3877, -4.4549, -0.5830,\n",
      "         -4.2153, -2.6917, -3.9555,  0.2509, -0.4089, -3.7122, -3.1464, -2.5165,\n",
      "         -3.3606, -1.8977, -2.1629, -1.0288, -1.3772, -1.5394, -0.9556,  0.2844,\n",
      "         -2.0601, -3.5385, -3.1416, -2.7007, -3.9367,  0.8988, -1.9003, -0.2569,\n",
      "         -0.9222, -3.6218, -1.8526, -1.9526, -0.9056, -3.3057, -1.7949, -0.1833,\n",
      "         -0.8996, -0.5932, -0.4984, -4.0820, -1.7534, -3.6232, -3.6286, -3.3312,\n",
      "          0.2847,  0.2833, -3.9436, -1.5931, -6.9776, -3.0294, -3.2154, -1.1033,\n",
      "         -2.5864, -3.1168, -2.5717, -1.8036, -3.7059, -1.3230, -5.7652, -2.5689,\n",
      "         -3.6966,  0.9856, -1.5436, -0.1493, -0.4208, -3.2593, -4.1666, -5.3879,\n",
      "         -2.9037, -0.2803, -3.8698, -4.0886, -2.6083, -4.0255, -2.5986, -3.6158,\n",
      "         -4.7576, -2.6397, -0.5453, -1.3378, -1.7309, -3.0519, -1.9995, -0.7429,\n",
      "         -3.6097, -1.1051, -3.2890, -3.9851, -2.5957, -0.7925, -0.9694, -2.9267,\n",
      "         -2.9029, -3.9563, -3.3260, -3.2501, -0.3264, -1.0382, -3.7020, -2.1864,\n",
      "         -0.5430, -2.0511, -3.8207, -3.7227,  0.3807, -0.8579, -1.5361, -0.4318,\n",
      "         -4.7258, -4.6455,  0.1607, -0.9774, -2.0973, -1.4150,  0.0399, -1.1768,\n",
      "         -3.0805, -4.4791, -2.8366, -1.0378, -1.1562, -2.7845, -2.4962, -0.8961,\n",
      "         -0.6321, -4.2905, -0.0732,  0.3435, -3.4916, -3.9652, -1.3706, -2.0571,\n",
      "         -3.1684, -2.3777, -1.9257, -3.1919, -2.8980, -3.5855, -5.6485, -1.5964,\n",
      "         -0.7585, -0.7949, -0.7420, -0.8689, -0.7907, -0.8396, -4.9757, -5.0061,\n",
      "         -5.3747, -4.9720, -4.8687, -4.5671, -4.4739, -4.6021, -4.6560, -4.6159,\n",
      "         -4.7198, -4.8230, -4.8855, -4.8909, -4.8359, -4.7492, -4.6993, -4.6654,\n",
      "         -4.6468, -4.6246, -4.6040, -4.6153, -4.6368, -4.6477, -4.6759, -4.7086,\n",
      "         -4.7126, -4.7149, -4.7247, -4.7086, -4.6841, -4.6939, -4.6959, -4.6835,\n",
      "         -4.6733, -4.6701, -4.6923, -4.7134, -4.7286, -4.7378, -4.7444, -4.7670,\n",
      "         -4.7953, -4.8221, -4.8288, -4.8351, -4.8424, -4.8414, -4.8305, -4.8228,\n",
      "         -4.8186, -4.8152, -4.8057, -4.7898, -4.7777, -4.7750, -4.7764, -4.7853,\n",
      "         -4.7738, -4.7438, -4.7415, -4.6868, -4.6427, -4.5917, -4.5447, -4.4381,\n",
      "         -4.3722, -4.3749, -4.3555, -4.3253, -4.3115, -4.3042]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6866, -0.7991], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8300,  0.9900,  0.9900, -0.8528, -0.7965,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8872e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1679, -0.1696, -0.1713,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1725, -0.1743, -0.1760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1611, -0.1628, -0.1644,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8095,  0.9656,  0.9656, -0.8318, -0.7769,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2066, -4.0016, -0.9811,  ..., -5.3810, -5.4030, -5.3803],\n",
      "        [-3.1743, -3.9945, -0.9615,  ..., -5.3446, -5.3428, -5.3232],\n",
      "        [-2.9223, -4.0192, -0.9516,  ..., -4.3084, -4.2913, -4.2833],\n",
      "        ...,\n",
      "        [-2.9164, -4.0144, -0.9469,  ..., -4.2573, -4.2377, -4.2287],\n",
      "        [-2.9305, -4.0176, -0.9513,  ..., -4.3312, -4.3131, -4.3079],\n",
      "        [-3.2052, -3.9965, -0.9830,  ..., -5.4019, -5.4344, -5.4071]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9514,  0.9210, -0.7627,  0.7370,  0.9333, -0.7520, -0.7692,  0.8905],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8397, -1.6706, -0.8293, -0.8084,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5997e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1699, -0.1716, -0.1733,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3414, -0.3448, -0.3483,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1678, -0.1695, -0.1712,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8190, -1.6375, -0.8089, -0.7884,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9341, -4.0220, -0.9516,  ..., -4.2668, -4.2508, -4.2423],\n",
      "        [-2.8289, -4.0313, -0.9131,  ..., -4.2559, -4.2573, -4.2556],\n",
      "        [-2.9292, -4.0176, -0.9473,  ..., -4.3035, -4.2859, -4.2762],\n",
      "        ...,\n",
      "        [-3.2203, -4.0093, -0.9870,  ..., -5.4789, -5.5076, -5.4837],\n",
      "        [-3.2131, -4.0062, -0.9796,  ..., -5.4294, -5.4553, -5.4374],\n",
      "        [-3.2131, -4.0062, -0.9796,  ..., -5.4294, -5.4553, -5.4374]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8076, -1.0436, -0.8133, -0.7824,  0.8930,  0.9288,  0.9631,  0.9631],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.7748, -1.1966, -0.8330, -0.8171,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2603e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.1244e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1685, -0.1702, -0.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1653, -0.1670, -0.1686,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.7557, -1.1441, -0.8124, -0.7969,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2127, -4.0070, -0.9811,  ..., -5.3782, -5.3962, -5.3679],\n",
      "        [-3.1818, -4.0132, -0.9753,  ..., -5.3609, -5.3694, -5.3582],\n",
      "        [-3.2906, -3.9667, -0.9958,  ..., -4.9055, -4.8629, -4.8429],\n",
      "        ...,\n",
      "        [-2.9273, -4.0257, -0.9511,  ..., -4.2659, -4.2476, -4.2386],\n",
      "        [-2.9223, -4.0186, -0.9473,  ..., -4.3153, -4.2955, -4.2854],\n",
      "        [-3.2125, -4.0054, -0.9793,  ..., -5.4447, -5.4709, -5.4379]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8917,  0.9175,  0.7276, -0.8070, -1.1089, -0.8107, -0.8049,  0.8883],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1425,  0.9900, -0.7020,  0.9900, -0.8580,  0.9900,  0.9900, -0.7778],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4044e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1578, -0.1594, -0.1610,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1209, -0.1221, -0.1234,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1573, -0.1589, -0.1605,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0925,  0.9656, -0.6847,  0.9656, -0.8369,  0.9656,  0.9656, -0.7586],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9558, -3.9832, -0.9347,  ..., -4.2763, -4.2948, -4.3118],\n",
      "        [-3.1761, -4.0168, -0.9765,  ..., -5.3604, -5.3689, -5.3607],\n",
      "        [-2.9604, -4.0113, -0.9304,  ..., -4.6272, -4.6143, -4.5878],\n",
      "        ...,\n",
      "        [-3.2874, -3.9713, -0.9969,  ..., -4.8766, -4.8340, -4.8186],\n",
      "        [-3.2073, -4.0088, -0.9809,  ..., -5.4226, -5.4463, -5.4201],\n",
      "        [-2.9225, -4.0264, -0.9537,  ..., -4.3233, -4.3093, -4.3025]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1207,  0.9103, -0.6917,  0.9599, -0.8170,  0.7934,  0.9599, -0.7794],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1000,  0.9900,  0.9900, -0.7886,  0.9900,  0.9900, -0.8656],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9611e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1751, -0.1769, -0.1787,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.0518,  0.9608,  0.9656, -0.7692,  0.9656,  0.9656, -0.8443],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1679, -4.0120, -0.9811,  ..., -5.2398, -5.2551, -5.2448],\n",
      "        [-2.9976, -3.9718, -0.9527,  ..., -0.9751, -1.0028, -1.2313],\n",
      "        [-3.2748, -3.9763, -0.9997,  ..., -4.9588, -4.8620, -4.8115],\n",
      "        ...,\n",
      "        [-3.1600, -4.0058, -0.9636,  ..., -5.3380, -5.3402, -5.3213],\n",
      "        [-3.2074, -4.0175, -0.9883,  ..., -5.4866, -5.5161, -5.4909],\n",
      "        [-2.9084, -4.0275, -0.9519,  ..., -4.2752, -4.2594, -4.2527]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9009, -1.0319,  0.7744,  0.9542, -0.7810,  0.9094,  0.9079, -0.8201],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8392,  0.9900, -0.8376,  0.9900,  0.9900, -0.7339, -0.7195],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2654e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1698, -0.1715, -0.1732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1264, -0.1277, -0.1290,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1239, -0.1252, -0.1264,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8185,  0.9656, -0.8169,  0.9656,  0.9656, -0.7158, -0.7017],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2789, -3.9754, -0.9961,  ..., -4.9141, -4.8674, -4.8448],\n",
      "        [-2.9084, -4.0287, -0.9490,  ..., -4.2581, -4.2464, -4.2414],\n",
      "        [-3.2810, -3.9761, -0.9960,  ..., -4.8853, -4.8429, -4.8224],\n",
      "        ...,\n",
      "        [-3.1551, -4.0073, -0.9633,  ..., -5.3408, -5.3461, -5.3252],\n",
      "        [-2.9467, -4.0116, -0.9380,  ..., -4.6854, -4.6722, -4.6442],\n",
      "        [-2.9501, -4.0164, -0.9311,  ..., -4.6249, -4.6125, -4.5809]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7122, -0.7996,  0.7889, -0.8159,  0.9035,  0.9109, -0.7052, -0.7012],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.7597e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1865, -4.0156, -0.9808, -2.3665, -0.1086, -2.4421, -4.5441, -0.8008,\n",
      "         -4.4502, -2.8281, -4.0077,  0.2600, -0.5140, -3.7779, -3.1088, -2.6698,\n",
      "         -3.4282, -1.9443, -2.1213, -1.1556, -1.4438, -1.6928, -1.0657,  0.3458,\n",
      "         -2.2245, -3.6064, -3.1165, -2.8244, -3.9284,  0.9205, -1.9950, -0.4045,\n",
      "         -0.8948, -3.6680, -1.8721, -1.8238, -0.8548, -3.3452, -1.9022, -0.3032,\n",
      "         -0.9073, -0.5488, -0.5246, -4.0663, -1.8901, -3.8287, -3.7979, -3.3794,\n",
      "          0.6643,  0.5310, -3.9779, -1.6645, -7.0076, -3.1175, -3.2356, -1.1821,\n",
      "         -2.5841, -3.1385, -2.6364, -1.9405, -3.7618, -1.3839, -5.8099, -2.6233,\n",
      "         -3.6958,  1.0210, -1.6286, -0.3016, -0.3696, -3.3076, -4.2469, -5.3968,\n",
      "         -2.9814, -0.2138, -3.9060, -4.1072, -2.6849, -4.0436, -2.6891, -3.6455,\n",
      "         -4.7578, -2.7056, -0.5645, -1.4745, -1.7953, -3.1771, -2.0919, -0.8854,\n",
      "         -3.6804, -1.1405, -3.3276, -4.0070, -2.6589, -0.7982, -1.1104, -2.8607,\n",
      "         -3.0275, -4.1291, -3.3922, -3.3423, -0.3622, -1.1731, -3.7055, -2.2181,\n",
      "         -0.4900, -2.2083, -3.9732, -3.8250,  0.4480, -1.1144, -1.3105, -0.8769,\n",
      "         -5.1327, -4.7816,  0.1103, -1.1191, -2.1453, -1.5001, -0.0541, -1.2959,\n",
      "         -3.2979, -4.5237, -2.8553, -1.0189, -1.2565, -2.8765, -2.5768, -0.9689,\n",
      "         -0.7508, -4.3008,  0.3138,  0.6384, -3.5446, -3.9980, -1.4250, -2.0517,\n",
      "         -3.1606, -2.4308, -2.0002, -3.1497, -3.2523, -5.7465, -1.9297, -2.3074,\n",
      "         -1.2537, -0.9259, -1.0305, -4.5615, -3.9790, -0.2184, -2.1063, -0.4518,\n",
      "         -0.9845, -0.4250, -0.2291, -0.6021, -4.4454, -3.6446, -4.9066, -1.5308,\n",
      "          0.9742,  0.9258,  0.9821,  0.9214,  0.8778,  1.0259, -4.7854, -4.9219,\n",
      "         -5.2435, -4.9994, -5.2168, -5.2841, -5.4032, -5.3952, -5.3232, -5.3166,\n",
      "         -5.3096, -5.2315, -5.1038, -4.9983, -4.9495, -4.9413, -4.9570, -4.9801,\n",
      "         -4.9748, -4.9614, -4.9658, -4.9939, -5.0543, -5.1284, -5.1854, -5.2063,\n",
      "         -5.2120, -5.2373, -5.3018, -5.3949, -5.4440, -5.4410, -5.4001, -5.3387,\n",
      "         -5.3057, -5.2752, -5.2677, -5.2791, -5.3026, -5.3453, -5.3800, -5.3960,\n",
      "         -5.3856, -5.3502, -5.3416, -5.3375, -5.3531, -5.3906, -5.4103, -5.4199,\n",
      "         -5.4137, -5.3987, -5.4008, -5.4047, -5.4236, -5.4033],\n",
      "        [-3.1944, -4.0172, -0.9816, -2.3730, -0.1103, -2.4397, -4.5495, -0.8157,\n",
      "         -4.4571, -2.8315, -4.0130,  0.2503, -0.5165, -3.7813, -3.1160, -2.6805,\n",
      "         -3.4269, -1.9458, -2.1304, -1.1571, -1.4489, -1.6977, -1.0697,  0.3411,\n",
      "         -2.2285, -3.6128, -3.1231, -2.8359, -3.9297,  0.9254, -1.9965, -0.4059,\n",
      "         -0.8933, -3.6727, -1.8731, -1.8236, -0.8492, -3.3420, -1.9085, -0.3067,\n",
      "         -0.9044, -0.5448, -0.5251, -4.0609, -1.8928, -3.8383, -3.8012, -3.3840,\n",
      "          0.6808,  0.5410, -3.9815, -1.6657, -7.0081, -3.1261, -3.2308, -1.1852,\n",
      "         -2.5867, -3.1369, -2.6428, -1.9442, -3.7658, -1.3833, -5.8110, -2.6333,\n",
      "         -3.6961,  1.0272, -1.6273, -0.3031, -0.3671, -3.3123, -4.2594, -5.4056,\n",
      "         -2.9909, -0.2137, -3.9109, -4.1087, -2.6936, -4.0403, -2.6916, -3.6500,\n",
      "         -4.7673, -2.7143, -0.5675, -1.4762, -1.8057, -3.1831, -2.0928, -0.8843,\n",
      "         -3.6844, -1.1370, -3.3349, -4.0075, -2.6680, -0.7988, -1.1111, -2.8583,\n",
      "         -3.0293, -4.1357, -3.3922, -3.3420, -0.3653, -1.1740, -3.7083, -2.2171,\n",
      "         -0.4938, -2.2171, -3.9804, -3.8312,  0.4468, -1.2798, -1.5145, -1.0473,\n",
      "         -4.5315, -4.6417,  0.1815, -1.1931, -2.1447, -1.5530, -0.0622, -1.3008,\n",
      "         -3.2651, -4.4808, -2.8857, -1.0261, -1.2563, -2.8568, -2.5939, -0.9782,\n",
      "         -0.7671, -4.3244,  0.3071,  0.6374, -3.5035, -4.0124, -1.4478, -2.0611,\n",
      "         -3.1716, -2.4328, -2.0046, -3.1713, -3.2578, -5.7268, -1.9090, -2.3183,\n",
      "         -1.2958, -1.0332, -0.6455, -4.1831, -3.9274, -0.2177, -2.1706, -0.1610,\n",
      "         -1.0583, -0.5090,  0.1187, -0.6046, -4.4273, -4.1388, -4.8789, -1.5584,\n",
      "          0.9461,  0.9436,  0.9705,  0.9518,  0.7061,  0.8756, -4.8017, -4.9272,\n",
      "         -5.2445, -5.0050, -5.1927, -5.1995, -5.3744, -5.3526, -5.2628, -5.2792,\n",
      "         -5.2723, -5.1789, -5.0539, -4.9567, -4.9124, -4.9100, -4.9295, -4.9579,\n",
      "         -4.9523, -4.9388, -4.9384, -4.9629, -5.0118, -5.0762, -5.1222, -5.1394,\n",
      "         -5.1464, -5.1705, -5.2417, -5.3341, -5.3840, -5.3877, -5.3548, -5.3001,\n",
      "         -5.2617, -5.2322, -5.2267, -5.2412, -5.2674, -5.3126, -5.3438, -5.3553,\n",
      "         -5.3430, -5.3059, -5.2965, -5.2985, -5.3178, -5.3523, -5.3691, -5.3737,\n",
      "         -5.3711, -5.3670, -5.3707, -5.3861, -5.3974, -5.3740]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9512, 0.8989], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.1000, -0.8747,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6092e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.0518, -0.8531,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1834, -4.0160, -0.9788,  ..., -5.4014, -5.4203, -5.3993],\n",
      "        [-3.1619, -4.0232, -0.9723,  ..., -5.3601, -5.3686, -5.3588],\n",
      "        [-2.9865, -3.9757, -0.9489,  ..., -0.9822, -1.0098, -1.2154],\n",
      "        ...,\n",
      "        [-3.1498, -4.0088, -0.9610,  ..., -5.3414, -5.3458, -5.3277],\n",
      "        [-3.1859, -4.0188, -0.9763,  ..., -5.4447, -5.4712, -5.4486],\n",
      "        [-3.1549, -4.0133, -0.9785,  ..., -5.2273, -5.2440, -5.2334]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9613,  0.9142, -1.0357, -0.7700,  0.9105,  0.9257,  0.9698,  0.9187],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8230, -0.8969, -0.8630,  0.9900, -0.8732,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2736e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1665, -0.1682, -0.1699,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1814, -0.1833, -0.1851,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1746, -0.1763, -0.1781,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8027, -0.8748, -0.8417,  0.9656, -0.8516,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8910, -4.0324, -0.9447,  ..., -4.3125, -4.2972, -4.2903],\n",
      "        [-2.8849, -4.0310, -0.9433,  ..., -4.2728, -4.2563, -4.2485],\n",
      "        [-2.8910, -4.0307, -0.9392,  ..., -4.2508, -4.2365, -4.2318],\n",
      "        ...,\n",
      "        [-3.1853, -4.0182, -0.9719,  ..., -5.4386, -5.4640, -5.4415],\n",
      "        [-3.2709, -3.9762, -0.9906,  ..., -4.8643, -4.8190, -4.7952],\n",
      "        [-3.1894, -4.0139, -0.9666,  ..., -5.4306, -5.4564, -5.4395]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7829, -0.8176, -0.7948,  0.7311, -0.8210,  0.9778,  0.8089,  0.9489],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7302, -0.8416, -0.8604,  0.9900, -0.8961, -1.6979, -1.1609],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.4802e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1258, -0.1270, -0.1283,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1703, -0.1720, -0.1737,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1813, -0.1831, -0.1850,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3470, -0.3505, -0.3540,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1603, -0.1619, -0.1635,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7122, -0.8209, -0.8392,  0.9656, -0.8740, -1.6643, -1.1100],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1891, -4.0136, -0.9693,  ..., -5.4384, -5.4583, -5.4291],\n",
      "        [-2.9343, -4.0154, -0.9223,  ..., -4.5964, -4.5788, -4.5484],\n",
      "        [-2.8913, -4.0364, -0.9350,  ..., -4.2592, -4.2451, -4.2393],\n",
      "        ...,\n",
      "        [-2.8813, -4.0304, -0.9390,  ..., -4.2712, -4.2563, -4.2467],\n",
      "        [-2.7893, -4.0432, -0.9017,  ..., -4.2431, -4.2410, -4.2397],\n",
      "        [-2.9346, -3.9884, -0.9257,  ..., -4.2600, -4.2787, -4.2975]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9261, -0.7042, -0.7914, -0.8070,  0.9905, -0.8168, -1.0258, -1.1094],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8555,  0.9900,  0.9900,  0.9900, -0.8199,  0.9900, -0.8207, -1.2165],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2396e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1731, -0.1748, -0.1766,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1660, -0.1677, -0.1694,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1993, -0.2013, -0.2033,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8344,  0.9656,  0.9656,  0.9656, -0.7997,  0.9656, -0.8005, -1.1632],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8743, -4.0351, -0.9395,  ..., -4.2848, -4.2700, -4.2619],\n",
      "        [-3.1398, -4.0076, -0.9496,  ..., -5.3334, -5.3334, -5.3135],\n",
      "        [-3.2597, -3.9818, -0.9864,  ..., -4.8870, -4.8371, -4.8114],\n",
      "        ...,\n",
      "        [-3.2661, -3.9781, -0.9821,  ..., -4.8420, -4.7960, -4.7725],\n",
      "        [-2.8770, -4.0285, -0.9377,  ..., -4.2971, -4.2795, -4.2709],\n",
      "        [-2.8712, -4.0022, -0.9193,  ..., -4.5270, -4.5266, -4.5379]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7663,  0.9428,  0.7228,  0.9270, -0.8087,  0.8150, -0.7749, -1.0942],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8138,  0.9900,  0.9900, -0.8590,  0.9900,  0.9900,  0.9900, -0.8655],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9162e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.9424e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1646, -0.1663, -0.1680,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1751, -0.1769, -0.1786,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7937,  0.9608,  0.9656, -0.8378,  0.9656,  0.9656,  0.9656, -0.8441],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8791, -4.0334, -0.9326,  ..., -4.3076, -4.2911, -4.2822],\n",
      "        [-3.2522, -3.9808, -0.9828,  ..., -4.9168, -4.8207, -4.7669],\n",
      "        [-3.2612, -3.9787, -0.9795,  ..., -4.8656, -4.8172, -4.7916],\n",
      "        ...,\n",
      "        [-3.1862, -4.0198, -0.9681,  ..., -5.4783, -5.5037, -5.4784],\n",
      "        [-3.1463, -4.0142, -0.9617,  ..., -5.2095, -5.2229, -5.2139],\n",
      "        [-2.8786, -4.0356, -0.9315,  ..., -4.2474, -4.2292, -4.2217]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7849,  0.8105,  0.7391, -0.8291,  0.9401,  0.9277,  0.9402, -0.8227],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8482,  0.9900, -0.7318,  0.9900, -0.7185,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1788e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(7.9800e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1716, -0.1733, -0.1751,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1261, -0.1273, -0.1286,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8273,  0.9656, -0.7138,  0.9656, -0.7008,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8685, -4.0293, -0.9299,  ..., -4.2986, -4.2806, -4.2720],\n",
      "        [-3.1718, -4.0124, -0.9639,  ..., -5.4044, -5.4336, -5.4005],\n",
      "        [-2.9220, -4.0138, -0.9218,  ..., -4.6572, -4.6393, -4.6117],\n",
      "        ...,\n",
      "        [-3.1821, -4.0149, -0.9627,  ..., -5.4214, -5.4404, -5.4119],\n",
      "        [-3.1331, -4.0094, -0.9457,  ..., -5.3283, -5.3282, -5.3147],\n",
      "        [-3.1748, -4.0201, -0.9615,  ..., -5.4333, -5.4586, -5.4365]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8150,  0.9053, -0.6982,  0.9276, -0.7097,  0.9882,  0.9423,  0.9851],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.8594e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1696, -4.0163, -0.9659, -2.3688, -0.0876, -2.4438, -4.5617, -0.7879,\n",
      "         -4.4321, -2.8271, -4.0082,  0.2597, -0.5304, -3.7677, -3.1185, -2.6592,\n",
      "         -3.4267, -1.9267, -2.1120, -1.1467, -1.4442, -1.6861, -1.0564,  0.3571,\n",
      "         -2.2059, -3.5944, -3.1238, -2.8120, -3.9243,  0.9387, -1.9774, -0.3910,\n",
      "         -0.8850, -3.6594, -1.8653, -1.8228, -0.8394, -3.3389, -1.8826, -0.2862,\n",
      "         -0.8860, -0.5298, -0.5042, -4.0654, -1.8725, -3.8083, -3.7945, -3.3707,\n",
      "          0.6613,  0.6020, -3.9680, -1.6572, -7.0166, -3.1086, -3.2287, -1.1606,\n",
      "         -2.5793, -3.1238, -2.6386, -1.9221, -3.7490, -1.3745, -5.8103, -2.6135,\n",
      "         -3.6896,  1.0419, -1.6078, -0.2874, -0.3557, -3.2948, -4.2430, -5.4124,\n",
      "         -2.9695, -0.2046, -3.8935, -4.1213, -2.6692, -4.0397, -2.6697, -3.6317,\n",
      "         -4.7712, -2.6918, -0.5407, -1.4627, -1.7985, -3.1697, -2.0732, -0.8734,\n",
      "         -3.6927, -1.1219, -3.3163, -4.0183, -2.6453, -0.7759, -1.0977, -2.8558,\n",
      "         -3.0098, -4.1116, -3.3995, -3.3394, -0.3387, -1.1615, -3.7040, -2.2066,\n",
      "         -0.4829, -2.1970, -3.9524, -3.8387,  0.4614, -1.1328, -1.3097, -0.8885,\n",
      "         -5.1194, -4.7873,  0.1356, -1.1061, -2.1440, -1.4908, -0.0546, -1.2753,\n",
      "         -3.2687, -4.5362, -2.8380, -1.0015, -1.2444, -2.8806, -2.5623, -0.9573,\n",
      "         -0.7389, -4.2889,  0.3124,  0.7082, -3.5361, -4.0104, -1.4081, -2.0441,\n",
      "         -3.1483, -2.4328, -1.9802, -3.1291, -3.2335, -5.7601, -1.9231, -2.2961,\n",
      "         -1.2767, -0.9269, -1.0461, -4.5594, -3.9787, -0.2204, -2.1007, -0.4753,\n",
      "         -0.9882, -0.4359, -0.2265, -0.5707, -4.4351, -3.6320, -4.9229, -1.5192,\n",
      "          0.9715,  1.0022,  0.9838,  0.9315,  0.9290,  1.0328, -4.7875, -4.9224,\n",
      "         -5.2457, -5.0005, -5.2142, -5.2848, -5.3968, -5.3901, -5.3275, -5.3206,\n",
      "         -5.3100, -5.2415, -5.1119, -5.0047, -4.9519, -4.9377, -4.9521, -4.9744,\n",
      "         -4.9661, -4.9562, -4.9559, -4.9856, -5.0465, -5.1210, -5.1765, -5.1970,\n",
      "         -5.2039, -5.2288, -5.2928, -5.3821, -5.4312, -5.4292, -5.3914, -5.3331,\n",
      "         -5.2979, -5.2736, -5.2655, -5.2787, -5.3024, -5.3494, -5.3846, -5.3981,\n",
      "         -5.3872, -5.3548, -5.3461, -5.3450, -5.3580, -5.3948, -5.4161, -5.4254,\n",
      "         -5.4166, -5.4045, -5.4053, -5.4096, -5.4249, -5.4010],\n",
      "        [-3.1433, -4.0244, -0.9625, -2.3400, -0.0759, -2.4471, -4.5602, -0.7421,\n",
      "         -4.4019, -2.7891, -3.9966,  0.2558, -0.5310, -3.7282, -3.1292, -2.6362,\n",
      "         -3.4405, -1.9103, -2.1378, -1.1204, -1.4304, -1.6621, -1.0398,  0.3336,\n",
      "         -2.1938, -3.5521, -3.1307, -2.7961, -3.9371,  0.9261, -1.9569, -0.3597,\n",
      "         -0.8872, -3.6545, -1.8733, -1.8340, -0.8389, -3.3612, -1.8559, -0.2836,\n",
      "         -0.8735, -0.5359, -0.5030, -4.0705, -1.8644, -3.7868, -3.7583, -3.3617,\n",
      "          0.5852,  0.5494, -3.9372, -1.6511, -7.0125, -3.0973, -3.2505, -1.1421,\n",
      "         -2.5758, -3.1208, -2.6338, -1.9151, -3.7119, -1.3658, -5.8017, -2.6041,\n",
      "         -3.6983,  1.0287, -1.5877, -0.2571, -0.3617, -3.2923, -4.2140, -5.4084,\n",
      "         -2.9600, -0.2262, -3.8591, -4.1247, -2.6590, -4.0650, -2.6482, -3.5964,\n",
      "         -4.7669, -2.6808, -0.5361, -1.4304, -1.7928, -3.1590, -2.0549, -0.8446,\n",
      "         -3.6818, -1.1171, -3.2781, -4.0192, -2.6332, -0.7709, -1.0684, -2.8585,\n",
      "         -3.0053, -4.0879, -3.3851, -3.3331, -0.3353, -1.1328, -3.7076, -2.2039,\n",
      "         -0.5025, -2.1712, -3.9302, -3.8228,  0.4388, -1.1913, -1.5235, -0.6796,\n",
      "         -4.9232, -4.6477,  0.1420, -1.1185, -2.1609, -1.5067, -0.0306, -1.2796,\n",
      "         -3.1659, -4.5622, -2.8245, -1.0055, -1.2343, -2.8784, -2.5904, -0.9703,\n",
      "         -0.7523, -4.3174,  0.1709,  0.6306, -3.5541, -4.0262, -1.4057, -2.0420,\n",
      "         -3.1649, -2.4328, -1.9892, -3.1156, -3.1943, -5.7778, -1.8841, -2.3124,\n",
      "         -1.1703, -1.1141, -0.5861, -4.4477, -3.9244, -0.2468, -2.1896, -0.2812,\n",
      "         -1.2838, -0.5021,  0.1186, -0.9581, -4.5200, -5.0879, -1.7800, -2.2139,\n",
      "         -0.0225, -0.0885, -0.0183, -4.1656, -3.9408, -0.3699, -1.4482, -4.3724,\n",
      "         -6.1379, -4.9232, -3.2530, -4.8985, -1.5265,  0.9060,  0.9699,  0.9980,\n",
      "          0.9525,  0.6707,  0.9854, -4.8375, -4.9611, -5.2615, -5.0514, -5.2675,\n",
      "         -5.1377, -5.1609, -5.1569, -5.1471, -5.1655, -5.1610, -5.1157, -5.0847,\n",
      "         -5.0654, -5.0656, -5.0768, -5.0916, -5.0874, -5.0802, -5.0862, -5.1094,\n",
      "         -5.1508, -5.1969, -5.2441, -5.2886, -5.3124, -5.3320, -5.3513, -5.3793,\n",
      "         -5.4048, -5.4034, -5.3911, -5.3581, -5.3157, -5.2815, -5.2574, -5.2527,\n",
      "         -5.2780, -5.3024, -5.3311, -5.3505, -5.3594, -5.3531]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9751, 0.9138], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8123, -0.8521, -0.8884,  0.9900,  0.9900,  0.9900,  0.9900, -1.1621],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3923e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1643, -0.1660, -0.1677,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1724, -0.1741, -0.1759,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1797, -0.1815, -0.1834,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1605, -0.1621, -0.1637,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7923, -0.8310, -0.8665,  0.9656,  0.9656,  0.9656,  0.9656, -1.1112],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8746, -4.0354, -0.9392,  ..., -4.3158, -4.3010, -4.2936],\n",
      "        [-2.8673, -4.0296, -0.9356,  ..., -4.3067, -4.2894, -4.2795],\n",
      "        [-2.8661, -4.0326, -0.9378,  ..., -4.2758, -4.2607, -4.2526],\n",
      "        ...,\n",
      "        [-3.1687, -4.0172, -0.9693,  ..., -5.4211, -5.4399, -5.4170],\n",
      "        [-3.1657, -4.0128, -0.9689,  ..., -5.4330, -5.4618, -5.4290],\n",
      "        [-2.9157, -3.9907, -0.9218,  ..., -4.2725, -4.2917, -4.3096]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8106, -0.8339, -0.8469,  0.9092,  0.9135,  0.9680,  0.8934, -1.1184],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8685,  0.9900,  0.9900,  0.9900, -1.2239],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.4253e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2005, -0.2025, -0.2046,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8471,  0.9656,  0.9656,  0.9656, -1.1703],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1731, -4.0172, -0.9714,  ..., -5.4027, -5.4169, -5.3942],\n",
      "        [-3.2523, -3.9778, -0.9855,  ..., -4.9244, -4.8760, -4.8512],\n",
      "        [-3.1760, -4.0144, -0.9709,  ..., -5.4586, -5.4805, -5.4515],\n",
      "        ...,\n",
      "        [-3.2563, -3.9770, -0.9849,  ..., -4.8964, -4.8511, -4.8289],\n",
      "        [-3.1675, -4.0204, -0.9674,  ..., -5.4734, -5.5008, -5.4769],\n",
      "        [-2.8570, -4.0039, -0.9199,  ..., -4.5649, -4.5665, -4.5739]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9193,  0.7188,  0.9689, -0.8572,  0.9627,  0.7643,  0.9680, -1.1084],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8908, -0.7493,  0.9900, -0.8537,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8824e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.4986e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1727, -0.1745, -0.1762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8688, -0.7308,  0.9656, -0.8327,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1428, -4.0227, -0.9683,  ..., -5.3650, -5.3753, -5.3720],\n",
      "        [-3.2514, -3.9800, -0.9888,  ..., -4.9496, -4.9041, -4.8805],\n",
      "        [-3.1359, -4.0132, -0.9718,  ..., -5.2402, -5.2553, -5.2472],\n",
      "        ...,\n",
      "        [-3.1759, -4.0157, -0.9720,  ..., -5.4082, -5.4231, -5.4007],\n",
      "        [-2.8698, -4.0305, -0.9376,  ..., -4.3157, -4.3007, -4.2902],\n",
      "        [-3.1311, -4.0084, -0.9533,  ..., -5.3509, -5.3563, -5.3419]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9064,  0.6955,  0.9207, -0.8587, -0.7082,  0.9185, -0.8442,  0.9252],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8694, -0.8325, -0.8552,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8701e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1730, -0.1747, -0.1765,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8479, -0.8120, -0.8341,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1766, -4.0192, -0.9803,  ..., -5.5144, -5.5409, -5.5173],\n",
      "        [-3.1766, -4.0192, -0.9803,  ..., -5.5144, -5.5409, -5.5173],\n",
      "        [-3.2556, -3.9743, -0.9888,  ..., -4.9084, -4.8647, -4.8415],\n",
      "        ...,\n",
      "        [-2.8694, -4.0306, -0.9375,  ..., -4.3086, -4.2905, -4.2846],\n",
      "        [-3.1749, -4.0109, -0.9657,  ..., -5.4622, -5.4868, -5.4712],\n",
      "        [-3.1283, -4.0061, -0.9564,  ..., -5.3565, -5.3577, -5.3433]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9112,  0.9112,  0.7557, -0.8110, -0.8286, -0.8704,  0.9328,  0.9281],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8057, -0.7260,  0.9900, -0.8479, -0.8586, -0.8407,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9118e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1630, -0.1646, -0.1663,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1251, -0.1263, -0.1276,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1701, -0.1718, -0.1735,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7858, -0.7081,  0.9608, -0.8270, -0.8374, -0.8200,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8723, -4.0296, -0.9436,  ..., -4.3352, -4.3188, -4.3122],\n",
      "        [-2.9153, -4.0134, -0.9222,  ..., -4.6323, -4.6142, -4.5867],\n",
      "        [-3.2429, -3.9735, -0.9940,  ..., -4.9787, -4.8815, -4.8330],\n",
      "        ...,\n",
      "        [-2.8631, -4.0320, -0.9468,  ..., -4.3119, -4.2975, -4.2893],\n",
      "        [-3.1727, -4.0096, -0.9733,  ..., -5.4503, -5.4686, -5.4418],\n",
      "        [-3.1706, -4.0107, -0.9738,  ..., -5.4737, -5.4937, -5.4621]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8225, -0.7149,  0.8189, -0.8367, -0.8642, -0.8037,  0.9720,  0.9108],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7209, -1.6817, -0.8029,  0.9900, -0.7997,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6749e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1242, -0.1254, -0.1267,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3436, -0.3471, -0.3506,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1624, -0.1641, -0.1657,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7031, -1.6484, -0.7831,  0.9656, -0.7799,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9132, -4.0094, -0.9207,  ..., -4.6281, -4.6120, -4.5844],\n",
      "        [-2.7663, -4.0381, -0.9008,  ..., -4.2621, -4.2599, -4.2599],\n",
      "        [-2.8669, -4.0237, -0.9431,  ..., -4.3282, -4.3082, -4.3017],\n",
      "        ...,\n",
      "        [-3.1641, -4.0115, -0.9712,  ..., -5.4573, -5.4833, -5.4622],\n",
      "        [-3.1240, -4.0020, -0.9554,  ..., -5.3458, -5.3504, -5.3327],\n",
      "        [-3.1321, -4.0080, -0.9733,  ..., -5.2429, -5.2594, -5.2496]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7109, -1.0386, -0.8127,  0.9733, -0.8756,  0.9733,  0.9377,  0.9248],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(1.8757e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101, -0.1112, -0.1124, -0.1135, -0.1146, -0.1158,\n",
      "         -0.1170, -0.1181, -0.1193, -0.1205, -0.1218, -0.1230, -0.1242, -0.1255,\n",
      "         -0.1268, -0.1280, -0.1293, -0.1306, -0.1320, -0.1333, -0.1346, -0.1360,\n",
      "         -0.1374, -0.1388, -0.1402, -0.1416, -0.1430, -0.1444, -0.1459, -0.1474,\n",
      "         -0.1489, -0.1504, -0.1519, -0.1534, -0.1550, -0.1565, -0.1581, -0.1597,\n",
      "         -0.1613, -0.1630, -0.1646, -0.1663, -0.1679, -0.1696, -0.1714, -0.1731,\n",
      "         -0.1748, -0.1766, -0.1784, -0.1802, -0.1820, -0.1838, -0.1857, -0.1876,\n",
      "         -0.1895, -0.1914, -0.1933, -0.1953, -0.1972, -0.1992, -0.2013, -0.2033,\n",
      "         -0.2053, -0.2074, -0.2095, -0.2116, -0.2138, -0.2159, -0.2181, -0.2203,\n",
      "         -0.2225, -0.2248, -0.2270, -0.2293, -0.2317, -0.2340, -0.2364, -0.2387,\n",
      "         -0.2412, -0.2436, -0.2461, -0.2485, -0.2511, -0.2536, -0.2562, -0.2587,\n",
      "         -0.2614, -0.2640, -0.2667, -0.2694, -0.2721, -0.2748, -0.2776, -0.2804,\n",
      "         -0.2832, -0.2861, -0.2890, -0.2919, -0.2949, -0.2978, -0.3008, -0.3039,\n",
      "         -0.3069, -0.3100, -0.3132, -0.3163, -0.3195, -0.3228, -0.3260, -0.3293,\n",
      "         -0.3326, -0.3360, -0.3394, -0.3428, -0.3463, -0.3498, -0.3533, -0.3569,\n",
      "         -0.3605, -0.3641, -0.3678, -0.3715, -0.3753, -0.3791, -0.3829, -0.3868,\n",
      "         -0.3907, -0.3946, -0.3986, -0.4026, -0.4067, -0.4108, -0.4150, -0.4192,\n",
      "         -0.4234, -0.4277, -0.4320, -0.4363, -0.4408, -0.4452, -0.4497, -0.4542,\n",
      "         -0.4588, -0.4635, -0.4681, -0.4729, -0.4777, -0.4825, -0.4874, -0.4923,\n",
      "         -0.4972, -0.5023, -0.5073, -0.5125, -0.5176, -0.5229, -0.5282, -0.5335,\n",
      "         -0.5389, -0.5443, -0.5498, -0.5554, -0.5610, -0.5667, -0.5724, -0.5782,\n",
      "         -0.5840, -0.5899, -0.5959, -0.6019, -0.6080, -0.6141, -0.6203, -0.6266,\n",
      "         -0.6329, -0.6393, -0.6457, -0.6523, -0.6589, -0.6655, -0.6722, -0.6790,\n",
      "         -0.6859, -0.6928, -0.6998, -0.7069, -0.7140, -0.7212, -0.7285, -0.7359,\n",
      "         -0.7433, -0.7508, -0.7584, -0.7661, -0.7738, -0.7816, -0.7895, -0.7975,\n",
      "         -0.8055, -0.8137, -0.8219, -0.8302, -0.8386, -0.8470, -0.8556, -0.8642,\n",
      "         -0.8730, -0.8818, -0.8907, -0.8997, -0.9088, -0.9180, -0.9272, -0.9366,\n",
      "         -0.9461, -0.9556, -0.9653, -0.9750, -0.9849, -0.9948, -1.0049, -1.0150,\n",
      "         -1.0253, -1.0356, -1.0461, -1.0567, -1.0673, -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9492, -3.9647, -0.9404, -2.2271, -0.0338, -2.4336, -4.5825, -0.5729,\n",
      "         -4.2373, -2.6640, -3.9935,  0.2445, -0.4708, -3.6964, -3.2154, -2.5816,\n",
      "         -3.3926, -1.8547, -2.1710, -1.0091, -1.3734, -1.5264, -0.9237,  0.2620,\n",
      "         -2.0784, -3.4986, -3.1872, -2.7580, -3.9818,  0.8723, -1.8731, -0.2399,\n",
      "         -0.8692, -3.6565, -1.7795, -1.9546, -0.8867, -3.3290, -1.7654, -0.1019,\n",
      "         -0.8361, -0.5733, -0.4960, -4.1234, -1.7589, -3.6107, -3.6082, -3.3624,\n",
      "          0.3523,  0.3718, -3.9009, -1.6259, -6.9870, -3.0725, -3.2352, -1.0734,\n",
      "         -2.5671, -3.0963, -2.6075, -1.7998, -3.6653, -1.3437, -5.7562, -2.5929,\n",
      "         -3.7348,  0.9647, -1.5148, -0.1389, -0.3656, -3.2832, -4.1483, -5.3671,\n",
      "         -2.9459, -0.2662, -3.8245, -4.1610, -2.6418, -4.0498, -2.5714, -3.5687,\n",
      "         -4.7319, -2.6668, -0.4865, -1.3077, -1.6923, -3.0896, -1.9723, -0.7253,\n",
      "         -3.5890, -1.0521, -3.2409, -4.0548, -2.6209, -0.7376, -0.9335, -2.8989,\n",
      "         -2.9033, -3.9400, -3.2930, -3.2727, -0.2771, -1.0020, -3.7219, -2.1621,\n",
      "         -0.5611, -2.0298, -3.7906, -3.7549,  0.4047, -5.1290, -1.6866, -2.8024,\n",
      "         -4.5382, -4.5810,  0.2949, -0.9381, -2.1041, -1.3709,  0.0199, -1.1354,\n",
      "         -3.0092, -4.6094, -2.7976, -1.0182, -1.0980, -2.7961, -2.4361, -0.9007,\n",
      "         -0.7142, -4.3038,  0.0463,  0.4557, -3.4914, -4.0437, -1.3668, -2.0398,\n",
      "         -3.1722, -2.4074, -1.8799, -3.1541, -3.1488, -5.7533, -1.7177, -6.0823,\n",
      "         -1.7094, -1.4555, -1.4761, -1.1603, -1.4153, -1.1545, -1.3405, -4.3561,\n",
      "         -4.7762, -0.3515, -2.0586, -4.4812, -5.7689, -4.9648, -5.3472, -1.9379,\n",
      "         -5.6348, -0.8543, -0.9987, -1.0735, -0.9717, -1.0491, -0.8892, -1.2413,\n",
      "         -4.1464, -3.8891,  0.0731, -1.6750, -3.6306, -4.6258, -3.7419, -4.8577,\n",
      "         -1.4390, -5.3384, -0.8687, -0.7877, -0.9509, -0.7733, -0.9179, -0.7244,\n",
      "         -1.0562, -4.0155, -3.7478, -0.0404, -1.4208, -3.6111, -4.7237, -3.6109,\n",
      "         -4.9091, -1.3266, -5.2046, -0.9108, -0.7974, -0.9744, -0.8066, -0.9622,\n",
      "         -0.7718, -1.0625, -3.9325, -3.7981, -0.1317, -1.4356, -3.6233, -4.7925,\n",
      "         -3.5450, -3.6037, -4.9814, -1.5208, -0.8684, -1.1036, -1.1165, -1.0884,\n",
      "         -1.1105, -1.0332, -1.0157, -1.0200, -1.0483, -1.2093],\n",
      "        [-3.2426, -3.9688, -0.9869, -2.3909, -0.0759, -2.4550, -4.6532, -0.8663,\n",
      "         -4.5189, -2.8988, -3.9971,  0.2667, -0.5268, -3.8376, -3.1190, -2.7577,\n",
      "         -3.4403, -1.9397, -2.1055, -1.1524, -1.4770, -1.7135, -1.0711,  0.3738,\n",
      "         -2.2467, -3.6591, -3.1339, -2.8982, -3.9379,  0.9394, -1.9845, -0.4146,\n",
      "         -0.8794, -3.6537, -1.8512, -1.8169, -0.7951, -3.3386, -1.9268, -0.3183,\n",
      "         -0.9056, -0.4979, -0.5056, -4.0509, -1.9136, -3.8811, -3.8669, -3.3624,\n",
      "          0.7879,  0.6682, -4.0199, -1.6538, -7.0115, -3.1796, -3.2340, -1.1904,\n",
      "         -2.5805, -3.0928, -2.6805, -1.9552, -3.8064, -1.3819, -5.8152, -2.6780,\n",
      "         -3.7060,  1.0448, -1.6210, -0.3225, -0.3452, -3.2947, -4.3234, -5.4833,\n",
      "         -3.0488, -0.1813, -3.9520, -4.1058, -2.7453, -4.0383, -2.7080, -3.6902,\n",
      "         -4.8443, -2.7694, -0.5651, -1.4894, -1.8633, -3.2239, -2.1027, -0.9003,\n",
      "         -3.7167, -1.1571, -3.3838, -4.0082, -2.7177, -0.7894, -1.1320, -2.8586,\n",
      "         -3.0471, -4.1714, -3.3719, -3.3616, -0.3621, -1.1990, -3.6496, -2.2104,\n",
      "         -0.4707, -2.2421, -4.0268, -3.8799,  0.4807, -2.1187, -1.5402, -2.6046,\n",
      "         -5.0526, -4.7059,  0.1562, -1.1939, -2.1683, -1.5608, -0.1095, -1.3006,\n",
      "         -3.2357, -4.5272, -2.8239, -0.9918, -1.2070, -2.8710, -2.6359, -1.0118,\n",
      "         -0.8477, -4.3057,  0.5405,  0.7585, -3.5148, -4.0772, -1.4271, -2.0448,\n",
      "         -3.1290, -2.4957, -2.0190, -3.1182, -2.8157, -3.7866, -5.6860, -1.7900,\n",
      "          0.7315,  0.6497,  0.9522,  0.8214,  0.1883,  1.0315, -4.8645, -4.9935,\n",
      "         -5.2670, -4.9988, -5.1466, -5.0265, -5.2901, -5.3035, -5.2900, -5.2414,\n",
      "         -5.2135, -5.1942, -5.1820, -5.1602, -5.1256, -5.1128, -5.1275, -5.1508,\n",
      "         -5.1789, -5.2013, -5.2268, -5.2540, -5.3080, -5.3609, -5.4128, -5.4354,\n",
      "         -5.4594, -5.4939, -5.5463, -5.5643, -5.5491, -5.5295, -5.5116, -5.5066,\n",
      "         -5.5196, -5.5332, -5.5534, -5.5723, -5.5879, -5.5991, -5.5976, -5.5969,\n",
      "         -5.5969, -5.6082, -5.6120, -5.6341, -5.6551, -5.6557, -5.6602, -5.6613,\n",
      "         -5.6571, -5.6520, -5.6510, -5.6456, -5.6297, -5.6046, -5.5899, -5.6036,\n",
      "         -5.6425, -5.6663, -5.6758, -5.6572, -5.6428, -5.5465, -5.4569, -5.0851,\n",
      "         -4.9944, -5.0741, -5.0207, -4.9362, -4.8913, -4.8701]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0614,  0.7291], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8389,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(4.1381e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1697, -0.1714, -0.1732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8182,  0.9656,  0.9656,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2399, -3.9645, -0.9842,  ..., -4.9066, -4.8624, -4.8432],\n",
      "        [-3.1489, -4.0030, -0.9689,  ..., -5.4096, -5.4274, -5.4044],\n",
      "        [-2.8548, -4.0213, -0.9336,  ..., -4.2811, -4.2698, -4.2645],\n",
      "        ...,\n",
      "        [-3.2265, -3.9672, -0.9889,  ..., -4.9783, -4.8800, -4.8328],\n",
      "        [-3.1572, -4.0025, -0.9678,  ..., -5.4296, -5.4510, -5.4244],\n",
      "        [-3.1236, -4.0119, -0.9618,  ..., -5.3631, -5.3694, -5.3635]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7577,  0.9637, -0.8270,  0.9253,  0.9200,  0.8341,  0.9737,  0.9176],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1000, -0.8040,  0.9900,  0.9900, -0.7403, -0.8047, -0.8064],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9145e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1627, -0.1643, -0.1660,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1275, -0.1288, -0.1301,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1628, -0.1644, -0.1661,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1631, -0.1648, -0.1664,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.0518, -0.7842,  0.9656,  0.9656, -0.7220, -0.7849, -0.7865],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1508, -3.9991, -0.9621,  ..., -5.4560, -5.4800, -5.4529],\n",
      "        [-2.9429, -3.9574, -0.9341,  ..., -1.0076, -1.0358, -1.1888],\n",
      "        [-2.8485, -4.0172, -0.9339,  ..., -4.3377, -4.3213, -4.3164],\n",
      "        ...,\n",
      "        [-2.8928, -3.9960, -0.9189,  ..., -4.6964, -4.6800, -4.6496],\n",
      "        [-2.8527, -4.0176, -0.9340,  ..., -4.3481, -4.3310, -4.3244],\n",
      "        [-2.8467, -4.0141, -0.9337,  ..., -4.3373, -4.3169, -4.3100]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9198, -1.0498, -0.8636,  0.9498,  0.9764, -0.6866, -0.8065, -0.7973],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8452,  0.9900, -0.8288,  0.9900, -1.1646, -0.8073, -0.8857],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8348e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.7907e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1710, -0.1727, -0.1744,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1608, -0.1624, -0.1641,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1633, -0.1650, -0.1666,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1792, -0.1810, -0.1828,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8243,  0.9656, -0.8084,  0.9656, -1.1136, -0.7874, -0.8638],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1469, -3.9954, -0.9587,  ..., -5.4605, -5.4829, -5.4551],\n",
      "        [-2.8433, -4.0103, -0.9248,  ..., -4.3409, -4.3215, -4.3117],\n",
      "        [-3.1059, -3.9902, -0.9424,  ..., -5.3403, -5.3417, -5.3216],\n",
      "        ...,\n",
      "        [-2.8911, -3.9718, -0.9108,  ..., -4.2921, -4.3094, -4.3261],\n",
      "        [-2.8514, -4.0153, -0.9298,  ..., -4.3510, -4.3325, -4.3256],\n",
      "        [-2.8415, -4.0131, -0.9276,  ..., -4.3051, -4.2883, -4.2804]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9264, -0.8204,  0.9572, -0.8091,  0.9092, -1.1538, -0.7991, -0.8396],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.8401,  0.9900,  0.9900, -0.7292,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5907e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.2567e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1700, -0.1717, -0.1734,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1256, -0.1269, -0.1282,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.8194,  0.9656,  0.9656, -0.7112,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1423, -3.9925, -0.9518,  ..., -5.4326, -5.4594, -5.4453],\n",
      "        [-3.2220, -3.9581, -0.9787,  ..., -4.9636, -4.9234, -4.9042],\n",
      "        [-2.8371, -4.0155, -0.9325,  ..., -4.3258, -4.3087, -4.3036],\n",
      "        ...,\n",
      "        [-2.8890, -3.9969, -0.9078,  ..., -4.6343, -4.6193, -4.5874],\n",
      "        [-3.0986, -3.9883, -0.9436,  ..., -5.3318, -5.3359, -5.3183],\n",
      "        [-3.1058, -3.9928, -0.9601,  ..., -5.2324, -5.2485, -5.2389]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9470,  0.7255, -0.7750,  0.9405,  0.9712, -0.6858,  0.9601,  0.9352],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8460, -1.2194, -0.8554,  0.9900, -0.7276,  0.9900, -1.6845],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8923e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1711, -0.1729, -0.1746,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1997, -0.2018, -0.2038,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1253, -0.1266, -0.1279,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3442, -0.3477, -0.3512,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8251, -1.1659, -0.8343,  0.9656, -0.7096,  0.9656, -1.6511],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.2196, -3.9582, -0.9842,  ..., -4.9534, -4.9143, -4.8907],\n",
      "        [-2.8371, -4.0154, -0.9309,  ..., -4.3239, -4.3081, -4.3005],\n",
      "        [-2.8199, -3.9857, -0.9177,  ..., -4.5864, -4.5895, -4.5977],\n",
      "        ...,\n",
      "        [-2.8810, -4.0008, -0.9167,  ..., -4.6336, -4.6174, -4.5868],\n",
      "        [-3.1403, -4.0034, -0.9751,  ..., -5.4863, -5.5152, -5.4955],\n",
      "        [-2.7351, -4.0281, -0.8973,  ..., -4.2683, -4.2684, -4.2680]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7475, -0.8542, -1.1431, -0.8521,  0.9228, -0.6915,  0.9259, -1.0343],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.8565, -0.8350,  0.9900, -0.8512],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9247e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1689, -0.1706, -0.1723,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1722, -0.1739, -0.1757,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.8354, -0.8144,  0.9656, -0.8302],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1288, -4.0011, -0.9744,  ..., -5.4192, -5.4405, -5.4180],\n",
      "        [-3.2104, -3.9615, -0.9918,  ..., -4.9222, -4.8817, -4.8653],\n",
      "        [-3.0944, -4.0102, -0.9705,  ..., -5.3444, -5.3531, -5.3472],\n",
      "        ...,\n",
      "        [-2.8268, -4.0168, -0.9401,  ..., -4.3389, -4.3210, -4.3109],\n",
      "        [-3.1211, -4.0051, -0.9726,  ..., -5.4391, -5.4662, -5.4480],\n",
      "        [-2.8332, -4.0243, -0.9431,  ..., -4.2869, -4.2675, -4.2629]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9591,  0.7466,  0.9105,  0.9564, -0.8102, -0.8451,  0.9564, -0.8615],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8756,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(1.8220e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1771, -0.1789, -0.1807, -0.1826, -0.1844, -0.1863, -0.1881, -0.1900,\n",
      "         -0.1920, -0.1939, -0.1959, -0.1978, -0.1998, -0.2019, -0.2039, -0.2060,\n",
      "         -0.2080, -0.2101, -0.2123, -0.2144, -0.2166, -0.2188, -0.2210, -0.2232,\n",
      "         -0.2255, -0.2277, -0.2300, -0.2324, -0.2347, -0.2371, -0.2395, -0.2419,\n",
      "         -0.2443, -0.2468, -0.2493, -0.2518, -0.2544, -0.2569, -0.2595, -0.2621,\n",
      "         -0.2648, -0.2675, -0.2702, -0.2729, -0.2757, -0.2784, -0.2813, -0.2841,\n",
      "         -0.2870, -0.2899, -0.2928, -0.2957, -0.2987, -0.3017, -0.3048, -0.3079,\n",
      "         -0.3110, -0.3141, -0.3173, -0.3205, -0.3237, -0.3270, -0.3303, -0.3337,\n",
      "         -0.3370, -0.3404, -0.3439, -0.3473, -0.3508, -0.3544, -0.3580, -0.3616,\n",
      "         -0.3652, -0.3689, -0.3727, -0.3764, -0.3802, -0.3841, -0.3879, -0.3919,\n",
      "         -0.3958, -0.3998, -0.4039, -0.4079, -0.4121, -0.4162, -0.4204, -0.4247,\n",
      "         -0.4290, -0.4333, -0.4377, -0.4421, -0.4466, -0.4511, -0.4556, -0.4602,\n",
      "         -0.4649, -0.4696, -0.4743, -0.4791, -0.4839, -0.4888, -0.4938, -0.4988,\n",
      "         -0.5038, -0.5089, -0.5140, -0.5192, -0.5245, -0.5298, -0.5351, -0.5405,\n",
      "         -0.5460, -0.5515, -0.5571, -0.5627, -0.5684, -0.5741, -0.5799, -0.5858,\n",
      "         -0.5917, -0.5977, -0.6037, -0.6098, -0.6160, -0.6222, -0.6285, -0.6348,\n",
      "         -0.6412, -0.6477, -0.6542, -0.6608, -0.6675, -0.6743, -0.6811, -0.6880,\n",
      "         -0.6949, -0.7019, -0.7090, -0.7162, -0.7234, -0.7307, -0.7381, -0.7456,\n",
      "         -0.7531, -0.7607, -0.7684, -0.7761, -0.7840, -0.7919, -0.7999, -0.8080,\n",
      "         -0.8161, -0.8244, -0.8327, -0.8411, -0.8496, -0.8582, -0.8669, -0.8756,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8540,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8175, -4.0220, -0.9455, -2.2281, -0.0293, -2.4201, -4.5206, -0.5551,\n",
      "         -4.1030, -2.6481, -3.9346,  0.2910, -0.4081, -3.7044, -3.2246, -2.5389,\n",
      "         -3.3505, -1.8559, -2.2007, -0.9816, -1.4288, -1.5185, -0.9329,  0.2543,\n",
      "         -2.0136, -3.5282, -3.2192, -2.7328, -3.9705,  0.8768, -1.8601, -0.2131,\n",
      "         -0.9191, -3.5938, -1.8317, -1.9808, -0.8984, -3.2967, -1.7878, -0.1362,\n",
      "         -0.8564, -0.5866, -0.4941, -4.1729, -1.7176, -3.5147, -3.5784, -3.3099,\n",
      "          0.2648,  0.2917, -3.9345, -1.5864, -6.9910, -3.0645, -3.2121, -1.0660,\n",
      "         -2.6047, -3.0932, -2.6240, -1.7696, -3.6938, -1.3176, -5.7808, -2.6066,\n",
      "         -3.7342,  0.9596, -1.5118, -0.1128, -0.4168, -3.2316, -4.1630, -5.4366,\n",
      "         -2.9426, -0.2828, -3.8640, -4.1778, -2.6439, -4.0255, -2.5989, -3.6139,\n",
      "         -4.8059, -2.6796, -0.5103, -1.2953, -1.7866, -3.0864, -1.9608, -0.7049,\n",
      "         -3.6331, -1.0693, -3.2835, -4.0725, -2.6351, -0.7553, -0.9264, -2.9824,\n",
      "         -2.8637, -3.8570, -3.3088, -3.2874, -0.2891, -1.0011, -3.7221, -2.1729,\n",
      "         -0.5766, -2.0372, -3.7236, -3.7289,  0.3842, -1.5803, -1.6197, -1.0494,\n",
      "         -4.5635, -4.7054,  0.2260, -0.9628, -2.1410, -1.3562,  0.0506, -1.1813,\n",
      "         -2.9792, -4.5682, -2.8093, -1.0103, -1.0602, -2.7682, -2.4576, -0.9112,\n",
      "         -0.5695, -4.2199, -0.1397,  0.3415, -3.4762, -4.0064, -1.3632, -2.0775,\n",
      "         -3.1624, -2.4229, -1.8973, -3.1154, -2.8284, -3.5501, -5.7111, -1.5911,\n",
      "         -0.7956, -0.7934, -0.9413, -0.8616, -0.9528, -0.8548, -5.0147, -5.0001,\n",
      "         -5.3306, -5.0057, -4.9342, -4.6158, -4.5387, -4.6197, -4.6988, -4.6768,\n",
      "         -4.7203, -4.7764, -4.8191, -4.8187, -4.7560, -4.6702, -4.6297, -4.6197,\n",
      "         -4.6201, -4.6149, -4.6088, -4.6190, -4.6346, -4.6449, -4.6626, -4.6867,\n",
      "         -4.6766, -4.6679, -4.6715, -4.6675, -4.6595, -4.6694, -4.6639, -4.6477,\n",
      "         -4.6422, -4.6386, -4.6610, -4.6839, -4.7062, -4.7213, -4.7324, -4.7622,\n",
      "         -4.7861, -4.8032, -4.8033, -4.7988, -4.8001, -4.7934, -4.7893, -4.7832,\n",
      "         -4.7776, -4.7755, -4.7722, -4.7650, -4.7655, -4.7760, -4.7722, -4.7699,\n",
      "         -4.7535, -4.7170, -4.7236, -4.6780, -4.6444, -4.5836, -4.5244, -4.4354,\n",
      "         -4.3629, -4.3616, -4.3355, -4.3013, -4.2848, -4.2775],\n",
      "        [-3.2015, -3.9647, -0.9956, -2.4192, -0.0743, -2.4617, -4.6617, -0.8664,\n",
      "         -4.4854, -2.8791, -3.9892,  0.2746, -0.5090, -3.8386, -3.1457, -2.7747,\n",
      "         -3.4415, -1.9349, -2.1183, -1.1401, -1.4924, -1.7052, -1.0704,  0.3511,\n",
      "         -2.2348, -3.6614, -3.1658, -2.9201, -3.9470,  0.9228, -1.9803, -0.4071,\n",
      "         -0.8868, -3.6495, -1.8551, -1.8287, -0.8009, -3.3411, -1.9449, -0.3124,\n",
      "         -0.9038, -0.5062, -0.5197, -4.0859, -1.9080, -3.8474, -3.8479, -3.3611,\n",
      "          0.7642,  0.6488, -4.0245, -1.6545, -7.0162, -3.2031, -3.2391, -1.1886,\n",
      "         -2.5971, -3.0916, -2.7019, -1.9456, -3.8105, -1.3827, -5.8250, -2.7011,\n",
      "         -3.7179,  1.0265, -1.6215, -0.3160, -0.3550, -3.2928, -4.3287, -5.5025,\n",
      "         -3.0738, -0.1820, -3.9588, -4.1320, -2.7679, -4.0423, -2.7266, -3.6982,\n",
      "         -4.8667, -2.7946, -0.5670, -1.4829, -1.8914, -3.2409, -2.0982, -0.8918,\n",
      "         -3.7144, -1.1539, -3.3904, -4.0369, -2.7435, -0.7904, -1.1224, -2.8798,\n",
      "         -3.0349, -4.1390, -3.3577, -3.3767, -0.3598, -1.1887, -3.6649, -2.2102,\n",
      "         -0.4923, -2.2427, -3.9962, -3.8696,  0.4801, -2.1160, -1.5494, -2.6631,\n",
      "         -5.0156, -4.7017,  0.1539, -1.1880, -2.1877, -1.5587, -0.1018, -1.3138,\n",
      "         -3.2068, -4.5327, -2.8272, -0.9971, -1.2099, -2.8914, -2.6260, -1.0166,\n",
      "         -0.8397, -4.3040,  0.5220,  0.7402, -3.5187, -4.0821, -1.4245, -2.0640,\n",
      "         -3.1327, -2.5215, -2.0103, -3.0918, -2.7883, -3.7505, -5.7042, -1.7848,\n",
      "          0.6992,  0.6345,  0.9671,  0.8293,  0.0905,  1.0286, -4.8559, -4.9854,\n",
      "         -5.2618, -4.9935, -5.1339, -5.0083, -5.2813, -5.2875, -5.2745, -5.2238,\n",
      "         -5.1917, -5.1679, -5.1542, -5.1358, -5.0982, -5.0828, -5.0980, -5.1238,\n",
      "         -5.1493, -5.1701, -5.1973, -5.2255, -5.2816, -5.3371, -5.3883, -5.4142,\n",
      "         -5.4418, -5.4764, -5.5304, -5.5503, -5.5336, -5.5118, -5.4925, -5.4878,\n",
      "         -5.5010, -5.5229, -5.5394, -5.5607, -5.5760, -5.5886, -5.5882, -5.5894,\n",
      "         -5.5882, -5.5960, -5.5982, -5.6214, -5.6435, -5.6476, -5.6529, -5.6549,\n",
      "         -5.6453, -5.6447, -5.6435, -5.6408, -5.6265, -5.5995, -5.5907, -5.6064,\n",
      "         -5.6432, -5.6728, -5.6837, -5.6615, -5.6467, -5.5405, -5.4524, -5.0851,\n",
      "         -5.0020, -5.0898, -5.0342, -4.9476, -4.9066, -4.8870]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8666,  0.7082], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7982, -1.2170, -0.8792, -0.7398, -0.7247,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9166e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1615, -0.1631, -0.1647,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1994, -0.2014, -0.2034,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1248, -0.1261, -0.1274,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7785, -1.1637, -0.8575, -0.7215, -0.7068,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1211, -4.0066, -0.9806,  ..., -5.4049, -5.4299, -5.4051],\n",
      "        [-2.8215, -4.0254, -0.9502,  ..., -4.3368, -4.3217, -4.3163],\n",
      "        [-2.8052, -3.9941, -0.9304,  ..., -4.5783, -4.5831, -4.5909],\n",
      "        ...,\n",
      "        [-2.8647, -4.0081, -0.9286,  ..., -4.6157, -4.6015, -4.5745],\n",
      "        [-3.1209, -4.0119, -0.9875,  ..., -5.4687, -5.5010, -5.4771],\n",
      "        [-3.1113, -4.0104, -0.9799,  ..., -5.4260, -5.4498, -5.4304]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9472, -0.9019, -1.1471, -0.8693, -0.7010, -0.7015,  0.9010,  0.9450],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8553,  0.9900, -0.8596, -0.8029, -1.6812,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5165e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.8691e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1730, -0.1748, -0.1765,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3435, -0.3470, -0.3505,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8342,  0.9656, -0.8384, -0.7831, -1.6479,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0722, -4.0011, -0.9641,  ..., -5.3100, -5.3126, -5.2937],\n",
      "        [-2.8235, -4.0298, -0.9514,  ..., -4.2899, -4.2696, -4.2619],\n",
      "        [-3.1136, -4.0105, -0.9819,  ..., -5.4168, -5.4406, -5.4226],\n",
      "        ...,\n",
      "        [-2.7180, -4.0365, -0.9124,  ..., -4.2684, -4.2705, -4.2686],\n",
      "        [-3.1174, -4.0072, -0.9832,  ..., -5.4282, -5.4505, -5.4226],\n",
      "        [-3.1094, -4.0076, -0.9855,  ..., -5.3737, -5.3929, -5.3709]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9245, -0.8719,  0.9460, -0.8283, -0.8252, -1.0475,  0.8972,  0.9038],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8385,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.8927e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.9241e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2003, 0.2023, 0.2043,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8178,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1073, -4.0110, -0.9816,  ..., -5.4099, -5.4362, -5.4163],\n",
      "        [-3.1027, -4.0042, -0.9846,  ..., -5.3811, -5.4121, -5.3854],\n",
      "        [-3.0765, -4.0076, -0.9819,  ..., -5.2136, -5.2273, -5.2139],\n",
      "        ...,\n",
      "        [-3.0807, -4.0153, -0.9779,  ..., -5.3331, -5.3409, -5.3331],\n",
      "        [-3.0807, -4.0153, -0.9779,  ..., -5.3331, -5.3409, -5.3331],\n",
      "        [-3.1925, -3.9719, -1.0038,  ..., -4.9411, -4.9008, -4.8860]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9401,  0.8702,  0.8879, -0.8639,  0.8587,  0.8953,  0.8953,  0.6883],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8012,  0.9900, -0.8012,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.2785e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1621, -0.1637, -0.1654,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1621, -0.1637, -0.1654,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7814,  0.9656, -0.7814,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8111, -4.0268, -0.9482,  ..., -4.3497, -4.3318, -4.3259],\n",
      "        [-3.0724, -4.0067, -0.9807,  ..., -5.2128, -5.2270, -5.2123],\n",
      "        [-2.8111, -4.0268, -0.9482,  ..., -4.3497, -4.3318, -4.3259],\n",
      "        ...,\n",
      "        [-3.1115, -4.0063, -0.9810,  ..., -5.3925, -5.4134, -5.3859],\n",
      "        [-3.1100, -4.0044, -0.9719,  ..., -5.3955, -5.4233, -5.4073],\n",
      "        [-3.1938, -3.9671, -0.9978,  ..., -4.9351, -4.8960, -4.8759]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8272,  0.8892, -0.8272,  0.7328,  0.9207,  0.9419,  0.9054,  0.7024],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8518, -0.8403,  0.9900,  0.9900,  0.9900, -0.8311, -0.7223,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2002e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1723, -0.1741, -0.1758,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1700, -0.1717, -0.1734,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1681, -0.1698, -0.1715,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1244, -0.1257, -0.1269,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8308, -0.8196,  0.9656,  0.9656,  0.9608, -0.8106, -0.7045,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8111, -4.0253, -0.9432,  ..., -4.2909, -4.2711, -4.2649],\n",
      "        [-2.8138, -4.0219, -0.9390,  ..., -4.2844, -4.2721, -4.2662],\n",
      "        [-3.1963, -3.9645, -0.9931,  ..., -4.9091, -4.8703, -4.8558],\n",
      "        ...,\n",
      "        [-2.8045, -4.0266, -0.9462,  ..., -4.3241, -4.3082, -4.3007],\n",
      "        [-2.8562, -4.0064, -0.9236,  ..., -4.6069, -4.5931, -4.5645],\n",
      "        [-3.1923, -3.9638, -0.9933,  ..., -4.9373, -4.8987, -4.8789]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8606, -0.8348,  0.7432,  0.8983,  0.8500, -0.8163, -0.6952,  0.7119],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.1000, -0.8155, -0.8382,  0.9900, -1.1566, -0.8747],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7666e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1597, -0.1613, -0.1629,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1770, -0.1787, -0.1805,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.0518, -0.7954, -0.8175,  0.9656, -1.1059, -0.8531],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0663, -3.9947, -0.9521,  ..., -5.3019, -5.3019, -5.2851],\n",
      "        [-3.1140, -4.0057, -0.9771,  ..., -5.4562, -5.4888, -5.4624],\n",
      "        [-2.9032, -3.9587, -0.9394,  ..., -0.9949, -1.0223, -1.1850],\n",
      "        ...,\n",
      "        [-3.1118, -4.0019, -0.9741,  ..., -5.3563, -5.3768, -5.3492],\n",
      "        [-2.8541, -3.9745, -0.9216,  ..., -4.2731, -4.2914, -4.3088],\n",
      "        [-2.8058, -4.0184, -0.9380,  ..., -4.3033, -4.2855, -4.2781]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9438,  0.9200, -1.0399, -0.8160, -0.8586,  0.8730, -1.1590, -0.8479],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8249], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4247e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(2.7497e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1669, -0.1686, -0.1703, -0.1720, -0.1737, -0.1755, -0.1772, -0.1790,\n",
      "         -0.1808, -0.1827, -0.1845, -0.1864, -0.1883, -0.1902, -0.1921, -0.1940,\n",
      "         -0.1960, -0.1980, -0.2000, -0.2020, -0.2040, -0.2061, -0.2082, -0.2103,\n",
      "         -0.2124, -0.2145, -0.2167, -0.2189, -0.2211, -0.2233, -0.2256, -0.2279,\n",
      "         -0.2302, -0.2325, -0.2349, -0.2372, -0.2396, -0.2420, -0.2445, -0.2470,\n",
      "         -0.2495, -0.2520, -0.2545, -0.2571, -0.2597, -0.2623, -0.2650, -0.2676,\n",
      "         -0.2703, -0.2731, -0.2758, -0.2786, -0.2814, -0.2843, -0.2871, -0.2900,\n",
      "         -0.2930, -0.2959, -0.2989, -0.3019, -0.3050, -0.3081, -0.3112, -0.3143,\n",
      "         -0.3175, -0.3207, -0.3239, -0.3272, -0.3305, -0.3339, -0.3372, -0.3406,\n",
      "         -0.3441, -0.3476, -0.3511, -0.3546, -0.3582, -0.3618, -0.3655, -0.3692,\n",
      "         -0.3729, -0.3767, -0.3805, -0.3843, -0.3882, -0.3921, -0.3961, -0.4001,\n",
      "         -0.4041, -0.4082, -0.4123, -0.4165, -0.4207, -0.4249, -0.4292, -0.4336,\n",
      "         -0.4379, -0.4424, -0.4468, -0.4513, -0.4559, -0.4605, -0.4652, -0.4699,\n",
      "         -0.4746, -0.4794, -0.4842, -0.4891, -0.4941, -0.4991, -0.5041, -0.5092,\n",
      "         -0.5143, -0.5195, -0.5248, -0.5301, -0.5354, -0.5408, -0.5463, -0.5518,\n",
      "         -0.5574, -0.5630, -0.5687, -0.5745, -0.5803, -0.5861, -0.5920, -0.5980,\n",
      "         -0.6041, -0.6102, -0.6163, -0.6226, -0.6288, -0.6352, -0.6416, -0.6481,\n",
      "         -0.6546, -0.6613, -0.6679, -0.6747, -0.6815, -0.6884, -0.6953, -0.7024,\n",
      "         -0.7095, -0.7166, -0.7239, -0.7312, -0.7386, -0.7460, -0.7536, -0.7612,\n",
      "         -0.7689, -0.7766, -0.7845, -0.7924, -0.8004, -0.8085, -0.8166, -0.8249,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8045], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.1121, -3.9961, -0.9663, -2.4145, -0.0891, -2.4540, -4.5971, -0.7834,\n",
      "         -4.3936, -2.8222, -3.9835,  0.2839, -0.5174, -3.7665, -3.1646, -2.6759,\n",
      "         -3.4274, -1.9329, -2.1161, -1.1407, -1.4933, -1.6901, -1.0651,  0.3210,\n",
      "         -2.1977, -3.5974, -3.1826, -2.8412, -3.9538,  0.9159, -1.9781, -0.3950,\n",
      "         -0.8843, -3.6366, -1.8894, -1.8346, -0.8357, -3.3412, -1.9077, -0.2818,\n",
      "         -0.9008, -0.5328, -0.5152, -4.1309, -1.8719, -3.7769, -3.7954, -3.3552,\n",
      "          0.6668,  0.5945, -3.9699, -1.6597, -7.0139, -3.1378, -3.2348, -1.1699,\n",
      "         -2.5793, -3.1007, -2.6894, -1.9144, -3.7526, -1.3792, -5.8111, -2.6394,\n",
      "         -3.7204,  1.0155, -1.6140, -0.2907, -0.3560, -3.2756, -4.2654, -5.4819,\n",
      "         -3.0015, -0.1732, -3.9050, -4.1648, -2.7013, -4.0458, -2.7042, -3.6459,\n",
      "         -4.8435, -2.7226, -0.5425, -1.4638, -1.8691, -3.2118, -2.0801, -0.8714,\n",
      "         -3.6887, -1.1351, -3.3214, -4.0616, -2.6771, -0.7764, -1.0977, -2.9020,\n",
      "         -3.0055, -4.0820, -3.3683, -3.3620, -0.3374, -1.1619, -3.7132, -2.2000,\n",
      "         -0.5269, -2.2107, -3.9305, -3.8485,  0.4934, -1.3559, -1.4686, -0.5813,\n",
      "         -4.8188, -4.6499,  0.1798, -1.0983, -2.2220, -1.4979, -0.0241, -1.2359,\n",
      "         -3.1084, -4.6475, -2.7874, -0.9627, -1.1869, -2.8644, -2.5873, -0.9667,\n",
      "         -0.7954, -4.2904,  0.3048,  0.6920, -3.5341, -4.0356, -1.4252, -2.0558,\n",
      "         -3.1586, -2.4741, -1.9666, -3.0113, -3.1947, -5.8353, -1.9005, -2.3194,\n",
      "         -1.2517, -0.9814, -0.5524, -4.3371, -4.0630, -0.2049, -2.0884, -0.5189,\n",
      "         -1.1211, -0.3181, -0.1931, -0.6448, -4.7131, -3.5526, -4.9633, -1.4696,\n",
      "          0.9394,  0.9762,  0.9242,  0.9991,  0.7259,  1.0233, -4.7757, -4.9285,\n",
      "         -5.2371, -5.0242, -5.2161, -5.2120, -5.4578, -5.3977, -5.3236, -5.3412,\n",
      "         -5.3341, -5.2395, -5.1128, -5.0099, -4.9776, -4.9750, -4.9852, -5.0010,\n",
      "         -4.9942, -4.9847, -4.9935, -5.0195, -5.0806, -5.1502, -5.2026, -5.2305,\n",
      "         -5.2333, -5.2545, -5.3039, -5.3846, -5.4307, -5.4367, -5.4099, -5.3599,\n",
      "         -5.3231, -5.2929, -5.2848, -5.2933, -5.3158, -5.3603, -5.3954, -5.4120,\n",
      "         -5.3984, -5.3629, -5.3590, -5.3602, -5.3752, -5.4100, -5.4297, -5.4317,\n",
      "         -5.4258, -5.4142, -5.4167, -5.4302, -5.4511, -5.4241],\n",
      "        [-2.8136, -4.0123, -0.9292, -2.2348, -0.0294, -2.4112, -4.5255, -0.5478,\n",
      "         -4.1134, -2.6544, -3.9419,  0.2968, -0.4190, -3.6939, -3.2255, -2.5254,\n",
      "         -3.3537, -1.8671, -2.1756, -0.9981, -1.4276, -1.5226, -0.9432,  0.2550,\n",
      "         -2.0128, -3.5196, -3.2266, -2.7221, -3.9651,  0.8892, -1.8647, -0.2313,\n",
      "         -0.9195, -3.5990, -1.8550, -1.9581, -0.8915, -3.2955, -1.7908, -0.1464,\n",
      "         -0.8555, -0.5777, -0.4959, -4.1724, -1.7191, -3.5237, -3.5923, -3.3168,\n",
      "          0.3035,  0.3444, -3.9309, -1.5845, -6.9866, -3.0561, -3.2129, -1.0712,\n",
      "         -2.6035, -3.0889, -2.6310, -1.7643, -3.6897, -1.3141, -5.7709, -2.5886,\n",
      "         -3.7273,  0.9770, -1.5096, -0.1220, -0.4154, -3.2332, -4.1671, -5.4623,\n",
      "         -2.9290, -0.2577, -3.8593, -4.1823, -2.6292, -4.0247, -2.6095, -3.6056,\n",
      "         -4.8293, -2.6632, -0.5120, -1.3130, -1.7982, -3.0935, -1.9646, -0.7149,\n",
      "         -3.6340, -1.0653, -3.2718, -4.0732, -2.6187, -0.7576, -0.9417, -2.9768,\n",
      "         -2.8647, -3.8632, -3.3154, -3.2848, -0.2885, -1.0148, -3.7296, -2.1645,\n",
      "         -0.5843, -2.0351, -3.7324, -3.7473,  0.4113, -1.3795, -1.6009, -0.8225,\n",
      "         -4.5101, -4.6597,  0.2139, -0.9991, -2.1551, -1.4183, -0.0172, -1.2301,\n",
      "         -3.0465, -4.5357, -2.8082, -1.0022, -1.1802, -2.8688, -2.4653, -0.8928,\n",
      "         -0.6282, -4.2898, -0.0716,  0.4028, -3.4919, -4.0078, -1.3551, -2.0672,\n",
      "         -3.1405, -2.4527, -1.8773, -3.1056, -2.8060, -3.5288, -5.6527, -1.5381,\n",
      "         -0.7797, -0.8235, -0.8594, -0.8398, -0.8438, -0.8588, -4.9931, -4.9979,\n",
      "         -5.3741, -4.9716, -4.8679, -4.5385, -4.4093, -4.5477, -4.6606, -4.6617,\n",
      "         -4.7554, -4.8218, -4.8523, -4.8438, -4.7879, -4.7196, -4.6924, -4.6867,\n",
      "         -4.6736, -4.6441, -4.6150, -4.6063, -4.6051, -4.6060, -4.6308, -4.6676,\n",
      "         -4.6776, -4.6627, -4.6474, -4.6218, -4.6085, -4.6302, -4.6407, -4.6406,\n",
      "         -4.6407, -4.6393, -4.6586, -4.6767, -4.6909, -4.7014, -4.7096, -4.7398,\n",
      "         -4.7762, -4.8127, -4.8272, -4.8319, -4.8341, -4.8207, -4.8093, -4.8026,\n",
      "         -4.7997, -4.7982, -4.7848, -4.7662, -4.7556, -4.7567, -4.7570, -4.7582,\n",
      "         -4.7376, -4.7102, -4.7152, -4.6722, -4.6288, -4.5873, -4.5590, -4.4573,\n",
      "         -4.3934, -4.3892, -4.3640, -4.3341, -4.3155, -4.3058]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9313, -0.8342], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6592, -0.6632, -1.1161, -0.6456, -0.6156,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9778e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1334, -0.1347, -0.1361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1342, -0.1355, -0.1369,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1245, -0.1258, -0.1271,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6430, -0.6468, -1.0672, -0.6297, -0.6004,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0631, -3.9953, -0.9425,  ..., -5.2373, -5.2445, -5.2641],\n",
      "        [-2.8552, -4.0161, -0.9136,  ..., -4.7802, -4.7632, -4.7537],\n",
      "        [-2.8509, -4.0172, -0.9175,  ..., -4.7559, -4.7429, -4.7258],\n",
      "        ...,\n",
      "        [-2.8600, -4.0143, -0.9207,  ..., -4.8124, -4.7969, -4.7876],\n",
      "        [-3.0235, -3.9869, -0.9178,  ..., -5.3341, -5.3607, -5.3584],\n",
      "        [-3.0664, -3.9977, -0.9393,  ..., -5.2338, -5.2402, -5.2583]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9012, -0.8311, -0.7890, -1.3730, -0.7795, -0.7792,  0.9409,  0.9538],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6098,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.5677e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1234, -0.1246, -0.1259,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5947,  0.9656,  0.9608,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8547, -4.0148, -0.9045,  ..., -4.6917, -4.6832, -4.6783],\n",
      "        [-3.0575, -3.9891, -0.9338,  ..., -5.2289, -5.2367, -5.2573],\n",
      "        [-3.1496, -3.9675, -0.9497,  ..., -5.5327, -5.5348, -5.5259],\n",
      "        ...,\n",
      "        [-3.0663, -3.9941, -0.9367,  ..., -5.2820, -5.2934, -5.3152],\n",
      "        [-3.0605, -3.9886, -0.9303,  ..., -5.2728, -5.2800, -5.3026],\n",
      "        [-3.0605, -3.9886, -0.9303,  ..., -5.2728, -5.2800, -5.3026]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7707,  0.9166,  0.8845,  0.8780,  0.9376,  0.9299,  0.9314,  0.9314],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6143, -0.4251,  0.9900, -0.5696,  0.9900,  0.9900, -0.5916],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6850e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.0574e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1243, -0.1255, -0.1268,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0732, -0.0740, -0.0747,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1197, -0.1209, -0.1221,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5991, -0.4146,  0.9656, -0.5555,  0.9656,  0.9656, -0.5770],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0637, -3.9805, -0.9221,  ..., -5.2282, -5.2379, -5.2638],\n",
      "        [-2.8306, -3.9985, -0.8980,  ..., -4.6939, -4.6815, -4.6730],\n",
      "        [-2.9014, -3.9738, -0.8995,  ..., -4.7167, -4.7349, -4.7409],\n",
      "        ...,\n",
      "        [-3.0540, -3.9789, -0.9260,  ..., -5.1854, -5.1909, -5.2087],\n",
      "        [-3.0601, -3.9800, -0.9153,  ..., -5.2140, -5.2199, -5.2371],\n",
      "        [-2.8437, -3.9997, -0.8964,  ..., -4.6647, -4.6570, -4.6540]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9859, -0.7417, -0.6497,  0.8040, -0.7368,  0.9201,  0.9559, -0.7506],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.1009,  0.9900, -1.3546, -0.3718, -0.5728, -0.5655,  0.9900, -0.5319],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4979e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1281, -0.1294, -0.1307,  ..., -1.0790, -1.0899, -1.1009],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2768, -0.2796, -0.2824,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1144, -0.1156, -0.1167,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1076, -0.1087, -0.1098,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0526,  0.9656, -1.3278, -0.3626, -0.5587, -0.5516,  0.9656, -0.5188],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8841, -3.9372, -0.9005,  ..., -1.0878, -1.1400, -1.0848],\n",
      "        [-3.1381, -3.9543, -0.9337,  ..., -5.5512, -5.5440, -5.5241],\n",
      "        [-2.7342, -4.0050, -0.8606,  ..., -4.5287, -4.5274, -4.5314],\n",
      "        ...,\n",
      "        [-2.8196, -3.9976, -0.8985,  ..., -4.6554, -4.6460, -4.6387],\n",
      "        [-3.0399, -3.9727, -0.9173,  ..., -5.2167, -5.2241, -5.2458],\n",
      "        [-2.8225, -3.9981, -0.8973,  ..., -4.6898, -4.6822, -4.6735]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1187,  0.7861, -0.9297, -0.6303, -0.7464, -0.7055,  0.9232, -0.7944],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5514, -1.3277, -0.5451, -0.5052,  0.9900, -1.3277,  0.9900, -0.3515],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8360e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1115, -0.1127, -0.1138,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2713, -0.2740, -0.2768,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1103, -0.1114, -0.1125,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2713, -0.2740, -0.2768,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0606, -0.0612, -0.0618,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5378, -1.3014, -0.5317, -0.4928,  0.9656, -1.3014,  0.9656, -0.3429],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7941, -3.9876, -0.8837,  ..., -4.6075, -4.5994, -4.5932],\n",
      "        [-2.7193, -4.0004, -0.8534,  ..., -4.4952, -4.4918, -4.4974],\n",
      "        [-2.7995, -3.9943, -0.8886,  ..., -4.5892, -4.5862, -4.5855],\n",
      "        ...,\n",
      "        [-2.7193, -4.0004, -0.8534,  ..., -4.4952, -4.4918, -4.4974],\n",
      "        [-2.9989, -3.9684, -0.9064,  ..., -5.1449, -5.1762, -5.1852],\n",
      "        [-2.8675, -3.9673, -0.8896,  ..., -4.5861, -4.6027, -4.6137]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7137, -0.9189, -0.7189, -0.6740,  0.9726, -0.9189,  0.9668, -0.6027],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4986, -0.5180,  0.9900, -0.4711,  0.9900,  0.9900, -0.4731, -0.5108],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0308e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.8965e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1009, -0.1019, -0.1029,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1048, -0.1059, -0.1069,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0957, -0.0967, -0.0977,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1033, -0.1044, -0.1054,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4863, -0.5053,  0.9656, -0.4594,  0.9656,  0.9608, -0.4614, -0.4982],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7628, -3.9829, -0.8718,  ..., -4.4895, -4.4877, -4.4864],\n",
      "        [-2.7575, -3.9817, -0.8704,  ..., -4.5228, -4.5187, -4.5185],\n",
      "        [-3.0736, -3.9511, -0.9183,  ..., -5.4991, -5.4897, -5.4715],\n",
      "        ...,\n",
      "        [-3.0685, -3.9522, -0.9199,  ..., -5.3818, -5.3730, -5.3543],\n",
      "        [-2.7605, -3.9840, -0.8763,  ..., -4.5645, -4.5578, -4.5571],\n",
      "        [-2.7614, -3.9871, -0.8762,  ..., -4.5275, -4.5247, -4.5260]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6519, -0.6891,  0.7900, -0.6480,  0.9753,  0.9325, -0.7230, -0.6798],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2767,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.9306e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0477, -0.0482, -0.0486, -0.0491, -0.0496, -0.0501, -0.0506, -0.0511,\n",
      "         -0.0517, -0.0522, -0.0527, -0.0532, -0.0538, -0.0543, -0.0549, -0.0554,\n",
      "         -0.0560, -0.0566, -0.0571, -0.0577, -0.0583, -0.0589, -0.0595, -0.0601,\n",
      "         -0.0607, -0.0613, -0.0619, -0.0625, -0.0632, -0.0638, -0.0644, -0.0651,\n",
      "         -0.0658, -0.0664, -0.0671, -0.0678, -0.0684, -0.0691, -0.0698, -0.0705,\n",
      "         -0.0713, -0.0720, -0.0727, -0.0734, -0.0742, -0.0749, -0.0757, -0.0765,\n",
      "         -0.0772, -0.0780, -0.0788, -0.0796, -0.0804, -0.0812, -0.0820, -0.0829,\n",
      "         -0.0837, -0.0845, -0.0854, -0.0863, -0.0871, -0.0880, -0.0889, -0.0898,\n",
      "         -0.0907, -0.0916, -0.0925, -0.0935, -0.0944, -0.0954, -0.0963, -0.0973,\n",
      "         -0.0983, -0.0993, -0.1003, -0.1013, -0.1023, -0.1034, -0.1044, -0.1055,\n",
      "         -0.1065, -0.1076, -0.1087, -0.1098, -0.1109, -0.1120, -0.1131, -0.1143,\n",
      "         -0.1154, -0.1166, -0.1178, -0.1190, -0.1202, -0.1214, -0.1226, -0.1238,\n",
      "         -0.1251, -0.1264, -0.1276, -0.1289, -0.1302, -0.1315, -0.1329, -0.1342,\n",
      "         -0.1356, -0.1369, -0.1383, -0.1397, -0.1411, -0.1426, -0.1440, -0.1455,\n",
      "         -0.1469, -0.1484, -0.1499, -0.1514, -0.1530, -0.1545, -0.1561, -0.1576,\n",
      "         -0.1592, -0.1608, -0.1625, -0.1641, -0.1658, -0.1674, -0.1691, -0.1708,\n",
      "         -0.1726, -0.1743, -0.1761, -0.1778, -0.1796, -0.1815, -0.1833, -0.1851,\n",
      "         -0.1870, -0.1889, -0.1908, -0.1927, -0.1947, -0.1966, -0.1986, -0.2006,\n",
      "         -0.2027, -0.2047, -0.2068, -0.2089, -0.2110, -0.2131, -0.2153, -0.2174,\n",
      "         -0.2196, -0.2218, -0.2241, -0.2264, -0.2286, -0.2309, -0.2333, -0.2356,\n",
      "         -0.2380, -0.2404, -0.2428, -0.2453, -0.2478, -0.2503, -0.2528, -0.2554,\n",
      "         -0.2579, -0.2606, -0.2632, -0.2658, -0.2685, -0.2712, -0.2740, -0.2767,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2699,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8012, -3.9584, -0.8732, -2.2684,  0.0341, -2.3723, -4.5648, -0.4973,\n",
      "         -4.0884, -2.5722, -3.9249,  0.4813, -0.2870, -3.6292, -3.2013, -2.4945,\n",
      "         -3.3212, -1.8124, -2.0991, -0.9794, -1.4077, -1.4921, -0.9291,  0.2678,\n",
      "         -1.9648, -3.4495, -3.2023, -2.6863, -3.9670,  0.9431, -1.8424, -0.2111,\n",
      "         -0.8871, -3.5724, -1.8451, -1.8179, -0.8904, -3.2532, -1.7665, -0.1380,\n",
      "         -0.7765, -0.5647, -0.4459, -4.1623, -1.6609, -3.4734, -3.5353, -3.2789,\n",
      "          0.3526,  0.3728, -3.8541, -1.5418, -6.9932, -3.0107, -3.1644, -1.0163,\n",
      "         -2.5964, -3.0625, -2.6169, -1.6894, -3.6094, -1.2630, -5.7604, -2.5266,\n",
      "         -3.7236,  1.0416, -1.4715, -0.0846, -0.3792, -3.1978, -4.1014, -5.4603,\n",
      "         -2.8660, -0.2021, -3.7780, -4.1853, -2.5610, -3.9710, -2.6001, -3.5195,\n",
      "         -4.8147, -2.5893, -0.4611, -1.2744, -1.7595, -3.0791, -1.9215, -0.6787,\n",
      "         -3.6044, -0.9832, -3.1718, -4.0624, -2.5439, -0.7109, -0.8946, -2.9591,\n",
      "         -2.7955, -3.8053, -3.2784, -3.2536, -0.2319, -0.9694, -3.7330, -2.0829,\n",
      "         -0.5738, -1.9837, -3.6572, -3.7055,  0.4880, -1.3920, -1.4333, -0.8499,\n",
      "         -4.5081, -4.6724,  0.2845, -0.9365, -2.1096, -1.3201,  0.0923, -1.0771,\n",
      "         -2.8890, -4.5646, -2.7486, -0.9341, -1.0307, -2.7614, -2.3759, -0.8820,\n",
      "         -0.5382, -4.1904, -0.0418,  0.4369, -3.4197, -4.0819, -1.3043, -2.0512,\n",
      "         -3.1240, -2.4093, -1.7966, -3.0049, -2.9879, -5.7977, -1.6927, -2.3098,\n",
      "         -1.3123, -0.9533, -0.6243, -4.1605, -3.9486, -0.2245, -2.0407, -0.5658,\n",
      "         -1.1841, -0.3717, -0.2245, -0.6672, -4.1166, -3.3689, -5.0248, -1.2693,\n",
      "         -0.4436, -0.4748, -0.6752, -0.6164, -0.4828, -0.5053, -4.9224, -4.9279,\n",
      "         -5.2120, -4.7652, -4.9495, -4.8698, -4.7908, -4.8872, -4.8997, -4.9660,\n",
      "         -5.0502, -5.1461, -5.0923, -4.9285, -4.7724, -4.6158, -4.4496, -4.3263,\n",
      "         -4.2456, -4.1756, -4.1295, -4.1349, -4.1650, -4.2004, -4.2253, -4.2419,\n",
      "         -4.2741, -4.3190, -4.3722, -4.4146, -4.4307, -4.4275, -4.4135, -4.4039,\n",
      "         -4.4227, -4.4554, -4.4821, -4.5039, -4.5154],\n",
      "        [-2.9590, -3.9639, -0.8932, -2.3395, -0.0274, -2.4224, -4.5920, -0.6431,\n",
      "         -4.2449, -2.7034, -3.9804,  0.4642, -0.3341, -3.6647, -3.1325, -2.5678,\n",
      "         -3.3871, -1.8819, -2.0277, -1.0780, -1.4371, -1.6087, -1.0176,  0.3278,\n",
      "         -2.0870, -3.4905, -3.1590, -2.7465, -3.9563,  0.9686, -1.9159, -0.3136,\n",
      "         -0.8755, -3.6232, -1.8673, -1.7349, -0.8121, -3.2985, -1.8372, -0.2303,\n",
      "         -0.8306, -0.5004, -0.4611, -4.1066, -1.7618, -3.6189, -3.6809, -3.3263,\n",
      "          0.5822,  0.5283, -3.8818, -1.6016, -7.0100, -3.0547, -3.2040, -1.0960,\n",
      "         -2.5862, -3.0598, -2.6679, -1.7899, -3.6492, -1.3192, -5.7851, -2.5524,\n",
      "         -3.7230,  1.0699, -1.5389, -0.1943, -0.3546, -3.2476, -4.1810, -5.5027,\n",
      "         -2.9110, -0.1511, -3.8108, -4.1716, -2.6049, -4.0136, -2.6671, -3.5483,\n",
      "         -4.8560, -2.6303, -0.4893, -1.3800, -1.8316, -3.1724, -2.0082, -0.7867,\n",
      "         -3.6807, -1.0585, -3.2103, -4.0555, -2.5821, -0.7318, -1.0060, -2.9136,\n",
      "         -2.9013, -3.9347, -3.3302, -3.2968, -0.2791, -1.0764, -3.7073, -2.1134,\n",
      "         -0.5258, -2.1115, -3.7761, -3.7720,  0.5387, -1.0582, -1.1999, -0.7383,\n",
      "         -4.9348, -4.7456,  0.1829, -1.0281, -2.1567, -1.4268,  0.0188, -1.2110,\n",
      "         -3.0857, -4.5296, -2.7722, -0.9590, -1.1870, -2.9033, -2.4674, -0.9169,\n",
      "         -0.6458, -4.2483,  0.2307,  0.6313, -3.4930, -4.0768, -1.3535, -2.0357,\n",
      "         -3.0923, -2.4680, -1.8574, -2.9831, -3.0445, -5.8100, -1.8249, -2.2129,\n",
      "         -1.1978, -0.8085, -0.8790, -4.3264, -4.0319, -0.1795, -2.0239, -0.4191,\n",
      "         -0.8748, -0.2770, -0.1505, -0.6859, -4.2201, -3.4406, -4.9798, -1.4182,\n",
      "          0.8861,  0.8889,  1.0745,  0.9826,  0.7239,  1.2031, -4.7145, -4.8661,\n",
      "         -5.1979, -4.9497, -5.1533, -5.2325, -5.2857, -5.3196, -5.2999, -5.2746,\n",
      "         -5.2187, -5.1671, -5.0912, -4.9927, -4.9259, -4.8923, -4.8855, -4.8970,\n",
      "         -4.8912, -4.8900, -4.9058, -4.9418, -4.9980, -5.0594, -5.1058, -5.1334,\n",
      "         -5.1436, -5.1658, -5.2062, -5.2529, -5.2735, -5.2746, -5.2645, -5.2327,\n",
      "         -5.2119, -5.1886, -5.1748, -5.1816, -5.1959]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5330,  0.9598], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.0316,  0.9900,  0.9900, -0.2624, -0.4561, -0.4295, -0.4771],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0361e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1201, -0.1213, -0.1225,  ..., -1.0111, -1.0213, -1.0316],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0923, -0.0932, -0.0941,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0869, -0.0878, -0.0887,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0965, -0.0975, -0.0985,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9864,  0.9656,  0.9656, -0.2559, -0.4449, -0.4189, -0.4653],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0247, -3.9434, -0.9092,  ..., -5.3757, -5.3566, -5.3325],\n",
      "        [-2.7836, -3.9196, -0.8764,  ..., -0.9855, -1.0130, -0.9280],\n",
      "        [-2.9117, -3.9554, -0.8883,  ..., -5.1096, -5.1357, -5.1339],\n",
      "        ...,\n",
      "        [-2.7102, -3.9747, -0.8641,  ..., -4.3760, -4.3737, -4.3739],\n",
      "        [-2.7050, -3.9771, -0.8692,  ..., -4.4461, -4.4419, -4.4406],\n",
      "        [-2.6987, -3.9754, -0.8649,  ..., -4.3877, -4.3901, -4.3946]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8397, -1.0089,  0.9740,  1.0167, -0.4877, -0.5716, -0.6449, -0.5610],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2540,  0.9900, -0.4221,  0.9900, -0.4608,  0.9900, -0.4671,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7951e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(9.5677e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0437, -0.0442, -0.0446,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0854, -0.0863, -0.0871,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0945, -0.0954, -0.0964,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2477,  0.9656, -0.4117,  0.9656, -0.4494,  0.9656, -0.4556,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7550, -3.9536, -0.8646,  ..., -4.4421, -4.4641, -4.4767],\n",
      "        [-2.9183, -3.9588, -0.8857,  ..., -5.1564, -5.1657, -5.1811],\n",
      "        [-2.6893, -3.9730, -0.8654,  ..., -4.3942, -4.3887, -4.3890],\n",
      "        ...,\n",
      "        [-2.9183, -3.9588, -0.8857,  ..., -5.1564, -5.1657, -5.1811],\n",
      "        [-2.6881, -3.9694, -0.8603,  ..., -4.3549, -4.3470, -4.3504],\n",
      "        [-2.9179, -3.9580, -0.8887,  ..., -5.1309, -5.1419, -5.1589]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4431,  0.9786, -0.6024,  1.0018, -0.5559,  0.9786, -0.5692,  0.9503],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4241, -0.4806,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.8971e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.3783e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0858, -0.0867, -0.0875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4136, -0.4687,  0.9656,  0.9656,  0.9608,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9303, -3.9654, -0.8974,  ..., -5.2095, -5.2248, -5.2433],\n",
      "        [-2.8966, -3.9613, -0.8810,  ..., -5.1913, -5.2111, -5.1989],\n",
      "        [-2.6935, -3.9674, -0.8704,  ..., -4.3376, -4.3336, -4.3378],\n",
      "        ...,\n",
      "        [-2.9925, -3.9499, -0.9124,  ..., -5.4218, -5.4065, -5.3922],\n",
      "        [-2.9852, -3.9481, -0.9107,  ..., -5.3258, -5.3133, -5.2957],\n",
      "        [-2.9940, -3.9441, -0.9094,  ..., -5.3943, -5.3753, -5.3588]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9866,  0.9793, -0.4836, -0.4823,  0.9649,  0.7761,  0.9388,  0.7864],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.9157, -1.2702, -0.4940,  0.9900,  0.9900,  0.9900, -0.4432,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7989e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1779, -0.1797, -0.1816,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2595, -0.2622, -0.2648,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0999, -0.1009, -0.1020,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0897, -0.0906, -0.0915,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8755, -1.2450, -0.4818,  0.9656,  0.9656,  0.9656, -0.4323,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6769, -3.9634, -0.8560,  ..., -4.2698, -4.2714, -4.2791],\n",
      "        [-2.6275, -3.9851, -0.8378,  ..., -4.2978, -4.2915, -4.2962],\n",
      "        [-2.7099, -3.9703, -0.8681,  ..., -4.3076, -4.2990, -4.3018],\n",
      "        ...,\n",
      "        [-2.9408, -3.9637, -0.8935,  ..., -5.1721, -5.1889, -5.2051],\n",
      "        [-2.7189, -3.9756, -0.8668,  ..., -4.2412, -4.2325, -4.2382],\n",
      "        [-2.9342, -3.9613, -0.8995,  ..., -5.1947, -5.2104, -5.2241]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1049, -0.9236, -0.4890,  0.9674,  0.9674,  0.9628, -0.4547,  0.8681],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5066,  0.9900, -0.5007, -0.4465, -0.2848, -1.2858,  0.9900, -0.5066],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5527e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1025, -0.1035, -0.1046,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1013, -0.1023, -0.1033,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2627, -0.2654, -0.2681,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1025, -0.1035, -0.1046,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4941,  0.9656, -0.4883, -0.4355, -0.2778, -1.2603,  0.9656, -0.4941],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7235, -3.9738, -0.8807,  ..., -4.2193, -4.2100, -4.2181],\n",
      "        [-2.9515, -3.9713, -0.9030,  ..., -5.1982, -5.2153, -5.2309],\n",
      "        [-2.7268, -3.9682, -0.8737,  ..., -4.2156, -4.2063, -4.2128],\n",
      "        ...,\n",
      "        [-2.6370, -3.9832, -0.8398,  ..., -4.2672, -4.2617, -4.2658],\n",
      "        [-2.9156, -3.9622, -0.8879,  ..., -5.1627, -5.1715, -5.1541],\n",
      "        [-2.7235, -3.9738, -0.8807,  ..., -4.2193, -4.2100, -4.2181]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4259,  0.9557, -0.4835, -0.4340, -0.4371, -0.9298,  0.9218, -0.4259],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5134, -1.3020, -0.4784,  0.9900, -0.5209,  0.9900, -0.5040, -0.4702],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4525e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1039, -0.1049, -0.1060,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2661, -0.2687, -0.2715,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0968, -0.0977, -0.0987,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1020, -0.1030, -0.1040,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0951, -0.0961, -0.0971,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5007, -1.2762, -0.4666,  0.9608, -0.5081,  0.9656, -0.4915, -0.4586],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7284, -3.9750, -0.8755,  ..., -4.1753, -4.1665, -4.1737],\n",
      "        [-2.6382, -3.9837, -0.8378,  ..., -4.2510, -4.2428, -4.2476],\n",
      "        [-2.7338, -3.9719, -0.8753,  ..., -4.1935, -4.1822, -4.1938],\n",
      "        ...,\n",
      "        [-2.8958, -3.9514, -0.8830,  ..., -5.2715, -5.2744, -5.2480],\n",
      "        [-2.7324, -3.9714, -0.8740,  ..., -4.1718, -4.1627, -4.1683],\n",
      "        [-2.7309, -3.9761, -0.8687,  ..., -4.1677, -4.1581, -4.1631]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4477, -0.9470, -0.4290,  0.8420, -0.4474,  0.8616, -0.4312, -0.4138],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3110,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5046e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(8.3045e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0536, -0.0541, -0.0546, -0.0552, -0.0558, -0.0563, -0.0569, -0.0575,\n",
      "         -0.0580, -0.0586, -0.0592, -0.0598, -0.0604, -0.0610, -0.0617, -0.0623,\n",
      "         -0.0629, -0.0635, -0.0642, -0.0648, -0.0655, -0.0661, -0.0668, -0.0675,\n",
      "         -0.0682, -0.0689, -0.0696, -0.0703, -0.0710, -0.0717, -0.0724, -0.0731,\n",
      "         -0.0739, -0.0746, -0.0754, -0.0761, -0.0769, -0.0777, -0.0785, -0.0793,\n",
      "         -0.0801, -0.0809, -0.0817, -0.0825, -0.0833, -0.0842, -0.0850, -0.0859,\n",
      "         -0.0868, -0.0876, -0.0885, -0.0894, -0.0903, -0.0912, -0.0922, -0.0931,\n",
      "         -0.0940, -0.0950, -0.0959, -0.0969, -0.0979, -0.0989, -0.0999, -0.1009,\n",
      "         -0.1019, -0.1029, -0.1040, -0.1050, -0.1061, -0.1072, -0.1082, -0.1093,\n",
      "         -0.1104, -0.1116, -0.1127, -0.1138, -0.1150, -0.1161, -0.1173, -0.1185,\n",
      "         -0.1197, -0.1209, -0.1221, -0.1233, -0.1246, -0.1259, -0.1271, -0.1284,\n",
      "         -0.1297, -0.1310, -0.1323, -0.1337, -0.1350, -0.1364, -0.1378, -0.1392,\n",
      "         -0.1406, -0.1420, -0.1434, -0.1449, -0.1463, -0.1478, -0.1493, -0.1508,\n",
      "         -0.1523, -0.1539, -0.1554, -0.1570, -0.1586, -0.1602, -0.1618, -0.1634,\n",
      "         -0.1651, -0.1668, -0.1684, -0.1701, -0.1719, -0.1736, -0.1753, -0.1771,\n",
      "         -0.1789, -0.1807, -0.1825, -0.1844, -0.1862, -0.1881, -0.1900, -0.1919,\n",
      "         -0.1939, -0.1958, -0.1978, -0.1998, -0.2018, -0.2039, -0.2059, -0.2080,\n",
      "         -0.2101, -0.2122, -0.2144, -0.2165, -0.2187, -0.2209, -0.2232, -0.2254,\n",
      "         -0.2277, -0.2300, -0.2323, -0.2347, -0.2371, -0.2394, -0.2419, -0.2443,\n",
      "         -0.2468, -0.2493, -0.2518, -0.2543, -0.2569, -0.2595, -0.2621, -0.2648,\n",
      "         -0.2674, -0.2701, -0.2729, -0.2756, -0.2784, -0.2812, -0.2841, -0.2869,\n",
      "         -0.2898, -0.2928, -0.2957, -0.2987, -0.3017, -0.3048, -0.3078, -0.3110,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3033,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7741, -3.9589, -0.8673, -2.3022,  0.0516, -2.3573, -4.6070, -0.5061,\n",
      "         -4.0572, -2.4615, -3.9392,  0.7333, -0.0321, -3.6379, -3.1940, -2.5171,\n",
      "         -3.2892, -1.8017, -2.0653, -0.9927, -1.4664, -1.4877, -0.9409,  0.2553,\n",
      "         -1.9464, -3.4529, -3.1954, -2.7079, -3.9918,  0.9321, -1.8220, -0.2202,\n",
      "         -0.8868, -3.5876, -1.8179, -1.6602, -0.9165, -3.2150, -1.7729, -0.1575,\n",
      "         -0.7230, -0.5864, -0.4501, -4.1906, -1.6383, -3.4523, -3.4218, -3.2883,\n",
      "          0.3395,  0.3154, -3.8572, -1.5087, -6.9736, -3.0268, -3.1272, -1.0091,\n",
      "         -2.5988, -3.0582, -2.6203, -1.6671, -3.6198, -1.2365, -5.7349, -2.5525,\n",
      "         -3.7556,  1.0338, -1.4459, -0.0909, -0.3744, -3.2077, -4.0869, -5.4245,\n",
      "         -2.8881, -0.2654, -3.7927, -4.1786, -2.5863, -3.9213, -2.6153, -3.5320,\n",
      "         -4.7750, -2.6118, -0.4584, -1.2790, -1.7487, -3.0862, -1.8963, -0.6843,\n",
      "         -3.6334, -0.9227, -3.1768, -4.0512, -2.5668, -0.7076, -0.8966, -2.9874,\n",
      "         -2.7674, -3.7844, -3.2881, -3.2532, -0.2260, -0.9810, -3.7430, -2.0005,\n",
      "         -0.5767, -1.9899, -3.6376, -3.8301,  0.4235, -1.3480, -1.2645, -0.8061,\n",
      "         -4.4933, -4.6793,  0.2822, -0.9480, -2.1576, -1.3200, -0.2220, -1.0438,\n",
      "         -2.8647, -4.6103, -2.7497, -0.9231, -1.0370, -2.7658, -2.3235, -0.9005,\n",
      "         -0.5533, -4.1957, -0.0528,  0.3817, -3.3687, -4.1317, -1.2922, -2.0584,\n",
      "         -3.1437, -2.4166, -1.7635, -2.9514, -2.9839, -5.8209, -1.6785, -2.2253,\n",
      "         -1.2502, -0.7632, -0.5314, -4.1530, -3.9309, -0.1812, -2.0269, -0.5483,\n",
      "         -1.0273, -0.3231, -0.2765, -0.5887, -4.0997, -3.3833, -5.0587, -1.2660,\n",
      "         -0.1304, -0.1400, -0.9020, -0.6349, -0.3549, -0.4403, -4.8723, -4.8913,\n",
      "         -5.1964, -4.7257, -4.8912, -4.8168, -4.7692, -4.8726, -4.9404, -4.9920,\n",
      "         -5.0548, -5.1092, -5.0378, -4.9113, -4.8231, -4.7395, -4.6195, -4.4818,\n",
      "         -4.3205, -4.1968, -4.1469, -4.1509, -4.1760, -4.2104, -4.2429, -4.2766,\n",
      "         -4.3176, -4.3617, -4.3982, -4.4207, -4.4234, -4.4198, -4.4153, -4.4170,\n",
      "         -4.4405, -4.4735, -4.5036, -4.5251, -4.5418],\n",
      "        [-2.9534, -3.9701, -0.9035, -2.3890, -0.0264, -2.3978, -4.6522, -0.6690,\n",
      "         -4.2335, -2.5893, -4.0081,  0.7046, -0.0708, -3.6838, -3.1554, -2.6059,\n",
      "         -3.3501, -1.8823, -2.0357, -1.0886, -1.5243, -1.6126, -1.0316,  0.3075,\n",
      "         -2.0627, -3.5050, -3.1818, -2.7758, -3.9803,  0.9500, -1.8958, -0.3244,\n",
      "         -0.8793, -3.6468, -1.8447, -1.5850, -0.8389, -3.2566, -1.8541, -0.2570,\n",
      "         -0.7568, -0.5247, -0.4629, -4.1367, -1.7345, -3.6194, -3.5593, -3.3432,\n",
      "          0.5810,  0.4857, -3.8932, -1.5792, -7.0110, -3.0771, -3.1570, -1.1038,\n",
      "         -2.6145, -3.0626, -2.6883, -1.7678, -3.6663, -1.3006, -5.7842, -2.5829,\n",
      "         -3.7526,  1.0578, -1.5166, -0.1986, -0.3526, -3.2686, -4.1792, -5.4786,\n",
      "         -2.9426, -0.2187, -3.8356, -4.1794, -2.6387, -3.9568, -2.6903, -3.5731,\n",
      "         -4.8294, -2.6569, -0.4893, -1.3824, -1.8354, -3.1870, -1.9858, -0.7877,\n",
      "         -3.7173, -0.9774, -3.2268, -4.0589, -2.6096, -0.7259, -1.0039, -2.9581,\n",
      "         -2.8684, -3.9395, -3.3425, -3.3014, -0.2712, -1.0874, -3.7229, -2.0326,\n",
      "         -0.5368, -2.1184, -3.7805, -3.9141,  0.4669, -1.1069, -1.2488, -0.9069,\n",
      "         -4.3225, -4.6092,  0.2590, -1.1100, -2.2175, -1.4873, -0.3158, -1.1697,\n",
      "         -3.0509, -4.5273, -2.8080, -0.9672, -1.1906, -2.8947, -2.4293, -0.9471,\n",
      "         -0.6805, -4.2843,  0.2121,  0.5805, -3.3990, -4.1560, -1.3745, -2.0765,\n",
      "         -3.1306, -2.4825, -1.8285, -2.9574, -3.0605, -5.8242, -1.7818, -2.1414,\n",
      "         -1.1012, -0.7226, -0.4572, -3.9076, -3.9583, -0.1438, -2.0648, -0.0099,\n",
      "         -0.7827, -0.3875,  0.1146, -0.6152, -4.1895, -3.9783, -4.9970, -1.4432,\n",
      "          0.8467,  0.8529,  0.6981,  0.8859,  0.5997,  0.9304, -4.6933, -4.8421,\n",
      "         -5.1874, -4.8977, -5.0713, -5.1147, -5.2117, -5.2370, -5.2029, -5.1945,\n",
      "         -5.1483, -5.1006, -5.0641, -5.0226, -4.9997, -4.9876, -5.0034, -5.0261,\n",
      "         -5.0327, -5.0297, -5.0314, -5.0568, -5.0901, -5.1291, -5.1402, -5.1325,\n",
      "         -5.1180, -5.1312, -5.1703, -5.2093, -5.2228, -5.2212, -5.2074, -5.1853,\n",
      "         -5.1702, -5.1634, -5.1640, -5.1816, -5.1937]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4338,  0.8023], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.4760, -0.2993, -0.5088,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8875e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1029, -0.1040, -0.1050,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656,  0.9656, -0.4643, -0.2919, -0.4963,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9963, -3.9526, -0.9242,  ..., -5.3356, -5.3249, -5.3146],\n",
      "        [-2.9989, -3.9551, -0.9259,  ..., -5.4020, -5.3914, -5.3905],\n",
      "        [-2.9428, -3.9683, -0.9062,  ..., -5.2335, -5.2503, -5.2633],\n",
      "        ...,\n",
      "        [-2.7094, -3.9745, -0.8768,  ..., -4.1691, -4.1593, -4.1663],\n",
      "        [-2.9512, -3.9722, -0.9157,  ..., -5.2735, -5.2904, -5.3099],\n",
      "        [-2.9107, -3.9643, -0.8934,  ..., -5.1761, -5.1889, -5.1725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7675,  0.6145,  0.8522, -0.4272, -0.4340, -0.4475,  0.8510,  0.8434],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4778, -0.8945, -0.4192,  0.9900, -0.4792, -0.4730, -0.2523],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2110e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0967, -0.0976, -0.0986,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1738, -0.1756, -0.1774,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0970, -0.0979, -0.0989,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0957, -0.0966, -0.0976,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0435, -0.0439, -0.0443,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4661, -0.8553, -0.4089,  0.9656, -0.4674, -0.4613, -0.2461],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8973, -3.9586, -0.9099,  ..., -5.1079, -5.1267, -5.1204],\n",
      "        [-2.6905, -3.9680, -0.8787,  ..., -4.1683, -4.1641, -4.1725],\n",
      "        [-2.6655, -3.9626, -0.8663,  ..., -4.2417, -4.2426, -4.2502],\n",
      "        ...,\n",
      "        [-2.6973, -3.9673, -0.8779,  ..., -4.2624, -4.2498, -4.2524],\n",
      "        [-2.7024, -3.9682, -0.8792,  ..., -4.2012, -4.1931, -4.1976],\n",
      "        [-2.7703, -3.9623, -0.8773,  ..., -4.4166, -4.4399, -4.4623]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8507, -0.4149, -0.9950, -0.4173,  0.7914, -0.4362, -0.4648, -0.4462],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4108, -0.4108,  0.9900, -0.4212,  0.9900, -0.3706],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.2762e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0831, -0.0840, -0.0848,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0852, -0.0861, -0.0869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0750, -0.0757, -0.0765,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656, -0.4007, -0.4007,  0.9656, -0.4108,  0.9656, -0.3615],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9826, -3.9492, -0.9374,  ..., -5.3326, -5.3204, -5.3066],\n",
      "        [-2.9963, -3.9437, -0.9383,  ..., -5.3331, -5.3168, -5.3046],\n",
      "        [-2.6971, -3.9647, -0.8856,  ..., -4.2222, -4.2133, -4.2146],\n",
      "        ...,\n",
      "        [-2.6930, -3.9699, -0.8870,  ..., -4.2295, -4.2187, -4.2207],\n",
      "        [-2.8942, -3.9625, -0.9061,  ..., -5.2110, -5.2321, -5.2215],\n",
      "        [-2.6920, -3.9618, -0.8890,  ..., -4.2641, -4.2571, -4.2627]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7370,  0.6709, -0.4314, -0.4314,  0.8939, -0.4565,  0.8551, -0.4177],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3282,  0.9900,  0.9900, -0.3752, -0.1628,  0.9900, -0.3282,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3069e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.5083e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0664, -0.0671, -0.0678,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0664, -0.0671, -0.0678,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3201,  0.9656,  0.9656, -0.3660, -0.1588,  0.9656, -0.3201,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6892, -3.9671, -0.8835,  ..., -4.2410, -4.2294, -4.2323],\n",
      "        [-2.9853, -3.9408, -0.9388,  ..., -5.3834, -5.3683, -5.3580],\n",
      "        [-2.9130, -3.9585, -0.9242,  ..., -5.2040, -5.2126, -5.2222],\n",
      "        ...,\n",
      "        [-2.9154, -3.9580, -0.9170,  ..., -5.2314, -5.2406, -5.2479],\n",
      "        [-2.6892, -3.9671, -0.8835,  ..., -4.2410, -4.2294, -4.2323],\n",
      "        [-2.9120, -3.9582, -0.9234,  ..., -5.2325, -5.2409, -5.2483]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4306,  0.6338,  0.8224, -0.4611, -0.4070,  0.8753, -0.4306,  0.8642],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.1252, -0.3020, -0.3020,  0.9900, -0.9377, -1.1252, -1.1252,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5218e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2299, -0.2322, -0.2346,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0611, -0.0617, -0.0623,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0611, -0.0617, -0.0623,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2299, -0.2322, -0.2346,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2299, -0.2322, -0.2346,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1029, -0.2946, -0.2946,  0.9656, -0.8966, -1.1029, -1.1029,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5877, -3.9723, -0.8573,  ..., -4.3052, -4.2954, -4.2998],\n",
      "        [-2.6796, -3.9602, -0.8932,  ..., -4.3180, -4.3044, -4.3013],\n",
      "        [-2.6796, -3.9602, -0.8932,  ..., -4.3180, -4.3044, -4.3013],\n",
      "        ...,\n",
      "        [-2.5877, -3.9723, -0.8573,  ..., -4.3052, -4.2954, -4.2998],\n",
      "        [-2.5877, -3.9723, -0.8573,  ..., -4.3052, -4.2954, -4.2998],\n",
      "        [-2.9018, -3.9533, -0.9243,  ..., -5.2464, -5.2504, -5.2564]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9355, -0.4765, -0.4765,  0.9195, -0.8093, -0.9355, -0.9355,  0.7616],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.3375, -0.3375, -0.1229, -0.3434,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5647e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(8.9483e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0212, -0.0214, -0.0216,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0695, -0.0702, -0.0709,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.3292, -0.3292, -0.1198, -0.3349,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8852, -3.9525, -0.9250,  ..., -5.2524, -5.2594, -5.2642],\n",
      "        [-2.8894, -3.9541, -0.9263,  ..., -5.2855, -5.2898, -5.2977],\n",
      "        [-2.8852, -3.9525, -0.9250,  ..., -5.2524, -5.2594, -5.2642],\n",
      "        ...,\n",
      "        [-2.7266, -3.9451, -0.8908,  ..., -4.4847, -4.5080, -4.5227],\n",
      "        [-2.6537, -3.9525, -0.8850,  ..., -4.3567, -4.3449, -4.3402],\n",
      "        [-2.8367, -3.9416, -0.9037,  ..., -5.3079, -5.3314, -5.3333]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8916,  0.9235,  0.8916, -0.3768, -0.3768, -0.3691, -0.4170,  0.8805],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3558,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5836e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0720, -0.0727, -0.0734, -0.0742, -0.0749, -0.0757, -0.0765, -0.0772,\n",
      "         -0.0780, -0.0788, -0.0796, -0.0804, -0.0812, -0.0820, -0.0829, -0.0837,\n",
      "         -0.0845, -0.0854, -0.0863, -0.0871, -0.0880, -0.0889, -0.0898, -0.0907,\n",
      "         -0.0916, -0.0925, -0.0935, -0.0944, -0.0954, -0.0963, -0.0973, -0.0983,\n",
      "         -0.0993, -0.1003, -0.1013, -0.1023, -0.1034, -0.1044, -0.1055, -0.1065,\n",
      "         -0.1076, -0.1087, -0.1098, -0.1109, -0.1120, -0.1131, -0.1143, -0.1154,\n",
      "         -0.1166, -0.1178, -0.1190, -0.1202, -0.1214, -0.1226, -0.1239, -0.1251,\n",
      "         -0.1264, -0.1276, -0.1289, -0.1302, -0.1316, -0.1329, -0.1342, -0.1356,\n",
      "         -0.1370, -0.1383, -0.1397, -0.1411, -0.1426, -0.1440, -0.1455, -0.1469,\n",
      "         -0.1484, -0.1499, -0.1514, -0.1530, -0.1545, -0.1561, -0.1576, -0.1592,\n",
      "         -0.1608, -0.1625, -0.1641, -0.1658, -0.1674, -0.1691, -0.1708, -0.1726,\n",
      "         -0.1743, -0.1761, -0.1779, -0.1796, -0.1815, -0.1833, -0.1851, -0.1870,\n",
      "         -0.1889, -0.1908, -0.1927, -0.1947, -0.1967, -0.1986, -0.2006, -0.2027,\n",
      "         -0.2047, -0.2068, -0.2089, -0.2110, -0.2131, -0.2153, -0.2174, -0.2196,\n",
      "         -0.2219, -0.2241, -0.2264, -0.2287, -0.2310, -0.2333, -0.2357, -0.2380,\n",
      "         -0.2404, -0.2429, -0.2453, -0.2478, -0.2503, -0.2528, -0.2554, -0.2580,\n",
      "         -0.2606, -0.2632, -0.2659, -0.2685, -0.2713, -0.2740, -0.2768, -0.2796,\n",
      "         -0.2824, -0.2852, -0.2881, -0.2910, -0.2940, -0.2969, -0.2999, -0.3030,\n",
      "         -0.3060, -0.3091, -0.3122, -0.3154, -0.3186, -0.3218, -0.3250, -0.3283,\n",
      "         -0.3316, -0.3350, -0.3384, -0.3418, -0.3452, -0.3487, -0.3523, -0.3558,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3470,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6263, -3.9540, -0.8814, -2.3622,  0.0422, -2.3857, -4.6675, -0.4996,\n",
      "         -3.8943, -2.4885, -3.9707,  0.6266, -0.1106, -3.6109, -3.1699, -2.4640,\n",
      "         -3.2492, -1.8031, -2.0329, -1.0293, -1.4665, -1.5044, -0.9272,  0.2786,\n",
      "         -1.9686, -3.4420, -3.1554, -2.6647, -3.9856,  0.9036, -1.8033, -0.2657,\n",
      "         -0.8860, -3.6010, -1.8316, -1.5907, -0.9226, -3.1651, -1.7198, -0.1166,\n",
      "         -0.7362, -0.5806, -0.4245, -4.2129, -1.6602, -3.3207, -3.4082, -3.2926,\n",
      "          0.2853,  0.3479, -3.8313, -1.5453, -6.9303, -2.9875, -3.0704, -1.0068,\n",
      "         -2.5856, -3.0436, -2.6020, -1.6874, -3.6084, -1.2771, -5.6986, -2.5288,\n",
      "         -3.7576,  1.0000, -1.4320, -0.1455, -0.3718, -3.2218, -4.0668, -5.3390,\n",
      "         -2.8631, -0.1688, -3.7912, -4.1953, -2.5652, -3.8810, -2.5749, -3.5333,\n",
      "         -4.6913, -2.5896, -0.4130, -1.3174, -1.7238, -3.0554, -1.8699, -0.7297,\n",
      "         -3.6201, -0.9068, -3.1679, -4.0638, -2.5427, -0.6818, -0.9439, -2.9821,\n",
      "         -2.7884, -3.6846, -3.2373, -3.2937, -0.1989, -1.0365, -3.7576, -2.0359,\n",
      "         -0.5421, -1.9817, -3.5239, -3.7916,  0.5247, -1.1229, -0.9478, -0.9740,\n",
      "         -4.3485, -4.5636,  0.2620, -1.0763, -2.1656, -1.4160, -0.2269, -1.0570,\n",
      "         -2.8910, -4.4681, -2.7835, -0.9787, -1.1415, -2.7794, -2.3437, -0.8884,\n",
      "         -0.7284, -4.2400, -0.0720,  0.4288, -3.3562, -4.1207, -1.2749, -2.0434,\n",
      "         -3.1396, -2.3994, -1.7787, -2.8452, -2.5716, -3.3715, -5.7051, -1.4624,\n",
      "         -0.4397, -0.4051, -0.5491, -0.2370, -0.5105, -0.2757, -4.8119, -4.8871,\n",
      "         -5.2323, -4.8831, -4.8110, -4.5975, -4.5687, -4.7058, -4.7475, -4.7080,\n",
      "         -4.7077, -4.7139, -4.7020, -4.6632, -4.5998, -4.5343, -4.4895, -4.4671,\n",
      "         -4.4544, -4.4536, -4.4619, -4.4741, -4.4895, -4.4881, -4.4995, -4.5114,\n",
      "         -4.5012, -4.4884, -4.4935, -4.5152, -4.5162, -4.4935, -4.4663, -4.4533,\n",
      "         -4.4399, -4.4301, -4.4423, -4.4572, -4.4532, -4.4316, -4.4119, -4.3898,\n",
      "         -4.3773, -4.3628, -4.3492, -4.3395, -4.3415, -4.3380, -4.3287, -4.3079,\n",
      "         -4.2911, -4.2951, -4.2943, -4.2928, -4.2970],\n",
      "        [-2.8710, -3.9543, -0.9264, -2.5013, -0.0308, -2.4376, -4.7189, -0.6907,\n",
      "         -4.1249, -2.6197, -4.0411,  0.6045, -0.2002, -3.6713, -3.1358, -2.5897,\n",
      "         -3.3564, -1.8767, -2.0154, -1.1551, -1.5287, -1.6507, -1.0354,  0.3141,\n",
      "         -2.1371, -3.5015, -3.1394, -2.7661, -3.9949,  0.9001, -1.9112, -0.3961,\n",
      "         -0.8826, -3.6689, -1.8793, -1.5042, -0.8673, -3.2483, -1.8364, -0.2483,\n",
      "         -0.7711, -0.5447, -0.4636, -4.1967, -1.8036, -3.5372, -3.5670, -3.3580,\n",
      "          0.5611,  0.5280, -3.8737, -1.6271, -6.9674, -3.0620, -3.1338, -1.1032,\n",
      "         -2.5881, -3.0759, -2.6896, -1.8355, -3.6635, -1.3486, -5.7488, -2.5791,\n",
      "         -3.7732,  1.0067, -1.5329, -0.2803, -0.3520, -3.2923, -4.1530, -5.3749,\n",
      "         -2.9355, -0.1118, -3.8377, -4.2122, -2.6355, -3.9449, -2.6746, -3.5753,\n",
      "         -4.7298, -2.6505, -0.4565, -1.4415, -1.8067, -3.1804, -1.9866, -0.8584,\n",
      "         -3.6761, -0.9656, -3.2218, -4.0899, -2.6012, -0.7106, -1.0704, -2.9422,\n",
      "         -2.9229, -3.8716, -3.2840, -3.3617, -0.2585, -1.1575, -3.7466, -2.0642,\n",
      "         -0.5170, -2.1291, -3.6946, -3.8816,  0.5800, -1.2698, -1.1003, -0.5015,\n",
      "         -4.5588, -4.5901,  0.2357, -1.1173, -2.2276, -1.4786, -0.1824, -1.1374,\n",
      "         -2.9344, -4.6260, -2.7687, -0.9503, -1.1508, -2.8611, -2.4768, -0.9560,\n",
      "         -0.8103, -4.3124,  0.2179,  0.6291, -3.4217, -4.1598, -1.3534, -2.0571,\n",
      "         -3.1726, -2.4772, -1.8763, -2.8092, -3.0216, -5.8568, -1.7900, -2.1782,\n",
      "         -1.1034, -0.5640, -0.3796, -4.0240, -4.0383, -0.1193, -2.0071, -0.4609,\n",
      "         -0.7402, -0.2662, -0.0928, -0.4010, -4.4537, -3.3897, -5.0046, -1.4054,\n",
      "          0.9232,  0.8828,  0.8075,  0.9763,  0.9081,  1.0861, -4.6728, -4.8607,\n",
      "         -5.1791, -4.9661, -5.1528, -5.2067, -5.2672, -5.3041, -5.3142, -5.2951,\n",
      "         -5.2451, -5.1849, -5.1285, -5.0642, -5.0221, -5.0031, -5.0120, -5.0350,\n",
      "         -5.0435, -5.0596, -5.0862, -5.1313, -5.1904, -5.2548, -5.3041, -5.3332,\n",
      "         -5.3517, -5.3736, -5.3982, -5.4192, -5.4154, -5.4078, -5.3935, -5.3696,\n",
      "         -5.3447, -5.3206, -5.3070, -5.3107, -5.3174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4028,  0.9307], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3813, -0.3705,  0.9900, -0.3892, -0.3863,  0.9900,  0.9900, -0.3499],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9526e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0771, -0.0779, -0.0787,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0750, -0.0757, -0.0765,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0708, -0.0715, -0.0722,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3718, -0.3614,  0.9656, -0.3796, -0.3768,  0.9656,  0.9656, -0.3412],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6186, -3.9566, -0.8808,  ..., -4.3365, -4.3272, -4.3300],\n",
      "        [-2.6258, -3.9530, -0.8788,  ..., -4.3225, -4.3124, -4.3157],\n",
      "        [-2.8642, -3.9531, -0.9267,  ..., -5.3254, -5.3287, -5.3336],\n",
      "        ...,\n",
      "        [-2.8585, -3.9525, -0.9270,  ..., -5.3140, -5.3186, -5.3247],\n",
      "        [-2.8643, -3.9584, -0.9229,  ..., -5.3294, -5.3285, -5.3336],\n",
      "        [-2.6215, -3.9534, -0.8811,  ..., -4.3592, -4.3485, -4.3547]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4427, -0.4049,  0.9557, -0.3952, -0.3738,  0.9110,  0.9468, -0.4212],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4245, -0.9657,  0.9900, -0.2087,  0.9900,  0.9900, -0.4245,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4568e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0859, -0.0867, -0.0876,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1124, -0.1135, -0.1147,  ..., -0.9465, -0.9561, -0.9657],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0859, -0.0867, -0.0876,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4140, -0.9234,  0.9656, -0.2036,  0.9656,  0.9656, -0.4140,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6030, -3.9521, -0.8735,  ..., -4.3526, -4.3454, -4.3498],\n",
      "        [-2.6800, -3.9085, -0.8949,  ..., -0.7444, -0.7695, -0.8271],\n",
      "        [-2.8484, -3.9537, -0.9283,  ..., -5.3553, -5.3574, -5.3641],\n",
      "        ...,\n",
      "        [-2.9341, -3.9315, -0.9457,  ..., -5.3653, -5.3541, -5.3395],\n",
      "        [-2.6030, -3.9521, -0.8735,  ..., -4.3526, -4.3454, -4.3498],\n",
      "        [-2.8524, -3.9536, -0.9275,  ..., -5.3102, -5.3158, -5.3208]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4345, -0.8026,  0.9200, -0.3189,  0.9140,  0.7281, -0.4345,  0.8798],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2336,  0.9900, -0.2425,  0.9900, -0.4058,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.2459e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2521, -0.2546, -0.2572,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0418, -0.0422, -0.0426,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.2092,  0.9656, -0.2365,  0.9656, -0.3958,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5212, -3.9647, -0.8494,  ..., -4.3615, -4.3591, -4.3639],\n",
      "        [-2.8503, -3.9564, -0.9309,  ..., -5.4025, -5.4032, -5.4086],\n",
      "        [-2.6856, -3.9447, -0.8851,  ..., -4.5122, -4.5246, -4.5352],\n",
      "        ...,\n",
      "        [-2.9153, -3.9365, -0.9429,  ..., -5.3728, -5.3660, -5.3543],\n",
      "        [-2.8238, -3.9512, -0.9089,  ..., -5.2265, -5.2572, -5.2699],\n",
      "        [-2.8468, -3.9537, -0.9290,  ..., -5.3907, -5.3927, -5.3966]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9265,  0.9565, -0.3031,  0.6663, -0.3850,  0.7047,  0.9456,  0.9318],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4285, -0.4285, -0.4607, -0.4733, -0.4786, -0.4689, -0.2581],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(5.1864e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0867, -0.0876, -0.0884,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0867, -0.0876, -0.0884,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0968, -0.0978, -0.0988,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0949, -0.0958, -0.0968,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0445, -0.0449, -0.0454,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4179, -0.4179, -0.4493, -0.4617, -0.4668, -0.4573, -0.2518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9287, -3.9347, -0.9445,  ..., -5.4615, -5.4555, -5.4520],\n",
      "        [-2.5972, -3.9571, -0.8639,  ..., -4.3690, -4.3645, -4.3740],\n",
      "        [-2.5972, -3.9571, -0.8639,  ..., -4.3690, -4.3645, -4.3740],\n",
      "        ...,\n",
      "        [-2.5894, -3.9501, -0.8663,  ..., -4.4391, -4.4330, -4.4358],\n",
      "        [-2.5943, -3.9552, -0.8702,  ..., -4.3975, -4.3890, -4.3934],\n",
      "        [-2.6888, -3.9473, -0.8760,  ..., -4.4458, -4.4584, -4.4707]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6501, -0.3991, -0.3991, -0.3832, -0.3621, -0.3923, -0.4243, -0.3024],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4839,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.5785e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0979, -0.0989, -0.0999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4720,  0.9656,  0.9656,  0.9656,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8575, -3.9560, -0.9296,  ..., -5.4511, -5.4501, -5.4564],\n",
      "        [-2.8601, -3.9544, -0.9205,  ..., -5.4010, -5.4070, -5.4145],\n",
      "        [-2.5906, -3.9516, -0.8661,  ..., -4.4092, -4.4092, -4.4113],\n",
      "        ...,\n",
      "        [-2.8620, -3.9544, -0.9309,  ..., -5.3785, -5.3798, -5.3892],\n",
      "        [-2.8304, -3.9492, -0.9072,  ..., -5.2280, -5.2598, -5.2728],\n",
      "        [-2.9242, -3.9312, -0.9397,  ..., -5.3936, -5.3865, -5.3771]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9785,  0.9662, -0.3650,  0.9524,  0.9662,  0.9135,  0.9711,  0.7071],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.8964, -1.2426,  0.9900, -0.4187, -0.2514,  0.9900, -0.4621, -1.2426],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.7302e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.4568e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1742, -0.1760, -0.1777,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2539, -0.2565, -0.2591,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0935, -0.0944, -0.0954,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2539, -0.2565, -0.2591,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8572, -1.2180,  0.9656, -0.4084, -0.2452,  0.9656, -0.4507, -1.2180],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5762, -3.9499, -0.8567,  ..., -4.4025, -4.4032, -4.4168],\n",
      "        [-2.5295, -3.9620, -0.8432,  ..., -4.4064, -4.4004, -4.4045],\n",
      "        [-2.8771, -3.9549, -0.9295,  ..., -5.4697, -5.4738, -5.4823],\n",
      "        ...,\n",
      "        [-2.8653, -3.9491, -0.9250,  ..., -5.4239, -5.4271, -5.4303],\n",
      "        [-2.6106, -3.9480, -0.8665,  ..., -4.4877, -4.4778, -4.4798],\n",
      "        [-2.5295, -3.9620, -0.8432,  ..., -4.4064, -4.4004, -4.4045]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9444, -0.8961,  0.9963, -0.4176, -0.2668,  0.8335, -0.3837, -0.8961],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3976, -0.3924], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0804, -0.0812, -0.0821, -0.0829, -0.0837, -0.0846, -0.0854, -0.0863,\n",
      "         -0.0872, -0.0881, -0.0889, -0.0898, -0.0907, -0.0917, -0.0926, -0.0935,\n",
      "         -0.0945, -0.0954, -0.0964, -0.0974, -0.0983, -0.0993, -0.1003, -0.1014,\n",
      "         -0.1024, -0.1034, -0.1045, -0.1055, -0.1066, -0.1077, -0.1087, -0.1098,\n",
      "         -0.1110, -0.1121, -0.1132, -0.1143, -0.1155, -0.1167, -0.1178, -0.1190,\n",
      "         -0.1202, -0.1215, -0.1227, -0.1239, -0.1252, -0.1264, -0.1277, -0.1290,\n",
      "         -0.1303, -0.1316, -0.1330, -0.1343, -0.1357, -0.1370, -0.1384, -0.1398,\n",
      "         -0.1412, -0.1426, -0.1441, -0.1455, -0.1470, -0.1485, -0.1500, -0.1515,\n",
      "         -0.1530, -0.1546, -0.1561, -0.1577, -0.1593, -0.1609, -0.1626, -0.1642,\n",
      "         -0.1659, -0.1675, -0.1692, -0.1709, -0.1727, -0.1744, -0.1762, -0.1779,\n",
      "         -0.1797, -0.1816, -0.1834, -0.1852, -0.1871, -0.1890, -0.1909, -0.1928,\n",
      "         -0.1948, -0.1968, -0.1987, -0.2007, -0.2028, -0.2048, -0.2069, -0.2090,\n",
      "         -0.2111, -0.2132, -0.2154, -0.2176, -0.2198, -0.2220, -0.2242, -0.2265,\n",
      "         -0.2288, -0.2311, -0.2334, -0.2358, -0.2382, -0.2406, -0.2430, -0.2454,\n",
      "         -0.2479, -0.2504, -0.2530, -0.2555, -0.2581, -0.2607, -0.2633, -0.2660,\n",
      "         -0.2687, -0.2714, -0.2741, -0.2769, -0.2797, -0.2825, -0.2854, -0.2883,\n",
      "         -0.2912, -0.2941, -0.2971, -0.3001, -0.3031, -0.3062, -0.3093, -0.3124,\n",
      "         -0.3155, -0.3187, -0.3220, -0.3252, -0.3285, -0.3318, -0.3352, -0.3385,\n",
      "         -0.3420, -0.3454, -0.3489, -0.3524, -0.3560, -0.3596, -0.3632, -0.3669,\n",
      "         -0.3706, -0.3743, -0.3781, -0.3819, -0.3858, -0.3897, -0.3936, -0.3976,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0794, -0.0802, -0.0810, -0.0818, -0.0826, -0.0835, -0.0843, -0.0852,\n",
      "         -0.0860, -0.0869, -0.0878, -0.0887, -0.0896, -0.0905, -0.0914, -0.0923,\n",
      "         -0.0932, -0.0942, -0.0951, -0.0961, -0.0971, -0.0980, -0.0990, -0.1000,\n",
      "         -0.1010, -0.1021, -0.1031, -0.1041, -0.1052, -0.1062, -0.1073, -0.1084,\n",
      "         -0.1095, -0.1106, -0.1117, -0.1128, -0.1140, -0.1151, -0.1163, -0.1175,\n",
      "         -0.1187, -0.1199, -0.1211, -0.1223, -0.1235, -0.1248, -0.1260, -0.1273,\n",
      "         -0.1286, -0.1299, -0.1312, -0.1325, -0.1339, -0.1352, -0.1366, -0.1380,\n",
      "         -0.1394, -0.1408, -0.1422, -0.1436, -0.1451, -0.1465, -0.1480, -0.1495,\n",
      "         -0.1510, -0.1526, -0.1541, -0.1557, -0.1572, -0.1588, -0.1604, -0.1620,\n",
      "         -0.1637, -0.1653, -0.1670, -0.1687, -0.1704, -0.1721, -0.1739, -0.1756,\n",
      "         -0.1774, -0.1792, -0.1810, -0.1828, -0.1847, -0.1865, -0.1884, -0.1903,\n",
      "         -0.1922, -0.1942, -0.1961, -0.1981, -0.2001, -0.2021, -0.2042, -0.2062,\n",
      "         -0.2083, -0.2104, -0.2126, -0.2147, -0.2169, -0.2191, -0.2213, -0.2235,\n",
      "         -0.2258, -0.2281, -0.2304, -0.2327, -0.2350, -0.2374, -0.2398, -0.2422,\n",
      "         -0.2447, -0.2471, -0.2496, -0.2522, -0.2547, -0.2573, -0.2599, -0.2625,\n",
      "         -0.2652, -0.2678, -0.2705, -0.2733, -0.2760, -0.2788, -0.2816, -0.2845,\n",
      "         -0.2874, -0.2903, -0.2932, -0.2962, -0.2991, -0.3022, -0.3052, -0.3083,\n",
      "         -0.3114, -0.3146, -0.3177, -0.3209, -0.3242, -0.3275, -0.3308, -0.3341,\n",
      "         -0.3375, -0.3409, -0.3443, -0.3478, -0.3513, -0.3549, -0.3585, -0.3621,\n",
      "         -0.3657, -0.3694, -0.3732, -0.3769, -0.3807, -0.3846, -0.3885, -0.3924,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3878, -0.3827], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6140, -3.9512, -0.8681, -2.3673,  0.0434, -2.3768, -4.6599, -0.4731,\n",
      "         -3.8791, -2.5160, -3.9502,  0.6572, -0.1225, -3.5864, -3.1519, -2.4234,\n",
      "         -3.2298, -1.8071, -1.9809, -1.0409, -1.4419, -1.5037, -0.9118,  0.2884,\n",
      "         -1.9894, -3.4088, -3.1361, -2.6242, -3.9819,  0.9014, -1.8050, -0.2763,\n",
      "         -0.8805, -3.5699, -1.8374, -1.6222, -0.9032, -3.1502, -1.6857, -0.1057,\n",
      "         -0.7520, -0.5578, -0.4228, -4.1980, -1.6847, -3.3222, -3.4464, -3.2666,\n",
      "          0.2538,  0.3501, -3.7946, -1.5433, -6.9094, -2.9509, -3.0550, -1.0137,\n",
      "         -2.5513, -3.0350, -2.5753, -1.7087, -3.5812, -1.2749, -5.6761, -2.4879,\n",
      "         -3.7545,  0.9966, -1.4376, -0.1582, -0.3669, -3.1943, -4.0589, -5.3249,\n",
      "         -2.8353, -0.1355, -3.7692, -4.2075, -2.5356, -3.8683, -2.5462, -3.5115,\n",
      "         -4.6760, -2.5580, -0.4110, -1.3340, -1.7131, -3.0316, -1.8720, -0.7463,\n",
      "         -3.5719, -0.9105, -3.1330, -4.0812, -2.5079, -0.6786, -0.9585, -2.9615,\n",
      "         -2.8115, -3.6986, -3.2361, -3.2837, -0.1978, -1.0570, -3.7461, -2.0391,\n",
      "         -0.5333, -1.9792, -3.5393, -3.6707,  0.5535, -0.6613, -1.1548, -0.4267,\n",
      "         -4.4362, -4.5743,  0.2542, -1.0120, -2.1339, -1.4100,  0.0849, -1.0839,\n",
      "         -2.8662, -4.4573, -2.7893, -0.9825, -1.1277, -2.7992, -2.3832, -0.8727,\n",
      "         -0.6690, -4.2438, -0.0589,  0.4227, -3.3123, -4.1128, -1.2619, -2.0206,\n",
      "         -3.1457, -2.3967, -1.8319, -2.9112, -2.6201, -3.3573, -5.7004, -1.4834,\n",
      "         -0.3829, -0.4297, -0.3206, -0.4075, -0.5566, -0.3945, -4.8741, -4.9509,\n",
      "         -5.3198, -4.9192, -4.8415, -4.6018, -4.5780, -4.7256, -4.7463, -4.6769,\n",
      "         -4.7254, -4.8066, -4.8530, -4.8336, -4.7735, -4.7000, -4.6522, -4.6235,\n",
      "         -4.6178, -4.6213, -4.6278, -4.6480, -4.6650, -4.6688, -4.6745, -4.6778,\n",
      "         -4.6575, -4.6447, -4.6522, -4.6761, -4.6781, -4.6761, -4.6528, -4.6327,\n",
      "         -4.6168, -4.6116, -4.6277, -4.6476, -4.6533, -4.6446, -4.6289, -4.6151,\n",
      "         -4.6028, -4.5890, -4.5748, -4.5649, -4.5616, -4.5594, -4.5468, -4.5292,\n",
      "         -4.5103, -4.5045, -4.4951, -4.4823, -4.4821],\n",
      "        [-2.6119, -3.9440, -0.8691, -2.3650,  0.0442, -2.3801, -4.6577, -0.4749,\n",
      "         -3.8774, -2.5132, -3.9466,  0.6623, -0.1226, -3.5858, -3.1459, -2.4195,\n",
      "         -3.2314, -1.8075, -1.9845, -1.0423, -1.4454, -1.5065, -0.9150,  0.2865,\n",
      "         -1.9933, -3.4090, -3.1315, -2.6213, -3.9816,  0.9002, -1.8063, -0.2778,\n",
      "         -0.8801, -3.5663, -1.8381, -1.6227, -0.9059, -3.1538, -1.6861, -0.1101,\n",
      "         -0.7558, -0.5608, -0.4234, -4.2029, -1.6865, -3.3204, -3.4456, -3.2623,\n",
      "          0.2521,  0.3505, -3.7935, -1.5442, -6.9101, -2.9477, -3.0559, -1.0136,\n",
      "         -2.5476, -3.0309, -2.5727, -1.7123, -3.5785, -1.2745, -5.6783, -2.4854,\n",
      "         -3.7547,  0.9946, -1.4395, -0.1594, -0.3657, -3.1905, -4.0541, -5.3213,\n",
      "         -2.8304, -0.1340, -3.7670, -4.2011, -2.5327, -3.8681, -2.5471, -3.5096,\n",
      "         -4.6705, -2.5555, -0.4103, -1.3350, -1.7109, -3.0340, -1.8744, -0.7457,\n",
      "         -3.5692, -0.9160, -3.1304, -4.0753, -2.5054, -0.6773, -0.9596, -2.9622,\n",
      "         -2.8145, -3.6957, -3.2315, -3.2839, -0.1963, -1.0587, -3.7442, -2.0413,\n",
      "         -0.5348, -1.9806, -3.5364, -3.6680,  0.5567, -1.1591, -1.2148, -0.2902,\n",
      "         -4.4881, -4.5487,  0.3038, -1.0304, -2.1534, -1.3825,  0.0710, -1.0597,\n",
      "         -2.8995, -4.4780, -2.7702, -0.9721, -1.1068, -2.7517, -2.3675, -0.8741,\n",
      "         -0.7005, -4.2176, -0.1028,  0.4361, -3.3310, -4.1061, -1.2821, -2.0091,\n",
      "         -3.1220, -2.3903, -1.8062, -2.8826, -2.6142, -3.3867, -5.6818, -1.4497,\n",
      "         -0.4061, -0.4344, -0.3503, -0.3329, -0.2736, -0.3936, -4.8504, -4.9100,\n",
      "         -5.2720, -4.9255, -4.8999, -4.6465, -4.6333, -4.7725, -4.7880, -4.7228,\n",
      "         -4.7639, -4.8194, -4.8366, -4.8149, -4.7582, -4.6917, -4.6481, -4.6248,\n",
      "         -4.6198, -4.6257, -4.6327, -4.6481, -4.6622, -4.6609, -4.6599, -4.6658,\n",
      "         -4.6383, -4.6124, -4.6157, -4.6439, -4.6592, -4.6560, -4.6382, -4.6239,\n",
      "         -4.6095, -4.6048, -4.6261, -4.6479, -4.6598, -4.6529, -4.6409, -4.6267,\n",
      "         -4.6092, -4.5916, -4.5751, -4.5614, -4.5617, -4.5607, -4.5564, -4.5405,\n",
      "         -4.5229, -4.5200, -4.5094, -4.5021, -4.5079]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4153, -0.3652], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4210, -1.2119,  0.9900,  0.9900, -0.4105,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3493e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.7492e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0852, -0.0860, -0.0869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2477, -0.2502, -0.2527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4106, -1.1879,  0.9608,  0.9608, -0.4004,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6142, -3.9501, -0.8624,  ..., -4.4905, -4.4812, -4.4864],\n",
      "        [-2.5209, -3.9595, -0.8334,  ..., -4.4312, -4.4236, -4.4313],\n",
      "        [-2.9419, -3.9231, -0.9288,  ..., -5.4149, -5.4053, -5.3920],\n",
      "        ...,\n",
      "        [-2.8370, -3.9436, -0.8959,  ..., -5.2281, -5.2652, -5.2813],\n",
      "        [-2.8675, -3.9465, -0.9169,  ..., -5.4469, -5.4503, -5.4603],\n",
      "        [-2.8659, -3.9429, -0.9152,  ..., -5.4425, -5.4446, -5.4565]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4091, -0.9228,  0.6821,  0.6821, -0.3695,  0.9609,  0.9703,  0.9534],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3738, -0.3783, -0.8677,  0.9900, -0.2125, -0.2125,  0.9900, -0.4217],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2473e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0756, -0.0764, -0.0772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0765, -0.0773, -0.0781,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.1703, -0.1720,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0366, -0.0370, -0.0373,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0853, -0.0862, -0.0870,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3646, -0.3689, -0.8297,  0.9656, -0.2072, -0.2072,  0.9656, -0.4113],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6099, -3.9462, -0.8715,  ..., -4.5473, -4.5389, -4.5427],\n",
      "        [-2.6178, -3.9559, -0.8659,  ..., -4.4645, -4.4567, -4.4619],\n",
      "        [-2.5723, -3.9466, -0.8556,  ..., -4.4548, -4.4543, -4.4630],\n",
      "        ...,\n",
      "        [-2.6939, -3.9391, -0.8771,  ..., -4.6266, -4.6381, -4.6463],\n",
      "        [-2.8716, -3.9451, -0.9153,  ..., -5.4055, -5.4124, -5.4221],\n",
      "        [-2.6059, -3.9469, -0.8664,  ..., -4.5359, -4.5243, -4.5239]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3754, -0.3971, -0.9527,  0.6514, -0.2795, -0.2795,  0.9422, -0.3969],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3842,  0.9900,  0.9900, -0.3809, -0.3765, -0.4176,  0.9900, -0.4276],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.7442e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0777, -0.0785, -0.0793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0845, -0.0853, -0.0862,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0865, -0.0874, -0.0883,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3747,  0.9656,  0.9656, -0.3715, -0.3672, -0.4073,  0.9656, -0.4170],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6098, -3.9504, -0.8779,  ..., -4.5328, -4.5246, -4.5302],\n",
      "        [-2.8749, -3.9463, -0.9325,  ..., -5.3699, -5.3810, -5.3926],\n",
      "        [-2.9600, -3.9156, -0.9450,  ..., -5.4251, -5.4122, -5.3984],\n",
      "        ...,\n",
      "        [-2.6015, -3.9523, -0.8792,  ..., -4.5009, -4.4900, -4.4945],\n",
      "        [-2.8282, -3.9383, -0.9189,  ..., -5.1563, -5.1993, -5.2231],\n",
      "        [-2.5943, -3.9503, -0.8734,  ..., -4.5037, -4.4983, -4.5012]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4108,  0.8880,  0.7169, -0.4022, -0.3836, -0.3768,  0.9084, -0.3687],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4175,  0.9900,  0.9900,  0.9900, -0.9330, -1.2140, -0.3832,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7483e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0845, -0.0853, -0.0862,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2481, -0.2506, -0.2531,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.0783, -0.0791,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4072,  0.9656,  0.9656,  0.9656, -0.8921, -1.1900, -0.3737,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5998, -3.9521, -0.8820,  ..., -4.5254, -4.5183, -4.5234],\n",
      "        [-2.8089, -3.9322, -0.9109,  ..., -5.4062, -5.4450, -5.4644],\n",
      "        [-2.8625, -3.9418, -0.9365,  ..., -5.4404, -5.4456, -5.4572],\n",
      "        ...,\n",
      "        [-2.5110, -3.9611, -0.8528,  ..., -4.4561, -4.4497, -4.4525],\n",
      "        [-2.5985, -3.9516, -0.8848,  ..., -4.5475, -4.5347, -4.5329],\n",
      "        [-2.8359, -3.9426, -0.9165,  ..., -5.2427, -5.2826, -5.3061]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4293,  0.9161,  0.9108,  0.9126, -0.8503, -0.9561, -0.4344,  0.9361],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.2084, -0.2191,  0.9900, -0.4329,  0.9900, -0.4090, -1.2159],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3210e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0359, -0.0363, -0.0366,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0377, -0.0381, -0.0385,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0827, -0.0836, -0.0844,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2485, -0.2510, -0.2535,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.2032, -0.2137,  0.9656, -0.4222,  0.9656, -0.3989, -1.1918],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8706, -3.9443, -0.9438,  ..., -5.4693, -5.4766, -5.4914],\n",
      "        [-2.6814, -3.9384, -0.8895,  ..., -4.5688, -4.5810, -4.5910],\n",
      "        [-2.6785, -3.9349, -0.8941,  ..., -4.6353, -4.6467, -4.6530],\n",
      "        ...,\n",
      "        [-2.8634, -3.9440, -0.9361,  ..., -5.4352, -5.4409, -5.4507],\n",
      "        [-2.5971, -3.9489, -0.8840,  ..., -4.5026, -4.4961, -4.5026],\n",
      "        [-2.5064, -3.9594, -0.8571,  ..., -4.4653, -4.4583, -4.4621]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9462, -0.2920, -0.2920,  0.9608, -0.3712,  0.9498, -0.3960, -0.9595],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4369, -0.4358,  0.9900,  0.9900, -0.4277,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8302e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0884, -0.0893, -0.0902,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0882, -0.0890, -0.0899,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0865, -0.0874, -0.0883,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4261, -0.4250,  0.9656,  0.9656, -0.4172,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9449, -3.9169, -0.9623,  ..., -5.5280, -5.5187, -5.5048],\n",
      "        [-2.5795, -3.9473, -0.8834,  ..., -4.5417, -4.5305, -4.5326],\n",
      "        [-2.5755, -3.9461, -0.8819,  ..., -4.5620, -4.5529, -4.5521],\n",
      "        ...,\n",
      "        [-2.5761, -3.9500, -0.8896,  ..., -4.5196, -4.5137, -4.5149],\n",
      "        [-2.8498, -3.9353, -0.9406,  ..., -5.4239, -5.4279, -5.4397],\n",
      "        [-2.8533, -3.9375, -0.9425,  ..., -5.4316, -5.4385, -5.4550]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.5926, -0.4349, -0.4197,  0.8818,  0.9226, -0.3877,  0.7764,  0.8940],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4004, -0.4463], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0810, -0.0818, -0.0826, -0.0835, -0.0843, -0.0852, -0.0860, -0.0869,\n",
      "         -0.0878, -0.0887, -0.0896, -0.0905, -0.0914, -0.0923, -0.0932, -0.0942,\n",
      "         -0.0951, -0.0961, -0.0971, -0.0980, -0.0990, -0.1000, -0.1010, -0.1021,\n",
      "         -0.1031, -0.1041, -0.1052, -0.1063, -0.1073, -0.1084, -0.1095, -0.1106,\n",
      "         -0.1117, -0.1129, -0.1140, -0.1151, -0.1163, -0.1175, -0.1187, -0.1199,\n",
      "         -0.1211, -0.1223, -0.1235, -0.1248, -0.1261, -0.1273, -0.1286, -0.1299,\n",
      "         -0.1312, -0.1325, -0.1339, -0.1352, -0.1366, -0.1380, -0.1394, -0.1408,\n",
      "         -0.1422, -0.1436, -0.1451, -0.1466, -0.1480, -0.1495, -0.1510, -0.1526,\n",
      "         -0.1541, -0.1557, -0.1572, -0.1588, -0.1604, -0.1621, -0.1637, -0.1653,\n",
      "         -0.1670, -0.1687, -0.1704, -0.1721, -0.1739, -0.1756, -0.1774, -0.1792,\n",
      "         -0.1810, -0.1828, -0.1847, -0.1865, -0.1884, -0.1903, -0.1922, -0.1942,\n",
      "         -0.1962, -0.1981, -0.2001, -0.2022, -0.2042, -0.2063, -0.2083, -0.2104,\n",
      "         -0.2126, -0.2147, -0.2169, -0.2191, -0.2213, -0.2235, -0.2258, -0.2281,\n",
      "         -0.2304, -0.2327, -0.2351, -0.2374, -0.2398, -0.2422, -0.2447, -0.2472,\n",
      "         -0.2497, -0.2522, -0.2547, -0.2573, -0.2599, -0.2625, -0.2652, -0.2679,\n",
      "         -0.2706, -0.2733, -0.2761, -0.2788, -0.2817, -0.2845, -0.2874, -0.2903,\n",
      "         -0.2932, -0.2962, -0.2992, -0.3022, -0.3052, -0.3083, -0.3114, -0.3146,\n",
      "         -0.3178, -0.3210, -0.3242, -0.3275, -0.3308, -0.3341, -0.3375, -0.3409,\n",
      "         -0.3444, -0.3478, -0.3514, -0.3549, -0.3585, -0.3621, -0.3658, -0.3695,\n",
      "         -0.3732, -0.3770, -0.3808, -0.3846, -0.3885, -0.3924, -0.3964, -0.4004,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0903, -0.0912, -0.0921, -0.0930, -0.0940, -0.0949, -0.0959, -0.0969,\n",
      "         -0.0978, -0.0988, -0.0998, -0.1008, -0.1019, -0.1029, -0.1039, -0.1050,\n",
      "         -0.1060, -0.1071, -0.1082, -0.1093, -0.1104, -0.1115, -0.1126, -0.1138,\n",
      "         -0.1149, -0.1161, -0.1172, -0.1184, -0.1196, -0.1208, -0.1220, -0.1233,\n",
      "         -0.1245, -0.1258, -0.1271, -0.1283, -0.1296, -0.1309, -0.1323, -0.1336,\n",
      "         -0.1350, -0.1363, -0.1377, -0.1391, -0.1405, -0.1419, -0.1433, -0.1448,\n",
      "         -0.1463, -0.1477, -0.1492, -0.1507, -0.1523, -0.1538, -0.1553, -0.1569,\n",
      "         -0.1585, -0.1601, -0.1617, -0.1633, -0.1650, -0.1667, -0.1683, -0.1700,\n",
      "         -0.1718, -0.1735, -0.1753, -0.1770, -0.1788, -0.1806, -0.1824, -0.1843,\n",
      "         -0.1861, -0.1880, -0.1899, -0.1918, -0.1938, -0.1957, -0.1977, -0.1997,\n",
      "         -0.2017, -0.2038, -0.2058, -0.2079, -0.2100, -0.2121, -0.2143, -0.2164,\n",
      "         -0.2186, -0.2208, -0.2231, -0.2253, -0.2276, -0.2299, -0.2322, -0.2346,\n",
      "         -0.2369, -0.2393, -0.2417, -0.2442, -0.2466, -0.2491, -0.2517, -0.2542,\n",
      "         -0.2568, -0.2594, -0.2620, -0.2646, -0.2673, -0.2700, -0.2727, -0.2755,\n",
      "         -0.2783, -0.2811, -0.2839, -0.2868, -0.2897, -0.2926, -0.2956, -0.2985,\n",
      "         -0.3016, -0.3046, -0.3077, -0.3108, -0.3139, -0.3171, -0.3203, -0.3235,\n",
      "         -0.3268, -0.3301, -0.3334, -0.3368, -0.3402, -0.3436, -0.3471, -0.3506,\n",
      "         -0.3542, -0.3577, -0.3614, -0.3650, -0.3687, -0.3724, -0.3762, -0.3800,\n",
      "         -0.3838, -0.3877, -0.3916, -0.3956, -0.3996, -0.4036, -0.4077, -0.4118,\n",
      "         -0.4159, -0.4202, -0.4244, -0.4287, -0.4330, -0.4374, -0.4418, -0.4463,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3905, -0.4353], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5758, -3.9479, -0.8890, -2.3407,  0.0269, -2.3870, -4.6237, -0.4648,\n",
      "         -3.8348, -2.4933, -3.9319,  0.6596, -0.0862, -3.6209, -3.1448, -2.4359,\n",
      "         -3.2220, -1.8194, -1.9950, -1.0328, -1.4587, -1.5013, -0.9083,  0.2711,\n",
      "         -2.0052, -3.4469, -3.1252, -2.6422, -3.9798,  0.8719, -1.8042, -0.2674,\n",
      "         -0.9043, -3.5539, -1.8303, -1.7496, -0.9268, -3.1454, -1.6936, -0.1009,\n",
      "         -0.7739, -0.5745, -0.4449, -4.2140, -1.7056, -3.2865, -3.4219, -3.2513,\n",
      "          0.2097,  0.2812, -3.8350, -1.5449, -6.9207, -2.9720, -3.0505, -1.0322,\n",
      "         -2.5555, -3.0394, -2.5505, -1.7348, -3.6225, -1.2748, -5.7019, -2.5158,\n",
      "         -3.7576,  0.9607, -1.4448, -0.1542, -0.3930, -3.1786, -4.0565, -5.3013,\n",
      "         -2.8627, -0.2911, -3.8137, -4.1921, -2.5641, -3.8628, -2.5629, -3.5607,\n",
      "         -4.6563, -2.5887, -0.4465, -1.3294, -1.7111, -3.0301, -1.8794, -0.7430,\n",
      "         -3.5484, -0.9295, -3.1808, -4.0697, -2.5425, -0.7100, -0.9564, -2.9863,\n",
      "         -2.8329, -3.6730, -3.2167, -3.2721, -0.2274, -1.0533, -3.7396, -2.0576,\n",
      "         -0.5473, -1.9824, -3.5147, -3.6754,  0.3918, -0.7480, -1.2928, -0.4089,\n",
      "         -4.4012, -4.5604,  0.2211, -1.0079, -2.1535, -1.4193,  0.0449, -1.1063,\n",
      "         -2.8645, -4.4385, -2.8039, -1.0140, -1.1376, -2.7655, -2.4029, -0.8826,\n",
      "         -0.6649, -4.2299, -0.1021,  0.3497, -3.3121, -4.0906, -1.2860, -2.0309,\n",
      "         -3.1517, -2.3717, -1.8628, -2.9147, -2.6287, -3.3331, -5.6673, -1.4875,\n",
      "         -0.3584, -0.3798, -0.3967, -0.5592, -0.5104, -0.3948, -4.8810, -4.9542,\n",
      "         -5.3262, -4.9253, -4.8125, -4.5346, -4.5430, -4.6961, -4.6759, -4.5854,\n",
      "         -4.6406, -4.7425, -4.8144, -4.8037, -4.7409, -4.6686, -4.6138, -4.5823,\n",
      "         -4.5713, -4.5659, -4.5646, -4.5822, -4.6101, -4.6302, -4.6495, -4.6566,\n",
      "         -4.6333, -4.6196, -4.6344, -4.6474, -4.6489, -4.6595, -4.6470, -4.6230,\n",
      "         -4.6112, -4.6045, -4.6245, -4.6432, -4.6570, -4.6568, -4.6500, -4.6509,\n",
      "         -4.6449, -4.6373, -4.6264, -4.6212, -4.6211, -4.6161, -4.6056, -4.5909,\n",
      "         -4.5795, -4.5786, -4.5726, -4.5613, -4.5557],\n",
      "        [-2.5773, -3.9449, -0.8859, -2.3309,  0.0294, -2.3900, -4.6126, -0.4594,\n",
      "         -3.8289, -2.4866, -3.9272,  0.6633, -0.0832, -3.6181, -3.1360, -2.4260,\n",
      "         -3.2182, -1.8185, -1.9949, -1.0324, -1.4587, -1.5012, -0.9109,  0.2675,\n",
      "         -2.0058, -3.4458, -3.1193, -2.6343, -3.9778,  0.8739, -1.8057, -0.2664,\n",
      "         -0.9076, -3.5505, -1.8307, -1.7534, -0.9242, -3.1448, -1.6975, -0.1042,\n",
      "         -0.7780, -0.5707, -0.4480, -4.2168, -1.7063, -3.2824, -3.4171, -3.2474,\n",
      "          0.2104,  0.2761, -3.8313, -1.5375, -6.9174, -2.9646, -3.0469, -1.0288,\n",
      "         -2.5518, -3.0348, -2.5450, -1.7355, -3.6214, -1.2694, -5.6978, -2.5081,\n",
      "         -3.7537,  0.9625, -1.4451, -0.1546, -0.3967, -3.1745, -4.0483, -5.2969,\n",
      "         -2.8540, -0.2936, -3.8105, -4.1839, -2.5565, -3.8578, -2.5649, -3.5578,\n",
      "         -4.6552, -2.5820, -0.4491, -1.3298, -1.7112, -3.0294, -1.8790, -0.7414,\n",
      "         -3.5496, -0.9332, -3.1775, -4.0629, -2.5345, -0.7136, -0.9574, -2.9850,\n",
      "         -2.8341, -3.6685, -3.2171, -3.2735, -0.2297, -1.0529, -3.7361, -2.0580,\n",
      "         -0.5511, -1.9792, -3.5092, -3.6684,  0.3898, -1.0992, -1.4144, -0.5419,\n",
      "         -4.4576, -4.5207,  0.1814, -1.0695, -2.1710, -1.4239,  0.0220, -1.1514,\n",
      "         -2.8615, -4.4861, -2.7730, -1.0082, -1.1516, -2.7695, -2.4169, -0.8981,\n",
      "         -0.6834, -4.2216, -0.1903,  0.3470, -3.3393, -4.0898, -1.2903, -2.0211,\n",
      "         -3.1497, -2.3583, -1.8519, -2.8745, -2.6078, -3.3464, -5.5866, -1.4211,\n",
      "         -0.4646, -0.3722, -0.4579, -0.5940, -0.3079, -0.3983, -4.8878, -4.9275,\n",
      "         -5.3017, -4.9129, -4.8059, -4.5663, -4.5119, -4.6321, -4.6019, -4.5486,\n",
      "         -4.6193, -4.6971, -4.7590, -4.7655, -4.7331, -4.6800, -4.6461, -4.6286,\n",
      "         -4.6299, -4.6317, -4.6277, -4.6374, -4.6516, -4.6648, -4.6799, -4.6945,\n",
      "         -4.6736, -4.6488, -4.6399, -4.6462, -4.6486, -4.6569, -4.6446, -4.6291,\n",
      "         -4.6172, -4.6114, -4.6271, -4.6462, -4.6560, -4.6514, -4.6434, -4.6372,\n",
      "         -4.6282, -4.6189, -4.6047, -4.5986, -4.5982, -4.5921, -4.5827, -4.5687,\n",
      "         -4.5558, -4.5575, -4.5485, -4.5409, -4.5403]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4332, -0.4325], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4305,  0.9900,  0.9900, -0.3949,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3627e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0871, -0.0880, -0.0889,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4199,  0.9608,  0.9656, -0.3852,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5789, -3.9478, -0.8950,  ..., -4.5427, -4.5329, -4.5343],\n",
      "        [-2.9390, -3.9087, -0.9643,  ..., -5.4500, -5.4378, -5.4249],\n",
      "        [-2.8552, -3.9310, -0.9486,  ..., -5.3898, -5.4002, -5.4145],\n",
      "        ...,\n",
      "        [-2.9506, -3.9038, -0.9643,  ..., -5.5169, -5.5043, -5.4889],\n",
      "        [-2.8608, -3.9314, -0.9377,  ..., -5.4024, -5.4161, -5.4322],\n",
      "        [-2.9547, -3.9034, -0.9634,  ..., -5.4400, -5.4293, -5.4108]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3830,  0.6475,  0.8902, -0.3852,  0.8891,  0.6375,  0.9271,  0.7110],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.3962,  0.9900, -1.2340,  0.9900, -0.9268,  0.9900, -0.4445],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7580e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0801, -0.0810, -0.0818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -0.9084, -0.9176, -0.9268],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0899, -0.0908, -0.0917,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3864,  0.9656, -1.2096,  0.9656, -0.8862,  0.9656, -0.4335],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8746, -3.9328, -0.9517,  ..., -5.4643, -5.4761, -5.4918],\n",
      "        [-2.5979, -3.9374, -0.8974,  ..., -4.6305, -4.6190, -4.6207],\n",
      "        [-2.8728, -3.9271, -0.9500,  ..., -5.4292, -5.4424, -5.4634],\n",
      "        ...,\n",
      "        [-2.6785, -3.8879, -0.9128,  ..., -0.8364, -0.8422, -0.8202],\n",
      "        [-2.8661, -3.9299, -0.9514,  ..., -5.4402, -5.4485, -5.4654],\n",
      "        [-2.5865, -3.9411, -0.8939,  ..., -4.5720, -4.5643, -4.5619]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9601, -0.3760,  0.9739, -0.9359,  0.9046, -0.8531,  0.8855, -0.3535],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.2392, -0.4410,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.8718e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0412, -0.0416, -0.0420,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0892, -0.0901, -0.0910,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.2333, -0.4302,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8717, -3.9233, -0.9494,  ..., -5.4325, -5.4422, -5.4570],\n",
      "        [-2.8710, -3.9245, -0.9503,  ..., -5.4367, -5.4461, -5.4631],\n",
      "        [-2.8779, -3.9252, -0.9415,  ..., -5.4081, -5.4208, -5.4406],\n",
      "        ...,\n",
      "        [-2.6858, -3.9209, -0.9040,  ..., -4.7135, -4.7247, -4.7301],\n",
      "        [-2.6056, -3.9381, -0.8940,  ..., -4.6084, -4.5962, -4.5936],\n",
      "        [-2.8165, -3.9171, -0.9226,  ..., -5.4026, -5.4411, -5.4550]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8095,  0.9115,  0.9491,  0.9115,  0.9544, -0.2863, -0.4032,  0.9358],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4278, -0.3999,  0.9900, -0.3928, -0.4386, -0.4034,  0.9900, -1.2367],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5154e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.8569e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0865, -0.0874, -0.0883,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0809, -0.0817, -0.0825,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0816, -0.0824, -0.0833,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2527, -0.2553, -0.2578,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4173, -0.3900,  0.9656, -0.3831, -0.4278, -0.3934,  0.9656, -1.2122],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6229, -3.9405, -0.8982,  ..., -4.5993, -4.5909, -4.5924],\n",
      "        [-2.6231, -3.9384, -0.8996,  ..., -4.6190, -4.6044, -4.6067],\n",
      "        [-2.8827, -3.9273, -0.9453,  ..., -5.4359, -5.4438, -5.4587],\n",
      "        ...,\n",
      "        [-2.6278, -3.9428, -0.8949,  ..., -4.5456, -4.5375, -4.5389],\n",
      "        [-2.8505, -3.9266, -0.9314,  ..., -5.2542, -5.3025, -5.3390],\n",
      "        [-2.5217, -3.9463, -0.8639,  ..., -4.4889, -4.4797, -4.4808]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3922, -0.3859,  0.9829, -0.4015, -0.4056, -0.3659,  0.9632, -0.9115],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2238, -1.2373, -0.4377, -0.4450, -0.2377, -0.4274, -0.4173, -0.4302],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0268e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0386, -0.0389, -0.0393,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2528, -0.2554, -0.2580,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0886, -0.0894, -0.0904,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0865, -0.0873, -0.0882,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0844, -0.0853, -0.0861,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0870, -0.0879, -0.0888,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2183, -1.2128, -0.4269, -0.4340, -0.2318, -0.4169, -0.4070, -0.4196],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6899, -3.9209, -0.8981,  ..., -4.6768, -4.6920, -4.6977],\n",
      "        [-2.5211, -3.9481, -0.8642,  ..., -4.4897, -4.4797, -4.4785],\n",
      "        [-2.6189, -3.9388, -0.8964,  ..., -4.6126, -4.5953, -4.5931],\n",
      "        ...,\n",
      "        [-2.6239, -3.9418, -0.8998,  ..., -4.5993, -4.5894, -4.5897],\n",
      "        [-2.6237, -3.9401, -0.8981,  ..., -4.5660, -4.5573, -4.5624],\n",
      "        [-2.6176, -3.9433, -0.9034,  ..., -4.5722, -4.5567, -4.5535]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2763, -0.9102, -0.4048, -0.3387, -0.2824, -0.3910, -0.3705, -0.3591],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4231,  0.9900, -0.4438, -0.4086, -0.2428, -0.3987,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6781e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.0865, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0418, -0.0422, -0.0427,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0807, -0.0815, -0.0823,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608, -0.4127,  0.9656, -0.4328, -0.3985, -0.2368, -0.3889,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9612, -3.8996, -0.9672,  ..., -5.4845, -5.4752, -5.4611],\n",
      "        [-2.6175, -3.9408, -0.8987,  ..., -4.5663, -4.5584, -4.5587],\n",
      "        [-2.9733, -3.9002, -0.9689,  ..., -5.5583, -5.5525, -5.5396],\n",
      "        ...,\n",
      "        [-2.6852, -3.9209, -0.9036,  ..., -4.7445, -4.7553, -4.7567],\n",
      "        [-2.6163, -3.9423, -0.9015,  ..., -4.6325, -4.6126, -4.6033],\n",
      "        [-2.8754, -3.9248, -0.9511,  ..., -5.4376, -5.4492, -5.4652]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6741, -0.3732,  0.6222, -0.3986, -0.3645, -0.2817, -0.3988,  0.8834],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9292], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5366e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1806, -0.1824, -0.1842, -0.1861, -0.1880, -0.1899, -0.1918, -0.1937,\n",
      "         -0.1957, -0.1977, -0.1997, -0.2017, -0.2037, -0.2058, -0.2078, -0.2099,\n",
      "         -0.2121, -0.2142, -0.2164, -0.2186, -0.2208, -0.2230, -0.2252, -0.2275,\n",
      "         -0.2298, -0.2321, -0.2345, -0.2369, -0.2392, -0.2417, -0.2441, -0.2466,\n",
      "         -0.2491, -0.2516, -0.2541, -0.2567, -0.2593, -0.2619, -0.2645, -0.2672,\n",
      "         -0.2699, -0.2726, -0.2754, -0.2782, -0.2810, -0.2838, -0.2867, -0.2896,\n",
      "         -0.2925, -0.2955, -0.2984, -0.3015, -0.3045, -0.3076, -0.3107, -0.3138,\n",
      "         -0.3170, -0.3202, -0.3234, -0.3267, -0.3300, -0.3333, -0.3367, -0.3401,\n",
      "         -0.3435, -0.3470, -0.3505, -0.3541, -0.3576, -0.3612, -0.3649, -0.3686,\n",
      "         -0.3723, -0.3761, -0.3799, -0.3837, -0.3876, -0.3915, -0.3954, -0.3994,\n",
      "         -0.4035, -0.4075, -0.4117, -0.4158, -0.4200, -0.4243, -0.4285, -0.4329,\n",
      "         -0.4373, -0.4417, -0.4461, -0.4506, -0.4552, -0.4598, -0.4644, -0.4691,\n",
      "         -0.4739, -0.4786, -0.4835, -0.4884, -0.4933, -0.4983, -0.5033, -0.5084,\n",
      "         -0.5135, -0.5187, -0.5240, -0.5293, -0.5346, -0.5400, -0.5455, -0.5510,\n",
      "         -0.5565, -0.5621, -0.5678, -0.5736, -0.5794, -0.5852, -0.5911, -0.5971,\n",
      "         -0.6031, -0.6092, -0.6154, -0.6216, -0.6279, -0.6342, -0.6406, -0.6471,\n",
      "         -0.6536, -0.6602, -0.6669, -0.6736, -0.6804, -0.6873, -0.6942, -0.7013,\n",
      "         -0.7083, -0.7155, -0.7227, -0.7300, -0.7374, -0.7448, -0.7524, -0.7600,\n",
      "         -0.7676, -0.7754, -0.7832, -0.7911, -0.7991, -0.8072, -0.8154, -0.8236,\n",
      "         -0.8319, -0.8403, -0.8488, -0.8574, -0.8660, -0.8748, -0.8836, -0.8925,\n",
      "         -0.9016, -0.9107, -0.9199, -0.9292,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8884], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8703, -3.9235, -0.9495, -2.4807, -0.0415, -2.4433, -4.6806, -0.6382,\n",
      "         -4.1218, -2.6500, -3.9835,  0.6236, -0.2027, -3.6530, -3.0796, -2.5224,\n",
      "         -3.3244, -1.9032, -1.9525, -1.1975, -1.5354, -1.6602, -1.0359,  0.3182,\n",
      "         -2.2091, -3.4998, -3.0700, -2.7055, -3.9648,  0.8682, -1.9407, -0.4352,\n",
      "         -0.9055, -3.5961, -1.8721, -1.5356, -0.9019, -3.2226, -1.8258, -0.2488,\n",
      "         -0.8303, -0.5632, -0.4918, -4.1846, -1.8769, -3.5460, -3.6205, -3.2940,\n",
      "          0.5160,  0.5161, -3.8593, -1.6228, -6.9538, -3.0012, -3.0975, -1.1316,\n",
      "         -2.5333, -3.0797, -2.6063, -1.9012, -3.6632, -1.3444, -5.7480, -2.5172,\n",
      "         -3.7447,  0.9744, -1.5656, -0.3234, -0.3685, -3.2243, -4.1267, -5.3359,\n",
      "         -2.8831, -0.0666, -3.8362, -4.1363, -2.5828, -3.9071, -2.6683, -3.5772,\n",
      "         -4.6919, -2.5979, -0.4906, -1.4893, -1.7733, -3.1653, -2.0142, -0.9026,\n",
      "         -3.6147, -1.0037, -3.2094, -4.0210, -2.5465, -0.7382, -1.1173, -2.9399,\n",
      "         -2.9921, -3.8940, -3.2593, -3.3414, -0.2868, -1.2079, -3.7136, -2.0960,\n",
      "         -0.5094, -2.1109, -3.7123, -3.8534,  0.6277, -0.9585, -0.9823, -0.7839,\n",
      "         -4.8482, -4.6835,  0.1506, -1.1787, -2.2077, -1.5027, -0.1739, -1.1983,\n",
      "         -3.1300, -4.4600, -2.8229, -1.0191, -1.2421, -2.8435, -2.5171, -0.9658,\n",
      "         -0.8004, -4.2374,  0.1851,  0.6213, -3.3996, -4.1051, -1.3715, -2.0067,\n",
      "         -3.1614, -2.4219, -1.9642, -2.8799, -3.0690, -5.7547, -1.7936, -2.1354,\n",
      "         -1.0292, -0.5894, -0.8244, -4.2427, -3.9250, -0.1084, -2.0361, -0.3731,\n",
      "         -0.6679, -0.3629, -0.0981, -0.6295, -4.0983, -3.3409, -4.9467, -1.5012,\n",
      "          0.9408,  0.9127,  0.9492,  1.0174,  0.7379,  0.9244, -4.7277, -4.8897,\n",
      "         -5.2196, -4.9687, -5.1948, -5.3335, -5.3142, -5.2880, -5.2350, -5.1881,\n",
      "         -5.1523, -5.0812, -5.0181, -4.9839, -4.9788, -4.9773, -4.9986, -5.0267,\n",
      "         -5.0340, -5.0289, -5.0366, -5.0683, -5.1280, -5.2099, -5.2785, -5.3217,\n",
      "         -5.3548, -5.3991, -5.4575, -5.5207, -5.5379, -5.5199, -5.4946, -5.4616,\n",
      "         -5.4406, -5.4292, -5.4296, -5.4434, -5.4622],\n",
      "        [-2.5802, -3.9335, -0.8787, -2.2979,  0.0316, -2.3770, -4.5856, -0.4088,\n",
      "         -3.8308, -2.4667, -3.9151,  0.6556, -0.1085, -3.5707, -3.1357, -2.3833,\n",
      "         -3.2180, -1.8228, -1.9813, -1.0316, -1.4434, -1.4855, -0.9093,  0.2779,\n",
      "         -2.0034, -3.3925, -3.1016, -2.5880, -3.9570,  0.8576, -1.8164, -0.2590,\n",
      "         -0.9032, -3.5369, -1.8252, -1.6458, -0.9651, -3.1410, -1.6742, -0.0919,\n",
      "         -0.7670, -0.5974, -0.4471, -4.2023, -1.7075, -3.2894, -3.4176, -3.2363,\n",
      "          0.1371,  0.2431, -3.7890, -1.5313, -6.9269, -2.9133, -3.0420, -1.0317,\n",
      "         -2.5197, -3.0426, -2.5147, -1.7320, -3.5739, -1.2556, -5.6964, -2.4563,\n",
      "         -3.7286,  0.9476, -1.4594, -0.1452, -0.3890, -3.1584, -3.9989, -5.3082,\n",
      "         -2.8021, -0.1381, -3.7564, -4.1514, -2.5040, -3.8510, -2.5423, -3.5080,\n",
      "         -4.6613, -2.5277, -0.4428, -1.3293, -1.6938, -3.0198, -1.8935, -0.7359,\n",
      "         -3.5238, -0.9180, -3.1226, -4.0255, -2.4823, -0.7042, -0.9517, -2.9781,\n",
      "         -2.8347, -3.6706, -3.2131, -3.2417, -0.2205, -1.0499, -3.7303, -2.0658,\n",
      "         -0.5422, -1.9307, -3.5115, -3.7469,  0.5568, -5.0483, -1.3601, -2.7324,\n",
      "         -4.2502, -4.4905,  0.3121, -1.0077, -2.1500, -1.3815, -0.0946, -1.0134,\n",
      "         -2.8120, -4.5711, -2.7654, -1.0070, -1.1152, -2.7011, -2.3842, -0.8969,\n",
      "         -0.7839, -4.2198, -0.1446,  0.3312, -3.2889, -4.0867, -1.3106, -1.9972,\n",
      "         -3.1810, -2.3241, -1.8235, -2.9148, -2.6266, -3.4256, -5.7070, -1.3623,\n",
      "         -1.0674, -0.8694, -0.9873, -0.9024, -0.9860, -0.8786, -0.9041, -0.8384,\n",
      "         -0.8595, -1.0732, -4.8081, -4.8761, -5.1643, -4.8705, -5.0064, -4.9318,\n",
      "         -4.8755, -4.9004, -4.8978, -4.9092, -4.9466, -4.9028, -4.8151, -4.7380,\n",
      "         -4.6746, -4.6186, -4.5577, -4.4927, -4.4194, -4.3677, -4.3455, -4.3578,\n",
      "         -4.3862, -4.4223, -4.4600, -4.4867, -4.4895, -4.4730, -4.4392, -4.4237,\n",
      "         -4.4274, -4.4514, -4.4946, -4.5335, -4.5630, -4.5748, -4.5768, -4.5807,\n",
      "         -4.5861, -4.5891, -4.5909, -4.6060, -4.6220, -4.6221, -4.6307, -4.6173,\n",
      "         -4.6079, -4.6000, -4.5902, -4.5857, -4.5955]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9137, -0.9366], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2582,  0.9900, -0.2604,  0.9900,  0.9900,  0.9900, -0.2604, -0.4431],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7298e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2571, -0.2597, -0.2623,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0449, -0.0453, -0.0458,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0449, -0.0453, -0.0458,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0896, -0.0905, -0.0915,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.2332,  0.9656, -0.2540,  0.9656,  0.9656,  0.9656, -0.2540, -0.4321],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5098, -3.9479, -0.8603,  ..., -4.4859, -4.4776, -4.4785],\n",
      "        [-2.8730, -3.9223, -0.9364,  ..., -5.3946, -5.4076, -5.4273],\n",
      "        [-2.6794, -3.9206, -0.9000,  ..., -4.7250, -4.7338, -4.7352],\n",
      "        ...,\n",
      "        [-2.9664, -3.8942, -0.9626,  ..., -5.5339, -5.5257, -5.5138],\n",
      "        [-2.6794, -3.9206, -0.9000,  ..., -4.7250, -4.7338, -4.7352],\n",
      "        [-2.6067, -3.9417, -0.8911,  ..., -4.5503, -4.5439, -4.5458]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9085,  0.9576, -0.2759,  0.9576,  0.9310,  0.6734, -0.2759, -0.3727],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4710,  0.9900,  0.9900,  0.9900,  0.9900, -1.2625],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.0814e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0953, -0.0963, -0.0972,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2580, -0.2606, -0.2632,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4594,  0.9656,  0.9656,  0.9656,  0.9656, -1.2375],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8586, -3.9213, -0.9418,  ..., -5.4108, -5.4228, -5.4429],\n",
      "        [-2.8689, -3.9265, -0.9458,  ..., -5.4399, -5.4531, -5.4733],\n",
      "        [-2.5894, -3.9374, -0.8833,  ..., -4.5973, -4.5866, -4.5847],\n",
      "        ...,\n",
      "        [-2.8625, -3.9252, -0.9375,  ..., -5.4104, -5.4204, -5.4333],\n",
      "        [-2.8312, -3.9286, -0.9246,  ..., -5.2633, -5.3158, -5.3535],\n",
      "        [-2.5030, -3.9481, -0.8556,  ..., -4.4836, -4.4743, -4.4743]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9199,  0.9790, -0.3942,  0.9317,  0.9199,  0.9550,  0.9618, -0.9074],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4274, -0.4725,  0.9900,  0.9900, -0.4615, -0.4357, -0.2488, -1.2604],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4511e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0865, -0.0873, -0.0882,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0956, -0.0965, -0.0975,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0881, -0.0890, -0.0899,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0429, -0.0433, -0.0437,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2576, -0.2602, -0.2628,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4169, -0.4608,  0.9608,  0.9656, -0.4501, -0.4250, -0.2427, -1.2355],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5810, -3.9401, -0.8825,  ..., -4.5990, -4.5821, -4.5762],\n",
      "        [-2.5792, -3.9382, -0.8786,  ..., -4.5717, -4.5608, -4.5604],\n",
      "        [-2.9422, -3.8950, -0.9543,  ..., -5.4721, -5.4667, -5.4544],\n",
      "        ...,\n",
      "        [-2.5860, -3.9436, -0.8756,  ..., -4.5229, -4.5184, -4.5203],\n",
      "        [-2.6625, -3.9205, -0.8841,  ..., -4.6413, -4.6504, -4.6565],\n",
      "        [-2.4942, -3.9461, -0.8502,  ..., -4.4799, -4.4723, -4.4731]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3885, -0.4061,  0.6847,  0.9853, -0.3892, -0.3561, -0.2673, -0.9148],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4598, -0.4524,  0.9900, -0.4524,  0.9900, -0.4656, -0.4400, -0.2493],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.3610e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0930, -0.0940, -0.0949,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0915, -0.0924, -0.0934,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0942, -0.0951, -0.0961,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0890, -0.0899, -0.0908,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0430, -0.0434, -0.0438,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4485, -0.4412,  0.9656, -0.4412,  0.9656, -0.4541, -0.4292, -0.2432],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5711, -3.9365, -0.8728,  ..., -4.5675, -4.5584, -4.5580],\n",
      "        [-2.5692, -3.9403, -0.8782,  ..., -4.5428, -4.5338, -4.5350],\n",
      "        [-2.7917, -3.9123, -0.9046,  ..., -5.4126, -5.4572, -5.4666],\n",
      "        ...,\n",
      "        [-2.5643, -3.9366, -0.8710,  ..., -4.5623, -4.5546, -4.5524],\n",
      "        [-2.5798, -3.9377, -0.8742,  ..., -4.5287, -4.5232, -4.5271],\n",
      "        [-2.6541, -3.9149, -0.8850,  ..., -4.6919, -4.7027, -4.7037]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4083, -0.3501,  0.9383, -0.3501,  0.9119, -0.3405, -0.3732, -0.2660],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9174, -0.4397, -0.4236, -0.3898,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.1362e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.8016e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1783, -0.1801, -0.1819,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0890, -0.0899, -0.0908,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8772, -0.4289, -0.4131, -0.3802,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9423, -3.8923, -0.9463,  ..., -5.5539, -5.5486, -5.5405],\n",
      "        [-2.5424, -3.9301, -0.8562,  ..., -4.5355, -4.5354, -4.5471],\n",
      "        [-2.5620, -3.9357, -0.8694,  ..., -4.5618, -4.5531, -4.5514],\n",
      "        ...,\n",
      "        [-2.9324, -3.8926, -0.9453,  ..., -5.4746, -5.4682, -5.4563],\n",
      "        [-2.9463, -3.8876, -0.9415,  ..., -5.4557, -5.4464, -5.4322],\n",
      "        [-2.8034, -3.9167, -0.9172,  ..., -5.1936, -5.2403, -5.2730]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6134, -0.9031, -0.3447, -0.3934, -0.3610,  0.6785,  0.7157,  0.9086],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3713, -0.3642,  0.9900, -0.3679, -0.4096,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4080e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0751, -0.0759, -0.0766,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0737, -0.0744, -0.0752,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3621, -0.3552,  0.9656, -0.3588, -0.3995,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5887, -3.9430, -0.8813,  ..., -4.5256, -4.5188, -4.5201],\n",
      "        [-2.5816, -3.9334, -0.8887,  ..., -4.6393, -4.6265, -4.6264],\n",
      "        [-2.8431, -3.9181, -0.9344,  ..., -5.3795, -5.3918, -5.4090],\n",
      "        ...,\n",
      "        [-2.8364, -3.9140, -0.9338,  ..., -5.3672, -5.3757, -5.3899],\n",
      "        [-2.8431, -3.9181, -0.9344,  ..., -5.3795, -5.3918, -5.4090],\n",
      "        [-2.8096, -3.9225, -0.9186,  ..., -5.2634, -5.3135, -5.3481]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3729, -0.3836,  0.8629, -0.4010, -0.4148,  0.8061,  0.8629,  0.9375],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3416, -0.8893], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0691, -0.0698, -0.0705, -0.0712, -0.0719, -0.0727, -0.0734, -0.0741,\n",
      "         -0.0749, -0.0757, -0.0764, -0.0772, -0.0780, -0.0788, -0.0796, -0.0804,\n",
      "         -0.0812, -0.0820, -0.0828, -0.0837, -0.0845, -0.0853, -0.0862, -0.0871,\n",
      "         -0.0880, -0.0888, -0.0897, -0.0907, -0.0916, -0.0925, -0.0934, -0.0944,\n",
      "         -0.0953, -0.0963, -0.0973, -0.0982, -0.0992, -0.1002, -0.1013, -0.1023,\n",
      "         -0.1033, -0.1044, -0.1054, -0.1065, -0.1075, -0.1086, -0.1097, -0.1108,\n",
      "         -0.1120, -0.1131, -0.1142, -0.1154, -0.1165, -0.1177, -0.1189, -0.1201,\n",
      "         -0.1213, -0.1226, -0.1238, -0.1250, -0.1263, -0.1276, -0.1289, -0.1302,\n",
      "         -0.1315, -0.1328, -0.1342, -0.1355, -0.1369, -0.1383, -0.1397, -0.1411,\n",
      "         -0.1425, -0.1439, -0.1454, -0.1469, -0.1483, -0.1498, -0.1514, -0.1529,\n",
      "         -0.1544, -0.1560, -0.1576, -0.1592, -0.1608, -0.1624, -0.1640, -0.1657,\n",
      "         -0.1674, -0.1690, -0.1708, -0.1725, -0.1742, -0.1760, -0.1778, -0.1796,\n",
      "         -0.1814, -0.1832, -0.1850, -0.1869, -0.1888, -0.1907, -0.1926, -0.1946,\n",
      "         -0.1966, -0.1985, -0.2005, -0.2026, -0.2046, -0.2067, -0.2088, -0.2109,\n",
      "         -0.2130, -0.2152, -0.2173, -0.2195, -0.2217, -0.2240, -0.2262, -0.2285,\n",
      "         -0.2308, -0.2332, -0.2355, -0.2379, -0.2403, -0.2427, -0.2452, -0.2477,\n",
      "         -0.2502, -0.2527, -0.2552, -0.2578, -0.2604, -0.2631, -0.2657, -0.2684,\n",
      "         -0.2711, -0.2739, -0.2766, -0.2794, -0.2822, -0.2851, -0.2880, -0.2909,\n",
      "         -0.2938, -0.2968, -0.2998, -0.3028, -0.3059, -0.3090, -0.3121, -0.3152,\n",
      "         -0.3184, -0.3216, -0.3249, -0.3282, -0.3315, -0.3348, -0.3382, -0.3416,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1035, -0.1046, -0.1056, -0.1067, -0.1078, -0.1088, -0.1099, -0.1111,\n",
      "         -0.1122, -0.1133, -0.1145, -0.1156, -0.1168, -0.1180, -0.1192, -0.1204,\n",
      "         -0.1216, -0.1228, -0.1240, -0.1253, -0.1266, -0.1278, -0.1291, -0.1304,\n",
      "         -0.1317, -0.1331, -0.1344, -0.1358, -0.1372, -0.1385, -0.1399, -0.1414,\n",
      "         -0.1428, -0.1442, -0.1457, -0.1471, -0.1486, -0.1501, -0.1517, -0.1532,\n",
      "         -0.1547, -0.1563, -0.1579, -0.1595, -0.1611, -0.1627, -0.1643, -0.1660,\n",
      "         -0.1677, -0.1694, -0.1711, -0.1728, -0.1746, -0.1763, -0.1781, -0.1799,\n",
      "         -0.1817, -0.1836, -0.1854, -0.1873, -0.1892, -0.1911, -0.1930, -0.1950,\n",
      "         -0.1969, -0.1989, -0.2009, -0.2030, -0.2050, -0.2071, -0.2092, -0.2113,\n",
      "         -0.2134, -0.2156, -0.2178, -0.2200, -0.2222, -0.2244, -0.2267, -0.2290,\n",
      "         -0.2313, -0.2336, -0.2360, -0.2384, -0.2408, -0.2432, -0.2457, -0.2482,\n",
      "         -0.2507, -0.2532, -0.2558, -0.2583, -0.2609, -0.2636, -0.2662, -0.2689,\n",
      "         -0.2716, -0.2744, -0.2772, -0.2800, -0.2828, -0.2856, -0.2885, -0.2914,\n",
      "         -0.2944, -0.2974, -0.3004, -0.3034, -0.3065, -0.3096, -0.3127, -0.3158,\n",
      "         -0.3190, -0.3223, -0.3255, -0.3288, -0.3321, -0.3355, -0.3389, -0.3423,\n",
      "         -0.3458, -0.3492, -0.3528, -0.3563, -0.3599, -0.3636, -0.3672, -0.3710,\n",
      "         -0.3747, -0.3785, -0.3823, -0.3862, -0.3901, -0.3940, -0.3980, -0.4020,\n",
      "         -0.4061, -0.4102, -0.4143, -0.4185, -0.4227, -0.4270, -0.4313, -0.4357,\n",
      "         -0.4401, -0.4445, -0.4490, -0.4535, -0.4581, -0.4627, -0.4674, -0.4721,\n",
      "         -0.4769, -0.4817, -0.4866, -0.4915, -0.4965, -0.5015, -0.5066, -0.5117,\n",
      "         -0.5168, -0.5221, -0.5273, -0.5327, -0.5380, -0.5435, -0.5490, -0.5545,\n",
      "         -0.5601, -0.5658, -0.5715, -0.5773, -0.5831, -0.5890, -0.5949, -0.6009,\n",
      "         -0.6070, -0.6131, -0.6193, -0.6256, -0.6319, -0.6383, -0.6447, -0.6512,\n",
      "         -0.6578, -0.6645, -0.6712, -0.6780, -0.6848, -0.6917, -0.6987, -0.7058,\n",
      "         -0.7129, -0.7201, -0.7274, -0.7347, -0.7421, -0.7496, -0.7572, -0.7649,\n",
      "         -0.7726, -0.7804, -0.7883, -0.7962, -0.8043, -0.8124, -0.8206, -0.8289,\n",
      "         -0.8373, -0.8457, -0.8543, -0.8629, -0.8716, -0.8804, -0.8893]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3332, -0.8503], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5892, -3.9392, -0.8977, -2.2920,  0.0499, -2.3750, -4.6111, -0.4029,\n",
      "         -3.8497, -2.4895, -3.8745,  0.6231, -0.1263, -3.5858, -3.1530, -2.3875,\n",
      "         -3.2156, -1.8150, -2.0028, -1.0353, -1.4704, -1.4914, -0.9117,  0.2902,\n",
      "         -1.9917, -3.4174, -3.1203, -2.5879, -3.9497,  0.8765, -1.8091, -0.2636,\n",
      "         -0.9130, -3.4841, -1.8427, -1.6478, -0.9312, -3.1393, -1.7020, -0.0962,\n",
      "         -0.7628, -0.5700, -0.4283, -4.2015, -1.6882, -3.2998, -3.4224, -3.1823,\n",
      "          0.1900,  0.2669, -3.8129, -1.5340, -6.9621, -2.9173, -3.0317, -1.0144,\n",
      "         -2.5586, -3.0300, -2.5244, -1.7107, -3.5927, -1.2617, -5.7333, -2.4581,\n",
      "         -3.7214,  0.9734, -1.4407, -0.1415, -0.3917, -3.1072, -4.0448, -5.3613,\n",
      "         -2.8044, -0.1296, -3.7748, -4.1523, -2.4995, -3.8409, -2.5672, -3.5263,\n",
      "         -4.7163, -2.5253, -0.4325, -1.3277, -1.7105, -3.0350, -1.8810, -0.7326,\n",
      "         -3.5634, -0.9133, -3.1454, -4.0242, -2.4782, -0.6974, -0.9529, -2.9889,\n",
      "         -2.8177, -3.6759, -3.2216, -3.2484, -0.2139, -1.0484, -3.7245, -2.0675,\n",
      "         -0.5299, -1.9306, -3.5080, -3.7859,  0.5684, -0.7502, -1.1932, -0.2716,\n",
      "         -4.4127, -4.5711,  0.2433, -1.0011, -2.1689, -1.4044, -0.1144, -1.0356,\n",
      "         -2.8346, -4.4684, -2.7731, -1.0101, -1.1381, -2.7396, -2.3755, -0.8807,\n",
      "         -0.6555, -4.1673, -0.1230,  0.3343, -3.3056, -4.1077, -1.2685, -2.0302,\n",
      "         -3.1378, -2.3438, -1.8314, -2.8681, -2.5719, -3.3280, -5.6742, -1.4300,\n",
      "         -0.3957, -0.4278, -0.3982, -0.4495, -0.3809, -0.4578, -4.8532, -4.9269,\n",
      "         -5.3013, -4.9068, -4.7973, -4.5003, -4.4551, -4.5892, -4.5705, -4.4990,\n",
      "         -4.5854, -4.7085, -4.7775, -4.7732, -4.7170, -4.6391, -4.5780, -4.5475,\n",
      "         -4.5432, -4.5449, -4.5439, -4.5671, -4.6044, -4.6338, -4.6625, -4.6727,\n",
      "         -4.6475, -4.6292, -4.6433, -4.6557, -4.6625, -4.6815, -4.6757, -4.6561,\n",
      "         -4.6403, -4.6329, -4.6502, -4.6718, -4.6907, -4.6985, -4.7031, -4.7142,\n",
      "         -4.7160, -4.7056, -4.6910, -4.6803, -4.6798, -4.6671, -4.6542, -4.6394,\n",
      "         -4.6321, -4.6273, -4.6123, -4.5921, -4.5858],\n",
      "        [-2.6527, -3.8777, -0.9077, -2.3183,  0.0461, -2.4047, -4.6601, -0.4104,\n",
      "         -3.9174, -2.4805, -3.9232,  0.5824, -0.1578, -3.5749, -3.1662, -2.4276,\n",
      "         -3.2531, -1.8104, -2.0122, -1.0376, -1.4379, -1.4863, -0.8999,  0.2720,\n",
      "         -2.0395, -3.3939, -3.1180, -2.6189, -3.9657,  0.8501, -1.8195, -0.2636,\n",
      "         -0.8812, -3.5366, -1.7932, -1.6401, -0.9199, -3.1726, -1.7025, -0.0567,\n",
      "         -0.7477, -0.5647, -0.4482, -4.1994, -1.7254, -3.3384, -3.4260, -3.2276,\n",
      "          0.2371,  0.3084, -3.7867, -1.5834, -6.9804, -2.9393, -3.0615, -1.0188,\n",
      "         -2.5332, -3.0285, -2.5340, -1.7391, -3.5669, -1.2994, -5.7417, -2.4649,\n",
      "         -3.7287,  0.9533, -1.4476, -0.1440, -0.3658, -3.1534, -4.0318, -5.3170,\n",
      "         -2.8230, -0.1093, -3.7465, -4.1641, -2.5110, -3.8717, -2.5677, -3.4918,\n",
      "         -4.6686, -2.5313, -0.4104, -1.3156, -1.6522, -3.0581, -1.8947, -0.7310,\n",
      "         -3.5184, -0.9013, -3.1156, -4.0356, -2.4842, -0.6741, -0.9382, -2.9540,\n",
      "         -2.8458, -3.7077, -3.1843, -3.2547, -0.1979, -1.0272, -3.7331, -2.0524,\n",
      "         -0.5417, -1.9195, -3.5318, -3.7955,  0.5888, -5.0643, -1.3505, -2.7667,\n",
      "         -4.2809, -4.5012,  0.3462, -0.9839, -2.1409, -1.3603, -0.1204, -0.9981,\n",
      "         -2.8113, -4.5584, -2.7526, -1.0135, -1.0968, -2.7225, -2.3620, -0.8947,\n",
      "         -0.7696, -4.2023, -0.0412,  0.4012, -3.3160, -4.1399, -1.2958, -2.0072,\n",
      "         -3.1613, -2.3453, -1.8165, -2.8835, -2.9578, -5.7682, -1.5670, -5.8517,\n",
      "         -1.1689, -1.0634, -0.9111, -0.7984, -0.8826, -0.8093, -0.8218, -4.1461,\n",
      "         -4.7345, -0.2157, -1.9560, -4.5181, -5.7006, -4.7141, -5.3419, -1.8854,\n",
      "         -5.4971, -0.3567, -0.7062, -0.6146, -0.6769, -0.5861, -0.5944, -0.7823,\n",
      "         -3.9997, -3.8482,  0.2319, -1.6631, -3.6438, -4.5949, -3.6258, -4.8371,\n",
      "         -1.4056, -5.1975, -0.3947, -0.4791, -0.4856, -0.4633, -0.4555, -0.4202,\n",
      "         -0.6021, -3.8563, -3.7022,  0.1249, -1.3941, -3.6066, -4.6874, -3.4952,\n",
      "         -3.4094, -4.9202, -1.5458, -0.7700, -0.8118, -0.7089, -0.9294, -0.7694,\n",
      "         -0.9434, -0.8411, -0.8387, -0.8028, -0.8332]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4183, -0.8249], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.1740,  0.9900,  0.9900,  0.9900, -0.3757,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.8721e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0300, -0.0303, -0.0306,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0760, -0.0768, -0.0776,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.1697,  0.9656,  0.9656,  0.9656, -0.3665,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8470, -3.9152, -0.9405,  ..., -5.3476, -5.3600, -5.3751],\n",
      "        [-2.6559, -3.9123, -0.9039,  ..., -4.7387, -4.7523, -4.7539],\n",
      "        [-2.8105, -3.9219, -0.9301,  ..., -5.2669, -5.3168, -5.3501],\n",
      "        ...,\n",
      "        [-2.5976, -3.9401, -0.9027,  ..., -4.5856, -4.5798, -4.5787],\n",
      "        [-2.8058, -3.9149, -0.9407,  ..., -5.1934, -5.2384, -5.2702],\n",
      "        [-2.8419, -3.9182, -0.9448,  ..., -5.3682, -5.3742, -5.3916]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9205, -0.2774,  0.9352,  0.9121,  0.9010, -0.4312,  0.9016,  0.9129],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1809,  0.9900, -0.9060, -0.3575, -0.3737,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6501e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2413, -0.2437, -0.2462,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0756, -0.0764, -0.0771,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1575,  0.9656, -0.8663, -0.3487, -0.3645,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8101, -3.9203, -0.9370,  ..., -5.2670, -5.3204, -5.3534],\n",
      "        [-2.4866, -3.9424, -0.8685,  ..., -4.4646, -4.4524, -4.4494],\n",
      "        [-2.8448, -3.9127, -0.9458,  ..., -5.3527, -5.3675, -5.3806],\n",
      "        ...,\n",
      "        [-2.6030, -3.9362, -0.9079,  ..., -4.5597, -4.5508, -4.5496],\n",
      "        [-2.8359, -3.9096, -0.9529,  ..., -5.3741, -5.3843, -5.4000],\n",
      "        [-2.7866, -3.9084, -0.9297,  ..., -5.3714, -5.4140, -5.4257]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9404, -0.9220,  0.9205, -0.8334, -0.3932, -0.4297,  0.8176,  0.9257],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.3631,  0.9900,  0.9900,  0.9900, -0.4012,  0.9900, -0.3619],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9464e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0735, -0.0742, -0.0750,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0812, -0.0820, -0.0828,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0732, -0.0740, -0.0747,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3542,  0.9656,  0.9656,  0.9656, -0.3913,  0.9656, -0.3530],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9445, -3.8804, -0.9739,  ..., -5.5469, -5.5372, -5.5244],\n",
      "        [-2.5962, -3.9300, -0.9141,  ..., -4.6628, -4.6490, -4.6458],\n",
      "        [-2.8343, -3.9110, -0.9567,  ..., -5.3892, -5.4009, -5.4232],\n",
      "        ...,\n",
      "        [-2.5893, -3.9380, -0.9156,  ..., -4.5658, -4.5504, -4.5505],\n",
      "        [-2.8447, -3.9151, -0.9594,  ..., -5.4214, -5.4315, -5.4550],\n",
      "        [-2.5949, -3.9343, -0.9126,  ..., -4.6258, -4.6078, -4.5944]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6354, -0.4281,  0.9091,  0.8680,  0.8680, -0.3940,  0.9649, -0.4291],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3925, -0.4391, -0.9215, -1.2202, -0.2094, -0.2232, -0.3963,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(7.2578e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0794, -0.0802, -0.0810,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0888, -0.0897, -0.0906,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1791, -0.1809, -0.1827,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0385, -0.0388, -0.0392,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0802, -0.0810, -0.0818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3828, -0.4283, -0.8811, -1.1960, -0.2043, -0.2177, -0.3865,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5957, -3.9312, -0.9110,  ..., -4.6136, -4.5992, -4.6026],\n",
      "        [-2.5795, -3.9298, -0.9070,  ..., -4.5884, -4.5778, -4.5711],\n",
      "        [-2.5446, -3.9227, -0.8878,  ..., -4.5539, -4.5543, -4.5663],\n",
      "        ...,\n",
      "        [-2.6509, -3.9078, -0.9120,  ..., -4.7462, -4.7567, -4.7585],\n",
      "        [-2.5999, -3.9366, -0.9060,  ..., -4.5299, -4.5222, -4.5222],\n",
      "        [-2.9396, -3.8817, -0.9758,  ..., -5.5813, -5.5738, -5.5641]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4215, -0.3928, -0.9161, -0.9120, -0.2683, -0.2646, -0.3928,  0.6172],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2485, -0.4542,  0.9900,  0.9900, -0.4479, -0.4125,  0.9900, -0.4584],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8806e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0428, -0.0432, -0.0437,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0919, -0.0928, -0.0938,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0834, -0.0843, -0.0851,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0927, -0.0937, -0.0946,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2423, -0.4430,  0.9656,  0.9608, -0.4368, -0.4023,  0.9608, -0.4471],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6489, -3.9046, -0.9101,  ..., -4.7350, -4.7452, -4.7470],\n",
      "        [-2.5783, -3.9273, -0.9031,  ..., -4.5934, -4.5808, -4.5800],\n",
      "        [-2.8299, -3.9032, -0.9573,  ..., -5.3597, -5.3720, -5.3903],\n",
      "        ...,\n",
      "        [-2.5818, -3.9287, -0.9091,  ..., -4.6154, -4.5977, -4.5878],\n",
      "        [-2.9281, -3.8791, -0.9736,  ..., -5.5140, -5.5047, -5.4977],\n",
      "        [-2.5761, -3.9255, -0.9020,  ..., -4.6014, -4.5895, -4.5877]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2559, -0.4575,  0.9424,  0.7107, -0.4381, -0.4208,  0.7107, -0.4318],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4607,  0.9900, -0.4339,  0.9900, -1.2661, -0.4719,  0.9900, -0.4838],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9776e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0932, -0.0941, -0.0951,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0878, -0.0887, -0.0896,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0955, -0.0964, -0.0974,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0979, -0.0989, -0.0999,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4493,  0.9656, -0.4232,  0.9656, -1.2411, -0.4602,  0.9656, -0.4718],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5772, -3.9246, -0.8999,  ..., -4.5398, -4.5324, -4.5382],\n",
      "        [-2.9395, -3.8701, -0.9685,  ..., -5.4829, -5.4765, -5.4666],\n",
      "        [-2.5735, -3.9193, -0.9045,  ..., -4.6421, -4.6315, -4.6332],\n",
      "        ...,\n",
      "        [-2.5678, -3.9267, -0.9043,  ..., -4.5560, -4.5441, -4.5433],\n",
      "        [-2.8324, -3.9014, -0.9503,  ..., -5.4064, -5.4190, -5.4407],\n",
      "        [-2.5624, -3.9222, -0.8985,  ..., -4.5829, -4.5732, -4.5695]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4279,  0.6909, -0.4276,  0.9376, -0.8919, -0.3820,  0.9939, -0.3879],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4898, -0.4887], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0991, -0.1001, -0.1011, -0.1021, -0.1032, -0.1042, -0.1053, -0.1063,\n",
      "         -0.1074, -0.1085, -0.1096, -0.1107, -0.1118, -0.1129, -0.1141, -0.1152,\n",
      "         -0.1164, -0.1176, -0.1187, -0.1199, -0.1212, -0.1224, -0.1236, -0.1249,\n",
      "         -0.1261, -0.1274, -0.1287, -0.1300, -0.1313, -0.1326, -0.1340, -0.1353,\n",
      "         -0.1367, -0.1381, -0.1395, -0.1409, -0.1423, -0.1437, -0.1452, -0.1466,\n",
      "         -0.1481, -0.1496, -0.1511, -0.1527, -0.1542, -0.1558, -0.1573, -0.1589,\n",
      "         -0.1605, -0.1622, -0.1638, -0.1654, -0.1671, -0.1688, -0.1705, -0.1722,\n",
      "         -0.1740, -0.1757, -0.1775, -0.1793, -0.1811, -0.1829, -0.1848, -0.1867,\n",
      "         -0.1885, -0.1904, -0.1924, -0.1943, -0.1963, -0.1983, -0.2003, -0.2023,\n",
      "         -0.2043, -0.2064, -0.2085, -0.2106, -0.2127, -0.2149, -0.2170, -0.2192,\n",
      "         -0.2214, -0.2237, -0.2259, -0.2282, -0.2305, -0.2328, -0.2352, -0.2376,\n",
      "         -0.2400, -0.2424, -0.2448, -0.2473, -0.2498, -0.2523, -0.2549, -0.2575,\n",
      "         -0.2601, -0.2627, -0.2653, -0.2680, -0.2707, -0.2735, -0.2762, -0.2790,\n",
      "         -0.2818, -0.2847, -0.2876, -0.2905, -0.2934, -0.2964, -0.2994, -0.3024,\n",
      "         -0.3054, -0.3085, -0.3116, -0.3148, -0.3180, -0.3212, -0.3244, -0.3277,\n",
      "         -0.3310, -0.3343, -0.3377, -0.3411, -0.3446, -0.3481, -0.3516, -0.3551,\n",
      "         -0.3587, -0.3623, -0.3660, -0.3697, -0.3734, -0.3772, -0.3810, -0.3849,\n",
      "         -0.3887, -0.3927, -0.3966, -0.4006, -0.4047, -0.4088, -0.4129, -0.4171,\n",
      "         -0.4213, -0.4256, -0.4298, -0.4342, -0.4386, -0.4430, -0.4475, -0.4520,\n",
      "         -0.4566, -0.4612, -0.4658, -0.4705, -0.4753, -0.4801, -0.4849, -0.4898,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0989, -0.0999, -0.1009, -0.1019, -0.1029, -0.1040, -0.1050, -0.1061,\n",
      "         -0.1071, -0.1082, -0.1093, -0.1104, -0.1115, -0.1127, -0.1138, -0.1150,\n",
      "         -0.1161, -0.1173, -0.1185, -0.1197, -0.1209, -0.1221, -0.1233, -0.1246,\n",
      "         -0.1258, -0.1271, -0.1284, -0.1297, -0.1310, -0.1323, -0.1337, -0.1350,\n",
      "         -0.1364, -0.1378, -0.1391, -0.1405, -0.1420, -0.1434, -0.1449, -0.1463,\n",
      "         -0.1478, -0.1493, -0.1508, -0.1523, -0.1539, -0.1554, -0.1570, -0.1586,\n",
      "         -0.1602, -0.1618, -0.1634, -0.1651, -0.1667, -0.1684, -0.1701, -0.1718,\n",
      "         -0.1736, -0.1753, -0.1771, -0.1789, -0.1807, -0.1825, -0.1844, -0.1862,\n",
      "         -0.1881, -0.1900, -0.1919, -0.1939, -0.1958, -0.1978, -0.1998, -0.2018,\n",
      "         -0.2039, -0.2059, -0.2080, -0.2101, -0.2122, -0.2144, -0.2165, -0.2187,\n",
      "         -0.2209, -0.2232, -0.2254, -0.2277, -0.2300, -0.2323, -0.2347, -0.2370,\n",
      "         -0.2394, -0.2418, -0.2443, -0.2468, -0.2492, -0.2518, -0.2543, -0.2569,\n",
      "         -0.2595, -0.2621, -0.2647, -0.2674, -0.2701, -0.2728, -0.2756, -0.2784,\n",
      "         -0.2812, -0.2840, -0.2869, -0.2898, -0.2927, -0.2957, -0.2987, -0.3017,\n",
      "         -0.3047, -0.3078, -0.3109, -0.3141, -0.3172, -0.3204, -0.3237, -0.3269,\n",
      "         -0.3302, -0.3336, -0.3370, -0.3404, -0.3438, -0.3473, -0.3508, -0.3543,\n",
      "         -0.3579, -0.3615, -0.3652, -0.3689, -0.3726, -0.3763, -0.3801, -0.3840,\n",
      "         -0.3879, -0.3918, -0.3957, -0.3997, -0.4038, -0.4078, -0.4120, -0.4161,\n",
      "         -0.4203, -0.4246, -0.4289, -0.4332, -0.4376, -0.4420, -0.4465, -0.4510,\n",
      "         -0.4555, -0.4601, -0.4648, -0.4695, -0.4742, -0.4790, -0.4838, -0.4887,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4778, -0.4767], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5613, -3.9183, -0.8912, -2.2675,  0.0587, -2.3862, -4.6084, -0.4104,\n",
      "         -3.8289, -2.4633, -3.8703,  0.6775, -0.0783, -3.5934, -3.1487, -2.3938,\n",
      "         -3.2263, -1.8071, -2.0210, -1.0209, -1.4870, -1.4822, -0.9179,  0.2883,\n",
      "         -1.9747, -3.4319, -3.1202, -2.5952, -3.9517,  0.8737, -1.8059, -0.2519,\n",
      "         -0.9040, -3.4879, -1.8348, -1.6989, -0.9194, -3.1555, -1.7170, -0.1070,\n",
      "         -0.7688, -0.5624, -0.4387, -4.2071, -1.6787, -3.2735, -3.4024, -3.1887,\n",
      "          0.2694,  0.3198, -3.8172, -1.5226, -6.9294, -2.9292, -3.0482, -1.0147,\n",
      "         -2.5730, -3.0074, -2.5295, -1.7024, -3.6073, -1.2495, -5.7059, -2.4680,\n",
      "         -3.7234,  0.9668, -1.4478, -0.1434, -0.3856, -3.1193, -4.0596, -5.3573,\n",
      "         -2.8211, -0.1739, -3.7940, -4.1420, -2.5190, -3.8563, -2.5769, -3.5446,\n",
      "         -4.7181, -2.5463, -0.4369, -1.3203, -1.7197, -3.0393, -1.8883, -0.7276,\n",
      "         -3.5492, -0.9239, -3.1693, -4.0195, -2.5002, -0.6996, -0.9495, -2.9931,\n",
      "         -2.8061, -3.6479, -3.2149, -3.2638, -0.2141, -1.0448, -3.7024, -2.0365,\n",
      "         -0.5306, -1.9384, -3.4894, -3.7262,  0.5149, -1.2088, -1.3240, -0.7127,\n",
      "         -4.2650, -4.5891,  0.2616, -1.0434, -2.1990, -1.4162, -0.0998, -1.0935,\n",
      "         -2.8970, -4.4858, -2.7592, -0.9804, -1.1847, -2.7671, -2.3797, -0.8780,\n",
      "         -0.6810, -4.1997, -0.0769,  0.3876, -3.3422, -4.1060, -1.2838, -2.0412,\n",
      "         -3.1071, -2.3561, -1.8138, -2.8880, -2.5893, -3.3412, -5.5755, -1.3916,\n",
      "         -0.3262, -0.3791, -0.4372, -0.4806, -0.4822, -0.4422, -4.8882, -4.9351,\n",
      "         -5.3104, -4.9058, -4.7985, -4.4956, -4.4393, -4.6002, -4.6128, -4.5714,\n",
      "         -4.6478, -4.7310, -4.7739, -4.7518, -4.6879, -4.6197, -4.5783, -4.5601,\n",
      "         -4.5663, -4.5701, -4.5652, -4.5740, -4.5949, -4.6181, -4.6480, -4.6604,\n",
      "         -4.6340, -4.6024, -4.5947, -4.5976, -4.6054, -4.6243, -4.6224, -4.6104,\n",
      "         -4.6016, -4.5947, -4.6083, -4.6231, -4.6345, -4.6376, -4.6418, -4.6487,\n",
      "         -4.6537, -4.6521, -4.6400, -4.6310, -4.6275, -4.6156, -4.6097, -4.6040,\n",
      "         -4.6008, -4.6032, -4.5977, -4.5866, -4.5885],\n",
      "        [-2.5655, -3.9207, -0.8938, -2.2646,  0.0611, -2.3855, -4.6026, -0.4096,\n",
      "         -3.8301, -2.4596, -3.8652,  0.6747, -0.0773, -3.5954, -3.1437, -2.3903,\n",
      "         -3.2249, -1.8076, -2.0209, -1.0247, -1.4851, -1.4870, -0.9158,  0.2854,\n",
      "         -1.9783, -3.4331, -3.1141, -2.5924, -3.9519,  0.8732, -1.8095, -0.2555,\n",
      "         -0.9093, -3.4834, -1.8326, -1.6999, -0.9211, -3.1548, -1.7216, -0.1020,\n",
      "         -0.7727, -0.5606, -0.4389, -4.2079, -1.6821, -3.2759, -3.3990, -3.1839,\n",
      "          0.2728,  0.3167, -3.8164, -1.5156, -6.9278, -2.9253, -3.0466, -1.0119,\n",
      "         -2.5719, -3.0060, -2.5263, -1.7076, -3.6097, -1.2429, -5.7021, -2.4668,\n",
      "         -3.7239,  0.9677, -1.4513, -0.1454, -0.3908, -3.1157, -4.0549, -5.3551,\n",
      "         -2.8180, -0.1741, -3.7937, -4.1409, -2.5170, -3.8558, -2.5806, -3.5454,\n",
      "         -4.7134, -2.5445, -0.4393, -1.3227, -1.7195, -3.0391, -1.8891, -0.7287,\n",
      "         -3.5533, -0.9298, -3.1726, -4.0200, -2.4989, -0.7016, -0.9524, -2.9926,\n",
      "         -2.8073, -3.6477, -3.2198, -3.2656, -0.2152, -1.0456, -3.7014, -2.0410,\n",
      "         -0.5324, -1.9399, -3.4881, -3.7206,  0.5125, -1.1546, -1.3697, -0.6253,\n",
      "         -4.4512, -4.5380,  0.2047, -1.0563, -2.2203, -1.4100, -0.0787, -1.0973,\n",
      "         -2.8196, -4.5246, -2.7441, -0.9988, -1.1562, -2.7477, -2.3820, -0.8975,\n",
      "         -0.6692, -4.1706, -0.1282,  0.3879, -3.3585, -4.1056, -1.2773, -2.0449,\n",
      "         -3.1214, -2.3369, -1.8171, -2.8309, -2.5570, -3.3237, -5.5631, -1.3770,\n",
      "         -0.4350, -0.3658, -0.5147, -0.5396, -0.4220, -0.4427, -4.8782, -4.9139,\n",
      "         -5.2893, -4.8961, -4.7997, -4.5411, -4.4415, -4.5610, -4.5373, -4.4836,\n",
      "         -4.5749, -4.6742, -4.7347, -4.7338, -4.6843, -4.6187, -4.5762, -4.5630,\n",
      "         -4.5717, -4.5877, -4.5957, -4.6175, -4.6435, -4.6638, -4.6848, -4.6926,\n",
      "         -4.6654, -4.6333, -4.6275, -4.6315, -4.6379, -4.6537, -4.6453, -4.6274,\n",
      "         -4.6142, -4.6098, -4.6279, -4.6473, -4.6598, -4.6648, -4.6645, -4.6632,\n",
      "         -4.6611, -4.6546, -4.6356, -4.6257, -4.6239, -4.6157, -4.6088, -4.5978,\n",
      "         -4.5905, -4.5897, -4.5799, -4.5690, -4.5710]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4246, -0.4533], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4761, -0.4935, -0.4986, -0.4867, -0.4495,  0.9900, -0.4508,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.1074e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0963, -0.0973, -0.0983,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0998, -0.1008, -0.1019,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1009, -0.1019, -0.1029,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0912, -0.0921, -0.0930,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4644, -0.4813, -0.4863, -0.4746, -0.4384,  0.9656, -0.4396,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5695, -3.9197, -0.8902,  ..., -4.5384, -4.5320, -4.5399],\n",
      "        [-2.5580, -3.9171, -0.8886,  ..., -4.5975, -4.5886, -4.5865],\n",
      "        [-2.5556, -3.9190, -0.8893,  ..., -4.5803, -4.5731, -4.5712],\n",
      "        ...,\n",
      "        [-2.8280, -3.9021, -0.9393,  ..., -5.4017, -5.4079, -5.4208],\n",
      "        [-2.5652, -3.9208, -0.8950,  ..., -4.6025, -4.5859, -4.5844],\n",
      "        [-2.9359, -3.8727, -0.9633,  ..., -5.5896, -5.5825, -5.5758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4321, -0.4290, -0.3932, -0.3835, -0.4344,  0.9439, -0.4170,  0.6517],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4432,  0.9900, -0.4783, -0.2618,  0.9900, -1.2695,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8298e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0897, -0.0906, -0.0915,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2594, -0.2620, -0.2647,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4323,  0.9656, -0.4665, -0.2553,  0.9656, -1.2443,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8392, -3.8953, -0.9403,  ..., -5.3955, -5.4061, -5.4273],\n",
      "        [-2.5729, -3.9200, -0.8921,  ..., -4.6092, -4.5927, -4.5875],\n",
      "        [-2.8273, -3.8936, -0.9385,  ..., -5.3882, -5.3966, -5.4090],\n",
      "        ...,\n",
      "        [-2.7834, -3.8928, -0.9157,  ..., -5.4352, -5.4904, -5.5050],\n",
      "        [-2.4794, -3.9259, -0.8572,  ..., -4.4785, -4.4670, -4.4675],\n",
      "        [-2.9322, -3.8703, -0.9610,  ..., -5.5173, -5.5107, -5.4983]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0091, -0.4231,  0.8762, -0.4464, -0.2581,  0.9715, -0.8762,  0.7481],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2483,  0.9900, -0.4593, -0.4722, -0.4226,  0.9900, -0.2545],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.2085e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2551, -0.2577, -0.2603,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0855, -0.0863, -0.0872,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0438, -0.0443, -0.0447,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.2236,  0.9656, -0.4479, -0.4606, -0.4121,  0.9656, -0.2482],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8340, -3.8971, -0.9350,  ..., -5.3974, -5.4049, -5.4217],\n",
      "        [-2.4823, -3.9251, -0.8542,  ..., -4.4760, -4.4697, -4.4686],\n",
      "        [-2.8394, -3.8956, -0.9284,  ..., -5.3580, -5.3667, -5.3858],\n",
      "        ...,\n",
      "        [-2.5785, -3.9143, -0.8900,  ..., -4.6495, -4.6380, -4.6392],\n",
      "        [-2.8347, -3.8956, -0.9341,  ..., -5.3829, -5.3898, -5.4077],\n",
      "        [-2.6557, -3.8963, -0.8928,  ..., -4.7007, -4.7080, -4.7142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9192, -0.8776,  0.9464, -0.4521, -0.4080, -0.4500,  0.9614, -0.2493],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4069, -0.4430,  0.9900,  0.9900,  0.9900, -0.2310,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1633e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0823, -0.0832, -0.0840,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0896, -0.0905, -0.0914,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0398, -0.0402, -0.0406,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3969, -0.4320,  0.9656,  0.9608,  0.9656, -0.2253,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5825, -3.9179, -0.8831,  ..., -4.6149, -4.5999, -4.6033],\n",
      "        [-2.5737, -3.9173, -0.8797,  ..., -4.5945, -4.5835, -4.5810],\n",
      "        [-2.8394, -3.8938, -0.9195,  ..., -5.3475, -5.3561, -5.3709],\n",
      "        ...,\n",
      "        [-2.6528, -3.8946, -0.8868,  ..., -4.7038, -4.7149, -4.7145],\n",
      "        [-2.8060, -3.9026, -0.9135,  ..., -5.2913, -5.3459, -5.3761],\n",
      "        [-2.9507, -3.8629, -0.9495,  ..., -5.4883, -5.4762, -5.4637]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4342, -0.4778,  0.9395,  0.7499,  0.9700, -0.2514,  0.9778,  0.6878],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4220, -0.9120,  0.9900, -0.4057,  0.9900, -0.2093, -0.4171],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8584e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.4512e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0854, -0.0862, -0.0871,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1772, -0.1790, -0.1808,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0360, -0.0364, -0.0368,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0844, -0.0852, -0.0861,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4116, -0.8721,  0.9656, -0.3957,  0.9656, -0.2041, -0.4068],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8317, -3.8916, -0.9215,  ..., -5.3539, -5.3613, -5.3809],\n",
      "        [-2.5775, -3.9153, -0.8748,  ..., -4.5966, -4.5840, -4.5823],\n",
      "        [-2.5365, -3.9072, -0.8552,  ..., -4.5364, -4.5390, -4.5525],\n",
      "        ...,\n",
      "        [-2.9479, -3.8615, -0.9426,  ..., -5.5601, -5.5514, -5.5430],\n",
      "        [-2.6534, -3.8919, -0.8795,  ..., -4.7036, -4.7152, -4.7164],\n",
      "        [-2.5733, -3.9185, -0.8783,  ..., -4.5756, -4.5603, -4.5584]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9572, -0.4792, -0.9260,  0.9572, -0.4511,  0.6577, -0.2506, -0.4014],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9396,  0.9900, -0.4136, -0.3760,  0.9900, -1.1882,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8259e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(9.3727e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1094, -0.1105, -0.1116,  ..., -0.9209, -0.9302, -0.9396],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2428, -0.2453, -0.2477,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8984,  0.9656, -0.4034, -0.3667,  0.9656, -1.1647,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8339, -3.8924, -0.9192,  ..., -5.3605, -5.3677, -5.3865],\n",
      "        [-2.6506, -3.8548, -0.8877,  ..., -0.8152, -0.9030, -0.8475],\n",
      "        [-2.8420, -3.8927, -0.9202,  ..., -5.2936, -5.3024, -5.3219],\n",
      "        ...,\n",
      "        [-2.8063, -3.8994, -0.9024,  ..., -5.2806, -5.3330, -5.3595],\n",
      "        [-2.4765, -3.9220, -0.8355,  ..., -4.4726, -4.4624, -4.4624],\n",
      "        [-2.7984, -3.8929, -0.9124,  ..., -5.1973, -5.2512, -5.2783]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9178, -0.8489,  0.9604, -0.4509, -0.3977,  0.9761, -0.8885,  0.9546],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3716,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.7587e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0752, -0.0759, -0.0767, -0.0775, -0.0783, -0.0791, -0.0799, -0.0807,\n",
      "         -0.0815, -0.0823, -0.0831, -0.0840, -0.0848, -0.0857, -0.0865, -0.0874,\n",
      "         -0.0883, -0.0892, -0.0901, -0.0910, -0.0919, -0.0929, -0.0938, -0.0947,\n",
      "         -0.0957, -0.0967, -0.0976, -0.0986, -0.0996, -0.1006, -0.1016, -0.1027,\n",
      "         -0.1037, -0.1048, -0.1058, -0.1069, -0.1080, -0.1090, -0.1102, -0.1113,\n",
      "         -0.1124, -0.1135, -0.1147, -0.1158, -0.1170, -0.1182, -0.1194, -0.1206,\n",
      "         -0.1218, -0.1230, -0.1243, -0.1255, -0.1268, -0.1281, -0.1294, -0.1307,\n",
      "         -0.1320, -0.1333, -0.1347, -0.1360, -0.1374, -0.1388, -0.1402, -0.1416,\n",
      "         -0.1430, -0.1445, -0.1459, -0.1474, -0.1489, -0.1504, -0.1519, -0.1535,\n",
      "         -0.1550, -0.1566, -0.1582, -0.1598, -0.1614, -0.1630, -0.1647, -0.1663,\n",
      "         -0.1680, -0.1697, -0.1714, -0.1731, -0.1749, -0.1767, -0.1784, -0.1802,\n",
      "         -0.1821, -0.1839, -0.1858, -0.1876, -0.1895, -0.1914, -0.1934, -0.1953,\n",
      "         -0.1973, -0.1993, -0.2013, -0.2033, -0.2054, -0.2075, -0.2096, -0.2117,\n",
      "         -0.2138, -0.2160, -0.2182, -0.2204, -0.2226, -0.2248, -0.2271, -0.2294,\n",
      "         -0.2317, -0.2341, -0.2364, -0.2388, -0.2412, -0.2437, -0.2461, -0.2486,\n",
      "         -0.2511, -0.2537, -0.2562, -0.2588, -0.2614, -0.2641, -0.2667, -0.2694,\n",
      "         -0.2722, -0.2749, -0.2777, -0.2805, -0.2833, -0.2862, -0.2891, -0.2920,\n",
      "         -0.2949, -0.2979, -0.3009, -0.3040, -0.3070, -0.3101, -0.3133, -0.3164,\n",
      "         -0.3196, -0.3229, -0.3261, -0.3294, -0.3327, -0.3361, -0.3395, -0.3429,\n",
      "         -0.3464, -0.3499, -0.3534, -0.3570, -0.3606, -0.3642, -0.3679, -0.3716,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3625,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5780e+00, -3.9206e+00, -8.6644e-01, -2.2387e+00,  8.8505e-02,\n",
      "         -2.3578e+00, -4.6041e+00, -3.6490e-01, -3.8485e+00, -2.4914e+00,\n",
      "         -3.8385e+00,  5.8602e-01, -1.6843e-01, -3.5739e+00, -3.1767e+00,\n",
      "         -2.3622e+00, -3.2304e+00, -1.7975e+00, -2.0211e+00, -1.0048e+00,\n",
      "         -1.4708e+00, -1.4620e+00, -8.9589e-01,  3.0594e-01, -1.9596e+00,\n",
      "         -3.4073e+00, -3.1408e+00, -2.5561e+00, -3.9285e+00,  9.0613e-01,\n",
      "         -1.7950e+00, -2.3008e-01, -8.8940e-01, -3.4421e+00, -1.8525e+00,\n",
      "         -1.6790e+00, -9.0247e-01, -3.1541e+00, -1.7038e+00, -7.7079e-02,\n",
      "         -7.5762e-01, -5.3654e-01, -4.0054e-01, -4.1854e+00, -1.6584e+00,\n",
      "         -3.2907e+00, -3.4312e+00, -3.1393e+00,  2.5731e-01,  3.2055e-01,\n",
      "         -3.8063e+00, -1.5243e+00, -6.9749e+00, -2.8915e+00, -3.0412e+00,\n",
      "         -9.8260e-01, -2.5716e+00, -2.9864e+00, -2.5180e+00, -1.6746e+00,\n",
      "         -3.5842e+00, -1.2455e+00, -5.7315e+00, -2.4254e+00, -3.6898e+00,\n",
      "          1.0079e+00, -1.4238e+00, -1.0563e-01, -3.6883e-01, -3.0657e+00,\n",
      "         -4.0662e+00, -5.4192e+00, -2.7799e+00, -1.5225e-01, -3.7651e+00,\n",
      "         -4.1589e+00, -2.4733e+00, -3.8540e+00, -2.5670e+00, -3.5148e+00,\n",
      "         -4.7678e+00, -2.5004e+00, -4.0699e-01, -1.2968e+00, -1.7123e+00,\n",
      "         -3.0190e+00, -1.8686e+00, -6.9808e-01, -3.5552e+00, -9.1081e-01,\n",
      "         -3.1390e+00, -4.0292e+00, -2.4534e+00, -6.7547e-01, -9.2255e-01,\n",
      "         -2.9806e+00, -2.7840e+00, -3.6582e+00, -3.2353e+00, -3.2407e+00,\n",
      "         -1.8612e-01, -1.0158e+00, -3.6969e+00, -2.0503e+00, -5.2146e-01,\n",
      "         -1.9072e+00, -3.4955e+00, -3.7442e+00,  5.3779e-01, -1.1516e+00,\n",
      "         -1.2212e+00, -5.4895e-01, -4.4686e+00, -4.6843e+00,  2.9553e-01,\n",
      "         -9.3893e-01, -2.1823e+00, -1.3295e+00, -5.3859e-03, -9.8738e-01,\n",
      "         -2.6897e+00, -4.7035e+00, -2.6496e+00, -9.4735e-01, -9.9259e-01,\n",
      "         -2.6626e+00, -2.3379e+00, -8.5651e-01, -6.0951e-01, -4.0850e+00,\n",
      "         -8.4936e-03,  4.2306e-01, -3.3481e+00, -4.1377e+00, -1.2309e+00,\n",
      "         -2.0325e+00, -3.0913e+00, -2.3175e+00, -1.7767e+00, -2.8161e+00,\n",
      "         -2.5499e+00, -3.3695e+00, -5.6966e+00, -1.4121e+00, -4.4689e-01,\n",
      "         -4.7690e-01, -3.2857e-01, -3.5105e-01, -3.3671e-01, -4.6170e-01,\n",
      "         -4.8720e+00, -4.9022e+00, -5.2817e+00, -4.9091e+00, -4.8518e+00,\n",
      "         -4.6266e+00, -4.6161e+00, -4.7074e+00, -4.6833e+00, -4.5955e+00,\n",
      "         -4.6030e+00, -4.6649e+00, -4.7175e+00, -4.7033e+00, -4.6307e+00,\n",
      "         -4.5317e+00, -4.4631e+00, -4.4382e+00, -4.4434e+00, -4.4715e+00,\n",
      "         -4.4950e+00, -4.5284e+00, -4.5626e+00, -4.5887e+00, -4.6193e+00,\n",
      "         -4.6286e+00, -4.5882e+00, -4.5443e+00, -4.5453e+00, -4.5676e+00,\n",
      "         -4.5769e+00, -4.5875e+00, -4.5682e+00, -4.5395e+00, -4.5159e+00,\n",
      "         -4.5080e+00, -4.5278e+00, -4.5439e+00, -4.5596e+00, -4.5644e+00,\n",
      "         -4.5737e+00, -4.5862e+00, -4.5953e+00, -4.5918e+00, -4.5752e+00,\n",
      "         -4.5616e+00, -4.5526e+00, -4.5452e+00, -4.5467e+00, -4.5387e+00,\n",
      "         -4.5345e+00, -4.5376e+00, -4.5297e+00, -4.5233e+00, -4.5249e+00],\n",
      "        [-2.8349e+00, -3.8963e+00, -9.2192e-01, -2.3992e+00,  1.2866e-02,\n",
      "         -2.4163e+00, -4.6658e+00, -5.7501e-01, -4.1047e+00, -2.6257e+00,\n",
      "         -3.9092e+00,  5.7419e-01, -2.6121e-01, -3.6352e+00, -3.0966e+00,\n",
      "         -2.4969e+00, -3.3247e+00, -1.8746e+00, -1.9862e+00, -1.1480e+00,\n",
      "         -1.5192e+00, -1.6177e+00, -1.0130e+00,  3.5563e-01, -2.1477e+00,\n",
      "         -3.4809e+00, -3.0798e+00, -2.6687e+00, -3.9368e+00,  9.0360e-01,\n",
      "         -1.9121e+00, -3.8300e-01, -8.7740e-01, -3.5122e+00, -1.8881e+00,\n",
      "         -1.5798e+00, -8.5334e-01, -3.2261e+00, -1.8273e+00, -2.1790e-01,\n",
      "         -7.9995e-01, -5.0749e-01, -4.5218e-01, -4.1604e+00, -1.8186e+00,\n",
      "         -3.5177e+00, -3.5981e+00, -3.2041e+00,  5.8638e-01,  5.4223e-01,\n",
      "         -3.8502e+00, -1.6077e+00, -6.9946e+00, -2.9801e+00, -3.0915e+00,\n",
      "         -1.0789e+00, -2.5580e+00, -3.0202e+00, -2.5922e+00, -1.8312e+00,\n",
      "         -3.6454e+00, -1.3217e+00, -5.7663e+00, -2.4847e+00, -3.7029e+00,\n",
      "          1.0189e+00, -1.5303e+00, -2.6184e-01, -3.3816e-01, -3.1346e+00,\n",
      "         -4.1485e+00, -5.4319e+00, -2.8590e+00, -8.6703e-02, -3.8177e+00,\n",
      "         -4.1348e+00, -2.5491e+00, -3.9058e+00, -2.6693e+00, -3.5566e+00,\n",
      "         -4.7820e+00, -2.5681e+00, -4.5483e-01, -1.4359e+00, -1.7801e+00,\n",
      "         -3.1488e+00, -1.9892e+00, -8.4147e-01, -3.6064e+00, -9.7668e-01,\n",
      "         -3.1962e+00, -4.0119e+00, -2.5178e+00, -7.0724e-01, -1.0660e+00,\n",
      "         -2.9357e+00, -2.9278e+00, -3.8513e+00, -3.2644e+00, -3.3200e+00,\n",
      "         -2.4753e-01, -1.1508e+00, -3.6870e+00, -2.0839e+00, -4.8246e-01,\n",
      "         -2.0609e+00, -3.6712e+00, -3.8317e+00,  6.0738e-01, -1.1812e+00,\n",
      "         -1.1760e+00, -5.4299e-01, -4.6497e+00, -4.5824e+00,  2.8432e-01,\n",
      "         -1.1106e+00, -2.2472e+00, -1.4551e+00, -9.0175e-02, -1.0794e+00,\n",
      "         -3.0079e+00, -4.4994e+00, -2.7614e+00, -9.7411e-01, -1.1822e+00,\n",
      "         -2.7801e+00, -2.4611e+00, -9.3861e-01, -8.0385e-01, -4.1713e+00,\n",
      "          2.3084e-01,  6.3742e-01, -3.3969e+00, -4.1448e+00, -1.3387e+00,\n",
      "         -2.0222e+00, -3.0953e+00, -2.4046e+00, -1.8826e+00, -2.8436e+00,\n",
      "         -3.0276e+00, -5.7827e+00, -1.7278e+00, -2.1576e+00, -1.2116e+00,\n",
      "         -7.0073e-01, -4.4289e-01, -4.2352e+00, -3.9212e+00, -1.2620e-01,\n",
      "         -2.0823e+00, -6.5115e-01, -9.2487e-01, -3.7089e-01,  1.1872e-01,\n",
      "         -4.4173e-01, -4.0683e+00, -3.2158e+00, -4.8952e+00, -1.4672e+00,\n",
      "          9.2865e-01,  9.9599e-01,  1.0025e+00,  9.4122e-01,  1.0336e+00,\n",
      "          1.0606e+00, -4.7836e+00, -4.9230e+00, -5.2430e+00, -5.0753e+00,\n",
      "         -5.2723e+00, -5.3088e+00, -5.3018e+00, -5.2811e+00, -5.2685e+00,\n",
      "         -5.2376e+00, -5.1939e+00, -5.1219e+00, -5.0571e+00, -5.0208e+00,\n",
      "         -5.0163e+00, -5.0268e+00, -5.0406e+00, -5.0608e+00, -5.0673e+00,\n",
      "         -5.0717e+00, -5.0903e+00, -5.1312e+00, -5.2037e+00, -5.2950e+00,\n",
      "         -5.3675e+00, -5.4055e+00, -5.4240e+00, -5.4530e+00, -5.4926e+00,\n",
      "         -5.5391e+00, -5.5478e+00, -5.5275e+00, -5.4940e+00, -5.4485e+00,\n",
      "         -5.4168e+00, -5.3899e+00, -5.3818e+00, -5.3925e+00, -5.4085e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4003,  0.9938], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.1995, -0.1995,  0.9900,  0.9900, -0.3687,  0.9900, -0.4067],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7673e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0344, -0.0347, -0.0351,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0344, -0.0347, -0.0351,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0746, -0.0753, -0.0761,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0823, -0.0831, -0.0839,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.1946, -0.1946,  0.9656,  0.9656, -0.3596,  0.9656, -0.3966],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8233, -3.8945, -0.9177,  ..., -5.2879, -5.2951, -5.3158],\n",
      "        [-2.6368, -3.8935, -0.8754,  ..., -4.6882, -4.7014, -4.7016],\n",
      "        [-2.6368, -3.8935, -0.8754,  ..., -4.6882, -4.7014, -4.7016],\n",
      "        ...,\n",
      "        [-2.5613, -3.9170, -0.8731,  ..., -4.6146, -4.5977, -4.5906],\n",
      "        [-2.7894, -3.8992, -0.9009,  ..., -5.2709, -5.3261, -5.3518],\n",
      "        [-2.5535, -3.9203, -0.8746,  ..., -4.5658, -4.5518, -4.5500]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9417, -0.2501, -0.2501,  0.9507,  0.9401, -0.4567,  0.9612, -0.4128],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.3752,  0.9900,  0.9900, -1.1944, -0.3752, -0.4181, -0.4253],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9054e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0759, -0.0767, -0.0774,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0759, -0.0767, -0.0774,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0846, -0.0854, -0.0863,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0860, -0.0869, -0.0878,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3660,  0.9656,  0.9656, -1.1708, -0.3660, -0.4077, -0.4148],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8073, -3.8947, -0.9103,  ..., -5.3276, -5.3336, -5.3470],\n",
      "        [-2.5497, -3.9100, -0.8710,  ..., -4.6415, -4.6306, -4.6318],\n",
      "        [-2.9272, -3.8590, -0.9372,  ..., -5.4778, -5.4630, -5.4507],\n",
      "        ...,\n",
      "        [-2.5497, -3.9100, -0.8710,  ..., -4.6415, -4.6306, -4.6318],\n",
      "        [-2.5497, -3.9146, -0.8664,  ..., -4.5800, -4.5682, -4.5667],\n",
      "        [-2.5413, -3.9142, -0.8637,  ..., -4.5819, -4.5740, -4.5704]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9229, -0.4630,  0.6703,  0.9046, -0.9021, -0.4630, -0.4863, -0.4214],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4226,  0.9900, -0.4312, -0.9160,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1244e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.7191e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0855, -0.0864, -0.0872,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4122,  0.9656, -0.4206, -0.8759,  0.9656,  0.9608,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7960, -3.8908, -0.9111,  ..., -5.3470, -5.3496, -5.3681],\n",
      "        [-2.5307, -3.9169, -0.8662,  ..., -4.5614, -4.5471, -4.5447],\n",
      "        [-2.7960, -3.8891, -0.9112,  ..., -5.3236, -5.3316, -5.3480],\n",
      "        ...,\n",
      "        [-2.7906, -3.8867, -0.9109,  ..., -5.3143, -5.3190, -5.3347],\n",
      "        [-2.9044, -3.8633, -0.9353,  ..., -5.4950, -5.4851, -5.4713],\n",
      "        [-2.9044, -3.8633, -0.9353,  ..., -5.4950, -5.4851, -5.4713]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9037, -0.4045,  0.9343, -0.4516, -0.9089,  0.8616,  0.7616,  0.7616],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4100,  0.9900, -0.2333, -1.2240,  0.9900, -0.4293],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8864e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0829, -0.0838, -0.0846,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2501, -0.2526, -0.2552,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0868, -0.0877, -0.0886,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.3999,  0.9656, -0.2275, -1.1997,  0.9656, -0.4187],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8005, -3.9025, -0.9264,  ..., -5.3770, -5.3875, -5.4033],\n",
      "        [-2.7884, -3.8978, -0.9227,  ..., -5.3328, -5.3392, -5.3559],\n",
      "        [-2.5361, -3.9220, -0.8735,  ..., -4.6054, -4.5911, -4.5976],\n",
      "        ...,\n",
      "        [-2.4387, -3.9289, -0.8388,  ..., -4.4661, -4.4592, -4.4608],\n",
      "        [-2.7965, -3.8977, -0.9207,  ..., -5.3416, -5.3472, -5.3671],\n",
      "        [-2.5390, -3.9212, -0.8706,  ..., -4.5395, -4.5317, -4.5359]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9725,  0.9290, -0.4317,  0.9433, -0.2346, -0.8988,  0.9759, -0.4445],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4226, -0.9786, -0.4506,  0.9900, -0.4412, -0.4134, -0.4630],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.0778e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0855, -0.0864, -0.0872,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1139, -0.1151, -0.1162,  ..., -0.9592, -0.9689, -0.9786],\n",
      "        ...,\n",
      "        [-0.0893, -0.0902, -0.0911,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0836, -0.0845, -0.0853,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0937, -0.0946, -0.0956,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4122, -0.9358, -0.4395,  0.9656, -0.4303, -0.4032, -0.4516],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9013, -3.8795, -0.9541,  ..., -5.5917, -5.5834, -5.5766],\n",
      "        [-2.5326, -3.9333, -0.8732,  ..., -4.5240, -4.5183, -4.5213],\n",
      "        [-2.6081, -3.8687, -0.8963,  ..., -0.8120, -0.8877, -0.7895],\n",
      "        ...,\n",
      "        [-2.5329, -3.9290, -0.8756,  ..., -4.5437, -4.5371, -4.5405],\n",
      "        [-2.5281, -3.9303, -0.8800,  ..., -4.6088, -4.5917, -4.5879],\n",
      "        [-2.5176, -3.9278, -0.8746,  ..., -4.5905, -4.5811, -4.5783]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6006, -0.4181, -0.8370, -0.4595,  0.9378, -0.4460, -0.4676, -0.4203],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4591, -0.2437, -0.4316,  0.9900,  0.9900,  0.9900,  0.9900, -0.4651],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3886e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0929, -0.0938, -0.0948,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0420, -0.0424, -0.0428,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0873, -0.0882, -0.0891,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0941, -0.0950, -0.0960,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4478, -0.2377, -0.4209,  0.9656,  0.9656,  0.9656,  0.9656, -0.4537],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5306, -3.9383, -0.8851,  ..., -4.6003, -4.5911, -4.5913],\n",
      "        [-2.6121, -3.9175, -0.8854,  ..., -4.6161, -4.6269, -4.6297],\n",
      "        [-2.5356, -3.9412, -0.8800,  ..., -4.5306, -4.5223, -4.5263],\n",
      "        ...,\n",
      "        [-2.7919, -3.9117, -0.9275,  ..., -5.3293, -5.3366, -5.3480],\n",
      "        [-2.7919, -3.9117, -0.9275,  ..., -5.3293, -5.3366, -5.3480],\n",
      "        [-2.5221, -3.9338, -0.8819,  ..., -4.6090, -4.5982, -4.5961]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4626, -0.2566, -0.4314,  0.9141,  0.6174,  0.9017,  0.9017, -0.4481],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4657, -1.2519], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(6.3139e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.2766e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0942, -0.0952, -0.0961, -0.0971, -0.0981, -0.0991, -0.1001, -0.1011,\n",
      "         -0.1021, -0.1031, -0.1042, -0.1052, -0.1063, -0.1074, -0.1084, -0.1095,\n",
      "         -0.1106, -0.1118, -0.1129, -0.1140, -0.1152, -0.1163, -0.1175, -0.1187,\n",
      "         -0.1199, -0.1211, -0.1223, -0.1236, -0.1248, -0.1261, -0.1274, -0.1286,\n",
      "         -0.1299, -0.1313, -0.1326, -0.1339, -0.1353, -0.1366, -0.1380, -0.1394,\n",
      "         -0.1408, -0.1422, -0.1437, -0.1451, -0.1466, -0.1481, -0.1496, -0.1511,\n",
      "         -0.1526, -0.1542, -0.1557, -0.1573, -0.1589, -0.1605, -0.1621, -0.1637,\n",
      "         -0.1654, -0.1671, -0.1688, -0.1705, -0.1722, -0.1739, -0.1757, -0.1774,\n",
      "         -0.1792, -0.1811, -0.1829, -0.1847, -0.1866, -0.1885, -0.1904, -0.1923,\n",
      "         -0.1942, -0.1962, -0.1982, -0.2002, -0.2022, -0.2043, -0.2063, -0.2084,\n",
      "         -0.2105, -0.2126, -0.2148, -0.2170, -0.2191, -0.2214, -0.2236, -0.2259,\n",
      "         -0.2281, -0.2304, -0.2328, -0.2351, -0.2375, -0.2399, -0.2423, -0.2448,\n",
      "         -0.2472, -0.2497, -0.2523, -0.2548, -0.2574, -0.2600, -0.2626, -0.2653,\n",
      "         -0.2679, -0.2706, -0.2734, -0.2761, -0.2789, -0.2817, -0.2846, -0.2875,\n",
      "         -0.2904, -0.2933, -0.2963, -0.2993, -0.3023, -0.3053, -0.3084, -0.3115,\n",
      "         -0.3147, -0.3179, -0.3211, -0.3243, -0.3276, -0.3309, -0.3342, -0.3376,\n",
      "         -0.3410, -0.3445, -0.3480, -0.3515, -0.3550, -0.3586, -0.3622, -0.3659,\n",
      "         -0.3696, -0.3733, -0.3771, -0.3809, -0.3847, -0.3886, -0.3925, -0.3965,\n",
      "         -0.4005, -0.4046, -0.4087, -0.4128, -0.4169, -0.4212, -0.4254, -0.4297,\n",
      "         -0.4341, -0.4384, -0.4429, -0.4473, -0.4519, -0.4564, -0.4610, -0.4657,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2558, -0.2584, -0.2610, -0.2637, -0.2663, -0.2690, -0.2717, -0.2745,\n",
      "         -0.2772, -0.2800, -0.2829, -0.2857, -0.2886, -0.2915, -0.2945, -0.2974,\n",
      "         -0.3004, -0.3035, -0.3065, -0.3096, -0.3128, -0.3159, -0.3191, -0.3223,\n",
      "         -0.3256, -0.3289, -0.3322, -0.3356, -0.3390, -0.3424, -0.3458, -0.3493,\n",
      "         -0.3529, -0.3564, -0.3600, -0.3637, -0.3673, -0.3710, -0.3748, -0.3786,\n",
      "         -0.3824, -0.3863, -0.3902, -0.3941, -0.3981, -0.4021, -0.4062, -0.4103,\n",
      "         -0.4144, -0.4186, -0.4228, -0.4271, -0.4314, -0.4358, -0.4402, -0.4446,\n",
      "         -0.4491, -0.4537, -0.4582, -0.4629, -0.4675, -0.4723, -0.4770, -0.4819,\n",
      "         -0.4867, -0.4916, -0.4966, -0.5016, -0.5067, -0.5118, -0.5170, -0.5222,\n",
      "         -0.5275, -0.5328, -0.5382, -0.5436, -0.5491, -0.5547, -0.5603, -0.5659,\n",
      "         -0.5716, -0.5774, -0.5832, -0.5891, -0.5951, -0.6011, -0.6072, -0.6133,\n",
      "         -0.6195, -0.6258, -0.6321, -0.6385, -0.6449, -0.6514, -0.6580, -0.6646,\n",
      "         -0.6714, -0.6781, -0.6850, -0.6919, -0.6989, -0.7060, -0.7131, -0.7203,\n",
      "         -0.7276, -0.7349, -0.7423, -0.7498, -0.7574, -0.7651, -0.7728, -0.7806,\n",
      "         -0.7885, -0.7964, -0.8045, -0.8126, -0.8208, -0.8291, -0.8375, -0.8460,\n",
      "         -0.8545, -0.8631, -0.8718, -0.8807, -0.8895, -0.8985, -0.9076, -0.9168,\n",
      "         -0.9260, -0.9354, -0.9448, -0.9544, -0.9640, -0.9738, -0.9836, -0.9935,\n",
      "         -1.0036, -1.0137, -1.0239, -1.0343, -1.0447, -1.0553, -1.0659, -1.0767,\n",
      "         -1.0876, -1.0986, -1.1097, -1.1209, -1.1322, -1.1436, -1.1552, -1.1669,\n",
      "         -1.1786, -1.1906, -1.2026, -1.2147, -1.2270, -1.2394, -1.2519,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4542, -1.2271], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5320, -3.9412, -0.8878, -2.3099,  0.0797, -2.3906, -4.6410, -0.3900,\n",
      "         -3.7962, -2.4469, -3.8642,  0.6483, -0.0947, -3.5947, -3.1778, -2.4063,\n",
      "         -3.2327, -1.7858, -2.0722, -1.0083, -1.4867, -1.4703, -0.9012,  0.3026,\n",
      "         -1.9624, -3.4279, -3.1391, -2.6026, -3.9444,  0.8588, -1.7873, -0.2395,\n",
      "         -0.8919, -3.4850, -1.8405, -1.6847, -0.9281, -3.1641, -1.7166, -0.0756,\n",
      "         -0.7302, -0.5701, -0.4400, -4.2643, -1.6669, -3.2502, -3.3792, -3.1833,\n",
      "          0.2447,  0.2345, -3.8134, -1.5233, -6.9387, -2.9347, -3.0510, -0.9912,\n",
      "         -2.5965, -2.9961, -2.5484, -1.6897, -3.6048, -1.2446, -5.7133, -2.4740,\n",
      "         -3.7135,  0.9538, -1.4313, -0.1335, -0.3722, -3.1185, -4.0525, -5.3903,\n",
      "         -2.8327, -0.1257, -3.7912, -4.1941, -2.5286, -3.8630, -2.5740, -3.5428,\n",
      "         -4.7470, -2.5550, -0.4171, -1.3075, -1.7440, -3.0372, -1.8640, -0.7140,\n",
      "         -3.5508, -0.8892, -3.1636, -4.0678, -2.5086, -0.6825, -0.9368, -2.9849,\n",
      "         -2.7832, -3.6196, -3.2342, -3.2726, -0.1934, -1.0326, -3.7134, -2.0794,\n",
      "         -0.5194, -1.9216, -3.4576, -3.7443,  0.5598, -1.1937, -1.3539, -0.7307,\n",
      "         -4.4174, -4.5556,  0.2265, -1.0384, -2.2242, -1.3928, -0.0946, -1.0955,\n",
      "         -2.8016, -4.5429, -2.7454, -0.9854, -1.1428, -2.7716, -2.3489, -0.8899,\n",
      "         -0.6533, -4.1744, -0.1532,  0.3019, -3.3592, -4.1170, -1.2511, -2.0704,\n",
      "         -3.1189, -2.3587, -1.7928, -2.8037, -2.5319, -3.3023, -5.5870, -1.3556,\n",
      "         -0.4871, -0.3854, -0.5461, -0.5090, -0.5162, -0.4259, -4.8738, -4.9097,\n",
      "         -5.2850, -4.9063, -4.8132, -4.5714, -4.4670, -4.5686, -4.5326, -4.4774,\n",
      "         -4.5652, -4.6676, -4.7396, -4.7423, -4.6885, -4.6188, -4.5748, -4.5592,\n",
      "         -4.5705, -4.5865, -4.5966, -4.6181, -4.6383, -4.6531, -4.6709, -4.6712,\n",
      "         -4.6398, -4.6109, -4.6084, -4.6099, -4.6211, -4.6407, -4.6354, -4.6199,\n",
      "         -4.6142, -4.6136, -4.6308, -4.6502, -4.6630, -4.6670, -4.6668, -4.6724,\n",
      "         -4.6747, -4.6699, -4.6540, -4.6448, -4.6432, -4.6366, -4.6287, -4.6193,\n",
      "         -4.6085, -4.6098, -4.5992, -4.5872, -4.5850],\n",
      "        [-2.4425, -3.9466, -0.8560, -2.2841,  0.1143, -2.3577, -4.5728, -0.3608,\n",
      "         -3.7021, -2.3983, -3.8314,  0.6595, -0.1003, -3.5567, -3.1945, -2.3545,\n",
      "         -3.2193, -1.7513, -2.0966, -0.9509, -1.4639, -1.4214, -0.8675,  0.2812,\n",
      "         -1.9446, -3.3837, -3.1468, -2.5620, -3.9535,  0.8420, -1.7787, -0.1777,\n",
      "         -0.8978, -3.4573, -1.8361, -1.7191, -0.9732, -3.1557, -1.6980, -0.0315,\n",
      "         -0.7230, -0.6034, -0.4353, -4.2907, -1.6473, -3.1611, -3.3310, -3.1603,\n",
      "          0.1067,  0.1051, -3.7708, -1.4965, -6.9452, -2.8932, -3.0438, -0.9552,\n",
      "         -2.5968, -3.0186, -2.5176, -1.6771, -3.5644, -1.2169, -5.7201, -2.4486,\n",
      "         -3.7203,  0.9383, -1.4315, -0.0670, -0.3842, -3.0888, -3.9935, -5.3710,\n",
      "         -2.7906, -0.1431, -3.7436, -4.2109, -2.4893, -3.8563, -2.5565, -3.4979,\n",
      "         -4.7272, -2.5176, -0.4080, -1.2385, -1.7088, -2.9793, -1.8543, -0.6543,\n",
      "         -3.4815, -0.8819, -3.1154, -4.0783, -2.4736, -0.6731, -0.8660, -2.9908,\n",
      "         -2.7591, -3.5339, -3.1952, -3.2409, -0.1728, -0.9601, -3.7074, -2.0611,\n",
      "         -0.5326, -1.8832, -3.3831, -3.6946,  0.5427, -1.2077, -1.4016, -2.4437,\n",
      "         -4.2505, -4.5906,  0.2640, -0.9405, -2.2069, -1.3107,  0.0079, -1.0384,\n",
      "         -2.6262, -4.6939, -2.6979, -0.9272, -1.0284, -2.6866, -2.3137, -0.8595,\n",
      "         -0.5408, -4.1367, -0.2782,  0.1782, -3.3435, -4.0815, -1.2212, -2.0718,\n",
      "         -3.1378, -2.3186, -1.7706, -2.7835, -2.5410, -3.2767, -5.6209, -1.3253,\n",
      "         -0.4747, -0.2911, -1.2290, -1.3004, -1.1745, -4.8796, -4.8894, -5.3342,\n",
      "         -4.8281, -4.6730, -4.7091, -4.8899, -5.0302, -4.9596, -4.8399, -4.7598,\n",
      "         -4.6784, -4.5896, -4.5205, -4.4759, -4.4510, -4.4141, -4.3734, -4.3368,\n",
      "         -4.3115, -4.2959, -4.2841, -4.2868, -4.3074, -4.3401, -4.3798, -4.4244,\n",
      "         -4.4610, -4.4799, -4.4892, -4.4897, -4.4926, -4.4979, -4.5062, -4.5208,\n",
      "         -4.5390, -4.5484, -4.5615, -4.5658, -4.5586, -4.5385, -4.5186, -4.5120,\n",
      "         -4.5143, -4.5130, -4.5073, -4.5029, -4.4997, -4.4930, -4.4919, -4.4854,\n",
      "         -4.4823, -4.4835, -4.4772, -4.4706, -4.4707]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4783, -0.8939], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4469,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.2509,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.5786e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0904, -0.0913, -0.0922,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2556, -0.2582, -0.2608,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4358,  0.9656,  0.9608,  0.9656,  0.9656,  0.9656, -1.2261,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5314, -3.9472, -0.8855,  ..., -4.5636, -4.5528, -4.5567],\n",
      "        [-2.7879, -3.9242, -0.9319,  ..., -5.3365, -5.3420, -5.3557],\n",
      "        [-2.8952, -3.8982, -0.9656,  ..., -5.5140, -5.5072, -5.4989],\n",
      "        ...,\n",
      "        [-2.7789, -3.9239, -0.9398,  ..., -5.3549, -5.3554, -5.3718],\n",
      "        [-2.4321, -3.9539, -0.8537,  ..., -4.4804, -4.4735, -4.4743],\n",
      "        [-2.7547, -3.9271, -0.9340,  ..., -5.2080, -5.2597, -5.2874]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4450,  0.8828,  0.7094,  0.8234,  0.5707,  0.8948, -0.9237,  0.9012],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4453, -0.4173,  0.9900,  0.9900, -0.4173, -0.2432,  0.9900, -0.4444],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9170e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0901, -0.0910, -0.0919,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0844, -0.0853, -0.0861,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0419, -0.0423, -0.0427,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0899, -0.0908, -0.0917,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4343, -0.4070,  0.9656,  0.9656, -0.4070, -0.2372,  0.9656, -0.4334],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5238, -3.9544, -0.8841,  ..., -4.6159, -4.6058, -4.6039],\n",
      "        [-2.5253, -3.9569, -0.8790,  ..., -4.5432, -4.5343, -4.5360],\n",
      "        [-2.7772, -3.9278, -0.9367,  ..., -5.3493, -5.3543, -5.3711],\n",
      "        ...,\n",
      "        [-2.6037, -3.9299, -0.8905,  ..., -4.7035, -4.7138, -4.7143],\n",
      "        [-2.7872, -3.9328, -0.9410,  ..., -5.3969, -5.4063, -5.4240],\n",
      "        [-2.5149, -3.9550, -0.8873,  ..., -4.5877, -4.5714, -4.5690]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4649, -0.4564,  0.8835,  0.8670, -0.4564, -0.2193,  0.9199, -0.4013],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.3908, -0.4304,  0.9900, -1.2258, -0.4363,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3517e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0791, -0.0799, -0.0807,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2505, -0.2530, -0.2556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0883, -0.0891, -0.0900,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.3811, -0.4198,  0.9656, -1.2016, -0.4255,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7786, -3.9329, -0.9325,  ..., -5.3687, -5.3726, -5.3908],\n",
      "        [-2.7791, -3.9345, -0.9291,  ..., -5.3486, -5.3536, -5.3663],\n",
      "        [-2.5218, -3.9555, -0.8826,  ..., -4.6362, -4.6159, -4.6076],\n",
      "        ...,\n",
      "        [-2.4216, -3.9625, -0.8430,  ..., -4.4812, -4.4716, -4.4744],\n",
      "        [-2.5198, -3.9544, -0.8774,  ..., -4.6152, -4.5990, -4.5975],\n",
      "        [-2.9021, -3.9009, -0.9589,  ..., -5.5738, -5.5637, -5.5566]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8606,  0.8597, -0.4722, -0.4627,  0.8595, -0.9612, -0.4731,  0.5698],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4061, -0.3773,  0.9900, -0.3801, -0.9251,  0.9900,  0.9900, -0.2144],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6810e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0822, -0.0830, -0.0838,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0763, -0.0771, -0.0779,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0369, -0.0373, -0.0377,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3961, -0.3680,  0.9656, -0.3707, -0.8845,  0.9656,  0.9656, -0.2091],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5239, -3.9552, -0.8718,  ..., -4.5798, -4.5673, -4.5680],\n",
      "        [-2.5210, -3.9559, -0.8756,  ..., -4.6419, -4.6181, -4.6108],\n",
      "        [-2.7536, -3.9404, -0.9098,  ..., -5.2744, -5.3326, -5.3611],\n",
      "        ...,\n",
      "        [-2.9048, -3.9007, -0.9507,  ..., -5.4995, -5.4893, -5.4725],\n",
      "        [-2.7879, -3.9337, -0.9288,  ..., -5.2997, -5.3060, -5.3261],\n",
      "        [-2.6002, -3.9330, -0.8794,  ..., -4.7214, -4.7324, -4.7347]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4327, -0.4672,  0.8815, -0.4401, -0.9113,  0.5980,  0.8603, -0.2087],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2041, -0.3769, -0.4150,  0.9900,  0.9900, -0.9624, -0.4232, -0.3708],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.3000e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0352, -0.0355, -0.0359,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0762, -0.0770, -0.0778,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0840, -0.0848, -0.0857,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1120, -0.1131, -0.1143,  ..., -0.9432, -0.9528, -0.9624],\n",
      "        [-0.0856, -0.0865, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0750, -0.0758, -0.0765,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.1991, -0.3676, -0.4048,  0.9656,  0.9656, -0.9202, -0.4127, -0.3616],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5999, -3.9325, -0.8732,  ..., -4.7292, -4.7382, -4.7421],\n",
      "        [-2.5232, -3.9559, -0.8684,  ..., -4.6399, -4.6221, -4.6268],\n",
      "        [-2.5145, -3.9528, -0.8632,  ..., -4.6312, -4.6183, -4.6112],\n",
      "        ...,\n",
      "        [-2.5956, -3.8944, -0.8848,  ..., -0.8214, -0.8877, -0.8057],\n",
      "        [-2.5117, -3.9537, -0.8648,  ..., -4.6172, -4.6060, -4.5997],\n",
      "        [-2.5220, -3.9499, -0.8698,  ..., -4.6763, -4.6611, -4.6590]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2005, -0.4161, -0.4185,  0.9238,  0.8676, -0.8431, -0.4053, -0.4294],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.1908, -0.4064,  0.9900, -1.2049,  0.9900, -0.4229, -0.4148,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0055e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0329, -0.0332, -0.0335,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0822, -0.0831, -0.0839,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0856, -0.0864, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0839, -0.0848, -0.0856,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.1861, -0.3964,  0.9656, -1.1811,  0.9608, -0.4125, -0.4046,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6030, -3.9338, -0.8592,  ..., -4.6652, -4.6791, -4.6826],\n",
      "        [-2.5169, -3.9556, -0.8645,  ..., -4.5957, -4.5816, -4.5781],\n",
      "        [-2.7831, -3.9301, -0.9132,  ..., -5.3444, -5.3471, -5.3658],\n",
      "        ...,\n",
      "        [-2.5133, -3.9526, -0.8577,  ..., -4.6180, -4.6069, -4.6008],\n",
      "        [-2.5167, -3.9512, -0.8568,  ..., -4.6306, -4.6209, -4.6128],\n",
      "        [-2.7345, -3.9265, -0.8868,  ..., -5.4017, -5.4484, -5.4589]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2315, -0.3694,  0.8902, -0.9681,  0.6722, -0.3963, -0.4077,  0.9047],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4169,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5023e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0843, -0.0852, -0.0860, -0.0869, -0.0878, -0.0887, -0.0896, -0.0905,\n",
      "         -0.0914, -0.0923, -0.0932, -0.0942, -0.0951, -0.0961, -0.0971, -0.0981,\n",
      "         -0.0990, -0.1000, -0.1011, -0.1021, -0.1031, -0.1041, -0.1052, -0.1063,\n",
      "         -0.1073, -0.1084, -0.1095, -0.1106, -0.1117, -0.1129, -0.1140, -0.1152,\n",
      "         -0.1163, -0.1175, -0.1187, -0.1199, -0.1211, -0.1223, -0.1236, -0.1248,\n",
      "         -0.1261, -0.1273, -0.1286, -0.1299, -0.1312, -0.1326, -0.1339, -0.1352,\n",
      "         -0.1366, -0.1380, -0.1394, -0.1408, -0.1422, -0.1437, -0.1451, -0.1466,\n",
      "         -0.1481, -0.1495, -0.1511, -0.1526, -0.1541, -0.1557, -0.1573, -0.1588,\n",
      "         -0.1604, -0.1621, -0.1637, -0.1654, -0.1670, -0.1687, -0.1704, -0.1721,\n",
      "         -0.1739, -0.1756, -0.1774, -0.1792, -0.1810, -0.1828, -0.1847, -0.1866,\n",
      "         -0.1884, -0.1903, -0.1923, -0.1942, -0.1962, -0.1982, -0.2002, -0.2022,\n",
      "         -0.2042, -0.2063, -0.2084, -0.2105, -0.2126, -0.2147, -0.2169, -0.2191,\n",
      "         -0.2213, -0.2236, -0.2258, -0.2281, -0.2304, -0.2327, -0.2351, -0.2374,\n",
      "         -0.2398, -0.2423, -0.2447, -0.2472, -0.2497, -0.2522, -0.2548, -0.2573,\n",
      "         -0.2599, -0.2626, -0.2652, -0.2679, -0.2706, -0.2733, -0.2761, -0.2789,\n",
      "         -0.2817, -0.2845, -0.2874, -0.2903, -0.2932, -0.2962, -0.2992, -0.3022,\n",
      "         -0.3053, -0.3084, -0.3115, -0.3146, -0.3178, -0.3210, -0.3242, -0.3275,\n",
      "         -0.3308, -0.3342, -0.3375, -0.3410, -0.3444, -0.3479, -0.3514, -0.3549,\n",
      "         -0.3585, -0.3621, -0.3658, -0.3695, -0.3732, -0.3770, -0.3808, -0.3847,\n",
      "         -0.3885, -0.3925, -0.3964, -0.4004, -0.4045, -0.4086, -0.4127, -0.4169,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4066,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5233, -3.9515, -0.8518, -2.2932,  0.1031, -2.3770, -4.6486, -0.3658,\n",
      "         -3.7862, -2.4522, -3.8681,  0.7021, -0.0833, -3.5723, -3.1690, -2.3710,\n",
      "         -3.2259, -1.7765, -2.0412, -1.0011, -1.4744, -1.4570, -0.8806,  0.3328,\n",
      "         -1.9419, -3.3956, -3.1272, -2.5673, -3.9352,  0.8736, -1.7691, -0.2290,\n",
      "         -0.8806, -3.4826, -1.8515, -1.6739, -0.9300, -3.1555, -1.6769, -0.0523,\n",
      "         -0.7090, -0.5618, -0.4169, -4.2734, -1.6468, -3.2446, -3.3870, -3.1813,\n",
      "          0.1264,  0.1194, -3.7897, -1.5102, -6.9551, -2.9055, -3.0411, -0.9737,\n",
      "         -2.5851, -2.9830, -2.5403, -1.6675, -3.5795, -1.2297, -5.7213, -2.4447,\n",
      "         -3.7004,  0.9732, -1.4049, -0.1143, -0.3562, -3.1135, -4.0222, -5.4308,\n",
      "         -2.8033, -0.1073, -3.7628, -4.2195, -2.5000, -3.8532, -2.5441, -3.5131,\n",
      "         -4.7806, -2.5228, -0.3930, -1.2988, -1.7518, -3.0132, -1.8432, -0.7002,\n",
      "         -3.5374, -0.8652, -3.1274, -4.0898, -2.4776, -0.6636, -0.9229, -2.9625,\n",
      "         -2.7672, -3.6153, -3.2508, -3.2456, -0.1697, -1.0273, -3.7075, -2.0983,\n",
      "         -0.4956, -1.9139, -3.4494, -3.7508,  0.5819, -1.0922, -1.3403, -0.6409,\n",
      "         -4.4155, -4.5611,  0.2504, -1.0283, -2.2070, -1.3765, -0.0861, -1.0708,\n",
      "         -2.7983, -4.5598, -2.7311, -0.9653, -1.1196, -2.7686, -2.3313, -0.8614,\n",
      "         -0.6355, -4.1711, -0.2942,  0.1791, -3.3445, -4.1243, -1.2370, -2.0594,\n",
      "         -3.1091, -2.3517, -1.7695, -2.7978, -2.5297, -3.3262, -5.5990, -1.3357,\n",
      "         -0.5040, -0.3880, -0.4116, -0.4809, -0.3820, -0.4444, -4.8787, -4.9178,\n",
      "         -5.2928, -4.9183, -4.8292, -4.5857, -4.4762, -4.5614, -4.5245, -4.4741,\n",
      "         -4.5548, -4.6535, -4.7277, -4.7475, -4.7146, -4.6578, -4.6134, -4.5919,\n",
      "         -4.5994, -4.6067, -4.6148, -4.6341, -4.6612, -4.6850, -4.7054, -4.7136,\n",
      "         -4.6856, -4.6578, -4.6547, -4.6545, -4.6648, -4.6880, -4.6828, -4.6688,\n",
      "         -4.6569, -4.6524, -4.6715, -4.6876, -4.6999, -4.7035, -4.7061, -4.7110,\n",
      "         -4.7124, -4.7056, -4.6877, -4.6798, -4.6798, -4.6698, -4.6584, -4.6452,\n",
      "         -4.6345, -4.6301, -4.6173, -4.6014, -4.5992],\n",
      "        [-2.7604, -3.9360, -0.8909, -2.4185,  0.0441, -2.4330, -4.7148, -0.5258,\n",
      "         -4.0120, -2.5600, -3.9193,  0.6895, -0.1742, -3.5946, -3.1320, -2.4815,\n",
      "         -3.3212, -1.8357, -2.0257, -1.1011, -1.5203, -1.5723, -0.9681,  0.3539,\n",
      "         -2.1048, -3.4306, -3.0999, -2.6635, -3.9406,  0.8764, -1.8573, -0.3313,\n",
      "         -0.8611, -3.5349, -1.8951, -1.5937, -0.8834, -3.2372, -1.7685, -0.1673,\n",
      "         -0.7309, -0.5380, -0.4484, -4.2518, -1.7853, -3.4525, -3.5181, -3.2280,\n",
      "          0.3783,  0.2959, -3.8071, -1.5925, -6.9904, -2.9755, -3.0996, -1.0478,\n",
      "         -2.5689, -3.0052, -2.5989, -1.8037, -3.6077, -1.3012, -5.7652, -2.4921,\n",
      "         -3.7086,  0.9858, -1.4837, -0.2208, -0.3248, -3.1666, -4.0904, -5.4372,\n",
      "         -2.8660, -0.0646, -3.7828, -4.2147, -2.5592, -3.9155, -2.6181, -3.5261,\n",
      "         -4.7869, -2.5752, -0.4205, -1.3916, -1.8016, -3.1218, -1.9382, -0.7992,\n",
      "         -3.5751, -0.9159, -3.1502, -4.0874, -2.5282, -0.6788, -1.0193, -2.9225,\n",
      "         -2.8947, -3.7888, -3.2694, -3.3110, -0.2071, -1.1160, -3.6986, -2.1268,\n",
      "         -0.4800, -2.0377, -3.6071, -3.8263,  0.6245, -1.0810, -1.2633, -0.6126,\n",
      "         -4.5844, -4.5808,  0.2338, -1.1138, -2.2498, -1.4573, -0.1151, -1.1555,\n",
      "         -2.9441, -4.5440, -2.7578, -0.9710, -1.1836, -2.8440, -2.4392, -0.9199,\n",
      "         -0.7378, -4.2176, -0.0416,  0.3632, -3.4056, -4.1551, -1.3030, -2.0478,\n",
      "         -3.1203, -2.4152, -1.8757, -2.8153, -2.9651, -5.8106, -1.6924, -2.1792,\n",
      "         -0.9560, -0.7467, -0.3088, -4.0431, -3.9514, -0.1566, -2.1268, -0.2561,\n",
      "         -1.0136, -0.4891,  0.2895, -0.6972, -4.1569, -5.0757, -1.6817, -2.1442,\n",
      "         -0.0306,  0.1432,  0.2939, -3.9661, -3.8783, -0.2179, -1.3968, -4.3487,\n",
      "         -6.1003, -4.5627, -3.0246, -4.9401, -1.4793,  0.8184,  0.7547,  0.9508,\n",
      "          0.9498,  0.9567,  0.9779, -4.7674, -4.9309, -5.2513, -5.0716, -5.2558,\n",
      "         -5.1557, -5.0344, -5.0436, -5.0739, -5.0614, -5.0130, -4.9803, -4.9783,\n",
      "         -4.9895, -5.0237, -5.0587, -5.0895, -5.1028, -5.1119, -5.1277, -5.1543,\n",
      "         -5.1872, -5.2252, -5.2710, -5.3282, -5.3530]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4352,  0.9014], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.9251, -0.3761, -0.3725, -0.4285,  0.9900, -0.3833, -0.4285,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2498e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1798, -0.1816, -0.1834,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0761, -0.0769, -0.0776,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0754, -0.0761, -0.0769,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0775, -0.0783, -0.0791,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0867, -0.0876, -0.0884,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8846, -0.3668, -0.3633, -0.4179,  0.9656, -0.3739, -0.4179,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4873, -3.9414, -0.8273,  ..., -4.5657, -4.5687, -4.5816],\n",
      "        [-2.5247, -3.9461, -0.8504,  ..., -4.6732, -4.6584, -4.6572],\n",
      "        [-2.5245, -3.9484, -0.8505,  ..., -4.6393, -4.6182, -4.6096],\n",
      "        ...,\n",
      "        [-2.5314, -3.9554, -0.8415,  ..., -4.5497, -4.5398, -4.5403],\n",
      "        [-2.5150, -3.9485, -0.8437,  ..., -4.6121, -4.6002, -4.5964],\n",
      "        [-2.7950, -3.9306, -0.9000,  ..., -5.3419, -5.3453, -5.3615]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9145, -0.3938, -0.4268, -0.3751,  0.9061, -0.4372, -0.3751,  0.8947],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4173, -0.4168, -1.2172, -0.4168, -0.4064,  0.9900,  0.9900, -1.2172],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6314e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0844, -0.0853, -0.0861,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.0852, -0.0860,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2487, -0.2512, -0.2538,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2487, -0.2512, -0.2538,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4070, -0.4065, -1.1931, -0.4065, -0.3963,  0.9656,  0.9656, -1.1931],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5218, -3.9489, -0.8439,  ..., -4.5917, -4.5757, -4.5735],\n",
      "        [-2.5288, -3.9495, -0.8414,  ..., -4.6154, -4.6053, -4.6061],\n",
      "        [-2.4270, -3.9565, -0.8028,  ..., -4.4784, -4.4709, -4.4725],\n",
      "        ...,\n",
      "        [-2.7466, -3.9194, -0.8682,  ..., -5.4092, -5.4496, -5.4603],\n",
      "        [-2.7703, -3.9319, -0.8781,  ..., -5.2753, -5.3278, -5.3530],\n",
      "        [-2.4270, -3.9565, -0.8028,  ..., -4.4784, -4.4709, -4.4725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3359, -0.4055, -0.9402, -0.4055, -0.3777,  0.9445,  0.9305, -0.9402],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.9677, -0.3776,  0.9900, -0.3881, -0.2106,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6608e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1126, -0.1138, -0.1149,  ..., -0.9484, -0.9580, -0.9677],\n",
      "        ...,\n",
      "        [-0.0785, -0.0793, -0.0801,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0363, -0.0366, -0.0370,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.9253, -0.3683,  0.9656, -0.3785, -0.2054,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7617, -3.9219, -0.8813,  ..., -5.1980, -5.2494, -5.2746],\n",
      "        [-2.8074, -3.9234, -0.8910,  ..., -5.2782, -5.2886, -5.3107],\n",
      "        [-2.6074, -3.8856, -0.8521,  ..., -0.8408, -0.8821, -0.7973],\n",
      "        ...,\n",
      "        [-2.5269, -3.9482, -0.8275,  ..., -4.5465, -4.5373, -4.5400],\n",
      "        [-2.6071, -3.9226, -0.8421,  ..., -4.7323, -4.7414, -4.7447],\n",
      "        [-2.7961, -3.9213, -0.8918,  ..., -5.2973, -5.3033, -5.3207]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9421,  0.9228, -0.8498, -0.4075,  0.9772, -0.4237, -0.1607,  0.9021],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4220,  0.9900,  0.9900,  0.9900,  0.9900, -0.4300,  0.9900, -0.2148],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0045e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.1069e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0854, -0.0862, -0.0871,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0870, -0.0879, -0.0888,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0370, -0.0374, -0.0377,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4115,  0.9656,  0.9656,  0.9656,  0.9608, -0.4194,  0.9656, -0.2095],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5177, -3.9444, -0.8304,  ..., -4.5881, -4.5723, -4.5713],\n",
      "        [-2.7981, -3.9192, -0.8813,  ..., -5.3409, -5.3471, -5.3658],\n",
      "        [-2.7939, -3.9174, -0.8828,  ..., -5.3348, -5.3417, -5.3584],\n",
      "        ...,\n",
      "        [-2.5163, -3.9397, -0.8249,  ..., -4.6247, -4.6134, -4.6107],\n",
      "        [-2.9230, -3.8897, -0.9047,  ..., -5.5591, -5.5501, -5.5393],\n",
      "        [-2.6096, -3.9208, -0.8345,  ..., -4.7348, -4.7451, -4.7482]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3229,  0.9505,  0.8698,  0.9694,  0.7007, -0.3607,  0.6239, -0.1555],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4390, -0.4374,  0.9900,  0.9900,  0.9900, -0.4010,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7724e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.7732e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0888, -0.0897, -0.0906,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0885, -0.0894, -0.0903,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0811, -0.0819, -0.0828,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4281, -0.4266,  0.9656,  0.9656,  0.9656, -0.3911,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5200, -3.9373, -0.8247,  ..., -4.6231, -4.6121, -4.6064],\n",
      "        [-2.5264, -3.9414, -0.8260,  ..., -4.6080, -4.5936, -4.5922],\n",
      "        [-2.8024, -3.9207, -0.8828,  ..., -5.3574, -5.3661, -5.3845],\n",
      "        ...,\n",
      "        [-2.5278, -3.9424, -0.8289,  ..., -4.6279, -4.6095, -4.6165],\n",
      "        [-2.9305, -3.8875, -0.9045,  ..., -5.4895, -5.4817, -5.4707],\n",
      "        [-2.9137, -3.8937, -0.9074,  ..., -5.5003, -5.4955, -5.4826]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3623, -0.3957,  0.9501,  0.5985,  0.9480, -0.3632,  0.6638,  0.7079],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.2224, -0.4283,  0.9900,  0.9900, -1.2426, -0.4463,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9234e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0383, -0.0387, -0.0391,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0866, -0.0875, -0.0884,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2539, -0.2565, -0.2591,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.2169, -0.4177,  0.9656,  0.9656, -1.2180, -0.4353,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8231, -3.9188, -0.8802,  ..., -5.3324, -5.3427, -5.3613],\n",
      "        [-2.6236, -3.9231, -0.8347,  ..., -4.6887, -4.6999, -4.7029],\n",
      "        [-2.5452, -3.9398, -0.8330,  ..., -4.5684, -4.5575, -4.5618],\n",
      "        ...,\n",
      "        [-2.4404, -3.9517, -0.7940,  ..., -4.4793, -4.4722, -4.4718],\n",
      "        [-2.5398, -3.9407, -0.8322,  ..., -4.6076, -4.5935, -4.5914],\n",
      "        [-2.7870, -3.9273, -0.8704,  ..., -5.2776, -5.3313, -5.3614]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9666, -0.2024, -0.3614,  0.9665,  0.9665, -0.9143, -0.3966,  0.9706],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2432, -0.4099], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(3.0557e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0419, -0.0423, -0.0427, -0.0432, -0.0436, -0.0440, -0.0445, -0.0449,\n",
      "         -0.0454, -0.0459, -0.0463, -0.0468, -0.0473, -0.0477, -0.0482, -0.0487,\n",
      "         -0.0492, -0.0497, -0.0502, -0.0507, -0.0512, -0.0517, -0.0523, -0.0528,\n",
      "         -0.0533, -0.0539, -0.0544, -0.0549, -0.0555, -0.0561, -0.0566, -0.0572,\n",
      "         -0.0578, -0.0584, -0.0590, -0.0596, -0.0602, -0.0608, -0.0614, -0.0620,\n",
      "         -0.0626, -0.0633, -0.0639, -0.0645, -0.0652, -0.0658, -0.0665, -0.0672,\n",
      "         -0.0679, -0.0685, -0.0692, -0.0699, -0.0706, -0.0714, -0.0721, -0.0728,\n",
      "         -0.0735, -0.0743, -0.0750, -0.0758, -0.0766, -0.0773, -0.0781, -0.0789,\n",
      "         -0.0797, -0.0805, -0.0813, -0.0821, -0.0830, -0.0838, -0.0847, -0.0855,\n",
      "         -0.0864, -0.0872, -0.0881, -0.0890, -0.0899, -0.0908, -0.0917, -0.0927,\n",
      "         -0.0936, -0.0946, -0.0955, -0.0965, -0.0974, -0.0984, -0.0994, -0.1004,\n",
      "         -0.1014, -0.1025, -0.1035, -0.1045, -0.1056, -0.1067, -0.1077, -0.1088,\n",
      "         -0.1099, -0.1110, -0.1122, -0.1133, -0.1144, -0.1156, -0.1168, -0.1179,\n",
      "         -0.1191, -0.1203, -0.1216, -0.1228, -0.1240, -0.1253, -0.1265, -0.1278,\n",
      "         -0.1291, -0.1304, -0.1317, -0.1331, -0.1344, -0.1358, -0.1371, -0.1385,\n",
      "         -0.1399, -0.1413, -0.1428, -0.1442, -0.1457, -0.1471, -0.1486, -0.1501,\n",
      "         -0.1516, -0.1532, -0.1547, -0.1563, -0.1579, -0.1595, -0.1611, -0.1627,\n",
      "         -0.1643, -0.1660, -0.1677, -0.1694, -0.1711, -0.1728, -0.1745, -0.1763,\n",
      "         -0.1781, -0.1799, -0.1817, -0.1835, -0.1854, -0.1873, -0.1892, -0.1911,\n",
      "         -0.1930, -0.1950, -0.1969, -0.1989, -0.2009, -0.2030, -0.2050, -0.2071,\n",
      "         -0.2092, -0.2113, -0.2134, -0.2156, -0.2177, -0.2199, -0.2222, -0.2244,\n",
      "         -0.2267, -0.2290, -0.2313, -0.2336, -0.2360, -0.2384, -0.2408, -0.2432,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0829, -0.0838, -0.0846, -0.0855, -0.0863, -0.0872, -0.0881, -0.0890,\n",
      "         -0.0899, -0.0908, -0.0917, -0.0926, -0.0936, -0.0945, -0.0955, -0.0964,\n",
      "         -0.0974, -0.0984, -0.0994, -0.1004, -0.1014, -0.1024, -0.1034, -0.1045,\n",
      "         -0.1055, -0.1066, -0.1077, -0.1088, -0.1099, -0.1110, -0.1121, -0.1132,\n",
      "         -0.1144, -0.1155, -0.1167, -0.1179, -0.1191, -0.1203, -0.1215, -0.1227,\n",
      "         -0.1240, -0.1252, -0.1265, -0.1278, -0.1290, -0.1304, -0.1317, -0.1330,\n",
      "         -0.1343, -0.1357, -0.1371, -0.1385, -0.1399, -0.1413, -0.1427, -0.1441,\n",
      "         -0.1456, -0.1471, -0.1485, -0.1500, -0.1516, -0.1531, -0.1546, -0.1562,\n",
      "         -0.1578, -0.1594, -0.1610, -0.1626, -0.1643, -0.1659, -0.1676, -0.1693,\n",
      "         -0.1710, -0.1727, -0.1745, -0.1762, -0.1780, -0.1798, -0.1816, -0.1835,\n",
      "         -0.1853, -0.1872, -0.1891, -0.1910, -0.1929, -0.1949, -0.1968, -0.1988,\n",
      "         -0.2008, -0.2028, -0.2049, -0.2070, -0.2091, -0.2112, -0.2133, -0.2155,\n",
      "         -0.2176, -0.2198, -0.2221, -0.2243, -0.2266, -0.2288, -0.2312, -0.2335,\n",
      "         -0.2359, -0.2382, -0.2406, -0.2431, -0.2455, -0.2480, -0.2505, -0.2530,\n",
      "         -0.2556, -0.2582, -0.2608, -0.2634, -0.2661, -0.2688, -0.2715, -0.2742,\n",
      "         -0.2770, -0.2798, -0.2826, -0.2855, -0.2884, -0.2913, -0.2942, -0.2972,\n",
      "         -0.3002, -0.3032, -0.3063, -0.3094, -0.3125, -0.3157, -0.3188, -0.3221,\n",
      "         -0.3253, -0.3286, -0.3319, -0.3353, -0.3387, -0.3421, -0.3455, -0.3490,\n",
      "         -0.3526, -0.3561, -0.3597, -0.3634, -0.3670, -0.3707, -0.3745, -0.3783,\n",
      "         -0.3821, -0.3859, -0.3898, -0.3938, -0.3978, -0.4018, -0.4058, -0.4099,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2372, -0.3998], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6338, -3.9211, -0.8451, -2.3596,  0.1012, -2.3757, -4.6631, -0.4033,\n",
      "         -3.9020, -2.4684, -3.8740,  0.7391, -0.1250, -3.5863, -3.1530, -2.3990,\n",
      "         -3.2578, -1.7850, -2.0338, -1.0476, -1.4764, -1.4794, -0.9045,  0.3404,\n",
      "         -1.9914, -3.4122, -3.1051, -2.5943, -3.9361,  0.8768, -1.8105, -0.2709,\n",
      "         -0.8739, -3.4896, -1.8727, -1.6563, -0.9553, -3.1846, -1.7064, -0.0926,\n",
      "         -0.7001, -0.5886, -0.4307, -4.2693, -1.6919, -3.3445, -3.4295, -3.1926,\n",
      "          0.2704,  0.3051, -3.7921, -1.5148, -6.9631, -2.9147, -3.0574, -0.9894,\n",
      "         -2.5791, -3.0021, -2.5407, -1.7024, -3.5894, -1.2237, -5.7246, -2.4417,\n",
      "         -3.6948,  0.9835, -1.4445, -0.1565, -0.3418, -3.1218, -4.0369, -5.3951,\n",
      "         -2.8064, -0.1080, -3.7681, -4.2050, -2.5005, -3.8688, -2.5628, -3.5173,\n",
      "         -4.7404, -2.5167, -0.3992, -1.3368, -1.7238, -3.0444, -1.8807, -0.7383,\n",
      "         -3.5071, -0.8659, -3.1242, -4.0730, -2.4746, -0.6649, -0.9569, -2.9515,\n",
      "         -2.8020, -3.6985, -3.2581, -3.2587, -0.1700, -1.0600, -3.7056, -2.0840,\n",
      "         -0.4940, -1.9408, -3.5288, -3.7543,  0.5857, -1.4259, -1.2724, -0.9150,\n",
      "         -4.3793, -4.6560,  0.3315, -1.0221, -2.1730, -1.3368, -0.0308, -1.0059,\n",
      "         -2.8560, -4.5769, -2.7456, -0.9187, -1.0403, -2.6907, -2.3307, -0.8740,\n",
      "         -0.6159, -4.1395, -0.1231,  0.3625, -3.3188, -4.1235, -1.2655, -2.0593,\n",
      "         -3.1237, -2.3510, -1.8035, -2.8649, -2.9677, -5.7753, -1.6162, -2.2179,\n",
      "         -1.2544, -0.7394, -0.5025, -4.0625, -3.9074, -0.1194, -2.0740, -0.6514,\n",
      "         -1.0287, -0.4501, -0.1645, -0.6282, -3.9694, -3.2706, -4.9879, -1.3122,\n",
      "         -0.0955, -0.1701, -0.1981, -0.2067, -0.1705, -0.2171, -4.8782, -4.9440,\n",
      "         -5.2334, -4.8010, -5.0612, -5.0472, -4.9123, -4.9958, -5.0455, -5.1469,\n",
      "         -5.2800, -5.2736, -5.0771, -4.9027, -4.7919, -4.6757, -4.5838, -4.5385,\n",
      "         -4.4897, -4.4535, -4.4597, -4.4991, -4.5487, -4.5864, -4.6033, -4.6211,\n",
      "         -4.6483, -4.6904, -4.7310, -4.7586, -4.7620, -4.7442, -4.7231, -4.7058,\n",
      "         -4.7122, -4.7325, -4.7515, -4.7592, -4.7633],\n",
      "        [-2.5515, -3.9370, -0.8418, -2.3095,  0.0924, -2.3757, -4.6524, -0.3737,\n",
      "         -3.8319, -2.4646, -3.8797,  0.7498, -0.0820, -3.5745, -3.1339, -2.3655,\n",
      "         -3.2279, -1.7972, -2.0097, -1.0324, -1.4765, -1.4723, -0.8983,  0.3540,\n",
      "         -1.9533, -3.4061, -3.0954, -2.5659, -3.9278,  0.8862, -1.7814, -0.2581,\n",
      "         -0.8639, -3.4871, -1.8592, -1.6602, -0.9256, -3.1554, -1.6758, -0.0740,\n",
      "         -0.7141, -0.5568, -0.4245, -4.2559, -1.6551, -3.2851, -3.4055, -3.1912,\n",
      "          0.2280,  0.2748, -3.7917, -1.5014, -6.9491, -2.9027, -3.0384, -0.9929,\n",
      "         -2.5730, -2.9794, -2.5418, -1.6705, -3.5897, -1.2198, -5.7096, -2.4356,\n",
      "         -3.6899,  0.9864, -1.4208, -0.1461, -0.3369, -3.1200, -4.0408, -5.4162,\n",
      "         -2.8013, -0.1115, -3.7734, -4.1979, -2.4975, -3.8511, -2.5438, -3.5239,\n",
      "         -4.7634, -2.5186, -0.3994, -1.3318, -1.7424, -3.0201, -1.8630, -0.7325,\n",
      "         -3.5384, -0.8754, -3.1329, -4.0687, -2.4756, -0.6673, -0.9568, -2.9575,\n",
      "         -2.7815, -3.6556, -3.2658, -3.2376, -0.1743, -1.0620, -3.6987, -2.0856,\n",
      "         -0.4803, -1.9306, -3.4920, -3.7514,  0.5807, -1.1909, -1.2685, -0.4238,\n",
      "         -4.4643, -4.5699,  0.3371, -1.0158, -2.2150, -1.3617, -0.0720, -0.9981,\n",
      "         -2.8668, -4.5406, -2.7380, -0.9485, -1.0993, -2.7191, -2.3131, -0.8509,\n",
      "         -0.6775, -4.1656, -0.1431,  0.3514, -3.3268, -4.1041, -1.2690, -2.0461,\n",
      "         -3.0789, -2.3580, -1.7620, -2.8264, -2.5573, -3.3848, -5.6648, -1.3869,\n",
      "         -0.3593, -0.4046, -0.2854, -0.3780, -0.3402, -0.4240, -4.8550, -4.9080,\n",
      "         -5.2776, -4.9307, -4.8741, -4.5890, -4.5508, -4.6772, -4.6516, -4.5811,\n",
      "         -4.6564, -4.7472, -4.7967, -4.7875, -4.7215, -4.6423, -4.5930, -4.5712,\n",
      "         -4.5756, -4.5788, -4.5793, -4.6017, -4.6377, -4.6650, -4.6833, -4.6791,\n",
      "         -4.6448, -4.6188, -4.6324, -4.6492, -4.6690, -4.6962, -4.6943, -4.6797,\n",
      "         -4.6680, -4.6614, -4.6803, -4.6979, -4.7143, -4.7231, -4.7288, -4.7415,\n",
      "         -4.7455, -4.7347, -4.7180, -4.7037, -4.7024, -4.6919, -4.6855, -4.6798,\n",
      "         -4.6730, -4.6725, -4.6619, -4.6492, -4.6490]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.1763, -0.3653], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2542,  0.9900,  0.9900, -0.4116,  0.9900, -0.4198, -0.9939],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8878e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.3727e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2563, -0.2589, -0.2615,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0849, -0.0858, -0.0866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1157, -0.1169, -0.1180,  ..., -0.9742, -0.9840, -0.9939]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.2293,  0.9656,  0.9656, -0.4015,  0.9656, -0.4094, -0.9504],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8409, -3.9225, -0.9012,  ..., -5.2927, -5.3049, -5.3269],\n",
      "        [-2.4520, -3.9524, -0.8017,  ..., -4.4808, -4.4699, -4.4702],\n",
      "        [-2.8301, -3.9215, -0.9012,  ..., -5.3203, -5.3304, -5.3495],\n",
      "        ...,\n",
      "        [-2.8325, -3.9256, -0.8942,  ..., -5.3591, -5.3659, -5.3848],\n",
      "        [-2.5592, -3.9444, -0.8447,  ..., -4.6253, -4.6100, -4.6150],\n",
      "        [-2.6394, -3.8850, -0.8601,  ..., -0.8893, -0.9021, -0.7993]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9722, -0.8963,  0.9540,  1.0263, -0.3704,  0.9710, -0.3708, -0.8811],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.2482, -0.4513,  0.9900, -0.4219,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9793e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0428, -0.0432, -0.0436,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0853, -0.0862, -0.0871,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9608, -0.2421, -0.4401,  0.9656, -0.4115,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9566, -3.8942, -0.9220,  ..., -5.5016, -5.4929, -5.4830],\n",
      "        [-2.9405, -3.8991, -0.9228,  ..., -5.5132, -5.5089, -5.5002],\n",
      "        [-2.6344, -3.9266, -0.8528,  ..., -4.7615, -4.7721, -4.7760],\n",
      "        ...,\n",
      "        [-2.5619, -3.9533, -0.8443,  ..., -4.5313, -4.5220, -4.5254],\n",
      "        [-2.8281, -3.9266, -0.9027,  ..., -5.3745, -5.3831, -5.4030],\n",
      "        [-2.7747, -3.9213, -0.8750,  ..., -5.3991, -5.4413, -5.4451]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7075,  0.7471, -0.2055, -0.3751,  0.6398, -0.4347,  0.9736,  0.9859],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4464, -0.4405,  0.9900, -0.4301,  0.9900, -0.4031, -0.4464],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0439e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0891, -0.0900, -0.0909,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0816, -0.0824, -0.0832,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4354, -0.4296,  0.9656, -0.4195,  0.9656, -0.3932, -0.4354],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8331, -3.9302, -0.8958,  ..., -5.3394, -5.3478, -5.3665],\n",
      "        [-2.5597, -3.9532, -0.8487,  ..., -4.6131, -4.5993, -4.5958],\n",
      "        [-2.5611, -3.9543, -0.8517,  ..., -4.6059, -4.5975, -4.5998],\n",
      "        ...,\n",
      "        [-2.7883, -3.9323, -0.8957,  ..., -5.1897, -5.2459, -5.2734],\n",
      "        [-2.5592, -3.9484, -0.8545,  ..., -4.6667, -4.6506, -4.6492],\n",
      "        [-2.5597, -3.9532, -0.8487,  ..., -4.6131, -4.5993, -4.5958]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9829, -0.4265, -0.4085,  1.0027, -0.3873,  0.9594, -0.3896, -0.4265],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4334, -0.4404, -0.4493,  0.9900, -0.3920, -1.2353, -0.2167, -0.4321],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(8.5864e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0877, -0.0886, -0.0895,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0891, -0.0900, -0.0909,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0909, -0.0918, -0.0927,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2524, -0.2550, -0.2575,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0373, -0.0377, -0.0381,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0874, -0.0883, -0.0892,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4227, -0.4296, -0.4382,  0.9608, -0.3823, -1.2108, -0.2113, -0.4215],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5528, -3.9598, -0.8569,  ..., -4.5841, -4.5689, -4.5665],\n",
      "        [-2.5507, -3.9548, -0.8493,  ..., -4.6272, -4.6129, -4.6076],\n",
      "        [-2.5464, -3.9564, -0.8498,  ..., -4.6045, -4.5926, -4.5890],\n",
      "        ...,\n",
      "        [-2.4477, -3.9655, -0.8112,  ..., -4.4742, -4.4642, -4.4640],\n",
      "        [-2.6351, -3.9384, -0.8514,  ..., -4.6920, -4.7033, -4.7065],\n",
      "        [-2.5587, -3.9602, -0.8544,  ..., -4.6066, -4.5994, -4.6016]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3990, -0.4150, -0.3839,  0.7618, -0.4466, -0.9007, -0.2429, -0.4138],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.9467, -0.3996,  0.9900,  0.9900,  0.9900, -0.4371,  0.9900, -0.2259],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5041e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.8876e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1840, -0.1858, -0.1877,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0808, -0.0817, -0.0825,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0884, -0.0893, -0.0902,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0389, -0.0393, -0.0397,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9052, -0.3898,  0.9656,  0.9656,  0.9656, -0.4264,  0.9656, -0.2203],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5102, -3.9521, -0.8317,  ..., -4.5683, -4.5700, -4.5829],\n",
      "        [-2.5573, -3.9644, -0.8493,  ..., -4.5371, -4.5290, -4.5302],\n",
      "        [-2.8212, -3.9377, -0.9071,  ..., -5.3455, -5.3529, -5.3733],\n",
      "        ...,\n",
      "        [-2.5458, -3.9580, -0.8488,  ..., -4.6266, -4.6136, -4.6093],\n",
      "        [-2.9504, -3.9074, -0.9281,  ..., -5.5689, -5.5633, -5.5521],\n",
      "        [-2.6311, -3.9390, -0.8581,  ..., -4.7403, -4.7522, -4.7537]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9689, -0.4509,  0.9453,  0.9734,  0.8793, -0.4238,  0.6892, -0.2401],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.3819,  0.9900,  0.9900, -0.2162,  0.9900, -0.4117],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3272e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0773, -0.0780, -0.0788,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0372, -0.0376, -0.0380,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0833, -0.0841, -0.0850,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.3725,  0.9656,  0.9656, -0.2108,  0.9656, -0.4015],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8076, -3.9406, -0.8920,  ..., -5.3125, -5.3222, -5.3399],\n",
      "        [-2.7992, -3.9404, -0.9000,  ..., -5.3308, -5.3357, -5.3590],\n",
      "        [-2.5277, -3.9630, -0.8498,  ..., -4.6212, -4.6005, -4.5905],\n",
      "        ...,\n",
      "        [-2.6095, -3.9413, -0.8517,  ..., -4.7216, -4.7329, -4.7342],\n",
      "        [-2.7992, -3.9404, -0.9000,  ..., -5.3308, -5.3357, -5.3590],\n",
      "        [-2.5332, -3.9620, -0.8446,  ..., -4.5600, -4.5524, -4.5522]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9558,  0.9225, -0.4603,  0.9492,  0.9451, -0.2469,  0.9225, -0.4038],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2101, -0.4338], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(6.8969e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.7726e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2473, -0.2498, -0.2523, -0.2548, -0.2574, -0.2600, -0.2626, -0.2653,\n",
      "         -0.2680, -0.2707, -0.2734, -0.2762, -0.2790, -0.2818, -0.2846, -0.2875,\n",
      "         -0.2904, -0.2933, -0.2963, -0.2993, -0.3023, -0.3054, -0.3085, -0.3116,\n",
      "         -0.3147, -0.3179, -0.3211, -0.3244, -0.3276, -0.3309, -0.3343, -0.3377,\n",
      "         -0.3411, -0.3445, -0.3480, -0.3515, -0.3551, -0.3587, -0.3623, -0.3659,\n",
      "         -0.3696, -0.3734, -0.3771, -0.3809, -0.3848, -0.3887, -0.3926, -0.3966,\n",
      "         -0.4006, -0.4046, -0.4087, -0.4128, -0.4170, -0.4212, -0.4255, -0.4298,\n",
      "         -0.4341, -0.4385, -0.4429, -0.4474, -0.4519, -0.4565, -0.4611, -0.4658,\n",
      "         -0.4705, -0.4752, -0.4800, -0.4849, -0.4898, -0.4947, -0.4997, -0.5048,\n",
      "         -0.5098, -0.5150, -0.5202, -0.5255, -0.5308, -0.5361, -0.5415, -0.5470,\n",
      "         -0.5525, -0.5581, -0.5638, -0.5694, -0.5752, -0.5810, -0.5869, -0.5928,\n",
      "         -0.5988, -0.6048, -0.6110, -0.6171, -0.6234, -0.6297, -0.6360, -0.6424,\n",
      "         -0.6489, -0.6555, -0.6621, -0.6688, -0.6755, -0.6824, -0.6893, -0.6962,\n",
      "         -0.7033, -0.7104, -0.7175, -0.7248, -0.7321, -0.7395, -0.7470, -0.7545,\n",
      "         -0.7621, -0.7698, -0.7776, -0.7855, -0.7934, -0.8014, -0.8095, -0.8177,\n",
      "         -0.8259, -0.8343, -0.8427, -0.8512, -0.8598, -0.8685, -0.8773, -0.8861,\n",
      "         -0.8951, -0.9041, -0.9133, -0.9225, -0.9318, -0.9412, -0.9507, -0.9603,\n",
      "         -0.9700, -0.9798, -0.9897, -0.9997, -1.0098, -1.0200, -1.0303, -1.0407,\n",
      "         -1.0513, -1.0619, -1.0726, -1.0834, -1.0944, -1.1054, -1.1166, -1.1279,\n",
      "         -1.1393, -1.1508, -1.1624, -1.1741, -1.1860, -1.1980, -1.2101,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0878, -0.0887, -0.0895, -0.0905, -0.0914, -0.0923, -0.0932, -0.0942,\n",
      "         -0.0951, -0.0961, -0.0970, -0.0980, -0.0990, -0.1000, -0.1010, -0.1020,\n",
      "         -0.1031, -0.1041, -0.1052, -0.1062, -0.1073, -0.1084, -0.1095, -0.1106,\n",
      "         -0.1117, -0.1128, -0.1140, -0.1151, -0.1163, -0.1175, -0.1186, -0.1198,\n",
      "         -0.1211, -0.1223, -0.1235, -0.1248, -0.1260, -0.1273, -0.1286, -0.1299,\n",
      "         -0.1312, -0.1325, -0.1339, -0.1352, -0.1366, -0.1380, -0.1393, -0.1408,\n",
      "         -0.1422, -0.1436, -0.1451, -0.1465, -0.1480, -0.1495, -0.1510, -0.1525,\n",
      "         -0.1541, -0.1556, -0.1572, -0.1588, -0.1604, -0.1620, -0.1637, -0.1653,\n",
      "         -0.1670, -0.1687, -0.1704, -0.1721, -0.1738, -0.1756, -0.1774, -0.1792,\n",
      "         -0.1810, -0.1828, -0.1846, -0.1865, -0.1884, -0.1903, -0.1922, -0.1942,\n",
      "         -0.1961, -0.1981, -0.2001, -0.2021, -0.2042, -0.2062, -0.2083, -0.2104,\n",
      "         -0.2125, -0.2147, -0.2168, -0.2190, -0.2212, -0.2235, -0.2257, -0.2280,\n",
      "         -0.2303, -0.2327, -0.2350, -0.2374, -0.2398, -0.2422, -0.2446, -0.2471,\n",
      "         -0.2496, -0.2521, -0.2547, -0.2572, -0.2598, -0.2625, -0.2651, -0.2678,\n",
      "         -0.2705, -0.2732, -0.2760, -0.2788, -0.2816, -0.2844, -0.2873, -0.2902,\n",
      "         -0.2932, -0.2961, -0.2991, -0.3021, -0.3052, -0.3083, -0.3114, -0.3145,\n",
      "         -0.3177, -0.3209, -0.3241, -0.3274, -0.3307, -0.3341, -0.3374, -0.3409,\n",
      "         -0.3443, -0.3478, -0.3513, -0.3548, -0.3584, -0.3620, -0.3657, -0.3694,\n",
      "         -0.3731, -0.3769, -0.3807, -0.3845, -0.3884, -0.3923, -0.3963, -0.4003,\n",
      "         -0.4044, -0.4084, -0.4126, -0.4167, -0.4209, -0.4252, -0.4295, -0.4338,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1861, -0.4231], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4009e+00, -3.9742e+00, -7.9679e-01, -2.2928e+00,  1.7005e-01,\n",
      "         -2.3266e+00, -4.5445e+00, -3.0745e-01, -3.6710e+00, -2.3825e+00,\n",
      "         -3.8318e+00,  6.6438e-01, -1.2558e-01, -3.5373e+00, -3.1990e+00,\n",
      "         -2.3092e+00, -3.1910e+00, -1.7289e+00, -2.0689e+00, -9.2778e-01,\n",
      "         -1.4207e+00, -1.3813e+00, -8.3397e-01,  3.2331e-01, -1.8971e+00,\n",
      "         -3.3597e+00, -3.1456e+00, -2.5167e+00, -3.9092e+00,  8.8688e-01,\n",
      "         -1.7336e+00, -1.4387e-01, -8.8613e-01, -3.4433e+00, -1.8578e+00,\n",
      "         -1.7211e+00, -9.7184e-01, -3.1264e+00, -1.6354e+00,  6.4528e-03,\n",
      "         -6.5960e-01, -5.9060e-01, -4.0261e-01, -4.2938e+00, -1.5994e+00,\n",
      "         -3.1328e+00, -3.3122e+00, -3.1474e+00, -3.9948e-02,  1.6208e-01,\n",
      "         -3.7586e+00, -1.4614e+00, -7.0320e+00, -2.8641e+00, -3.0091e+00,\n",
      "         -9.0820e-01, -2.6068e+00, -3.0077e+00, -2.5006e+00, -1.6215e+00,\n",
      "         -3.5489e+00, -1.1756e+00, -5.7819e+00, -2.4134e+00, -3.6659e+00,\n",
      "          9.9120e-01, -1.3723e+00, -1.7934e-02, -3.6551e-01, -3.0678e+00,\n",
      "         -3.9939e+00, -5.4268e+00, -2.7598e+00, -1.7691e-01, -3.7235e+00,\n",
      "         -4.2646e+00, -2.4524e+00, -3.8287e+00, -2.5029e+00, -3.4760e+00,\n",
      "         -4.7662e+00, -2.4826e+00, -3.6438e-01, -1.2116e+00, -1.6917e+00,\n",
      "         -2.9412e+00, -1.8067e+00, -6.1712e-01, -3.4739e+00, -8.1524e-01,\n",
      "         -3.0798e+00, -4.1222e+00, -2.4365e+00, -6.3542e-01, -8.3014e-01,\n",
      "         -2.9964e+00, -2.7118e+00, -3.5066e+00, -3.2360e+00, -3.1960e+00,\n",
      "         -1.2345e-01, -9.3249e-01, -3.7295e+00, -2.0729e+00, -5.0529e-01,\n",
      "         -1.8549e+00, -3.3560e+00, -3.6878e+00,  5.1287e-01, -1.0675e+00,\n",
      "         -1.4074e+00, -2.3289e+00, -4.2355e+00, -4.5917e+00,  3.1613e-01,\n",
      "         -9.0482e-01, -2.1629e+00, -1.2705e+00,  6.0315e-02, -9.6969e-01,\n",
      "         -2.5633e+00, -4.7384e+00, -2.6672e+00, -8.8009e-01, -9.8767e-01,\n",
      "         -2.6763e+00, -2.2514e+00, -8.3560e-01, -4.9447e-01, -4.1214e+00,\n",
      "         -4.5653e-01,  2.3537e-01, -3.3043e+00, -4.0636e+00, -1.1809e+00,\n",
      "         -2.0833e+00, -3.1284e+00, -2.3046e+00, -1.6993e+00, -2.7788e+00,\n",
      "         -2.5445e+00, -3.2829e+00, -5.6479e+00, -1.2738e+00, -7.6694e-01,\n",
      "         -2.5891e-01, -1.2337e+00, -1.2006e+00, -1.2744e+00, -4.8638e+00,\n",
      "         -4.8771e+00, -5.3348e+00, -4.8265e+00, -4.6946e+00, -4.7584e+00,\n",
      "         -4.9130e+00, -5.0245e+00, -4.9462e+00, -4.8236e+00, -4.7388e+00,\n",
      "         -4.6532e+00, -4.5658e+00, -4.4970e+00, -4.4479e+00, -4.4121e+00,\n",
      "         -4.3814e+00, -4.3496e+00, -4.3250e+00, -4.3057e+00, -4.2908e+00,\n",
      "         -4.2764e+00, -4.2774e+00, -4.2927e+00, -4.3209e+00, -4.3516e+00,\n",
      "         -4.3946e+00, -4.4311e+00, -4.4469e+00, -4.4591e+00, -4.4603e+00,\n",
      "         -4.4648e+00, -4.4715e+00, -4.4819e+00, -4.5024e+00, -4.5179e+00,\n",
      "         -4.5307e+00, -4.5461e+00, -4.5525e+00, -4.5443e+00, -4.5268e+00,\n",
      "         -4.5060e+00, -4.5010e+00, -4.5052e+00, -4.5019e+00, -4.4983e+00,\n",
      "         -4.4920e+00, -4.4861e+00, -4.4763e+00, -4.4768e+00, -4.4688e+00,\n",
      "         -4.4638e+00, -4.4642e+00, -4.4570e+00, -4.4474e+00, -4.4479e+00],\n",
      "        [-2.4929e+00, -3.9642e+00, -8.3529e-01, -2.3324e+00,  1.3166e-01,\n",
      "         -2.3619e+00, -4.6246e+00, -3.4829e-01, -3.7751e+00, -2.4413e+00,\n",
      "         -3.8690e+00,  6.5174e-01, -1.2491e-01, -3.5759e+00, -3.1934e+00,\n",
      "         -2.3654e+00, -3.2169e+00, -1.7645e+00, -2.0527e+00, -9.8744e-01,\n",
      "         -1.4515e+00, -1.4370e+00, -8.7350e-01,  3.4058e-01, -1.9201e+00,\n",
      "         -3.4053e+00, -3.1491e+00, -2.5615e+00, -3.9039e+00,  9.0046e-01,\n",
      "         -1.7447e+00, -2.1066e-01, -8.7186e-01, -3.4744e+00, -1.8649e+00,\n",
      "         -1.6828e+00, -9.2672e-01, -3.1442e+00, -1.6534e+00, -4.2244e-02,\n",
      "         -6.6778e-01, -5.5838e-01, -4.0839e-01, -4.2708e+00, -1.6214e+00,\n",
      "         -3.2304e+00, -3.3704e+00, -3.1774e+00,  1.1185e-01,  3.0871e-01,\n",
      "         -3.8051e+00, -1.5013e+00, -7.0358e+00, -2.9059e+00, -3.0251e+00,\n",
      "         -9.4905e-01, -2.6054e+00, -2.9893e+00, -2.5378e+00, -1.6368e+00,\n",
      "         -3.5924e+00, -1.2156e+00, -5.7839e+00, -2.4404e+00, -3.6620e+00,\n",
      "          1.0052e+00, -1.3757e+00, -8.8076e-02, -3.4527e-01, -3.0983e+00,\n",
      "         -4.0592e+00, -5.4454e+00, -2.8019e+00, -1.5533e-01, -3.7714e+00,\n",
      "         -4.2485e+00, -2.4934e+00, -3.8437e+00, -2.5204e+00, -3.5205e+00,\n",
      "         -4.7874e+00, -2.5175e+00, -3.7177e-01, -1.2829e+00, -1.7294e+00,\n",
      "         -3.0030e+00, -1.8197e+00, -6.8115e-01, -3.5441e+00, -8.2387e-01,\n",
      "         -3.1332e+00, -4.1122e+00, -2.4707e+00, -6.3900e-01, -9.0224e-01,\n",
      "         -2.9903e+00, -2.7395e+00, -3.6004e+00, -3.2715e+00, -3.2315e+00,\n",
      "         -1.4209e-01, -1.0077e+00, -3.7360e+00, -2.0919e+00, -4.9290e-01,\n",
      "         -1.8993e+00, -3.4398e+00, -3.7438e+00,  5.3498e-01, -1.1282e+00,\n",
      "         -1.0659e+00, -9.6409e-01, -4.2747e+00, -4.5813e+00,  3.2140e-01,\n",
      "         -1.0298e+00, -2.1776e+00, -1.3683e+00, -1.0140e-01, -9.7023e-01,\n",
      "         -2.8177e+00, -4.5359e+00, -2.7403e+00, -9.4043e-01, -1.1074e+00,\n",
      "         -2.7178e+00, -2.2770e+00, -8.6051e-01, -6.8050e-01, -4.1376e+00,\n",
      "         -2.7339e-01,  3.8446e-01, -3.3201e+00, -4.0846e+00, -1.2305e+00,\n",
      "         -2.0799e+00, -3.1024e+00, -2.3417e+00, -1.7238e+00, -2.8152e+00,\n",
      "         -2.5505e+00, -3.3566e+00, -5.7085e+00, -1.3704e+00, -4.2794e-01,\n",
      "         -4.4414e-01, -3.7043e-01, -3.5267e-01, -3.8461e-01, -3.8857e-01,\n",
      "         -4.8286e+00, -4.8911e+00, -5.2503e+00, -4.9007e+00, -4.8470e+00,\n",
      "         -4.6277e+00, -4.5627e+00, -4.6569e+00, -4.6236e+00, -4.5844e+00,\n",
      "         -4.6366e+00, -4.6856e+00, -4.7152e+00, -4.6875e+00, -4.6049e+00,\n",
      "         -4.5150e+00, -4.4653e+00, -4.4484e+00, -4.4519e+00, -4.4582e+00,\n",
      "         -4.4691e+00, -4.4925e+00, -4.5205e+00, -4.5469e+00, -4.5743e+00,\n",
      "         -4.5836e+00, -4.5582e+00, -4.5346e+00, -4.5451e+00, -4.5585e+00,\n",
      "         -4.5652e+00, -4.5810e+00, -4.5767e+00, -4.5585e+00, -4.5488e+00,\n",
      "         -4.5491e+00, -4.5654e+00, -4.5801e+00, -4.5871e+00, -4.5849e+00,\n",
      "         -4.5828e+00, -4.5897e+00, -4.5929e+00, -4.5892e+00, -4.5794e+00,\n",
      "         -4.5752e+00, -4.5796e+00, -4.5770e+00, -4.5689e+00, -4.5638e+00,\n",
      "         -4.5602e+00, -4.5747e+00, -4.5779e+00, -4.5684e+00, -4.5641e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9469, -0.3947], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2013, -0.1898, -0.3741, -0.3722, -1.2013,  0.9900, -0.4120,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2826e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0347, -0.0350, -0.0354,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0327, -0.0330, -0.0334,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0757, -0.0764, -0.0772,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0834, -0.0842, -0.0850,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.1963, -0.1851, -0.3649, -0.3630, -1.1775,  0.9656, -0.4019,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5584, -3.9450, -0.8347,  ..., -4.6854, -4.6929, -4.6962],\n",
      "        [-2.5579, -3.9475, -0.8283,  ..., -4.6214, -4.6319, -4.6371],\n",
      "        [-2.4701, -3.9629, -0.8293,  ..., -4.6253, -4.6155, -4.6143],\n",
      "        ...,\n",
      "        [-2.8769, -3.9218, -0.9079,  ..., -5.5978, -5.5923, -5.5833],\n",
      "        [-2.4707, -3.9706, -0.8280,  ..., -4.5702, -4.5659, -4.5649],\n",
      "        [-2.7540, -3.9457, -0.8828,  ..., -5.3126, -5.3200, -5.3424]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2513, -0.2530, -0.4115, -0.4658, -0.9930,  0.5456, -0.4295,  0.9289],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900, 0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.6938e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656, 0.9656, 0.9656, 0.9656, 0.9656, 0.9656, 0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7230, -3.9430, -0.8647,  ..., -5.2776, -5.2861, -5.3054],\n",
      "        [-2.7105, -3.9412, -0.8725,  ..., -5.2855, -5.2931, -5.3127],\n",
      "        [-2.6877, -3.9517, -0.8532,  ..., -5.2248, -5.2840, -5.3082],\n",
      "        ...,\n",
      "        [-2.7142, -3.9439, -0.8711,  ..., -5.2947, -5.3003, -5.3209],\n",
      "        [-2.7142, -3.9439, -0.8711,  ..., -5.2947, -5.3003, -5.3209],\n",
      "        [-2.7280, -3.9465, -0.8740,  ..., -5.2346, -5.2456, -5.2694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.8718, 0.7584, 0.8640, 0.8718, 0.8638, 0.8393, 0.8393, 0.8548],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4017, -0.4293, -0.3821,  0.9900, -0.4135,  0.9900,  0.9900, -0.4126],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6872e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0813, -0.0821, -0.0829,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0868, -0.0877, -0.0886,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0773, -0.0781, -0.0789,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0835, -0.0843, -0.0852,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3918, -0.4187, -0.3727,  0.9656, -0.4033,  0.9656,  0.9608, -0.4024],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4178, -3.9623, -0.8046,  ..., -4.5201, -4.5145, -4.5167],\n",
      "        [-2.4021, -3.9623, -0.8045,  ..., -4.5528, -4.5463, -4.5420],\n",
      "        [-2.4144, -3.9673, -0.8000,  ..., -4.4947, -4.4890, -4.4923],\n",
      "        ...,\n",
      "        [-2.6431, -3.9361, -0.8363,  ..., -5.3679, -5.4182, -5.4269],\n",
      "        [-2.8174, -3.9158, -0.8895,  ..., -5.4933, -5.4892, -5.4766],\n",
      "        [-2.4138, -3.9657, -0.8082,  ..., -4.5588, -4.5523, -4.5532]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3963, -0.3888, -0.4354,  0.8569, -0.4192,  0.8392,  0.6299, -0.4193],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4036,  0.9900,  0.9900,  0.9900, -0.3835, -0.3841, -1.1997,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5649e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0816, -0.0825, -0.0833,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0777, -0.0785, -0.0793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2451, -0.2476, -0.2501,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3936,  0.9656,  0.9656,  0.9656, -0.3740, -0.3747, -1.1759,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4005, -3.9588, -0.7975,  ..., -4.5170, -4.5087, -4.5130],\n",
      "        [-2.6797, -3.9395, -0.8537,  ..., -5.2764, -5.2872, -5.3045],\n",
      "        [-2.6503, -3.9433, -0.8385,  ..., -5.2210, -5.2786, -5.3079],\n",
      "        ...,\n",
      "        [-2.3987, -3.9600, -0.7997,  ..., -4.5779, -4.5634, -4.5675],\n",
      "        [-2.2953, -3.9685, -0.7591,  ..., -4.4408, -4.4333, -4.4326],\n",
      "        [-2.6436, -3.9359, -0.8480,  ..., -5.1428, -5.1964, -5.2236]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3849,  0.8550,  0.8601,  0.8843, -0.4232, -0.3868, -1.0428,  0.8325],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.9759,  0.9900,  0.9900,  0.9900, -0.9282, -0.2039, -0.4262, -0.4193],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5571e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1136, -0.1147, -0.1159,  ..., -0.9565, -0.9662, -0.9759],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0351, -0.0355, -0.0358,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0862, -0.0871, -0.0880,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0848, -0.0857, -0.0865,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9332,  0.9656,  0.9656,  0.9656, -0.8875, -0.1988, -0.4157, -0.4090],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4787, -3.8945, -0.8136,  ..., -0.8882, -0.8963, -0.7653],\n",
      "        [-2.8057, -3.9032, -0.8735,  ..., -5.5679, -5.5626, -5.5516],\n",
      "        [-2.6680, -3.9304, -0.8560,  ..., -5.2442, -5.2547, -5.2750],\n",
      "        ...,\n",
      "        [-2.4777, -3.9317, -0.8026,  ..., -4.6735, -4.6813, -4.6824],\n",
      "        [-2.3805, -3.9509, -0.7910,  ..., -4.5758, -4.5624, -4.5616],\n",
      "        [-2.3854, -3.9541, -0.7905,  ..., -4.5521, -4.5406, -4.5385]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8658,  0.5120,  0.8339,  0.8511, -0.9249, -0.2159, -0.4051, -0.4266],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3807,  0.9900,  0.9900, -0.4200, -0.4303, -0.2094, -0.3788, -0.4236],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(9.8141e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0598e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.1692e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0770, -0.0778, -0.0786,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0361, -0.0364, -0.0368,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0766, -0.0774, -0.0782,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0857, -0.0866, -0.0874,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3714,  0.9608,  0.9656, -0.4097, -0.4197, -0.2042, -0.3694, -0.4132],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3865, -3.9444, -0.7941,  ..., -4.6110, -4.5985, -4.5978],\n",
      "        [-2.7912, -3.9004, -0.8690,  ..., -5.4982, -5.4947, -5.4846],\n",
      "        [-2.8055, -3.8956, -0.8678,  ..., -5.4958, -5.4886, -5.4735],\n",
      "        ...,\n",
      "        [-2.4751, -3.9272, -0.7966,  ..., -4.6803, -4.6885, -4.6891],\n",
      "        [-2.3859, -3.9491, -0.7929,  ..., -4.5767, -4.5562, -4.5511],\n",
      "        [-2.3837, -3.9491, -0.7876,  ..., -4.5545, -4.5432, -4.5405]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3624,  0.6535,  0.5941, -0.3830, -0.3914, -0.2042, -0.4131, -0.4127],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4418, -1.2133], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(7.0645e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0894, -0.0903, -0.0912, -0.0921, -0.0930, -0.0940, -0.0949, -0.0959,\n",
      "         -0.0969, -0.0978, -0.0988, -0.0998, -0.1008, -0.1019, -0.1029, -0.1039,\n",
      "         -0.1050, -0.1060, -0.1071, -0.1082, -0.1093, -0.1104, -0.1115, -0.1126,\n",
      "         -0.1138, -0.1149, -0.1161, -0.1172, -0.1184, -0.1196, -0.1208, -0.1221,\n",
      "         -0.1233, -0.1245, -0.1258, -0.1271, -0.1283, -0.1296, -0.1310, -0.1323,\n",
      "         -0.1336, -0.1350, -0.1363, -0.1377, -0.1391, -0.1405, -0.1419, -0.1433,\n",
      "         -0.1448, -0.1463, -0.1477, -0.1492, -0.1507, -0.1523, -0.1538, -0.1554,\n",
      "         -0.1569, -0.1585, -0.1601, -0.1617, -0.1634, -0.1650, -0.1667, -0.1684,\n",
      "         -0.1701, -0.1718, -0.1735, -0.1753, -0.1770, -0.1788, -0.1806, -0.1825,\n",
      "         -0.1843, -0.1862, -0.1880, -0.1899, -0.1919, -0.1938, -0.1958, -0.1977,\n",
      "         -0.1997, -0.2017, -0.2038, -0.2058, -0.2079, -0.2100, -0.2121, -0.2143,\n",
      "         -0.2164, -0.2186, -0.2208, -0.2231, -0.2253, -0.2276, -0.2299, -0.2322,\n",
      "         -0.2346, -0.2369, -0.2393, -0.2417, -0.2442, -0.2467, -0.2491, -0.2517,\n",
      "         -0.2542, -0.2568, -0.2594, -0.2620, -0.2646, -0.2673, -0.2700, -0.2727,\n",
      "         -0.2755, -0.2783, -0.2811, -0.2839, -0.2868, -0.2897, -0.2926, -0.2956,\n",
      "         -0.2986, -0.3016, -0.3046, -0.3077, -0.3108, -0.3139, -0.3171, -0.3203,\n",
      "         -0.3236, -0.3268, -0.3301, -0.3335, -0.3368, -0.3402, -0.3437, -0.3471,\n",
      "         -0.3506, -0.3542, -0.3578, -0.3614, -0.3650, -0.3687, -0.3724, -0.3762,\n",
      "         -0.3800, -0.3838, -0.3877, -0.3916, -0.3956, -0.3996, -0.4036, -0.4077,\n",
      "         -0.4118, -0.4160, -0.4202, -0.4244, -0.4287, -0.4330, -0.4374, -0.4418,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2479, -0.2504, -0.2530, -0.2555, -0.2581, -0.2607, -0.2633, -0.2660,\n",
      "         -0.2687, -0.2714, -0.2742, -0.2769, -0.2797, -0.2825, -0.2854, -0.2883,\n",
      "         -0.2912, -0.2941, -0.2971, -0.3001, -0.3031, -0.3062, -0.3093, -0.3124,\n",
      "         -0.3156, -0.3188, -0.3220, -0.3252, -0.3285, -0.3318, -0.3352, -0.3386,\n",
      "         -0.3420, -0.3454, -0.3489, -0.3525, -0.3560, -0.3596, -0.3632, -0.3669,\n",
      "         -0.3706, -0.3744, -0.3781, -0.3820, -0.3858, -0.3897, -0.3937, -0.3976,\n",
      "         -0.4017, -0.4057, -0.4098, -0.4139, -0.4181, -0.4224, -0.4266, -0.4309,\n",
      "         -0.4353, -0.4397, -0.4441, -0.4486, -0.4531, -0.4577, -0.4623, -0.4670,\n",
      "         -0.4717, -0.4765, -0.4813, -0.4862, -0.4911, -0.4960, -0.5010, -0.5061,\n",
      "         -0.5112, -0.5164, -0.5216, -0.5269, -0.5322, -0.5376, -0.5430, -0.5485,\n",
      "         -0.5540, -0.5596, -0.5653, -0.5710, -0.5767, -0.5826, -0.5885, -0.5944,\n",
      "         -0.6004, -0.6065, -0.6126, -0.6188, -0.6250, -0.6313, -0.6377, -0.6442,\n",
      "         -0.6507, -0.6572, -0.6639, -0.6706, -0.6774, -0.6842, -0.6911, -0.6981,\n",
      "         -0.7051, -0.7123, -0.7195, -0.7267, -0.7341, -0.7415, -0.7490, -0.7565,\n",
      "         -0.7642, -0.7719, -0.7797, -0.7876, -0.7955, -0.8036, -0.8117, -0.8199,\n",
      "         -0.8282, -0.8365, -0.8450, -0.8535, -0.8621, -0.8708, -0.8796, -0.8885,\n",
      "         -0.8975, -0.9066, -0.9157, -0.9250, -0.9343, -0.9438, -0.9533, -0.9629,\n",
      "         -0.9726, -0.9825, -0.9924, -1.0024, -1.0125, -1.0228, -1.0331, -1.0435,\n",
      "         -1.0541, -1.0647, -1.0755, -1.0863, -1.0973, -1.1084, -1.1196, -1.1309,\n",
      "         -1.1423, -1.1539, -1.1655, -1.1773, -1.1892, -1.2012, -1.2133,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4309, -1.1893], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3971e+00, -3.9386e+00, -7.8538e-01, -2.2723e+00,  1.6848e-01,\n",
      "         -2.3320e+00, -4.5745e+00, -2.7332e-01, -3.6851e+00, -2.4374e+00,\n",
      "         -3.8605e+00,  7.0793e-01, -8.4274e-02, -3.5370e+00, -3.1856e+00,\n",
      "         -2.2853e+00, -3.1907e+00, -1.7455e+00, -2.0040e+00, -9.2862e-01,\n",
      "         -1.4126e+00, -1.3899e+00, -8.3796e-01,  3.5005e-01, -1.8669e+00,\n",
      "         -3.3567e+00, -3.1425e+00, -2.4826e+00, -3.8762e+00,  9.3870e-01,\n",
      "         -1.7104e+00, -1.4425e-01, -8.6932e-01, -3.4582e+00, -1.8707e+00,\n",
      "         -1.6896e+00, -9.3331e-01, -3.1223e+00, -1.5934e+00, -5.5726e-03,\n",
      "         -6.4958e-01, -5.5745e-01, -3.6592e-01, -4.2628e+00, -1.5664e+00,\n",
      "         -3.1349e+00, -3.3712e+00, -3.1624e+00, -1.4291e-02,  6.4462e-02,\n",
      "         -3.7726e+00, -1.4813e+00, -7.0704e+00, -2.8390e+00, -3.0057e+00,\n",
      "         -9.1159e-01, -2.5826e+00, -2.9547e+00, -2.4981e+00, -1.5747e+00,\n",
      "         -3.5482e+00, -1.1978e+00, -5.7960e+00, -2.3686e+00, -3.6308e+00,\n",
      "          1.0442e+00, -1.3343e+00, -7.9522e-03, -3.4060e-01, -3.0764e+00,\n",
      "         -4.0192e+00, -5.5069e+00, -2.7305e+00, -1.0248e-01, -3.7253e+00,\n",
      "         -4.2520e+00, -2.4181e+00, -3.8255e+00, -2.4775e+00, -3.4718e+00,\n",
      "         -4.8409e+00, -2.4440e+00, -3.4507e-01, -1.2234e+00, -1.7211e+00,\n",
      "         -2.9609e+00, -1.7852e+00, -6.1317e-01, -3.5311e+00, -8.0249e-01,\n",
      "         -3.0760e+00, -4.1103e+00, -2.3952e+00, -6.1872e-01, -8.3589e-01,\n",
      "         -2.9938e+00, -2.6881e+00, -3.5116e+00, -3.2434e+00, -3.1739e+00,\n",
      "         -1.1080e-01, -9.4481e-01, -3.7212e+00, -2.1036e+00, -4.8652e-01,\n",
      "         -1.8489e+00, -3.3529e+00, -3.7429e+00,  5.9505e-01, -1.0389e+00,\n",
      "         -1.0696e+00, -8.8725e-01, -4.2019e+00, -4.5651e+00,  3.5920e-01,\n",
      "         -9.6147e-01, -2.1340e+00, -1.3189e+00, -9.1077e-02, -8.9753e-01,\n",
      "         -2.6989e+00, -4.5463e+00, -2.6933e+00, -9.0456e-01, -1.0702e+00,\n",
      "         -2.6829e+00, -2.2442e+00, -8.2430e-01, -6.1253e-01, -4.1228e+00,\n",
      "         -4.1979e-01,  1.2900e-01, -3.2976e+00, -4.0864e+00, -1.1995e+00,\n",
      "         -2.0473e+00, -3.0716e+00, -2.3045e+00, -1.6669e+00, -2.7673e+00,\n",
      "         -2.5049e+00, -3.2812e+00, -5.7395e+00, -1.3043e+00, -3.7302e-01,\n",
      "         -4.1230e-01, -2.8936e-01, -3.5146e-01, -3.0865e-01, -2.9588e-01,\n",
      "         -4.7970e+00, -4.8695e+00, -5.2326e+00, -4.8804e+00, -4.8370e+00,\n",
      "         -4.6298e+00, -4.5561e+00, -4.6500e+00, -4.6112e+00, -4.5663e+00,\n",
      "         -4.6209e+00, -4.6746e+00, -4.7039e+00, -4.6784e+00, -4.5853e+00,\n",
      "         -4.4897e+00, -4.4403e+00, -4.4210e+00, -4.4292e+00, -4.4351e+00,\n",
      "         -4.4454e+00, -4.4671e+00, -4.4978e+00, -4.5211e+00, -4.5488e+00,\n",
      "         -4.5553e+00, -4.5254e+00, -4.4994e+00, -4.5068e+00, -4.5184e+00,\n",
      "         -4.5306e+00, -4.5468e+00, -4.5418e+00, -4.5244e+00, -4.5200e+00,\n",
      "         -4.5172e+00, -4.5351e+00, -4.5507e+00, -4.5582e+00, -4.5556e+00,\n",
      "         -4.5545e+00, -4.5605e+00, -4.5657e+00, -4.5621e+00, -4.5522e+00,\n",
      "         -4.5451e+00, -4.5487e+00, -4.5463e+00, -4.5386e+00, -4.5328e+00,\n",
      "         -4.5316e+00, -4.5482e+00, -4.5499e+00, -4.5405e+00, -4.5358e+00],\n",
      "        [-2.2989e+00, -3.9492e+00, -7.4804e-01, -2.2319e+00,  2.0770e-01,\n",
      "         -2.2971e+00, -4.4923e+00, -2.3437e-01, -3.5769e+00, -2.3746e+00,\n",
      "         -3.8201e+00,  7.2102e-01, -8.2040e-02, -3.4966e+00, -3.1913e+00,\n",
      "         -2.2280e+00, -3.1638e+00, -1.7070e+00, -2.0212e+00, -8.6647e-01,\n",
      "         -1.3812e+00, -1.3324e+00, -7.9597e-01,  3.3145e-01, -1.8418e+00,\n",
      "         -3.3102e+00, -3.1385e+00, -2.4374e+00, -3.8817e+00,  9.2471e-01,\n",
      "         -1.6994e+00, -7.7178e-02, -8.8450e-01, -3.4240e+00, -1.8642e+00,\n",
      "         -1.7250e+00, -9.7947e-01, -3.1014e+00, -1.5752e+00,  4.5882e-02,\n",
      "         -6.4402e-01, -5.8886e-01, -3.5774e-01, -4.2859e+00, -1.5447e+00,\n",
      "         -3.0325e+00, -3.3081e+00, -3.1323e+00, -1.6851e-01, -8.9707e-02,\n",
      "         -3.7239e+00, -1.4413e+00, -7.0672e+00, -2.7956e+00, -2.9865e+00,\n",
      "         -8.7093e-01, -2.5852e+00, -2.9753e+00, -2.4597e+00, -1.5578e+00,\n",
      "         -3.5024e+00, -1.1547e+00, -5.7941e+00, -2.3433e+00, -3.6343e+00,\n",
      "          1.0304e+00, -1.3293e+00,  6.3088e-02, -3.6034e-01, -3.0422e+00,\n",
      "         -3.9514e+00, -5.4841e+00, -2.6865e+00, -1.2739e-01, -3.6772e+00,\n",
      "         -4.2706e+00, -2.3764e+00, -3.8094e+00, -2.4590e+00, -3.4253e+00,\n",
      "         -4.8171e+00, -2.4070e+00, -3.3570e-01, -1.1526e+00, -1.6828e+00,\n",
      "         -2.8980e+00, -1.7706e+00, -5.4848e-01, -3.4594e+00, -7.9232e-01,\n",
      "         -3.0242e+00, -4.1236e+00, -2.3594e+00, -6.1254e-01, -7.6241e-01,\n",
      "         -2.9992e+00, -2.6588e+00, -3.4139e+00, -3.2067e+00, -3.1352e+00,\n",
      "         -8.9253e-02, -8.6790e-01, -3.7134e+00, -2.0853e+00, -4.9921e-01,\n",
      "         -1.8022e+00, -3.2650e+00, -3.6849e+00,  5.6997e-01, -9.9463e-01,\n",
      "         -1.4124e+00, -2.2308e+00, -4.1570e+00, -4.5748e+00,  3.5146e-01,\n",
      "         -8.3764e-01, -2.1205e+00, -1.2190e+00,  7.3587e-02, -8.9566e-01,\n",
      "         -2.4406e+00, -4.7439e+00, -2.6198e+00, -8.4180e-01, -9.4964e-01,\n",
      "         -2.6385e+00, -2.2179e+00, -8.0065e-01, -4.2371e-01, -4.1054e+00,\n",
      "         -6.0666e-01, -2.5069e-02, -3.2801e+00, -4.0641e+00, -1.1492e+00,\n",
      "         -2.0505e+00, -3.0995e+00, -2.2651e+00, -1.6407e+00, -2.7305e+00,\n",
      "         -2.4988e+00, -3.2073e+00, -5.6780e+00, -1.2039e+00, -9.2942e-01,\n",
      "         -5.8814e-01, -1.1575e+00, -1.1332e+00, -1.1549e+00, -4.8351e+00,\n",
      "         -4.8562e+00, -5.3184e+00, -4.8047e+00, -4.6747e+00, -4.7343e+00,\n",
      "         -4.8971e+00, -5.0087e+00, -4.9231e+00, -4.7849e+00, -4.6914e+00,\n",
      "         -4.6089e+00, -4.5270e+00, -4.4622e+00, -4.4165e+00, -4.3857e+00,\n",
      "         -4.3544e+00, -4.3228e+00, -4.2977e+00, -4.2789e+00, -4.2659e+00,\n",
      "         -4.2526e+00, -4.2512e+00, -4.2682e+00, -4.2939e+00, -4.3295e+00,\n",
      "         -4.3714e+00, -4.4104e+00, -4.4277e+00, -4.4396e+00, -4.4410e+00,\n",
      "         -4.4483e+00, -4.4559e+00, -4.4644e+00, -4.4854e+00, -4.5042e+00,\n",
      "         -4.5142e+00, -4.5298e+00, -4.5367e+00, -4.5294e+00, -4.5097e+00,\n",
      "         -4.4877e+00, -4.4819e+00, -4.4873e+00, -4.4847e+00, -4.4796e+00,\n",
      "         -4.4752e+00, -4.4694e+00, -4.4578e+00, -4.4606e+00, -4.4536e+00,\n",
      "         -4.4490e+00, -4.4489e+00, -4.4414e+00, -4.4326e+00, -4.4318e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3384, -0.9926], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4414, -0.4020, -0.3915, -1.2222, -1.2222, -0.4362],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6515e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0893, -0.0902, -0.0911,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2498, -0.2523, -0.2548,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2498, -0.2523, -0.2548,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0882, -0.0891, -0.0900,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4305, -0.3920, -0.3818, -1.1980, -1.1980, -0.4254],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7156, -3.9140, -0.8502,  ..., -5.3543, -5.3682, -5.3926],\n",
      "        [-2.7062, -3.9140, -0.8417,  ..., -5.3034, -5.3117, -5.3326],\n",
      "        [-2.4138, -3.9296, -0.7829,  ..., -4.5784, -4.5677, -4.5649],\n",
      "        ...,\n",
      "        [-2.3141, -3.9424, -0.7433,  ..., -4.4422, -4.4344, -4.4313],\n",
      "        [-2.3141, -3.9424, -0.7433,  ..., -4.4422, -4.4344, -4.4313],\n",
      "        [-2.4205, -3.9320, -0.7830,  ..., -4.5584, -4.5433, -4.5451]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9562,  0.9216, -0.3699, -0.3737, -0.3884, -0.9882, -0.9882, -0.3910],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4274, -0.4469, -0.9929,  0.9900, -0.2298, -0.4374, -0.4075, -0.2159],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(5.7316e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0865, -0.0873, -0.0882,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0904, -0.0913, -0.0922,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1156, -0.1167, -0.1179,  ..., -0.9731, -0.9830, -0.9929],\n",
      "        ...,\n",
      "        [-0.0885, -0.0894, -0.0903,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0824, -0.0833, -0.0841,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0372, -0.0376, -0.0379,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4169, -0.4359, -0.9494,  0.9656, -0.2241, -0.4266, -0.3975, -0.2106],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4402, -3.9272, -0.7815,  ..., -4.5195, -4.5115, -4.5155],\n",
      "        [-2.4287, -3.9251, -0.7797,  ..., -4.5775, -4.5673, -4.5665],\n",
      "        [-2.5256, -3.8670, -0.8010,  ..., -0.8898, -0.9227, -0.7246],\n",
      "        ...,\n",
      "        [-2.4377, -3.9313, -0.7835,  ..., -4.5595, -4.5515, -4.5532],\n",
      "        [-2.4386, -3.9286, -0.7845,  ..., -4.5742, -4.5589, -4.5651],\n",
      "        [-2.5286, -3.9093, -0.7816,  ..., -4.6341, -4.6430, -4.6467]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3382, -0.3693, -0.8766,  0.6430, -0.1874, -0.3586, -0.3395, -0.1819],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.2403,  0.9900, -1.2402, -0.4486,  0.9900, -0.4486],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6236e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.2611e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0414, -0.0418, -0.0422,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0908, -0.0917, -0.0926,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0908, -0.0917, -0.0926,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.2344,  0.9656, -1.2156, -0.4375,  0.9656, -0.4375],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7111, -3.9107, -0.8203,  ..., -5.2369, -5.2981, -5.3246],\n",
      "        [-2.8784, -3.8730, -0.8590,  ..., -5.5829, -5.5811, -5.5697],\n",
      "        [-2.5431, -3.9045, -0.7860,  ..., -4.6941, -4.7017, -4.7040],\n",
      "        ...,\n",
      "        [-2.4481, -3.9290, -0.7840,  ..., -4.5349, -4.5218, -4.5226],\n",
      "        [-2.7474, -3.9029, -0.8305,  ..., -5.3135, -5.3300, -5.3514],\n",
      "        [-2.4481, -3.9290, -0.7840,  ..., -4.5349, -4.5218, -4.5226]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9415,  0.5913, -0.1899,  0.9526, -0.9992, -0.3619,  0.9526, -0.3619],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4396,  0.9900, -0.4538, -0.4097, -0.4665, -0.4497,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.7582e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0889, -0.0898, -0.0907,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0944, -0.0953, -0.0963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0910, -0.0919, -0.0928,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4288,  0.9608, -0.4426, -0.3996, -0.4550, -0.4387,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7570, -3.9017, -0.8428,  ..., -5.3428, -5.3557, -5.3745],\n",
      "        [-2.4747, -3.9256, -0.7828,  ..., -4.5254, -4.5164, -4.5229],\n",
      "        [-2.8869, -3.8777, -0.8628,  ..., -5.5186, -5.5169, -5.5026],\n",
      "        ...,\n",
      "        [-2.4597, -3.9254, -0.7813,  ..., -4.5579, -4.5473, -4.5452],\n",
      "        [-2.4729, -3.9285, -0.7854,  ..., -4.5627, -4.5576, -4.5611],\n",
      "        [-2.7600, -3.9025, -0.8419,  ..., -5.3461, -5.3573, -5.3837]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8270, -0.3523,  0.6636, -0.4017, -0.3939, -0.3507, -0.3710,  0.9215],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.9641,  0.9900, -0.2380],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.8233e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1874, -0.1892, -0.1912,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0410, -0.0414, -0.0418,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9608,  0.9656,  0.9656, -0.9219,  0.9656, -0.2321],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7312, -3.8981, -0.8181,  ..., -5.3742, -5.4229, -5.4307],\n",
      "        [-2.7440, -3.9046, -0.8365,  ..., -5.1637, -5.2186, -5.2458],\n",
      "        [-2.9113, -3.8780, -0.8653,  ..., -5.5149, -5.5099, -5.4986],\n",
      "        ...,\n",
      "        [-2.4505, -3.9201, -0.7658,  ..., -4.5274, -4.5339, -4.5472],\n",
      "        [-2.7858, -3.9038, -0.8496,  ..., -5.3039, -5.3169, -5.3382],\n",
      "        [-2.5844, -3.9046, -0.7935,  ..., -4.6863, -4.6974, -4.6982]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9230,  0.9124,  0.6696,  0.9438,  0.9287, -0.9628,  0.9124, -0.2058],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3971, -0.3971,  0.9900, -0.4055,  0.9900,  0.9900,  0.9900, -0.4523],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4226e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0803, -0.0811, -0.0820,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0803, -0.0811, -0.0820,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0915, -0.0924, -0.0933,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3873, -0.3873,  0.9656, -0.3955,  0.9656,  0.9656,  0.9656, -0.4411],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5119, -3.9215, -0.7936,  ..., -4.6206, -4.6091, -4.6102],\n",
      "        [-2.5119, -3.9215, -0.7936,  ..., -4.6206, -4.6091, -4.6102],\n",
      "        [-2.8151, -3.9033, -0.8473,  ..., -5.3431, -5.3554, -5.3816],\n",
      "        ...,\n",
      "        [-2.8050, -3.9029, -0.8476,  ..., -5.3395, -5.3535, -5.3759],\n",
      "        [-2.9386, -3.8793, -0.8699,  ..., -5.6084, -5.6060, -5.5983],\n",
      "        [-2.4991, -3.9256, -0.7863,  ..., -4.5582, -4.5476, -4.5454]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3643, -0.3643,  0.9983, -0.3956,  0.9636,  0.9416,  0.5869, -0.3730],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.4265e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7952, -3.9097, -0.8299, -2.4035,  0.0617, -2.3866, -4.6200, -0.4744,\n",
      "         -4.1008, -2.5654, -3.9414,  0.7197, -0.1651, -3.5889, -3.0457, -2.4261,\n",
      "         -3.3053, -1.8591, -1.9591, -1.0900, -1.4497, -1.5457, -0.9632,  0.4049,\n",
      "         -2.0954, -3.4420, -3.0201, -2.6199, -3.8753,  0.9377, -1.8535, -0.3066,\n",
      "         -0.8572, -3.5406, -1.9325, -1.6437, -0.9077, -3.2218, -1.7288, -0.1811,\n",
      "         -0.7192, -0.5541, -0.4201, -4.2166, -1.7679, -3.5122, -3.5436, -3.2437,\n",
      "          0.4640,  0.3064, -3.8203, -1.5552, -7.0310, -2.9383, -3.0774, -1.0467,\n",
      "         -2.5347, -2.9813, -2.5676, -1.7775, -3.6297, -1.2593, -5.7714, -2.4467,\n",
      "         -3.6332,  1.0502, -1.4764, -0.1884, -0.3142, -3.1706, -4.1172, -5.4624,\n",
      "         -2.8256, -0.0723, -3.7920, -4.1656, -2.5118, -3.9065, -2.5942, -3.5347,\n",
      "         -4.7993, -2.5325, -0.4269, -1.3873, -1.7658, -3.0988, -1.9466, -0.7758,\n",
      "         -3.5846, -0.9050, -3.1559, -4.0363, -2.4820, -0.6937, -1.0077, -2.9351,\n",
      "         -2.8931, -3.8444, -3.2822, -3.2595, -0.2033, -1.1015, -3.6752, -2.1362,\n",
      "         -0.4444, -2.0148, -3.6695, -3.8418,  0.6315, -0.9931, -1.3158, -0.6052,\n",
      "         -4.6688, -4.5448,  0.2498, -1.0891, -2.1908, -1.4334, -0.1475, -1.0781,\n",
      "         -2.9486, -4.5639, -2.7107, -0.9087, -1.1868, -2.8186, -2.4077, -0.9122,\n",
      "         -0.7035, -4.2414,  0.0122,  0.3765, -3.3977, -4.1150, -1.3019, -2.0047,\n",
      "         -3.0932, -2.3842, -1.8390, -2.9000, -2.9997, -5.8170, -1.6580, -2.1839,\n",
      "         -0.8836, -0.7913, -0.2974, -4.1253, -3.9562, -0.1604, -2.1185, -0.1373,\n",
      "         -1.0379, -0.4583,  0.2921, -0.6259, -4.2331, -5.0722, -1.6543, -2.1258,\n",
      "         -0.0768,  0.1328,  0.3278, -4.0094, -3.8813, -0.2248, -1.3736, -4.3160,\n",
      "         -6.0725, -4.6426, -3.0625, -4.9398, -1.4516,  1.0504,  0.8536,  0.9314,\n",
      "          0.9806,  1.0261,  1.0203, -4.7264, -4.9027, -5.2323, -5.0666, -5.2466,\n",
      "         -5.1624, -5.0045, -5.0200, -5.0565, -5.0494, -4.9947, -4.9550, -4.9506,\n",
      "         -4.9667, -5.0037, -5.0401, -5.0680, -5.0770, -5.0804, -5.0985, -5.1218,\n",
      "         -5.1554, -5.1952, -5.2416, -5.3007, -5.3270],\n",
      "        [-2.8314, -3.9027, -0.8477, -2.4642,  0.0406, -2.3837, -4.6373, -0.5474,\n",
      "         -4.1517, -2.6104, -3.9609,  0.7254, -0.1638, -3.6432, -3.0156, -2.4645,\n",
      "         -3.3042, -1.8854, -1.9361, -1.1328, -1.4715, -1.5906, -0.9975,  0.4333,\n",
      "         -2.1231, -3.4987, -3.0015, -2.6467, -3.8726,  0.9398, -1.8880, -0.3625,\n",
      "         -0.8532, -3.5547, -1.9243, -1.6250, -0.9029, -3.2093, -1.7736, -0.2110,\n",
      "         -0.7432, -0.5494, -0.4299, -4.2088, -1.7901, -3.5542, -3.5905, -3.2559,\n",
      "          0.5728,  0.3867, -3.8589, -1.5618, -7.0263, -2.9600, -3.0637, -1.0770,\n",
      "         -2.5395, -2.9917, -2.5874, -1.7980, -3.6744, -1.2711, -5.7756, -2.4647,\n",
      "         -3.6317,  1.0554, -1.5075, -0.2398, -0.3039, -3.1844, -4.1519, -5.4654,\n",
      "         -2.8484, -0.0515, -3.8373, -4.1463, -2.5368, -3.8894, -2.6305, -3.5780,\n",
      "         -4.8020, -2.5521, -0.4400, -1.4368, -1.7795, -3.1254, -1.9775, -0.8250,\n",
      "         -3.6004, -0.9219, -3.2005, -4.0196, -2.5030, -0.7035, -1.0562, -2.9303,\n",
      "         -2.9105, -3.8859, -3.2975, -3.2788, -0.2227, -1.1499, -3.6683, -2.1309,\n",
      "         -0.4191, -2.0591, -3.7109, -3.8694,  0.6534, -1.2354, -1.2350, -0.3356,\n",
      "         -4.6109, -4.5617,  0.2801, -1.0879, -2.1928, -1.4326, -0.1535, -0.9999,\n",
      "         -2.9273, -4.6730, -2.6925, -0.8678, -1.1335, -2.7653, -2.4160, -0.9124,\n",
      "         -0.7680, -4.2333,  0.1740,  0.4752, -3.3776, -4.1084, -1.3289, -2.0175,\n",
      "         -3.1039, -2.3787, -1.8294, -2.8490, -3.0494, -5.8557, -1.6948, -2.1896,\n",
      "         -1.0442, -0.6725, -0.1290, -4.1013, -4.0459, -0.0982, -1.9928, -0.4226,\n",
      "         -0.8464, -0.0934, -0.0277, -0.4926, -4.4975, -3.4054, -4.9635, -1.3574,\n",
      "          1.0991,  0.8994,  0.8774,  1.0517,  0.8638,  1.0747, -4.6353, -4.8489,\n",
      "         -5.1797, -5.0059, -5.1979, -5.2781, -5.3023, -5.2830, -5.2880, -5.2699,\n",
      "         -5.1973, -5.0857, -4.9877, -4.9295, -4.9151, -4.9222, -4.9481, -4.9885,\n",
      "         -5.0189, -5.0474, -5.0804, -5.1317, -5.2076, -5.3029, -5.3709, -5.4065,\n",
      "         -5.4182, -5.4435, -5.4842, -5.5188, -5.5190, -5.4903, -5.4553, -5.4138,\n",
      "         -5.3831, -5.3581, -5.3477, -5.3572, -5.3799]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9771, 0.9777], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1970, -0.3958,  0.9900, -0.9257,  0.9900, -0.1920, -0.3662],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0190e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2446, -0.2471, -0.2496,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0801, -0.0809, -0.0817,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0331, -0.0334, -0.0337,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0741, -0.0748, -0.0756,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1733, -0.3861,  0.9656, -0.8851,  0.9656, -0.1873, -0.3572],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9757, -3.8768, -0.8703,  ..., -5.5968, -5.5934, -5.5809],\n",
      "        [-2.4447, -3.9362, -0.7503,  ..., -4.4364, -4.4273, -4.4292],\n",
      "        [-2.5502, -3.9217, -0.7897,  ..., -4.5294, -4.5244, -4.5271],\n",
      "        ...,\n",
      "        [-2.8447, -3.9016, -0.8480,  ..., -5.3238, -5.3353, -5.3597],\n",
      "        [-2.6444, -3.9022, -0.7985,  ..., -4.6593, -4.6701, -4.6696],\n",
      "        [-2.5483, -3.9235, -0.7941,  ..., -4.5844, -4.5702, -4.5658]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6154, -0.9596, -0.3901,  0.9663, -0.9759,  0.9663, -0.2314, -0.4249],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.1834,  0.9900,  0.9900, -0.3672, -0.3983, -0.3983, -0.1834],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0109e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.7903e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0316, -0.0319, -0.0322,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0806, -0.0814, -0.0822,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0806, -0.0814, -0.0822,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0316, -0.0319, -0.0322,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.1789,  0.9656,  0.9656, -0.3582, -0.3885, -0.3885, -0.1789],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8570, -3.8986, -0.8457,  ..., -5.3343, -5.3463, -5.3670],\n",
      "        [-2.6518, -3.8992, -0.7971,  ..., -4.6501, -4.6598, -4.6613],\n",
      "        [-2.8575, -3.9022, -0.8421,  ..., -5.3274, -5.3334, -5.3562],\n",
      "        ...,\n",
      "        [-2.5564, -3.9238, -0.7907,  ..., -4.5616, -4.5583, -4.5573],\n",
      "        [-2.5564, -3.9238, -0.7907,  ..., -4.5616, -4.5583, -4.5573],\n",
      "        [-2.6518, -3.8992, -0.7971,  ..., -4.6501, -4.6598, -4.6613]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9950, -0.2358,  0.9846,  0.6700, -0.4093, -0.4083, -0.4083, -0.2358],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3597,  0.9900,  0.9900, -0.3676, -0.4031, -0.3990, -1.1894, -0.4073],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4773e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0728, -0.0735, -0.0743,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0807, -0.0815, -0.0824,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2430, -0.2455, -0.2480,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0824, -0.0832, -0.0841,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3509,  0.9656,  0.9608, -0.3585, -0.3931, -0.3892, -1.1658, -0.3972],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5606, -3.9108, -0.7925,  ..., -4.6149, -4.6029, -4.6043],\n",
      "        [-2.8701, -3.8981, -0.8513,  ..., -5.3725, -5.3850, -5.4130],\n",
      "        [-2.9819, -3.8665, -0.8677,  ..., -5.4878, -5.4800, -5.4637],\n",
      "        ...,\n",
      "        [-2.5543, -3.9188, -0.7921,  ..., -4.5406, -4.5292, -4.5319],\n",
      "        [-2.4520, -3.9278, -0.7476,  ..., -4.4275, -4.4196, -4.4208],\n",
      "        [-2.5538, -3.9131, -0.7858,  ..., -4.5836, -4.5726, -4.5728]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3882,  1.0308,  0.7421, -0.4119, -0.4494, -0.4132, -0.9390, -0.4303],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4092, -0.3612,  0.9900,  0.9900, -0.4181, -0.1748],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8004e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0828, -0.0836, -0.0845,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0846, -0.0854, -0.0863,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0301, -0.0304, -0.0307,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.3991, -0.3523,  0.9656,  0.9656, -0.4077, -0.1705],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7955, -3.8840, -0.8153,  ..., -5.3755, -5.4209, -5.4267],\n",
      "        [-2.8577, -3.8873, -0.8357,  ..., -5.3185, -5.3287, -5.3507],\n",
      "        [-2.5489, -3.9087, -0.7848,  ..., -4.5833, -4.5740, -4.5737],\n",
      "        ...,\n",
      "        [-2.8566, -3.8888, -0.8429,  ..., -5.3394, -5.3468, -5.3702],\n",
      "        [-2.5417, -3.9089, -0.7853,  ..., -4.5518, -4.5459, -4.5401],\n",
      "        [-2.6515, -3.8923, -0.7881,  ..., -4.5877, -4.5962, -4.5991]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9833,  1.0260, -0.4307, -0.4313,  0.9043,  1.0103, -0.4035, -0.2367],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4082,  0.9900,  0.9900, -0.4243, -0.3791,  0.9900, -0.3962,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7723e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0826, -0.0834, -0.0843,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0802, -0.0810, -0.0818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3982,  0.9656,  0.9608, -0.4138, -0.3697,  0.9656, -0.3865,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5414, -3.9080, -0.7880,  ..., -4.5385, -4.5259, -4.5278],\n",
      "        [-2.8090, -3.8901, -0.8224,  ..., -5.2285, -5.2829, -5.3131],\n",
      "        [-2.9679, -3.8542, -0.8622,  ..., -5.4878, -5.4764, -5.4616],\n",
      "        ...,\n",
      "        [-2.8521, -3.8832, -0.8409,  ..., -5.3391, -5.3502, -5.3748],\n",
      "        [-2.5515, -3.9048, -0.7820,  ..., -4.5220, -4.5165, -4.5219],\n",
      "        [-2.8507, -3.8830, -0.8324,  ..., -5.3222, -5.3304, -5.3533]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4108,  1.0164,  0.7710, -0.4026, -0.3948,  1.0495, -0.4037,  1.0293],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4198,  0.9900,  0.9900,  0.9900,  0.9900, -0.9801,  0.9900, -0.3773],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.2948e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0849, -0.0858, -0.0867,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1141, -0.1152, -0.1164,  ..., -0.9606, -0.9703, -0.9801],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0763, -0.0771, -0.0779,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4095,  0.9656,  0.9656,  0.9656,  0.9656, -0.9371,  0.9656, -0.3680],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5353, -3.9000, -0.7787,  ..., -4.5595, -4.5481, -4.5509],\n",
      "        [-2.8315, -3.8792, -0.8378,  ..., -5.3430, -5.3488, -5.3755],\n",
      "        [-2.8353, -3.8786, -0.8409,  ..., -5.3020, -5.3127, -5.3336],\n",
      "        ...,\n",
      "        [-2.6300, -3.8423, -0.8001,  ..., -0.8944, -0.9306, -0.8091],\n",
      "        [-2.7993, -3.8863, -0.8203,  ..., -5.2285, -5.2840, -5.3155],\n",
      "        [-2.5387, -3.8963, -0.7857,  ..., -4.6167, -4.6060, -4.6078]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4505,  1.0000,  0.9864,  0.9793,  0.7633, -0.8940,  1.0185, -0.3889],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2206], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(3.4779e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2494, -0.2519, -0.2545, -0.2571, -0.2597, -0.2623, -0.2649, -0.2676,\n",
      "         -0.2703, -0.2730, -0.2758, -0.2786, -0.2814, -0.2842, -0.2871, -0.2900,\n",
      "         -0.2929, -0.2959, -0.2989, -0.3019, -0.3050, -0.3080, -0.3111, -0.3143,\n",
      "         -0.3175, -0.3207, -0.3239, -0.3272, -0.3305, -0.3338, -0.3372, -0.3406,\n",
      "         -0.3440, -0.3475, -0.3510, -0.3546, -0.3582, -0.3618, -0.3654, -0.3691,\n",
      "         -0.3728, -0.3766, -0.3804, -0.3843, -0.3881, -0.3921, -0.3960, -0.4000,\n",
      "         -0.4041, -0.4081, -0.4123, -0.4164, -0.4206, -0.4249, -0.4292, -0.4335,\n",
      "         -0.4379, -0.4423, -0.4468, -0.4513, -0.4559, -0.4605, -0.4651, -0.4698,\n",
      "         -0.4746, -0.4793, -0.4842, -0.4891, -0.4940, -0.4990, -0.5040, -0.5091,\n",
      "         -0.5143, -0.5195, -0.5247, -0.5300, -0.5354, -0.5408, -0.5462, -0.5518,\n",
      "         -0.5573, -0.5630, -0.5687, -0.5744, -0.5802, -0.5861, -0.5920, -0.5980,\n",
      "         -0.6040, -0.6101, -0.6163, -0.6225, -0.6288, -0.6351, -0.6415, -0.6480,\n",
      "         -0.6546, -0.6612, -0.6679, -0.6746, -0.6814, -0.6883, -0.6953, -0.7023,\n",
      "         -0.7094, -0.7165, -0.7238, -0.7311, -0.7385, -0.7459, -0.7535, -0.7611,\n",
      "         -0.7688, -0.7765, -0.7844, -0.7923, -0.8003, -0.8084, -0.8165, -0.8248,\n",
      "         -0.8331, -0.8415, -0.8500, -0.8586, -0.8673, -0.8761, -0.8849, -0.8939,\n",
      "         -0.9029, -0.9120, -0.9212, -0.9305, -0.9399, -0.9494, -0.9590, -0.9687,\n",
      "         -0.9785, -0.9884, -0.9983, -1.0084, -1.0186, -1.0289, -1.0393, -1.0498,\n",
      "         -1.0604, -1.0711, -1.0819, -1.0929, -1.1039, -1.1150, -1.1263, -1.1377,\n",
      "         -1.1492, -1.1608, -1.1725, -1.1843, -1.1963, -1.2084, -1.2206,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1964], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8370, -3.8781, -0.8390, -2.4479,  0.0760, -2.3797, -4.6325, -0.5376,\n",
      "         -4.1675, -2.5837, -3.9561,  0.6520, -0.1702, -3.6430, -3.0186, -2.4761,\n",
      "         -3.3035, -1.8566, -1.9391, -1.1153, -1.4554, -1.5702, -0.9862,  0.4487,\n",
      "         -2.1020, -3.5038, -3.0032, -2.6532, -3.8502,  0.9642, -1.8708, -0.3450,\n",
      "         -0.8494, -3.5435, -1.9199, -1.6116, -0.8951, -3.2025, -1.7653, -0.1966,\n",
      "         -0.7206, -0.5395, -0.4021, -4.1935, -1.7678, -3.5614, -3.5672, -3.2471,\n",
      "          0.5400,  0.6224, -3.8619, -1.5445, -7.0146, -2.9620, -3.0567, -1.0481,\n",
      "         -2.5417, -2.9484, -2.6012, -1.7742, -3.6803, -1.2476, -5.7579, -2.4630,\n",
      "         -3.6075,  1.0842, -1.4905, -0.2267, -0.2955, -3.1752, -4.1586, -5.4656,\n",
      "         -2.8509, -0.0815, -3.8363, -4.1522, -2.5380, -3.8845, -2.6239, -3.5815,\n",
      "         -4.7991, -2.5551, -0.4235, -1.4266, -1.7670, -3.1188, -1.9608, -0.8107,\n",
      "         -3.6071, -0.9037, -3.2056, -4.0276, -2.5053, -0.6880, -1.0467, -2.9297,\n",
      "         -2.8915, -3.8922, -3.2913, -3.2869, -0.2028, -1.1368, -3.6682, -2.0994,\n",
      "         -0.4044, -2.0430, -3.7140, -3.8470,  0.6285, -1.1918, -1.2833, -0.8914,\n",
      "         -4.2525, -4.5297,  0.3300, -1.1534, -2.1617, -1.4654, -0.1534, -1.0548,\n",
      "         -3.0466, -4.4762, -2.7390, -0.8948, -1.1891, -2.8175, -2.3715, -0.8970,\n",
      "         -0.7041, -4.2295,  0.1416,  0.7193, -3.3403, -4.1048, -1.3080, -2.0124,\n",
      "         -3.0536, -2.4003, -1.8152, -2.9465, -3.0557, -5.7834, -1.6779, -2.1736,\n",
      "         -1.1596, -0.7020, -0.3264, -3.9050, -3.9500, -0.1121, -2.0639, -0.0966,\n",
      "         -0.7999, -0.3480,  0.2113, -0.3282, -4.0683, -3.8612, -4.9148, -1.3977,\n",
      "          0.9746,  1.2207,  0.9861,  1.0260,  0.8878,  0.9525, -4.6810, -4.8557,\n",
      "         -5.1931, -4.9871, -5.1821, -5.2946, -5.2989, -5.2703, -5.2487, -5.2211,\n",
      "         -5.1432, -5.0315, -4.9522, -4.8931, -4.8716, -4.8811, -4.9101, -4.9625,\n",
      "         -4.9886, -5.0013, -5.0157, -5.0498, -5.1115, -5.1893, -5.2478, -5.2844,\n",
      "         -5.3006, -5.3372, -5.3855, -5.4259, -5.4214, -5.4006, -5.3710, -5.3325,\n",
      "         -5.3090, -5.2882, -5.2788, -5.2930, -5.3175],\n",
      "        [-2.4224, -3.9085, -0.7349, -2.2171,  0.1977, -2.2824, -4.4590, -0.2359,\n",
      "         -3.7308, -2.3579, -3.8347,  0.6795, -0.0795, -3.5104, -3.1076, -2.2484,\n",
      "         -3.1846, -1.7289, -2.0025, -0.8948, -1.3590, -1.3399, -0.8119,  0.3730,\n",
      "         -1.8762, -3.3508, -3.0588, -2.4658, -3.8616,  0.9382, -1.7245, -0.1039,\n",
      "         -0.8768, -3.4331, -1.8762, -1.7490, -0.9981, -3.1210, -1.5928,  0.0129,\n",
      "         -0.6627, -0.6060, -0.3460, -4.2642, -1.5761, -3.1722, -3.3113, -3.1470,\n",
      "         -0.0365,  0.1610, -3.7456, -1.4222, -6.9981, -2.8161, -2.9979, -0.8972,\n",
      "         -2.5508, -2.9373, -2.4777, -1.5928, -3.5461, -1.1312, -5.7215, -2.3634,\n",
      "         -3.6136,  1.0406, -1.3646,  0.0205, -0.3517, -3.0573, -3.9732, -5.4408,\n",
      "         -2.7118, -0.1709, -3.7107, -4.2056, -2.4030, -3.8304, -2.4818, -3.4656,\n",
      "         -4.7713, -2.4349, -0.3659, -1.1953, -1.6574, -2.9019, -1.8082, -0.5840,\n",
      "         -3.4642, -0.8213, -3.0656, -4.0640, -2.3900, -0.6502, -0.8067, -2.9914,\n",
      "         -2.7080, -3.5513, -3.2128, -3.1571, -0.1155, -0.9072, -3.6757, -2.0611,\n",
      "         -0.4605, -1.8185, -3.4017, -3.6787,  0.5311, -1.0656, -1.4313, -2.2951,\n",
      "         -4.2821, -4.5504,  0.3308, -0.8713, -2.1132, -1.2363,  0.0385, -0.9044,\n",
      "         -2.5931, -4.7231, -2.6076, -0.8175, -0.9774, -2.6549, -2.2137, -0.7923,\n",
      "         -0.4539, -4.1249, -0.4703,  0.2414, -3.3077, -4.0269, -1.1655, -2.0137,\n",
      "         -3.0649, -2.2773, -1.6732, -2.8742, -2.6433, -3.3222, -5.6408, -1.2147,\n",
      "         -0.6582, -0.2914, -1.2305, -1.2348, -1.2128, -4.8367, -4.8615, -5.3270,\n",
      "         -4.8165, -4.6906, -4.7587, -4.9155, -5.0204, -4.9319, -4.8002, -4.7049,\n",
      "         -4.6177, -4.5281, -4.4621, -4.4140, -4.3850, -4.3530, -4.3169, -4.2846,\n",
      "         -4.2618, -4.2481, -4.2373, -4.2403, -4.2605, -4.2900, -4.3230, -4.3635,\n",
      "         -4.4020, -4.4188, -4.4324, -4.4367, -4.4413, -4.4484, -4.4551, -4.4745,\n",
      "         -4.4924, -4.5020, -4.5174, -4.5223, -4.5126, -4.4937, -4.4702, -4.4626,\n",
      "         -4.4668, -4.4625, -4.4550, -4.4496, -4.4446, -4.4360, -4.4383, -4.4324,\n",
      "         -4.4280, -4.4308, -4.4220, -4.4144, -4.4168]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0079, -0.9256], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4528,  0.9900,  0.9900,  0.9900, -0.9571, -0.2134, -0.4419],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7913e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.3157e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0916, -0.0925, -0.0935,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1860, -0.1879, -0.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0368, -0.0371, -0.0375,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0894, -0.0903, -0.0912,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4416,  0.9656,  0.9656,  0.9656, -0.9151, -0.2081, -0.4310],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8086, -3.8730, -0.8175,  ..., -5.3410, -5.3519, -5.3706],\n",
      "        [-2.4938, -3.8946, -0.7671,  ..., -4.5549, -4.5479, -4.5459],\n",
      "        [-2.7713, -3.8812, -0.8094,  ..., -5.2231, -5.2811, -5.3144],\n",
      "        ...,\n",
      "        [-2.4586, -3.8881, -0.7446,  ..., -4.5105, -4.5169, -4.5306],\n",
      "        [-2.6003, -3.8785, -0.7666,  ..., -4.5889, -4.5991, -4.6030],\n",
      "        [-2.4991, -3.8935, -0.7649,  ..., -4.5923, -4.5841, -4.5802]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0019, -0.4001,  0.9935,  0.9711,  0.9740, -0.9388, -0.2342, -0.4243],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4389,  0.9900, -1.0008,  0.9900, -0.4096,  0.9900, -0.4394, -0.2309],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9993e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0888, -0.0897, -0.0906,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1165, -0.1177, -0.1189,  ..., -0.9809, -0.9908, -1.0008],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0889, -0.0898, -0.0907,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0398, -0.0402, -0.0406,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4281,  0.9656, -0.9569,  0.9656, -0.3995,  0.9608, -0.4286, -0.2252],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4833, -3.8991, -0.7656,  ..., -4.5691, -4.5641, -4.5632],\n",
      "        [-2.7772, -3.8733, -0.8258,  ..., -5.3529, -5.3640, -5.3876],\n",
      "        [-2.5764, -3.8364, -0.7848,  ..., -0.8757, -0.9286, -0.7901],\n",
      "        ...,\n",
      "        [-2.9064, -3.8456, -0.8446,  ..., -5.4990, -5.4933, -5.4794],\n",
      "        [-2.4761, -3.9012, -0.7687,  ..., -4.5395, -4.5269, -4.5313],\n",
      "        [-2.5770, -3.8764, -0.7699,  ..., -4.6506, -4.6571, -4.6600]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4233,  0.9418, -0.8798,  0.9711, -0.4026,  0.7624, -0.4093, -0.2313],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4439,  0.9900, -0.4002,  0.9900, -0.4118, -0.4002, -0.2327,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.7757e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0898, -0.0907, -0.0916,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0810, -0.0818, -0.0826,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0810, -0.0818, -0.0826,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0401, -0.0405, -0.0409,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4330,  0.9656, -0.3903,  0.9656, -0.4016, -0.3903, -0.2270,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4622, -3.8985, -0.7601,  ..., -4.5693, -4.5595, -4.5584],\n",
      "        [-2.7644, -3.8799, -0.8191,  ..., -5.3645, -5.3724, -5.3934],\n",
      "        [-2.4664, -3.8999, -0.7651,  ..., -4.5895, -4.5735, -4.5683],\n",
      "        ...,\n",
      "        [-2.4664, -3.8999, -0.7651,  ..., -4.5895, -4.5735, -4.5683],\n",
      "        [-2.5610, -3.8784, -0.7670,  ..., -4.6510, -4.6593, -4.6597],\n",
      "        [-2.9031, -3.8433, -0.8415,  ..., -5.5520, -5.5485, -5.5385]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4663,  0.9182, -0.4379,  0.9034, -0.4058, -0.4379, -0.2346,  0.6675],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2275, -1.2275,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.6966e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2508, -0.2534, -0.2559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2508, -0.2534, -0.2559,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.2032, -1.2032,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3446, -3.9118, -0.7164,  ..., -4.4288, -4.4196, -4.4197],\n",
      "        [-2.3446, -3.9118, -0.7164,  ..., -4.4288, -4.4196, -4.4197],\n",
      "        [-2.7527, -3.8769, -0.8205,  ..., -5.3542, -5.3631, -5.3854],\n",
      "        ...,\n",
      "        [-2.7160, -3.8773, -0.8104,  ..., -5.1428, -5.2046, -5.2353],\n",
      "        [-2.8964, -3.8424, -0.8392,  ..., -5.4888, -5.4782, -5.4669],\n",
      "        [-2.8929, -3.8498, -0.8440,  ..., -5.5916, -5.5855, -5.5783]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0536, -1.0536,  0.8974,  0.8110,  0.9441,  0.8842,  0.6700,  0.5975],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2158, -0.4410, -0.4148,  0.9900, -0.4245,  0.9900,  0.9900, -0.3947],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8815e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0372, -0.0376, -0.0379,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0892, -0.0901, -0.0910,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0839, -0.0848, -0.0856,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0798, -0.0807, -0.0815,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2105, -0.4302, -0.4045,  0.9656, -0.4141,  0.9656,  0.9608, -0.3850],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5548, -3.8791, -0.7645,  ..., -4.6616, -4.6721, -4.6732],\n",
      "        [-2.4513, -3.9005, -0.7575,  ..., -4.5738, -4.5650, -4.5619],\n",
      "        [-2.4663, -3.8995, -0.7576,  ..., -4.5495, -4.5399, -4.5447],\n",
      "        ...,\n",
      "        [-2.7575, -3.8778, -0.8174,  ..., -5.3608, -5.3699, -5.3904],\n",
      "        [-2.8890, -3.8484, -0.8403,  ..., -5.5106, -5.5023, -5.4909],\n",
      "        [-2.4655, -3.9021, -0.7620,  ..., -4.6035, -4.5913, -4.5992]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2352, -0.4192, -0.4204,  0.8769, -0.4165,  0.9041,  0.7421, -0.4043],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3733,  0.9900, -0.4215,  0.9900, -0.3733, -0.4036, -1.2032,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0907e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.5498e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0755, -0.0763, -0.0771,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0853, -0.0861, -0.0870,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0817, -0.0825, -0.0833,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2459, -0.2483, -0.2509,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3641,  0.9656, -0.4111,  0.9656, -0.3641, -0.3937, -1.1793,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4719, -3.8956, -0.7621,  ..., -4.6550, -4.6415, -4.6408],\n",
      "        [-2.7662, -3.8778, -0.8163,  ..., -5.3580, -5.3639, -5.3881],\n",
      "        [-2.4627, -3.8978, -0.7545,  ..., -4.6135, -4.6061, -4.6021],\n",
      "        ...,\n",
      "        [-2.4757, -3.9012, -0.7576,  ..., -4.5544, -4.5467, -4.5507],\n",
      "        [-2.3522, -3.9132, -0.7143,  ..., -4.4328, -4.4253, -4.4249],\n",
      "        [-2.7763, -3.8780, -0.8179,  ..., -5.3479, -5.3580, -5.3863]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4034,  0.9041, -0.4388,  0.9134, -0.4034, -0.4186, -1.0544,  0.9364],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4074, -0.4052], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.5093e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0824, -0.0833, -0.0841, -0.0849, -0.0858, -0.0867, -0.0875, -0.0884,\n",
      "         -0.0893, -0.0902, -0.0911, -0.0921, -0.0930, -0.0939, -0.0949, -0.0958,\n",
      "         -0.0968, -0.0978, -0.0988, -0.0998, -0.1008, -0.1018, -0.1028, -0.1039,\n",
      "         -0.1049, -0.1060, -0.1070, -0.1081, -0.1092, -0.1103, -0.1114, -0.1126,\n",
      "         -0.1137, -0.1148, -0.1160, -0.1172, -0.1184, -0.1196, -0.1208, -0.1220,\n",
      "         -0.1232, -0.1245, -0.1257, -0.1270, -0.1283, -0.1296, -0.1309, -0.1322,\n",
      "         -0.1335, -0.1349, -0.1362, -0.1376, -0.1390, -0.1404, -0.1418, -0.1433,\n",
      "         -0.1447, -0.1462, -0.1476, -0.1491, -0.1506, -0.1522, -0.1537, -0.1553,\n",
      "         -0.1568, -0.1584, -0.1600, -0.1616, -0.1633, -0.1649, -0.1666, -0.1683,\n",
      "         -0.1700, -0.1717, -0.1734, -0.1752, -0.1769, -0.1787, -0.1805, -0.1823,\n",
      "         -0.1842, -0.1860, -0.1879, -0.1898, -0.1917, -0.1937, -0.1956, -0.1976,\n",
      "         -0.1996, -0.2016, -0.2037, -0.2057, -0.2078, -0.2099, -0.2120, -0.2141,\n",
      "         -0.2163, -0.2185, -0.2207, -0.2229, -0.2252, -0.2275, -0.2298, -0.2321,\n",
      "         -0.2344, -0.2368, -0.2392, -0.2416, -0.2440, -0.2465, -0.2490, -0.2515,\n",
      "         -0.2540, -0.2566, -0.2592, -0.2618, -0.2645, -0.2671, -0.2698, -0.2726,\n",
      "         -0.2753, -0.2781, -0.2809, -0.2837, -0.2866, -0.2895, -0.2924, -0.2954,\n",
      "         -0.2984, -0.3014, -0.3044, -0.3075, -0.3106, -0.3137, -0.3169, -0.3201,\n",
      "         -0.3234, -0.3266, -0.3299, -0.3332, -0.3366, -0.3400, -0.3434, -0.3469,\n",
      "         -0.3504, -0.3540, -0.3575, -0.3611, -0.3648, -0.3685, -0.3722, -0.3760,\n",
      "         -0.3798, -0.3836, -0.3875, -0.3914, -0.3953, -0.3993, -0.4034, -0.4074,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0820, -0.0828, -0.0836, -0.0845, -0.0853, -0.0862, -0.0871, -0.0879,\n",
      "         -0.0888, -0.0897, -0.0906, -0.0915, -0.0925, -0.0934, -0.0943, -0.0953,\n",
      "         -0.0963, -0.0972, -0.0982, -0.0992, -0.1002, -0.1012, -0.1022, -0.1033,\n",
      "         -0.1043, -0.1054, -0.1064, -0.1075, -0.1086, -0.1097, -0.1108, -0.1119,\n",
      "         -0.1131, -0.1142, -0.1154, -0.1165, -0.1177, -0.1189, -0.1201, -0.1213,\n",
      "         -0.1225, -0.1238, -0.1250, -0.1263, -0.1275, -0.1288, -0.1301, -0.1315,\n",
      "         -0.1328, -0.1341, -0.1355, -0.1368, -0.1382, -0.1396, -0.1410, -0.1425,\n",
      "         -0.1439, -0.1454, -0.1468, -0.1483, -0.1498, -0.1513, -0.1528, -0.1544,\n",
      "         -0.1559, -0.1575, -0.1591, -0.1607, -0.1623, -0.1640, -0.1656, -0.1673,\n",
      "         -0.1690, -0.1707, -0.1724, -0.1742, -0.1759, -0.1777, -0.1795, -0.1813,\n",
      "         -0.1832, -0.1850, -0.1869, -0.1888, -0.1907, -0.1926, -0.1945, -0.1965,\n",
      "         -0.1985, -0.2005, -0.2025, -0.2046, -0.2066, -0.2087, -0.2108, -0.2130,\n",
      "         -0.2151, -0.2173, -0.2195, -0.2217, -0.2239, -0.2262, -0.2285, -0.2308,\n",
      "         -0.2331, -0.2355, -0.2378, -0.2402, -0.2427, -0.2451, -0.2476, -0.2501,\n",
      "         -0.2526, -0.2552, -0.2578, -0.2604, -0.2630, -0.2656, -0.2683, -0.2710,\n",
      "         -0.2738, -0.2765, -0.2793, -0.2822, -0.2850, -0.2879, -0.2908, -0.2937,\n",
      "         -0.2967, -0.2997, -0.3027, -0.3058, -0.3089, -0.3120, -0.3151, -0.3183,\n",
      "         -0.3215, -0.3248, -0.3281, -0.3314, -0.3347, -0.3381, -0.3415, -0.3450,\n",
      "         -0.3485, -0.3520, -0.3555, -0.3591, -0.3628, -0.3664, -0.3701, -0.3739,\n",
      "         -0.3776, -0.3815, -0.3853, -0.3892, -0.3931, -0.3971, -0.4011, -0.4052,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3974, -0.3952], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4742e+00, -3.8990e+00, -7.5583e-01, -2.1995e+00,  1.8378e-01,\n",
      "         -2.3001e+00, -4.4982e+00, -2.2422e-01, -3.7835e+00, -2.4064e+00,\n",
      "         -3.8539e+00,  6.7562e-01, -8.1352e-02, -3.5288e+00, -3.1190e+00,\n",
      "         -2.2777e+00, -3.2060e+00, -1.7541e+00, -1.9839e+00, -9.1360e-01,\n",
      "         -1.3729e+00, -1.3607e+00, -8.2362e-01,  3.9484e-01, -1.8775e+00,\n",
      "         -3.3676e+00, -3.0809e+00, -2.4840e+00, -3.8418e+00,  9.5308e-01,\n",
      "         -1.7113e+00, -1.2787e-01, -8.7119e-01, -3.4458e+00, -1.8975e+00,\n",
      "         -1.7053e+00, -9.4619e-01, -3.1395e+00, -1.5931e+00,  4.8034e-04,\n",
      "         -6.5450e-01, -5.6178e-01, -3.3864e-01, -4.2580e+00, -1.5824e+00,\n",
      "         -3.2217e+00, -3.3632e+00, -3.1531e+00, -3.8978e-03,  3.5238e-02,\n",
      "         -3.7767e+00, -1.4507e+00, -7.0142e+00, -2.8463e+00, -3.0205e+00,\n",
      "         -9.1440e-01, -2.5553e+00, -2.9087e+00, -2.5129e+00, -1.5880e+00,\n",
      "         -3.5681e+00, -1.1577e+00, -5.7313e+00, -2.3755e+00, -3.5939e+00,\n",
      "          1.0581e+00, -1.3386e+00, -3.8950e-03, -3.3943e-01, -3.0684e+00,\n",
      "         -4.0094e+00, -5.5081e+00, -2.7389e+00, -1.5907e-01, -3.7389e+00,\n",
      "         -4.2096e+00, -2.4255e+00, -3.8462e+00, -2.4918e+00, -3.4916e+00,\n",
      "         -4.8375e+00, -2.4553e+00, -3.6753e-01, -1.2278e+00, -1.7025e+00,\n",
      "         -2.9440e+00, -1.7965e+00, -6.0372e-01, -3.5240e+00, -8.1397e-01,\n",
      "         -3.0939e+00, -4.0728e+00, -2.4094e+00, -6.5188e-01, -8.3806e-01,\n",
      "         -2.9833e+00, -2.7247e+00, -3.6010e+00, -3.2461e+00, -3.1783e+00,\n",
      "         -1.2377e-01, -9.4325e-01, -3.6856e+00, -2.1127e+00, -4.5431e-01,\n",
      "         -1.8402e+00, -3.4417e+00, -3.7322e+00,  5.4028e-01, -1.0673e+00,\n",
      "         -1.3668e+00, -6.8914e-01, -4.4239e+00, -4.5251e+00,  3.0351e-01,\n",
      "         -9.3318e-01, -2.1318e+00, -1.2949e+00, -4.6909e-02, -9.5417e-01,\n",
      "         -2.7340e+00, -4.5972e+00, -2.6475e+00, -8.7477e-01, -1.0759e+00,\n",
      "         -2.7496e+00, -2.2300e+00, -7.9155e-01, -5.2933e-01, -4.1507e+00,\n",
      "         -4.6133e-01,  9.5386e-02, -3.3338e+00, -4.0750e+00, -1.1812e+00,\n",
      "         -2.0155e+00, -3.0443e+00, -2.3254e+00, -1.6907e+00, -2.8800e+00,\n",
      "         -2.6189e+00, -3.3150e+00, -5.6401e+00, -1.2210e+00, -5.7392e-01,\n",
      "         -4.0591e-01, -4.0267e-01, -4.7187e-01, -4.5614e-01, -4.5312e-01,\n",
      "         -4.8228e+00, -4.8828e+00, -5.2782e+00, -4.8944e+00, -4.8050e+00,\n",
      "         -4.5705e+00, -4.4595e+00, -4.5267e+00, -4.4699e+00, -4.4229e+00,\n",
      "         -4.5233e+00, -4.6261e+00, -4.7057e+00, -4.7325e+00, -4.6973e+00,\n",
      "         -4.6283e+00, -4.5815e+00, -4.5624e+00, -4.5701e+00, -4.5755e+00,\n",
      "         -4.5774e+00, -4.5967e+00, -4.6206e+00, -4.6375e+00, -4.6540e+00,\n",
      "         -4.6588e+00, -4.6315e+00, -4.6036e+00, -4.6027e+00, -4.6018e+00,\n",
      "         -4.6102e+00, -4.6322e+00, -4.6276e+00, -4.6122e+00, -4.6030e+00,\n",
      "         -4.5996e+00, -4.6184e+00, -4.6366e+00, -4.6483e+00, -4.6532e+00,\n",
      "         -4.6542e+00, -4.6611e+00, -4.6596e+00, -4.6549e+00, -4.6404e+00,\n",
      "         -4.6338e+00, -4.6370e+00, -4.6245e+00, -4.6143e+00, -4.6068e+00,\n",
      "         -4.6019e+00, -4.6050e+00, -4.5958e+00, -4.5855e+00, -4.5826e+00],\n",
      "        [-2.4798e+00, -3.9024e+00, -7.5984e-01, -2.2085e+00,  1.8086e-01,\n",
      "         -2.2964e+00, -4.5104e+00, -2.2819e-01, -3.7897e+00, -2.4098e+00,\n",
      "         -3.8512e+00,  6.7279e-01, -8.0851e-02, -3.5302e+00, -3.1261e+00,\n",
      "         -2.2850e+00, -3.2107e+00, -1.7551e+00, -1.9892e+00, -9.1350e-01,\n",
      "         -1.3783e+00, -1.3602e+00, -8.2290e-01,  3.9321e-01, -1.8813e+00,\n",
      "         -3.3681e+00, -3.0857e+00, -2.4925e+00, -3.8466e+00,  9.4847e-01,\n",
      "         -1.7129e+00, -1.2797e-01, -8.5984e-01, -3.4422e+00, -1.8936e+00,\n",
      "         -1.7045e+00, -9.4848e-01, -3.1428e+00, -1.5932e+00,  4.1227e-03,\n",
      "         -6.5298e-01, -5.6414e-01, -3.4083e-01, -4.2623e+00, -1.5825e+00,\n",
      "         -3.2302e+00, -3.3655e+00, -3.1531e+00, -2.0796e-04,  4.1110e-02,\n",
      "         -3.7753e+00, -1.4559e+00, -7.0187e+00, -2.8489e+00, -3.0246e+00,\n",
      "         -9.1765e-01, -2.5523e+00, -2.9131e+00, -2.5169e+00, -1.5890e+00,\n",
      "         -3.5678e+00, -1.1621e+00, -5.7338e+00, -2.3799e+00, -3.5978e+00,\n",
      "          1.0554e+00, -1.3409e+00, -2.6491e-03, -3.2952e-01, -3.0669e+00,\n",
      "         -4.0142e+00, -5.5042e+00, -2.7440e+00, -1.5761e-01, -3.7370e+00,\n",
      "         -4.2094e+00, -2.4305e+00, -3.8496e+00, -2.4906e+00, -3.4899e+00,\n",
      "         -4.8319e+00, -2.4594e+00, -3.6507e-01, -1.2258e+00, -1.6975e+00,\n",
      "         -2.9438e+00, -1.7977e+00, -6.0316e-01, -3.5187e+00, -8.1097e-01,\n",
      "         -3.0946e+00, -4.0736e+00, -2.4139e+00, -6.4788e-01, -8.3529e-01,\n",
      "         -2.9805e+00, -2.7233e+00, -3.6083e+00, -3.2480e+00, -3.1791e+00,\n",
      "         -1.2247e-01, -9.4138e-01, -3.6849e+00, -2.1094e+00, -4.5530e-01,\n",
      "         -1.8428e+00, -3.4501e+00, -3.7372e+00,  5.4174e-01, -1.2703e+00,\n",
      "         -1.3132e+00, -4.5098e-01, -4.3275e+00, -4.5430e+00,  3.5171e-01,\n",
      "         -8.8560e-01, -2.1253e+00, -1.2586e+00, -1.4230e-02, -8.7038e-01,\n",
      "         -2.6710e+00, -4.7165e+00, -2.6172e+00, -8.3755e-01, -9.9119e-01,\n",
      "         -2.6788e+00, -2.2118e+00, -7.7437e-01, -5.4568e-01, -4.1327e+00,\n",
      "         -4.0755e-01,  1.1424e-01, -3.3312e+00, -4.0650e+00, -1.1914e+00,\n",
      "         -2.0248e+00, -3.0484e+00, -2.3082e+00, -1.6719e+00, -2.8510e+00,\n",
      "         -2.6005e+00, -3.3396e+00, -5.7832e+00, -1.2476e+00, -4.5552e-01,\n",
      "         -4.0814e-01, -4.1064e-01, -3.9001e-01, -4.1315e-01, -4.7457e-01,\n",
      "         -4.7998e+00, -4.8766e+00, -5.2392e+00, -4.8949e+00, -4.8781e+00,\n",
      "         -4.6245e+00, -4.5493e+00, -4.6083e+00, -4.5756e+00, -4.5302e+00,\n",
      "         -4.5861e+00, -4.6595e+00, -4.7142e+00, -4.7037e+00, -4.6197e+00,\n",
      "         -4.5107e+00, -4.4537e+00, -4.4434e+00, -4.4563e+00, -4.4687e+00,\n",
      "         -4.4750e+00, -4.4958e+00, -4.5207e+00, -4.5367e+00, -4.5571e+00,\n",
      "         -4.5639e+00, -4.5371e+00, -4.5183e+00, -4.5309e+00, -4.5477e+00,\n",
      "         -4.5619e+00, -4.5774e+00, -4.5664e+00, -4.5481e+00, -4.5431e+00,\n",
      "         -4.5451e+00, -4.5705e+00, -4.5882e+00, -4.6006e+00, -4.6079e+00,\n",
      "         -4.6103e+00, -4.6207e+00, -4.6259e+00, -4.6210e+00, -4.6098e+00,\n",
      "         -4.6014e+00, -4.6054e+00, -4.6008e+00, -4.5988e+00, -4.5895e+00,\n",
      "         -4.5847e+00, -4.5958e+00, -4.6007e+00, -4.5952e+00, -4.5957e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4606, -0.4253], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3704,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.3895],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.7655e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0749, -0.0757, -0.0765,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0788, -0.0796, -0.0804,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3613,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.3799],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4823, -3.8998, -0.7584,  ..., -4.6253, -4.6106, -4.6175],\n",
      "        [-2.7814, -3.8761, -0.8146,  ..., -5.3516, -5.3565, -5.3804],\n",
      "        [-2.7902, -3.8764, -0.8186,  ..., -5.2741, -5.2874, -5.3114],\n",
      "        ...,\n",
      "        [-2.7792, -3.8754, -0.8157,  ..., -5.3382, -5.3446, -5.3648],\n",
      "        [-2.7792, -3.8754, -0.8157,  ..., -5.3382, -5.3446, -5.3648],\n",
      "        [-2.4851, -3.8987, -0.7551,  ..., -4.5632, -4.5563, -4.5595]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3846,  0.9114,  0.8710,  0.9000,  0.9433,  0.9000,  0.9000, -0.4030],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1954, -1.1954,  0.9900, -0.4241, -0.1969,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9602e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2443, -0.2467, -0.2492,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2443, -0.2467, -0.2492,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0339, -0.0343, -0.0346,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.1717, -1.1717,  0.9656, -0.4137, -0.1921,  0.9608,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7893, -3.8767, -0.8102,  ..., -5.3572, -5.3624, -5.3809],\n",
      "        [-2.3703, -3.9088, -0.7105,  ..., -4.4410, -4.4331, -4.4343],\n",
      "        [-2.3703, -3.9088, -0.7105,  ..., -4.4410, -4.4331, -4.4343],\n",
      "        ...,\n",
      "        [-2.5809, -3.8738, -0.7603,  ..., -4.6785, -4.6876, -4.6884],\n",
      "        [-2.9198, -3.8444, -0.8382,  ..., -5.5290, -5.5196, -5.5047],\n",
      "        [-2.7959, -3.8724, -0.8062,  ..., -5.3414, -5.3482, -5.3706]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9096, -1.0251, -1.0251,  0.9237, -0.3920, -0.2144,  0.7806,  0.9326],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3888, -0.4196, -0.3795,  0.9900, -0.1962, -0.4271, -0.4365, -1.2080],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(7.2558e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0787, -0.0794, -0.0802,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0849, -0.0857, -0.0866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0768, -0.0775, -0.0783,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0864, -0.0873, -0.0882,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0883, -0.0892, -0.0901,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2468, -0.2493, -0.2519,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3792, -0.4092, -0.3701,  0.9656, -0.1914, -0.4165, -0.4258, -1.1841],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4956, -3.8967, -0.7460,  ..., -4.5396, -4.5335, -4.5376],\n",
      "        [-2.4817, -3.8956, -0.7528,  ..., -4.5859, -4.5705, -4.5717],\n",
      "        [-2.4906, -3.8886, -0.7542,  ..., -4.6710, -4.6574, -4.6566],\n",
      "        ...,\n",
      "        [-2.4829, -3.8897, -0.7475,  ..., -4.6324, -4.6213, -4.6180],\n",
      "        [-2.4788, -3.8929, -0.7486,  ..., -4.5983, -4.5898, -4.5829],\n",
      "        [-2.3721, -3.9048, -0.7068,  ..., -4.4445, -4.4372, -4.4378]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3617, -0.3701, -0.3663,  0.7058, -0.2219, -0.3993, -0.3820, -1.0127],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4292, -0.3875,  0.9900, -0.4287, -1.0037,  0.9900, -0.3989],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8897e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.7090e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0868, -0.0877, -0.0886,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0784, -0.0792, -0.0800,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1168, -0.1180, -0.1192,  ..., -0.9837, -0.9936, -1.0037],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0807, -0.0815, -0.0823,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4186, -0.3779,  0.9656, -0.4182, -0.9597,  0.9656, -0.3891],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7537, -3.8679, -0.7993,  ..., -5.1672, -5.2263, -5.2607],\n",
      "        [-2.4833, -3.8926, -0.7502,  ..., -4.5837, -4.5729, -4.5728],\n",
      "        [-2.4914, -3.8895, -0.7500,  ..., -4.6366, -4.6183, -4.6131],\n",
      "        ...,\n",
      "        [-2.5855, -3.8291, -0.7693,  ..., -0.8398, -0.9389, -0.8497],\n",
      "        [-2.7609, -3.8747, -0.7896,  ..., -5.2438, -5.3067, -5.3418],\n",
      "        [-2.4953, -3.8933, -0.7439,  ..., -4.5391, -4.5361, -4.5408]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9171, -0.3618, -0.3863,  0.8548, -0.3851, -0.8752,  0.9482, -0.3537],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4397,  0.9900, -0.2331, -0.2331,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.2764e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0890, -0.0898, -0.0908,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0401, -0.0406, -0.0410,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4288,  0.9608, -0.2273, -0.2273,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4895, -3.8893, -0.7462,  ..., -4.6148, -4.6036, -4.6036],\n",
      "        [-2.9299, -3.8366, -0.8335,  ..., -5.5486, -5.5389, -5.5260],\n",
      "        [-2.5866, -3.8669, -0.7567,  ..., -4.6962, -4.7033, -4.7058],\n",
      "        ...,\n",
      "        [-2.7979, -3.8657, -0.8144,  ..., -5.3355, -5.3419, -5.3660],\n",
      "        [-2.8058, -3.8673, -0.8115,  ..., -5.3594, -5.3701, -5.3955],\n",
      "        [-2.7405, -3.8607, -0.7805,  ..., -5.4276, -5.4714, -5.4755]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4161,  0.8126, -0.2004, -0.2004,  0.7158,  0.9335,  0.9798,  0.9370],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.3984, -0.3999, -0.9706,  0.9900, -0.4464,  0.9900, -0.4398],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8127e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0806, -0.0814, -0.0822,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0809, -0.0817, -0.0825,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0890, -0.0899, -0.0908,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3886, -0.3901, -0.9280,  0.9656, -0.4354,  0.9656, -0.4289],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9433, -3.8368, -0.8327,  ..., -5.6359, -5.6320, -5.6260],\n",
      "        [-2.4990, -3.8880, -0.7545,  ..., -4.6426, -4.6278, -4.6193],\n",
      "        [-2.5028, -3.8835, -0.7563,  ..., -4.6817, -4.6695, -4.6703],\n",
      "        ...,\n",
      "        [-2.4924, -3.8843, -0.7478,  ..., -4.6433, -4.6336, -4.6312],\n",
      "        [-2.8101, -3.8649, -0.8040,  ..., -5.3602, -5.3690, -5.3923],\n",
      "        [-2.5017, -3.8897, -0.7537,  ..., -4.6237, -4.6165, -4.6161]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6793, -0.3868, -0.3621, -0.9304,  0.9643, -0.3935,  0.9684, -0.3855],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4418, -0.4303], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(2.8595e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0894, -0.0903, -0.0912, -0.0921, -0.0930, -0.0940, -0.0949, -0.0959,\n",
      "         -0.0969, -0.0978, -0.0988, -0.0998, -0.1008, -0.1018, -0.1029, -0.1039,\n",
      "         -0.1050, -0.1060, -0.1071, -0.1082, -0.1093, -0.1104, -0.1115, -0.1126,\n",
      "         -0.1137, -0.1149, -0.1161, -0.1172, -0.1184, -0.1196, -0.1208, -0.1220,\n",
      "         -0.1233, -0.1245, -0.1258, -0.1270, -0.1283, -0.1296, -0.1309, -0.1323,\n",
      "         -0.1336, -0.1349, -0.1363, -0.1377, -0.1391, -0.1405, -0.1419, -0.1433,\n",
      "         -0.1448, -0.1462, -0.1477, -0.1492, -0.1507, -0.1522, -0.1538, -0.1553,\n",
      "         -0.1569, -0.1585, -0.1601, -0.1617, -0.1633, -0.1650, -0.1667, -0.1683,\n",
      "         -0.1700, -0.1718, -0.1735, -0.1752, -0.1770, -0.1788, -0.1806, -0.1824,\n",
      "         -0.1843, -0.1861, -0.1880, -0.1899, -0.1918, -0.1938, -0.1957, -0.1977,\n",
      "         -0.1997, -0.2017, -0.2038, -0.2058, -0.2079, -0.2100, -0.2121, -0.2143,\n",
      "         -0.2164, -0.2186, -0.2208, -0.2230, -0.2253, -0.2276, -0.2299, -0.2322,\n",
      "         -0.2345, -0.2369, -0.2393, -0.2417, -0.2442, -0.2466, -0.2491, -0.2516,\n",
      "         -0.2542, -0.2567, -0.2593, -0.2620, -0.2646, -0.2673, -0.2700, -0.2727,\n",
      "         -0.2755, -0.2782, -0.2810, -0.2839, -0.2868, -0.2897, -0.2926, -0.2955,\n",
      "         -0.2985, -0.3015, -0.3046, -0.3077, -0.3108, -0.3139, -0.3171, -0.3203,\n",
      "         -0.3235, -0.3268, -0.3301, -0.3334, -0.3368, -0.3402, -0.3436, -0.3471,\n",
      "         -0.3506, -0.3541, -0.3577, -0.3613, -0.3650, -0.3687, -0.3724, -0.3761,\n",
      "         -0.3799, -0.3838, -0.3877, -0.3916, -0.3955, -0.3995, -0.4036, -0.4076,\n",
      "         -0.4118, -0.4159, -0.4201, -0.4244, -0.4286, -0.4330, -0.4374, -0.4418,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0871, -0.0879, -0.0888, -0.0897, -0.0906, -0.0915, -0.0925, -0.0934,\n",
      "         -0.0943, -0.0953, -0.0963, -0.0972, -0.0982, -0.0992, -0.1002, -0.1012,\n",
      "         -0.1022, -0.1033, -0.1043, -0.1054, -0.1064, -0.1075, -0.1086, -0.1097,\n",
      "         -0.1108, -0.1119, -0.1131, -0.1142, -0.1154, -0.1165, -0.1177, -0.1189,\n",
      "         -0.1201, -0.1213, -0.1225, -0.1238, -0.1250, -0.1263, -0.1276, -0.1288,\n",
      "         -0.1301, -0.1315, -0.1328, -0.1341, -0.1355, -0.1368, -0.1382, -0.1396,\n",
      "         -0.1410, -0.1425, -0.1439, -0.1454, -0.1468, -0.1483, -0.1498, -0.1513,\n",
      "         -0.1528, -0.1544, -0.1559, -0.1575, -0.1591, -0.1607, -0.1623, -0.1640,\n",
      "         -0.1656, -0.1673, -0.1690, -0.1707, -0.1724, -0.1742, -0.1759, -0.1777,\n",
      "         -0.1795, -0.1813, -0.1832, -0.1850, -0.1869, -0.1888, -0.1907, -0.1926,\n",
      "         -0.1945, -0.1965, -0.1985, -0.2005, -0.2025, -0.2046, -0.2066, -0.2087,\n",
      "         -0.2108, -0.2130, -0.2151, -0.2173, -0.2195, -0.2217, -0.2239, -0.2262,\n",
      "         -0.2285, -0.2308, -0.2331, -0.2355, -0.2378, -0.2402, -0.2427, -0.2451,\n",
      "         -0.2476, -0.2501, -0.2526, -0.2552, -0.2578, -0.2604, -0.2630, -0.2657,\n",
      "         -0.2683, -0.2710, -0.2738, -0.2765, -0.2793, -0.2822, -0.2850, -0.2879,\n",
      "         -0.2908, -0.2937, -0.2967, -0.2997, -0.3027, -0.3058, -0.3089, -0.3120,\n",
      "         -0.3151, -0.3183, -0.3215, -0.3248, -0.3281, -0.3314, -0.3347, -0.3381,\n",
      "         -0.3415, -0.3450, -0.3485, -0.3520, -0.3555, -0.3591, -0.3628, -0.3664,\n",
      "         -0.3701, -0.3739, -0.3776, -0.3815, -0.3853, -0.3892, -0.3931, -0.3971,\n",
      "         -0.4011, -0.4052, -0.4093, -0.4134, -0.4176, -0.4218, -0.4260, -0.4303,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4309, -0.4197], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5026, -3.8855, -0.7505, -2.2093,  0.1604, -2.3084, -4.5218, -0.2500,\n",
      "         -3.8265, -2.4258, -3.8711,  0.7573, -0.0656, -3.5191, -3.0870, -2.2695,\n",
      "         -3.2180, -1.7828, -1.9509, -0.9545, -1.3879, -1.3889, -0.8543,  0.4121,\n",
      "         -1.9056, -3.3724, -3.0509, -2.4791, -3.8400,  0.9622, -1.7348, -0.1696,\n",
      "         -0.8568, -3.4571, -1.9022, -1.6991, -0.9348, -3.1530, -1.6076, -0.0320,\n",
      "         -0.6792, -0.5495, -0.3456, -4.2411, -1.6107, -3.2641, -3.3893, -3.1744,\n",
      "          0.1647,  0.1499, -3.7718, -1.4430, -6.9827, -2.8341, -3.0294, -0.9468,\n",
      "         -2.5443, -2.9076, -2.5327, -1.6140, -3.5753, -1.1521, -5.6970, -2.3573,\n",
      "         -3.5920,  1.0688, -1.3672, -0.0515, -0.3238, -3.0885, -4.0074, -5.4826,\n",
      "         -2.7282, -0.1261, -3.7419, -4.1669, -2.4150, -3.8542, -2.5102, -3.4969,\n",
      "         -4.8077, -2.4437, -0.3791, -1.2747, -1.7005, -2.9600, -1.8259, -0.6481,\n",
      "         -3.5371, -0.8446, -3.0985, -4.0341, -2.3983, -0.6632, -0.8864, -2.9735,\n",
      "         -2.7576, -3.6411, -3.2661, -3.1924, -0.1357, -0.9929, -3.6702, -2.0911,\n",
      "         -0.4435, -1.8659, -3.4860, -3.7570,  0.5780, -1.0466, -1.3625, -0.6533,\n",
      "         -4.4664, -4.5263,  0.2968, -0.9809, -2.1585, -1.3264, -0.0978, -0.9665,\n",
      "         -2.7671, -4.5984, -2.6549, -0.8656, -1.1059, -2.7687, -2.2566, -0.7928,\n",
      "         -0.5737, -4.1738, -0.2883,  0.2155, -3.3497, -4.0803, -1.2126, -2.0042,\n",
      "         -3.0449, -2.3418, -1.7171, -2.8704, -2.6009, -3.3471, -5.6145, -1.2506,\n",
      "         -0.4520, -0.3868, -0.3628, -0.4562, -0.4056, -0.4353, -4.8354, -4.8970,\n",
      "         -5.2903, -4.9013, -4.8127, -4.5747, -4.4706, -4.5267, -4.4704, -4.4321,\n",
      "         -4.5430, -4.6561, -4.7448, -4.7721, -4.7310, -4.6566, -4.6102, -4.5915,\n",
      "         -4.5991, -4.6012, -4.5979, -4.6181, -4.6464, -4.6678, -4.6865, -4.6896,\n",
      "         -4.6618, -4.6314, -4.6306, -4.6298, -4.6428, -4.6658, -4.6604, -4.6431,\n",
      "         -4.6353, -4.6297, -4.6483, -4.6646, -4.6770, -4.6842, -4.6831, -4.6911,\n",
      "         -4.6902, -4.6837, -4.6674, -4.6597, -4.6617, -4.6479, -4.6394, -4.6292,\n",
      "         -4.6271, -4.6277, -4.6213, -4.6091, -4.6090],\n",
      "        [-2.5108, -3.8864, -0.7516, -2.2155,  0.1604, -2.3090, -4.5326, -0.2533,\n",
      "         -3.8354, -2.4326, -3.8722,  0.7556, -0.0629, -3.5220, -3.0866, -2.2708,\n",
      "         -3.2188, -1.7846, -1.9534, -0.9518, -1.3914, -1.3874, -0.8549,  0.4163,\n",
      "         -1.9044, -3.3734, -3.0520, -2.4811, -3.8405,  0.9631, -1.7339, -0.1660,\n",
      "         -0.8515, -3.4602, -1.9028, -1.6976, -0.9371, -3.1525, -1.6059, -0.0352,\n",
      "         -0.6759, -0.5516, -0.3449, -4.2361, -1.6094, -3.2696, -3.3949, -3.1758,\n",
      "          0.1729,  0.1600, -3.7721, -1.4468, -6.9864, -2.8348, -3.0302, -0.9506,\n",
      "         -2.5473, -2.9095, -2.5302, -1.6126, -3.5741, -1.1559, -5.7002, -2.3587,\n",
      "         -3.5912,  1.0703, -1.3649, -0.0488, -0.3191, -3.0909, -4.0119, -5.4785,\n",
      "         -2.7269, -0.1232, -3.7411, -4.1634, -2.4155, -3.8554, -2.5083, -3.4965,\n",
      "         -4.8060, -2.4444, -0.3753, -1.2724, -1.7017, -2.9601, -1.8251, -0.6465,\n",
      "         -3.5384, -0.8426, -3.0984, -4.0314, -2.3991, -0.6599, -0.8855, -2.9700,\n",
      "         -2.7573, -3.6461, -3.2640, -3.1955, -0.1321, -0.9904, -3.6738, -2.0885,\n",
      "         -0.4403, -1.8657, -3.4920, -3.7617,  0.5799, -1.1539, -1.3703, -0.9046,\n",
      "         -4.0310, -4.5198,  0.3729, -1.0122, -2.1356, -1.3397, -0.0957, -0.9648,\n",
      "         -2.8219, -4.5233, -2.7022, -0.8762, -1.0933, -2.7316, -2.2292, -0.7767,\n",
      "         -0.5395, -4.1638, -0.2343,  0.2280, -3.3078, -4.0828, -1.2297, -2.0132,\n",
      "         -3.0365, -2.3337, -1.7083, -2.9170, -2.6032, -3.3886, -5.6513, -1.2676,\n",
      "         -0.3809, -0.3592, -0.3788, -0.3467, -0.4008, -0.4008, -4.8099, -4.8792,\n",
      "         -5.2517, -4.8670, -4.7856, -4.5114, -4.4669, -4.5372, -4.5321, -4.5000,\n",
      "         -4.5849, -4.6846, -4.7484, -4.7423, -4.6639, -4.5623, -4.5152, -4.5179,\n",
      "         -4.5438, -4.5560, -4.5603, -4.5864, -4.6258, -4.6568, -4.6781, -4.6841,\n",
      "         -4.6618, -4.6414, -4.6462, -4.6514, -4.6665, -4.6775, -4.6603, -4.6316,\n",
      "         -4.6161, -4.6012, -4.6152, -4.6287, -4.6418, -4.6473, -4.6491, -4.6580,\n",
      "         -4.6580, -4.6507, -4.6343, -4.6266, -4.6255, -4.6111, -4.6051, -4.5953,\n",
      "         -4.5896, -4.5884, -4.5818, -4.5745, -4.5789]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4165, -0.3779], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.2372, -0.4395, -0.3984,  0.9900,  0.9900, -1.2319],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9802e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0409, -0.0413, -0.0417,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2517, -0.2543, -0.2568,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.2313, -0.4287, -0.3885,  0.9656,  0.9656, -1.2075],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8126, -3.8618, -0.8168,  ..., -5.3652, -5.3746, -5.3970],\n",
      "        [-2.8142, -3.8615, -0.8196,  ..., -5.3367, -5.3482, -5.3682],\n",
      "        [-2.6044, -3.8635, -0.7615,  ..., -4.7078, -4.7168, -4.7178],\n",
      "        ...,\n",
      "        [-2.8210, -3.8607, -0.8068,  ..., -5.3621, -5.3695, -5.3910],\n",
      "        [-2.8210, -3.8607, -0.8068,  ..., -5.3621, -5.3695, -5.3910],\n",
      "        [-2.3896, -3.8962, -0.7087,  ..., -4.4576, -4.4491, -4.4510]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9804,  0.9754, -0.2055, -0.3670, -0.3908,  0.9921,  0.9921, -0.9415],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.3961, -0.4068,  0.9900, -0.4375, -0.2331, -0.3961],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9898e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(3.8252e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0801, -0.0809, -0.0818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0885, -0.0894, -0.0903,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.0406, -0.0410,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0801, -0.0809, -0.0818,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9608, -0.3863, -0.3967,  0.9656, -0.4267, -0.2274, -0.3863],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9456, -3.8301, -0.8352,  ..., -5.5596, -5.5520, -5.5392],\n",
      "        [-2.9456, -3.8301, -0.8352,  ..., -5.5596, -5.5520, -5.5392],\n",
      "        [-2.5125, -3.8796, -0.7582,  ..., -4.6896, -4.6760, -4.6763],\n",
      "        ...,\n",
      "        [-2.5089, -3.8830, -0.7517,  ..., -4.6281, -4.6145, -4.6119],\n",
      "        [-2.6027, -3.8623, -0.7617,  ..., -4.7112, -4.7195, -4.7195],\n",
      "        [-2.5125, -3.8796, -0.7582,  ..., -4.6896, -4.6760, -4.6763]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8584,  0.8584, -0.3739, -0.3611,  0.7472, -0.4270, -0.2102, -0.3739],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -1.2227, -0.4310,  0.9900, -0.2142, -0.4375],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0113e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0369, -0.0373, -0.0376,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0885, -0.0894, -0.0903,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -1.1985, -0.4204,  0.9656, -0.2089, -0.4267],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8220, -3.8602, -0.8154,  ..., -5.3537, -5.3627, -5.3894],\n",
      "        [-2.8152, -3.8631, -0.8112,  ..., -5.3686, -5.3729, -5.3951],\n",
      "        [-2.7720, -3.8604, -0.8043,  ..., -5.1756, -5.2359, -5.2683],\n",
      "        ...,\n",
      "        [-2.8236, -3.8651, -0.8193,  ..., -5.4038, -5.4152, -5.4380],\n",
      "        [-2.6036, -3.8634, -0.7524,  ..., -4.6549, -4.6648, -4.6682],\n",
      "        [-2.5062, -3.8795, -0.7489,  ..., -4.6538, -4.6428, -4.6407]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0252,  0.9838,  0.9635, -0.9301, -0.3999,  1.0220, -0.2334, -0.4085],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4166,  0.9900, -0.4166, -0.4450,  0.9900, -0.2237, -0.3837, -0.3966],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4518e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0843, -0.0851, -0.0860,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.0851, -0.0860,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0385, -0.0389, -0.0393,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0776, -0.0784, -0.0792,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0802, -0.0810, -0.0819,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4063,  0.9656, -0.4063, -0.4340,  0.9656, -0.2182, -0.3742, -0.3868],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5091, -3.8812, -0.7487,  ..., -4.5918, -4.5818, -4.5872],\n",
      "        [-2.8043, -3.8576, -0.8122,  ..., -5.3569, -5.3619, -5.3878],\n",
      "        [-2.5091, -3.8812, -0.7487,  ..., -4.5918, -4.5818, -4.5872],\n",
      "        ...,\n",
      "        [-2.5968, -3.8583, -0.7570,  ..., -4.7094, -4.7194, -4.7211],\n",
      "        [-2.5061, -3.8814, -0.7538,  ..., -4.6551, -4.6406, -4.6331],\n",
      "        [-2.5104, -3.8870, -0.7456,  ..., -4.5551, -4.5497, -4.5545]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4013,  0.9888, -0.4013, -0.4017,  0.9918, -0.2162, -0.4062, -0.3690],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9571, -0.4229, -0.3931, -1.0085,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6539e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1860, -0.1879, -0.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.0864, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9152, -0.4125, -0.3834, -0.9643,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7636, -3.8656, -0.7885,  ..., -5.2488, -5.3138, -5.3505],\n",
      "        [-2.4425, -3.8736, -0.7227,  ..., -4.5566, -4.5607, -4.5760],\n",
      "        [-2.4998, -3.8834, -0.7487,  ..., -4.6331, -4.6267, -4.6256],\n",
      "        ...,\n",
      "        [-2.8095, -3.8584, -0.8118,  ..., -5.2808, -5.2913, -5.3198],\n",
      "        [-2.7937, -3.8563, -0.8082,  ..., -5.3740, -5.3806, -5.4008],\n",
      "        [-2.7992, -3.8588, -0.8093,  ..., -5.3644, -5.3704, -5.3933]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9884, -0.9524, -0.4118, -0.3775, -0.9113,  0.9275,  0.9161,  0.9921],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4252,  0.9900,  0.9900, -0.4424,  0.9900,  0.9900,  0.9900, -0.4234],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0730e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(8.7514e-12, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0860, -0.0869, -0.0878,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.0865, -0.0874,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4147,  0.9656,  0.9656, -0.4315,  0.9656,  0.9656,  0.9656, -0.4129],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4812, -3.8792, -0.7408,  ..., -4.6252, -4.6120, -4.6122],\n",
      "        [-2.7879, -3.8554, -0.8063,  ..., -5.3490, -5.3550, -5.3780],\n",
      "        [-2.7335, -3.8511, -0.7731,  ..., -5.4339, -5.4790, -5.4863],\n",
      "        ...,\n",
      "        [-2.9408, -3.8201, -0.8226,  ..., -5.5400, -5.5318, -5.5176],\n",
      "        [-2.7909, -3.8567, -0.8064,  ..., -5.3636, -5.3689, -5.3913],\n",
      "        [-2.4781, -3.8831, -0.7479,  ..., -4.6033, -4.5894, -4.5892]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4500,  0.9817,  0.9679, -0.4123,  0.7219,  0.7701,  0.9861, -0.3901],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4307, -1.2117], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(6.8886e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.3921e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0871, -0.0880, -0.0889, -0.0898, -0.0907, -0.0916, -0.0925, -0.0935,\n",
      "         -0.0944, -0.0954, -0.0963, -0.0973, -0.0983, -0.0993, -0.1003, -0.1013,\n",
      "         -0.1023, -0.1034, -0.1044, -0.1055, -0.1065, -0.1076, -0.1087, -0.1098,\n",
      "         -0.1109, -0.1120, -0.1132, -0.1143, -0.1155, -0.1166, -0.1178, -0.1190,\n",
      "         -0.1202, -0.1214, -0.1226, -0.1239, -0.1251, -0.1264, -0.1277, -0.1289,\n",
      "         -0.1302, -0.1316, -0.1329, -0.1342, -0.1356, -0.1370, -0.1383, -0.1397,\n",
      "         -0.1412, -0.1426, -0.1440, -0.1455, -0.1469, -0.1484, -0.1499, -0.1514,\n",
      "         -0.1530, -0.1545, -0.1561, -0.1577, -0.1592, -0.1609, -0.1625, -0.1641,\n",
      "         -0.1658, -0.1675, -0.1691, -0.1709, -0.1726, -0.1743, -0.1761, -0.1779,\n",
      "         -0.1797, -0.1815, -0.1833, -0.1852, -0.1870, -0.1889, -0.1908, -0.1928,\n",
      "         -0.1947, -0.1967, -0.1987, -0.2007, -0.2027, -0.2047, -0.2068, -0.2089,\n",
      "         -0.2110, -0.2131, -0.2153, -0.2175, -0.2197, -0.2219, -0.2241, -0.2264,\n",
      "         -0.2287, -0.2310, -0.2333, -0.2357, -0.2380, -0.2405, -0.2429, -0.2453,\n",
      "         -0.2478, -0.2503, -0.2528, -0.2554, -0.2580, -0.2606, -0.2632, -0.2659,\n",
      "         -0.2686, -0.2713, -0.2740, -0.2768, -0.2796, -0.2824, -0.2853, -0.2881,\n",
      "         -0.2910, -0.2940, -0.2970, -0.3000, -0.3030, -0.3060, -0.3091, -0.3123,\n",
      "         -0.3154, -0.3186, -0.3218, -0.3251, -0.3284, -0.3317, -0.3350, -0.3384,\n",
      "         -0.3418, -0.3453, -0.3488, -0.3523, -0.3558, -0.3594, -0.3631, -0.3667,\n",
      "         -0.3704, -0.3742, -0.3780, -0.3818, -0.3856, -0.3895, -0.3935, -0.3974,\n",
      "         -0.4015, -0.4055, -0.4096, -0.4137, -0.4179, -0.4221, -0.4264, -0.4307,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2476, -0.2501, -0.2526, -0.2552, -0.2578, -0.2604, -0.2630, -0.2656,\n",
      "         -0.2683, -0.2710, -0.2738, -0.2765, -0.2793, -0.2822, -0.2850, -0.2879,\n",
      "         -0.2908, -0.2937, -0.2967, -0.2997, -0.3027, -0.3058, -0.3089, -0.3120,\n",
      "         -0.3151, -0.3183, -0.3215, -0.3248, -0.3281, -0.3314, -0.3347, -0.3381,\n",
      "         -0.3415, -0.3450, -0.3485, -0.3520, -0.3555, -0.3591, -0.3627, -0.3664,\n",
      "         -0.3701, -0.3739, -0.3776, -0.3814, -0.3853, -0.3892, -0.3931, -0.3971,\n",
      "         -0.4011, -0.4052, -0.4092, -0.4134, -0.4176, -0.4218, -0.4260, -0.4303,\n",
      "         -0.4347, -0.4391, -0.4435, -0.4480, -0.4525, -0.4571, -0.4617, -0.4664,\n",
      "         -0.4711, -0.4758, -0.4806, -0.4855, -0.4904, -0.4954, -0.5004, -0.5054,\n",
      "         -0.5105, -0.5157, -0.5209, -0.5261, -0.5315, -0.5368, -0.5422, -0.5477,\n",
      "         -0.5533, -0.5588, -0.5645, -0.5702, -0.5760, -0.5818, -0.5876, -0.5936,\n",
      "         -0.5996, -0.6056, -0.6118, -0.6179, -0.6242, -0.6305, -0.6368, -0.6433,\n",
      "         -0.6498, -0.6563, -0.6630, -0.6697, -0.6764, -0.6833, -0.6902, -0.6971,\n",
      "         -0.7042, -0.7113, -0.7185, -0.7257, -0.7331, -0.7405, -0.7480, -0.7555,\n",
      "         -0.7631, -0.7708, -0.7786, -0.7865, -0.7944, -0.8025, -0.8106, -0.8188,\n",
      "         -0.8270, -0.8354, -0.8438, -0.8523, -0.8610, -0.8697, -0.8784, -0.8873,\n",
      "         -0.8963, -0.9053, -0.9145, -0.9237, -0.9330, -0.9425, -0.9520, -0.9616,\n",
      "         -0.9713, -0.9811, -0.9910, -1.0010, -1.0112, -1.0214, -1.0317, -1.0421,\n",
      "         -1.0526, -1.0633, -1.0740, -1.0849, -1.0958, -1.1069, -1.1181, -1.1294,\n",
      "         -1.1408, -1.1523, -1.1639, -1.1757, -1.1876, -1.1995, -1.2117,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4201, -1.1877], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4633, -3.8737, -0.7407, -2.2049,  0.1778, -2.3036, -4.5256, -0.2346,\n",
      "         -3.7982, -2.4373, -3.8611,  0.6714, -0.1104, -3.5084, -3.1122, -2.2609,\n",
      "         -3.2205, -1.7681, -1.9630, -0.9351, -1.3623, -1.3668, -0.8404,  0.4196,\n",
      "         -1.8822, -3.3599, -3.0722, -2.4671, -3.8227,  0.9764, -1.7182, -0.1528,\n",
      "         -0.8598, -3.4445, -1.9117, -1.7451, -0.9291, -3.1537, -1.5874, -0.0144,\n",
      "         -0.6744, -0.5422, -0.3309, -4.2371, -1.5893, -3.2372, -3.3962, -3.1601,\n",
      "          0.1107,  0.2571, -3.7688, -1.4487, -7.0143, -2.8273, -3.0296, -0.9271,\n",
      "         -2.5532, -2.8923, -2.5317, -1.5860, -3.5619, -1.1540, -5.7194, -2.3469,\n",
      "         -3.5693,  1.0888, -1.3431, -0.0306, -0.3288, -3.0707, -4.0071, -5.4982,\n",
      "         -2.7177, -0.1363, -3.7286, -4.1880, -2.4012, -3.8548, -2.4953, -3.4838,\n",
      "         -4.8203, -2.4305, -0.3541, -1.2571, -1.6938, -2.9457, -1.8110, -0.6289,\n",
      "         -3.5289, -0.8308, -3.0836, -4.0520, -2.3823, -0.6442, -0.8698, -2.9766,\n",
      "         -2.7319, -3.6141, -3.2672, -3.1912, -0.1113, -0.9766, -3.6861, -2.0873,\n",
      "         -0.4383, -1.8511, -3.4597, -3.7641,  0.5670, -1.2502, -1.3498, -0.6407,\n",
      "         -4.2543, -4.5764,  0.3854, -0.9515, -2.1122, -1.3130, -0.0977, -0.9428,\n",
      "         -2.8103, -4.5777, -2.6504, -0.8323, -1.1232, -2.7915, -2.2308, -0.7536,\n",
      "         -0.5730, -4.1832, -0.2840,  0.3233, -3.3315, -4.0838, -1.2005, -2.0079,\n",
      "         -3.0151, -2.3627, -1.6856, -2.9062, -2.6117, -3.3479, -5.6771, -1.2592,\n",
      "         -0.4069, -0.4563, -0.4245, -0.4534, -0.3918, -0.4330, -4.8448, -4.9210,\n",
      "         -5.3177, -4.9099, -4.8147, -4.5301, -4.4760, -4.5720, -4.5723, -4.5332,\n",
      "         -4.6122, -4.7031, -4.7697, -4.7654, -4.7014, -4.6170, -4.5708, -4.5530,\n",
      "         -4.5490, -4.5370, -4.5257, -4.5303, -4.5486, -4.5673, -4.5940, -4.6082,\n",
      "         -4.5871, -4.5518, -4.5463, -4.5432, -4.5534, -4.5856, -4.5944, -4.5880,\n",
      "         -4.5829, -4.5814, -4.5983, -4.6125, -4.6191, -4.6234, -4.6278, -4.6476,\n",
      "         -4.6644, -4.6736, -4.6702, -4.6660, -4.6634, -4.6518, -4.6459, -4.6470,\n",
      "         -4.6442, -4.6564, -4.6518, -4.6430, -4.6407],\n",
      "        [-2.3491, -3.8878, -0.7005, -2.1650,  0.2209, -2.2632, -4.4428, -0.1903,\n",
      "         -3.6706, -2.3673, -3.8153,  0.6779, -0.1054, -3.4673, -3.1243, -2.1952,\n",
      "         -3.1924, -1.7259, -1.9892, -0.8720, -1.3364, -1.3074, -0.7925,  0.3956,\n",
      "         -1.8577, -3.3110, -3.0709, -2.4139, -3.8363,  0.9591, -1.7049, -0.0817,\n",
      "         -0.8714, -3.4050, -1.8993, -1.7849, -0.9860, -3.1323, -1.5663,  0.0500,\n",
      "         -0.6659, -0.5817, -0.3169, -4.2678, -1.5621, -3.1176, -3.3280, -3.1228,\n",
      "         -0.0643,  0.0790, -3.7161, -1.4067, -7.0170, -2.7746, -3.0094, -0.8795,\n",
      "         -2.5565, -2.9137, -2.4853, -1.5678, -3.5149, -1.1097, -5.7202, -2.3158,\n",
      "         -3.5797,  1.0707, -1.3363,  0.0478, -0.3443, -3.0270, -3.9342, -5.4726,\n",
      "         -2.6648, -0.1629, -3.6744, -4.2080, -2.3518, -3.8393, -2.4726, -3.4322,\n",
      "         -4.7901, -2.3856, -0.3416, -1.1802, -1.6482, -2.8757, -1.7902, -0.5584,\n",
      "         -3.4467, -0.8188, -3.0258, -4.0635, -2.3400, -0.6346, -0.7881, -2.9833,\n",
      "         -2.6994, -3.5018, -3.2295, -3.1561, -0.0845, -0.8914, -3.6794, -2.0748,\n",
      "         -0.4517, -1.8020, -3.3581, -3.6997,  0.5382, -1.0787, -1.4601, -2.2850,\n",
      "         -4.2430, -4.5625,  0.3622, -0.8540, -2.1120, -1.2106,  0.0404, -0.8717,\n",
      "         -2.5174, -4.7576, -2.5876, -0.7879, -0.9601, -2.6667, -2.1844, -0.7407,\n",
      "         -0.4269, -4.1037, -0.5058,  0.1538, -3.3203, -4.0393, -1.1510, -2.0157,\n",
      "         -3.0524, -2.2867, -1.6568, -2.8227, -2.5853, -3.2731, -5.6855, -1.1867,\n",
      "         -0.7644, -0.4217, -1.1719, -1.1816, -1.2168, -4.8446, -4.8761, -5.3476,\n",
      "         -4.8289, -4.7131, -4.7814, -4.9284, -5.0155, -4.9255, -4.8017, -4.7093,\n",
      "         -4.6091, -4.5152, -4.4466, -4.3973, -4.3679, -4.3359, -4.3035, -4.2796,\n",
      "         -4.2618, -4.2522, -4.2434, -4.2469, -4.2675, -4.2945, -4.3275, -4.3687,\n",
      "         -4.4076, -4.4280, -4.4442, -4.4473, -4.4532, -4.4611, -4.4700, -4.4918,\n",
      "         -4.5112, -4.5260, -4.5424, -4.5499, -4.5420, -4.5218, -4.5023, -4.4956,\n",
      "         -4.5032, -4.4969, -4.4920, -4.4877, -4.4853, -4.4758, -4.4784, -4.4715,\n",
      "         -4.4674, -4.4680, -4.4602, -4.4540, -4.4548]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4276, -0.9513], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3828, -0.4129,  0.9900,  0.9900, -0.3934,  0.9900,  0.9900, -0.4253],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4738e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.7088e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0774, -0.0782, -0.0790,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0835, -0.0844, -0.0852,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0860, -0.0869, -0.0878,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3733, -0.4027,  0.9656,  0.9656, -0.3837,  0.9656,  0.9656, -0.4148],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4461, -3.8694, -0.7449,  ..., -4.6882, -4.6776, -4.6763],\n",
      "        [-2.4489, -3.8756, -0.7395,  ..., -4.5928, -4.5852, -4.5915],\n",
      "        [-2.9045, -3.8161, -0.8236,  ..., -5.5296, -5.5223, -5.5077],\n",
      "        ...,\n",
      "        [-2.7193, -3.8593, -0.7826,  ..., -5.2420, -5.3095, -5.3418],\n",
      "        [-2.7469, -3.8494, -0.8035,  ..., -5.3551, -5.3635, -5.3810],\n",
      "        [-2.4419, -3.8742, -0.7381,  ..., -4.6273, -4.6148, -4.6143]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4076, -0.4222,  0.7509,  0.9581, -0.3891,  0.9605,  0.8910, -0.4624],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4214, -0.3781,  0.9900,  0.9900, -0.4203, -1.2091,  0.9900, -1.2091],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.2984e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0853, -0.0861, -0.0870,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0765, -0.0773, -0.0780,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2471, -0.2496, -0.2521,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2471, -0.2496, -0.2521,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4110, -0.3688,  0.9656,  0.9656, -0.4100, -1.1851,  0.9656, -1.1851],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4268, -3.8817, -0.7464,  ..., -4.6341, -4.6268, -4.6276],\n",
      "        [-2.4238, -3.8774, -0.7480,  ..., -4.6531, -4.6391, -4.6318],\n",
      "        [-2.7283, -3.8546, -0.8092,  ..., -5.3303, -5.3362, -5.3564],\n",
      "        ...,\n",
      "        [-2.3023, -3.8914, -0.7014,  ..., -4.4577, -4.4518, -4.4518],\n",
      "        [-2.7359, -3.8519, -0.8003,  ..., -5.3349, -5.3405, -5.3616],\n",
      "        [-2.3023, -3.8914, -0.7014,  ..., -4.4577, -4.4518, -4.4518]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4399, -0.4329,  0.9371,  0.9123, -0.4068, -1.0149,  0.9427, -1.0149],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2148,  0.9900, -0.3781, -0.3876,  0.9900,  0.9900, -0.4368, -0.9566],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9072e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0370, -0.0374, -0.0378,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0765, -0.0773, -0.0780,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0884, -0.0893, -0.0902,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1859, -0.1878, -0.1897,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2095,  0.9656, -0.3688, -0.3780,  0.9656,  0.9656, -0.4260, -0.9147],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4979, -3.8582, -0.7543,  ..., -4.6864, -4.6938, -4.6960],\n",
      "        [-2.7161, -3.8581, -0.8143,  ..., -5.2606, -5.2681, -5.2962],\n",
      "        [-2.4023, -3.8755, -0.7525,  ..., -4.6908, -4.6774, -4.6756],\n",
      "        ...,\n",
      "        [-2.7034, -3.8566, -0.8127,  ..., -5.3221, -5.3274, -5.3492],\n",
      "        [-2.3894, -3.8796, -0.7453,  ..., -4.6182, -4.6062, -4.6024],\n",
      "        [-2.3518, -3.8757, -0.7246,  ..., -4.5519, -4.5573, -4.5733]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2219,  0.8527, -0.4259, -0.4005,  0.9121,  0.9121, -0.4382, -0.9566],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.2137, -1.2047,  0.9900, -0.3858, -0.4148,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7505e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0368, -0.0372, -0.0376,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0780, -0.0788, -0.0796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0839, -0.0848, -0.0856,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.2085, -1.1809,  0.9656, -0.3763, -0.4046,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6577, -3.8640, -0.7935,  ..., -5.2156, -5.2859, -5.3216],\n",
      "        [-2.6862, -3.8571, -0.8154,  ..., -5.3362, -5.3392, -5.3599],\n",
      "        [-2.4797, -3.8592, -0.7576,  ..., -4.6845, -4.6912, -4.6945],\n",
      "        ...,\n",
      "        [-2.3879, -3.8838, -0.7446,  ..., -4.5611, -4.5554, -4.5594],\n",
      "        [-2.3746, -3.8854, -0.7529,  ..., -4.6066, -4.5932, -4.5916],\n",
      "        [-2.6957, -3.8573, -0.8137,  ..., -5.3191, -5.3238, -5.3509]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8988,  0.8864, -0.2210, -1.0781,  0.9228, -0.4033, -0.4167,  0.9223],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4318, -0.4127,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.2116],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1142e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0874, -0.0882, -0.0891,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0835, -0.0843, -0.0852,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0364, -0.0368, -0.0372,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4212, -0.4025,  0.9656,  0.9656,  0.9608,  0.9656,  0.9656, -0.2064],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3592, -3.8820, -0.7493,  ..., -4.6187, -4.6072, -4.6005],\n",
      "        [-2.3728, -3.8844, -0.7531,  ..., -4.6346, -4.6276, -4.6269],\n",
      "        [-2.6181, -3.8539, -0.7822,  ..., -5.4177, -5.4670, -5.4711],\n",
      "        ...,\n",
      "        [-2.6797, -3.8565, -0.8075,  ..., -5.3266, -5.3340, -5.3510],\n",
      "        [-2.6712, -3.8587, -0.8147,  ..., -5.3327, -5.3362, -5.3588],\n",
      "        [-2.4634, -3.8608, -0.7584,  ..., -4.6848, -4.6930, -4.6947]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4426, -0.4549,  0.8703,  0.8749,  0.7916,  0.8873,  0.8703, -0.2188],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4195, -0.4195, -0.1972,  0.9900, -1.0113, -0.4021,  0.9900, -0.4136],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(9.1037e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0849, -0.0857, -0.0866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0849, -0.0857, -0.0866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0340, -0.0343, -0.0347,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0813, -0.0822, -0.0830,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0837, -0.0845, -0.0854,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4092, -0.4092, -0.1924,  0.9656, -0.9670, -0.3921,  0.9656, -0.4034],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3586, -3.8799, -0.7487,  ..., -4.6564, -4.6435, -4.6379],\n",
      "        [-2.3586, -3.8799, -0.7487,  ..., -4.6564, -4.6435, -4.6379],\n",
      "        [-2.4578, -3.8624, -0.7540,  ..., -4.6230, -4.6334, -4.6335],\n",
      "        ...,\n",
      "        [-2.3686, -3.8834, -0.7520,  ..., -4.5925, -4.5822, -4.5883],\n",
      "        [-2.8221, -3.8257, -0.8427,  ..., -5.5787, -5.5736, -5.5629],\n",
      "        [-2.3620, -3.8827, -0.7504,  ..., -4.6287, -4.6163, -4.6121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4560, -0.4560, -0.2423,  0.6206, -0.9109, -0.4492,  0.6347, -0.4929],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3740,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5883e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(2.7401e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0757, -0.0764, -0.0772, -0.0780, -0.0788, -0.0796, -0.0804, -0.0812,\n",
      "         -0.0820, -0.0828, -0.0837, -0.0845, -0.0854, -0.0862, -0.0871, -0.0880,\n",
      "         -0.0889, -0.0898, -0.0907, -0.0916, -0.0925, -0.0934, -0.0944, -0.0953,\n",
      "         -0.0963, -0.0973, -0.0983, -0.0993, -0.1003, -0.1013, -0.1023, -0.1033,\n",
      "         -0.1044, -0.1054, -0.1065, -0.1076, -0.1087, -0.1097, -0.1109, -0.1120,\n",
      "         -0.1131, -0.1143, -0.1154, -0.1166, -0.1177, -0.1189, -0.1201, -0.1214,\n",
      "         -0.1226, -0.1238, -0.1251, -0.1263, -0.1276, -0.1289, -0.1302, -0.1315,\n",
      "         -0.1328, -0.1342, -0.1355, -0.1369, -0.1383, -0.1397, -0.1411, -0.1425,\n",
      "         -0.1440, -0.1454, -0.1469, -0.1484, -0.1499, -0.1514, -0.1529, -0.1545,\n",
      "         -0.1560, -0.1576, -0.1592, -0.1608, -0.1624, -0.1641, -0.1657, -0.1674,\n",
      "         -0.1691, -0.1708, -0.1725, -0.1743, -0.1760, -0.1778, -0.1796, -0.1814,\n",
      "         -0.1832, -0.1851, -0.1870, -0.1888, -0.1908, -0.1927, -0.1946, -0.1966,\n",
      "         -0.1986, -0.2006, -0.2026, -0.2047, -0.2067, -0.2088, -0.2109, -0.2130,\n",
      "         -0.2152, -0.2174, -0.2196, -0.2218, -0.2240, -0.2263, -0.2286, -0.2309,\n",
      "         -0.2332, -0.2356, -0.2380, -0.2404, -0.2428, -0.2452, -0.2477, -0.2502,\n",
      "         -0.2527, -0.2553, -0.2579, -0.2605, -0.2631, -0.2658, -0.2685, -0.2712,\n",
      "         -0.2739, -0.2767, -0.2795, -0.2823, -0.2851, -0.2880, -0.2909, -0.2939,\n",
      "         -0.2968, -0.2998, -0.3029, -0.3059, -0.3090, -0.3121, -0.3153, -0.3185,\n",
      "         -0.3217, -0.3249, -0.3282, -0.3315, -0.3349, -0.3383, -0.3417, -0.3451,\n",
      "         -0.3486, -0.3521, -0.3557, -0.3593, -0.3629, -0.3666, -0.3703, -0.3740,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,\n",
      "          0.2149,  0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,\n",
      "          0.2329,  0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,\n",
      "          0.2524,  0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,\n",
      "          0.2735,  0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,\n",
      "          0.2964,  0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,\n",
      "          0.3212,  0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,\n",
      "          0.3481,  0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,\n",
      "          0.3772,  0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,\n",
      "          0.4088,  0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,\n",
      "          0.4430,  0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,\n",
      "          0.4801,  0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,\n",
      "          0.5203,  0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,\n",
      "          0.5639,  0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,\n",
      "          0.6111,  0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,\n",
      "          0.6623,  0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,\n",
      "          0.7177,  0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,\n",
      "          0.7778,  0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,\n",
      "          0.8429,  0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,\n",
      "          0.9135,  0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,\n",
      "          0.9900,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3648,  0.9608], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3743e+00, -3.8822e+00, -7.6046e-01, -2.2063e+00,  2.0282e-01,\n",
      "         -2.3045e+00, -4.5037e+00, -1.9093e-01, -3.6969e+00, -2.3920e+00,\n",
      "         -3.8225e+00,  6.7001e-01, -6.2298e-02, -3.4907e+00, -3.1510e+00,\n",
      "         -2.2528e+00, -3.2033e+00, -1.7341e+00, -2.0141e+00, -8.6643e-01,\n",
      "         -1.3170e+00, -1.3263e+00, -7.9591e-01,  3.8705e-01, -1.8320e+00,\n",
      "         -3.3350e+00, -3.1150e+00, -2.4654e+00, -3.8319e+00,  9.5408e-01,\n",
      "         -1.6780e+00, -8.5613e-02, -8.7936e-01, -3.4151e+00, -1.8908e+00,\n",
      "         -1.7633e+00, -9.4614e-01, -3.1414e+00, -1.5745e+00,  3.3420e-02,\n",
      "         -6.4001e-01, -5.6028e-01, -3.4120e-01, -4.2915e+00, -1.5428e+00,\n",
      "         -3.1356e+00, -3.3460e+00, -3.1301e+00, -3.8447e-02, -5.3526e-03,\n",
      "         -3.7500e+00, -1.4427e+00, -7.0616e+00, -2.8305e+00, -3.0274e+00,\n",
      "         -8.9478e-01, -2.5903e+00, -2.9023e+00, -2.5277e+00, -1.5425e+00,\n",
      "         -3.5413e+00, -1.1471e+00, -5.7750e+00, -2.3617e+00, -3.5864e+00,\n",
      "          1.0622e+00, -1.3084e+00,  3.1692e-02, -3.5721e-01, -3.0422e+00,\n",
      "         -3.9770e+00, -5.5184e+00, -2.7296e+00, -1.5730e-01, -3.7136e+00,\n",
      "         -4.2213e+00, -2.4133e+00, -3.8498e+00, -2.4857e+00, -3.4689e+00,\n",
      "         -4.8425e+00, -2.4457e+00, -3.4318e-01, -1.1918e+00, -1.7138e+00,\n",
      "         -2.9298e+00, -1.7718e+00, -5.6327e-01, -3.5185e+00, -8.0063e-01,\n",
      "         -3.0646e+00, -4.0849e+00, -2.3962e+00, -6.3409e-01, -8.0161e-01,\n",
      "         -3.0004e+00, -2.6820e+00, -3.5201e+00, -3.2521e+00, -3.1859e+00,\n",
      "         -9.5683e-02, -9.1046e-01, -3.7131e+00, -2.1235e+00, -4.6600e-01,\n",
      "         -1.8158e+00, -3.3710e+00, -3.7176e+00,  5.4592e-01, -8.2283e-01,\n",
      "         -1.2935e+00, -4.5258e-01, -4.2984e+00, -4.5736e+00,  3.7209e-01,\n",
      "         -8.4077e-01, -2.0785e+00, -1.2704e+00, -6.9642e-03, -8.9184e-01,\n",
      "         -2.6534e+00, -4.5712e+00, -2.6604e+00, -8.8246e-01, -1.0514e+00,\n",
      "         -2.7615e+00, -2.1739e+00, -7.4978e-01, -4.8442e-01, -4.1370e+00,\n",
      "         -3.8949e-01,  5.2549e-02, -3.3035e+00, -4.0808e+00, -1.1605e+00,\n",
      "         -2.0584e+00, -3.0445e+00, -2.3608e+00, -1.6673e+00, -2.8691e+00,\n",
      "         -2.5778e+00, -3.2250e+00, -5.7869e+00, -1.2833e+00, -3.8103e-01,\n",
      "         -3.9911e-01, -4.1943e-01, -4.8740e-01, -5.2360e-01, -4.1218e-01,\n",
      "         -4.8007e+00, -4.9056e+00, -5.3025e+00, -4.9041e+00, -4.8185e+00,\n",
      "         -4.5572e+00, -4.4901e+00, -4.5653e+00, -4.5266e+00, -4.4624e+00,\n",
      "         -4.5893e+00, -4.7479e+00, -4.8408e+00, -4.8350e+00, -4.7410e+00,\n",
      "         -4.6162e+00, -4.5388e+00, -4.5076e+00, -4.5061e+00, -4.5041e+00,\n",
      "         -4.5024e+00, -4.5234e+00, -4.5524e+00, -4.5685e+00, -4.5837e+00,\n",
      "         -4.5893e+00, -4.5666e+00, -4.5458e+00, -4.5583e+00, -4.5622e+00,\n",
      "         -4.5725e+00, -4.5984e+00, -4.5984e+00, -4.5824e+00, -4.5703e+00,\n",
      "         -4.5653e+00, -4.5844e+00, -4.6053e+00, -4.6169e+00, -4.6267e+00,\n",
      "         -4.6333e+00, -4.6562e+00, -4.6712e+00, -4.6750e+00, -4.6722e+00,\n",
      "         -4.6675e+00, -4.6723e+00, -4.6655e+00, -4.6588e+00, -4.6531e+00,\n",
      "         -4.6572e+00, -4.6611e+00, -4.6560e+00, -4.6371e+00, -4.6295e+00],\n",
      "        [-2.8146e+00, -3.8292e+00, -8.4809e-01, -2.4496e+00,  1.1691e-01,\n",
      "         -2.3721e+00, -4.6750e+00, -5.4927e-01, -4.1873e+00, -2.6290e+00,\n",
      "         -3.9189e+00,  6.4743e-01, -1.5592e-01, -3.6585e+00, -3.0252e+00,\n",
      "         -2.5044e+00, -3.3222e+00, -1.8535e+00, -1.9486e+00, -1.0829e+00,\n",
      "         -1.3988e+00, -1.5570e+00, -9.6354e-01,  4.7614e-01, -2.0926e+00,\n",
      "         -3.5220e+00, -3.0275e+00, -2.6784e+00, -3.8116e+00,  9.9021e-01,\n",
      "         -1.8439e+00, -3.3242e-01, -8.7260e-01, -3.4954e+00, -1.9031e+00,\n",
      "         -1.6346e+00, -8.4460e-01, -3.2096e+00, -1.7888e+00, -1.7212e-01,\n",
      "         -7.3067e-01, -4.8783e-01, -3.9792e-01, -4.2108e+00, -1.7664e+00,\n",
      "         -3.5535e+00, -3.6226e+00, -3.2050e+00,  5.8983e-01,  4.9454e-01,\n",
      "         -3.8799e+00, -1.5271e+00, -7.0768e+00, -2.9857e+00, -3.0689e+00,\n",
      "         -1.0395e+00, -2.5771e+00, -2.9186e+00, -2.6395e+00, -1.7454e+00,\n",
      "         -3.7004e+00, -1.2320e+00, -5.8155e+00, -2.4771e+00, -3.5662e+00,\n",
      "          1.1212e+00, -1.4608e+00, -2.2357e-01, -3.1757e-01, -3.1270e+00,\n",
      "         -4.1672e+00, -5.5368e+00, -2.8782e+00, -7.4335e-02, -3.8566e+00,\n",
      "         -4.1200e+00, -2.5568e+00, -3.8794e+00, -2.6536e+00, -3.6009e+00,\n",
      "         -4.8595e+00, -2.5797e+00, -4.1779e-01, -1.4192e+00, -1.8079e+00,\n",
      "         -3.1208e+00, -1.9473e+00, -7.9166e-01, -3.6304e+00, -9.1584e-01,\n",
      "         -3.2289e+00, -3.9995e+00, -2.5250e+00, -6.8623e-01, -1.0404e+00,\n",
      "         -2.9272e+00, -2.8759e+00, -3.8712e+00, -3.3045e+00, -3.3138e+00,\n",
      "         -1.9247e-01, -1.1375e+00, -3.6658e+00, -2.1430e+00, -3.9603e-01,\n",
      "         -2.0421e+00, -3.7093e+00, -3.8785e+00,  6.3966e-01, -2.1708e+00,\n",
      "         -1.2349e+00, -1.2439e+00, -4.6059e+00, -4.7783e+00,  3.7422e-01,\n",
      "         -9.5347e-01, -2.1581e+00, -1.3292e+00, -1.1427e-01, -1.0102e+00,\n",
      "         -2.8375e+00, -4.7490e+00, -2.5712e+00, -7.7124e-01, -9.3455e-01,\n",
      "         -2.7904e+00, -2.3087e+00, -8.6743e-01, -7.2805e-01, -4.1351e+00,\n",
      "          2.7712e-01,  6.0345e-01, -3.3608e+00, -4.1811e+00, -1.2613e+00,\n",
      "         -2.0754e+00, -3.0299e+00, -2.4415e+00, -1.7407e+00, -2.7674e+00,\n",
      "         -2.4813e+00, -3.5447e+00, -5.7728e+00, -1.4781e+00,  4.9113e-01,\n",
      "          1.6304e-01,  1.0054e+00,  9.9078e-01,  9.6129e-01,  9.6742e-01,\n",
      "          1.0152e+00, -4.6295e+00, -4.8632e+00, -5.1267e+00, -4.8759e+00,\n",
      "         -5.0142e+00, -4.9566e+00, -5.1481e+00, -5.2246e+00, -5.2109e+00,\n",
      "         -5.1458e+00, -5.1237e+00, -5.1212e+00, -5.1352e+00, -5.1604e+00,\n",
      "         -5.2073e+00, -5.2611e+00, -5.2995e+00, -5.3308e+00, -5.3418e+00,\n",
      "         -5.3571e+00, -5.3637e+00, -5.3822e+00, -5.3955e+00, -5.4333e+00,\n",
      "         -5.4676e+00, -5.5021e+00, -5.5283e+00, -5.5472e+00, -5.5497e+00,\n",
      "         -5.5540e+00, -5.5459e+00, -5.5379e+00, -5.5499e+00, -5.5732e+00,\n",
      "         -5.5849e+00, -5.5912e+00, -5.5937e+00, -5.5929e+00, -5.5874e+00,\n",
      "         -5.5724e+00, -5.5595e+00, -5.5496e+00, -5.5403e+00, -5.5315e+00,\n",
      "         -5.5419e+00, -5.5481e+00, -5.5353e+00, -5.5364e+00, -5.5378e+00,\n",
      "         -5.5421e+00, -5.5447e+00, -5.5356e+00, -5.5256e+00, -5.5131e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4371,  0.7992], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4262,  0.9900,  0.9900,  0.9900, -0.4257, -0.3959,  0.9900, -0.4455],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4468e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0862, -0.0871, -0.0880,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0801, -0.0809, -0.0817,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0901, -0.0910, -0.0920,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4157,  0.9656,  0.9656,  0.9656, -0.4152, -0.3862,  0.9656, -0.4345],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3857, -3.8809, -0.7604,  ..., -4.6238, -4.6123, -4.6090],\n",
      "        [-2.6602, -3.8644, -0.8066,  ..., -5.1999, -5.2698, -5.3065],\n",
      "        [-2.6903, -3.8570, -0.8264,  ..., -5.3328, -5.3380, -5.3597],\n",
      "        ...,\n",
      "        [-2.3955, -3.8847, -0.7597,  ..., -4.5495, -4.5452, -4.5464],\n",
      "        [-2.6854, -3.8542, -0.8274,  ..., -5.3432, -5.3483, -5.3684],\n",
      "        [-2.3875, -3.8787, -0.7679,  ..., -4.6376, -4.6280, -4.6286]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4728,  0.9200,  0.8970,  0.8951, -0.4348, -0.3866,  0.8135, -0.4386],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4009,  0.9900, -0.4018,  0.9900, -0.4504,  0.9900,  0.9900, -0.3869],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4315e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0811, -0.0819, -0.0827,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0813, -0.0821, -0.0829,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0783, -0.0791, -0.0799,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3910,  0.9656, -0.3919,  0.9608, -0.4393,  0.9656,  0.9656, -0.3773],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4104, -3.8801, -0.7714,  ..., -4.6453, -4.6307, -4.6324],\n",
      "        [-2.7204, -3.8601, -0.8371,  ..., -5.3694, -5.3761, -5.3998],\n",
      "        [-2.4151, -3.8825, -0.7644,  ..., -4.5447, -4.5401, -4.5405],\n",
      "        ...,\n",
      "        [-2.8588, -3.8282, -0.8559,  ..., -5.6102, -5.6066, -5.5973],\n",
      "        [-2.7048, -3.8532, -0.8324,  ..., -5.3429, -5.3507, -5.3697],\n",
      "        [-2.4096, -3.8797, -0.7716,  ..., -4.6472, -4.6328, -4.6234]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3803,  0.9725, -0.3741,  0.8480, -0.4079,  0.7022,  0.8397, -0.4073],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.0275,  0.9900, -0.2191, -0.4531, -0.4341, -1.2282,  0.9900, -0.3927],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2811e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1196, -0.1208, -0.1220,  ..., -1.0070, -1.0172, -1.0275],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0377, -0.0381, -0.0385,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2510, -0.2535, -0.2561,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0795, -0.0803, -0.0811,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9824,  0.9656, -0.2137, -0.4419, -0.4234, -1.2038,  0.9656, -0.3830],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5258, -3.8167, -0.7917,  ..., -0.9168, -0.9790, -0.8597],\n",
      "        [-2.8837, -3.8262, -0.8589,  ..., -5.6083, -5.6046, -5.5945],\n",
      "        [-2.5272, -3.8586, -0.7722,  ..., -4.6251, -4.6329, -4.6356],\n",
      "        ...,\n",
      "        [-2.3040, -3.8883, -0.7245,  ..., -4.4402, -4.4333, -4.4327],\n",
      "        [-2.8864, -3.8210, -0.8550,  ..., -5.5686, -5.5643, -5.5523],\n",
      "        [-2.4340, -3.8710, -0.7782,  ..., -4.6821, -4.6677, -4.6646]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9210,  0.7415, -0.2211, -0.3958, -0.4079, -0.9769,  0.7518, -0.3853],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4462, -0.4300,  0.9900, -0.4390, -0.4102,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6659e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0870, -0.0879, -0.0888,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4352, -0.4194,  0.9656, -0.4282, -0.4001,  0.9656,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4445, -3.8694, -0.7717,  ..., -4.6442, -4.6343, -4.6286],\n",
      "        [-2.4552, -3.8737, -0.7734,  ..., -4.5791, -4.5727, -4.5746],\n",
      "        [-2.9077, -3.8182, -0.8580,  ..., -5.5663, -5.5631, -5.5518],\n",
      "        ...,\n",
      "        [-2.7245, -3.8584, -0.8165,  ..., -5.2078, -5.2730, -5.3104],\n",
      "        [-2.7567, -3.8547, -0.8344,  ..., -5.3371, -5.3413, -5.3608],\n",
      "        [-2.8964, -3.8206, -0.8603,  ..., -5.5236, -5.5149, -5.5029]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3976, -0.3967,  0.7879, -0.3574, -0.3575,  1.0046,  0.9978,  0.9007],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.4445, -0.4445,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.2520e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.4335, -0.4335,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7868, -3.8505, -0.8424,  ..., -5.2476, -5.2588, -5.2863],\n",
      "        [-2.7812, -3.8474, -0.8304,  ..., -5.3222, -5.3293, -5.3517],\n",
      "        [-2.7221, -3.8445, -0.8069,  ..., -5.4169, -5.4604, -5.4637],\n",
      "        ...,\n",
      "        [-2.7775, -3.8489, -0.8427,  ..., -5.2993, -5.3078, -5.3278],\n",
      "        [-2.7726, -3.8490, -0.8399,  ..., -5.3160, -5.3231, -5.3481],\n",
      "        [-2.7775, -3.8489, -0.8427,  ..., -5.2993, -5.3078, -5.3278]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9809,  1.0377,  1.0133, -0.4245, -0.4245,  1.0272,  1.0404,  1.0272],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4362,  0.9900,  0.9900,  0.9900,  0.9900, -0.8633,  0.9900, -0.4448],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3550e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0883, -0.0891, -0.0900,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1192, -0.1204, -0.1216,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0900, -0.0909, -0.0918,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4255,  0.9656,  0.9656,  0.9656,  0.9656, -0.8255,  0.9656, -0.4338],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4740, -3.8719, -0.7788,  ..., -4.5888, -4.5771, -4.5731],\n",
      "        [-2.9410, -3.8120, -0.8541,  ..., -5.4972, -5.4893, -5.4737],\n",
      "        [-2.7883, -3.8507, -0.8342,  ..., -5.3264, -5.3324, -5.3539],\n",
      "        ...,\n",
      "        [-2.5741, -3.8203, -0.7833,  ..., -4.1036, -4.0885, -4.0701],\n",
      "        [-2.7485, -3.8481, -0.8271,  ..., -5.1357, -5.1972, -5.2299],\n",
      "        [-2.4747, -3.8656, -0.7714,  ..., -4.6417, -4.6290, -4.6252]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3413,  0.8568,  1.0361,  1.0528,  0.8568, -0.8925,  1.0180, -0.3834],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.2268], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(3.3425e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(1.9724e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0391, -0.0395, -0.0399, -0.0403, -0.0407, -0.0411, -0.0415, -0.0419,\n",
      "         -0.0423, -0.0428, -0.0432, -0.0436, -0.0441, -0.0445, -0.0450, -0.0454,\n",
      "         -0.0459, -0.0463, -0.0468, -0.0473, -0.0478, -0.0482, -0.0487, -0.0492,\n",
      "         -0.0497, -0.0502, -0.0507, -0.0512, -0.0518, -0.0523, -0.0528, -0.0533,\n",
      "         -0.0539, -0.0544, -0.0550, -0.0555, -0.0561, -0.0567, -0.0572, -0.0578,\n",
      "         -0.0584, -0.0590, -0.0596, -0.0602, -0.0608, -0.0614, -0.0620, -0.0626,\n",
      "         -0.0633, -0.0639, -0.0646, -0.0652, -0.0659, -0.0665, -0.0672, -0.0679,\n",
      "         -0.0686, -0.0693, -0.0700, -0.0707, -0.0714, -0.0721, -0.0728, -0.0736,\n",
      "         -0.0743, -0.0751, -0.0758, -0.0766, -0.0774, -0.0781, -0.0789, -0.0797,\n",
      "         -0.0805, -0.0814, -0.0822, -0.0830, -0.0838, -0.0847, -0.0855, -0.0864,\n",
      "         -0.0873, -0.0882, -0.0891, -0.0900, -0.0909, -0.0918, -0.0927, -0.0936,\n",
      "         -0.0946, -0.0955, -0.0965, -0.0975, -0.0985, -0.0995, -0.1005, -0.1015,\n",
      "         -0.1025, -0.1035, -0.1046, -0.1056, -0.1067, -0.1078, -0.1089, -0.1100,\n",
      "         -0.1111, -0.1122, -0.1133, -0.1145, -0.1156, -0.1168, -0.1180, -0.1192,\n",
      "         -0.1204, -0.1216, -0.1228, -0.1241, -0.1253, -0.1266, -0.1279, -0.1292,\n",
      "         -0.1305, -0.1318, -0.1331, -0.1345, -0.1358, -0.1372, -0.1386, -0.1400,\n",
      "         -0.1414, -0.1428, -0.1443, -0.1457, -0.1472, -0.1487, -0.1502, -0.1517,\n",
      "         -0.1532, -0.1548, -0.1563, -0.1579, -0.1595, -0.1611, -0.1628, -0.1644,\n",
      "         -0.1661, -0.1677, -0.1694, -0.1711, -0.1729, -0.1746, -0.1764, -0.1782,\n",
      "         -0.1800, -0.1818, -0.1836, -0.1855, -0.1873, -0.1892, -0.1911, -0.1931,\n",
      "         -0.1950, -0.1970, -0.1990, -0.2010, -0.2030, -0.2051, -0.2072, -0.2092,\n",
      "         -0.2114, -0.2135, -0.2156, -0.2178, -0.2200, -0.2222, -0.2245, -0.2268,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.2212], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7998, -3.8460, -0.8331, -2.4518,  0.0749, -2.4040, -4.6445, -0.5286,\n",
      "         -4.1754, -2.5979, -3.9343,  0.7142, -0.1414, -3.5944, -2.9917, -2.4285,\n",
      "         -3.3032, -1.8730, -1.9237, -1.1281, -1.4144, -1.5787, -1.0005,  0.4642,\n",
      "         -2.1013, -3.4740, -2.9821, -2.6139, -3.8443,  0.9798, -1.8693, -0.3707,\n",
      "         -0.8459, -3.5114, -1.9197, -1.5884, -0.8737, -3.2069, -1.7701, -0.2061,\n",
      "         -0.7422, -0.5098, -0.4042, -4.2046, -1.7700, -3.5654, -3.5980, -3.2283,\n",
      "          0.6952,  0.7133, -3.8238, -1.5144, -7.0276, -2.9170, -3.0630, -1.0650,\n",
      "         -2.5459, -2.9493, -2.6413, -1.7621, -3.6576, -1.2164, -5.7559, -2.4097,\n",
      "         -3.5986,  1.1059, -1.4877, -0.2651, -0.2927, -3.1453, -4.1072, -5.4486,\n",
      "         -2.8053, -0.0474, -3.8057, -4.0825, -2.4877, -3.8818, -2.6445, -3.5520,\n",
      "         -4.7713, -2.5049, -0.4116, -1.4585, -1.7592, -3.1171, -1.9695, -0.8341,\n",
      "         -3.6223, -0.9291, -3.1701, -3.9602, -2.4558, -0.6876, -1.0808, -2.9348,\n",
      "         -2.8983, -3.8925, -3.3286, -3.3031, -0.1886, -1.1757, -3.6688, -2.0798,\n",
      "         -0.4016, -2.0452, -3.7211, -3.8503,  0.6724, -1.1920, -0.9269, -0.9977,\n",
      "         -4.5914, -4.5725,  0.3214, -1.1838, -2.1764, -1.4893, -0.2408, -0.9761,\n",
      "         -3.0435, -4.5362, -2.7116, -0.8702, -1.2332, -2.8373, -2.3615, -0.8785,\n",
      "         -0.8569, -4.2055,  0.2936,  0.8091, -3.3794, -4.1070, -1.3329, -2.0155,\n",
      "         -3.0683, -2.4327, -1.8034, -2.8704, -3.0638, -5.8201, -1.7109, -2.2366,\n",
      "         -1.1222, -0.5844, -0.3614, -4.2045, -3.8628, -0.0872, -2.0189, -0.2507,\n",
      "         -0.5746, -0.3445,  0.2133, -0.3724, -4.0268, -3.4262, -4.9143, -1.4296,\n",
      "          1.1114,  1.2652,  0.9448,  1.0902,  1.0075,  1.0237, -4.6651, -4.8553,\n",
      "         -5.1861, -5.0066, -5.2221, -5.3123, -5.3124, -5.3034, -5.2828, -5.2055,\n",
      "         -5.0806, -4.9744, -4.9163, -4.8895, -4.9006, -4.9203, -4.9602, -5.0116,\n",
      "         -5.0435, -5.0626, -5.0824, -5.1266, -5.2010, -5.2965, -5.3665, -5.3984,\n",
      "         -5.4014, -5.4139, -5.4540, -5.5030, -5.5055, -5.4799, -5.4422, -5.3944,\n",
      "         -5.3589, -5.3234, -5.3031, -5.3112, -5.3378],\n",
      "        [-2.5849, -3.8454, -0.7812, -2.3087,  0.1643, -2.3394, -4.5744, -0.3006,\n",
      "         -3.9283, -2.4379, -3.8478,  0.7110, -0.0990, -3.5090, -3.0897, -2.2939,\n",
      "         -3.2394, -1.7740, -1.9978, -0.9870, -1.3502, -1.4198, -0.8759,  0.3934,\n",
      "         -1.9415, -3.3704, -3.0469, -2.5032, -3.8533,  0.9570, -1.7669, -0.2099,\n",
      "         -0.8748, -3.4387, -1.8997, -1.6828, -0.9588, -3.1692, -1.6493, -0.0674,\n",
      "         -0.6765, -0.5756, -0.3648, -4.2655, -1.6369, -3.3440, -3.4236, -3.1590,\n",
      "          0.3174,  0.4305, -3.7514, -1.4479, -7.0327, -2.8281, -3.0368, -0.9543,\n",
      "         -2.5663, -2.9392, -2.5570, -1.6339, -3.5644, -1.1467, -5.7455, -2.3462,\n",
      "         -3.5995,  1.0768, -1.3957, -0.0979, -0.3408, -3.0700, -3.9890, -5.4237,\n",
      "         -2.7182, -0.1028, -3.7224, -4.1327, -2.4005, -3.8553, -2.5402, -3.4746,\n",
      "         -4.7470, -2.4238, -0.3622, -1.3038, -1.6754, -2.9943, -1.8557, -0.6778,\n",
      "         -3.5253, -0.8492, -3.0732, -3.9996, -2.3799, -0.6501, -0.9168, -2.9820,\n",
      "         -2.7667, -3.6995, -3.2779, -3.2329, -0.1202, -1.0198, -3.6983, -2.0607,\n",
      "         -0.4601, -1.8804, -3.5399, -3.7508,  0.6123, -1.4436, -1.2819, -0.8469,\n",
      "         -4.4048, -4.6306,  0.4015, -0.9739, -2.0929, -1.2964, -0.0527, -0.8886,\n",
      "         -2.7985, -4.6236, -2.6631, -0.8350, -1.0280, -2.7083, -2.2247, -0.8083,\n",
      "         -0.5660, -4.1098, -0.1052,  0.4958, -3.3116, -4.1020, -1.2402, -2.0368,\n",
      "         -3.0786, -2.3599, -1.7224, -2.9007, -2.9333, -5.8024, -1.5490, -2.3423,\n",
      "         -1.2862, -0.7354, -0.4400, -4.0731, -3.9314, -0.1696, -1.9955, -0.6793,\n",
      "         -1.0264, -0.3920, -0.1272, -0.6438, -4.0210, -3.3018, -4.9901, -1.1957,\n",
      "         -0.1499, -0.1612, -0.2392, -0.1753, -0.1550, -0.2369, -4.8177, -4.9075,\n",
      "         -5.2089, -4.7613, -5.0363, -5.0553, -4.9300, -4.9762, -5.0390, -5.1557,\n",
      "         -5.2824, -5.2484, -5.0435, -4.8702, -4.7680, -4.6812, -4.6174, -4.5723,\n",
      "         -4.5062, -4.4517, -4.4435, -4.4714, -4.5077, -4.5366, -4.5463, -4.5511,\n",
      "         -4.5738, -4.6140, -4.6558, -4.6863, -4.6889, -4.6658, -4.6391, -4.6173,\n",
      "         -4.6190, -4.6413, -4.6551, -4.6647, -4.6660]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0738, -0.1863], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4301, -1.0248, -0.3863,  0.9900, -0.3974, -0.2195,  0.9900, -0.3982],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5862e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0870, -0.0879, -0.0888,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1193, -0.1205, -0.1217,  ..., -1.0044, -1.0145, -1.0248],\n",
      "        [-0.0781, -0.0789, -0.0797,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0378, -0.0382, -0.0386,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0805, -0.0814, -0.0822,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4195, -0.9799, -0.3767,  0.9656, -0.3876, -0.2141,  0.9656, -0.3883],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4821, -3.8654, -0.7632,  ..., -4.6025, -4.5922, -4.5883],\n",
      "        [-2.5882, -3.8049, -0.7910,  ..., -0.9218, -0.9798, -0.8953],\n",
      "        [-2.4856, -3.8607, -0.7722,  ..., -4.6652, -4.6513, -4.6500],\n",
      "        ...,\n",
      "        [-2.5847, -3.8445, -0.7774,  ..., -4.6463, -4.6565, -4.6561],\n",
      "        [-2.7865, -3.8451, -0.8296,  ..., -5.2929, -5.2947, -5.3203],\n",
      "        [-2.4866, -3.8661, -0.7678,  ..., -4.6317, -4.6194, -4.6241]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4103, -0.9320, -0.3510,  1.0525, -0.3335, -0.1912,  1.0525, -0.3375],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2006,  0.9900, -0.4428,  0.9900, -0.4217,  0.9900,  0.9900, -0.4248],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7966e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.1425e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0346, -0.0349, -0.0353,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0896, -0.0905, -0.0914,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0859, -0.0868, -0.0877,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.1957,  0.9656, -0.4319,  0.9656, -0.4113,  0.9656,  0.9656, -0.4143],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5822, -3.8459, -0.7660,  ..., -4.5818, -4.5907, -4.5913],\n",
      "        [-2.7354, -3.8391, -0.7943,  ..., -5.4346, -5.4812, -5.4875],\n",
      "        [-2.4788, -3.8632, -0.7677,  ..., -4.6220, -4.6134, -4.6180],\n",
      "        ...,\n",
      "        [-2.9370, -3.8183, -0.8486,  ..., -5.6166, -5.6110, -5.6008],\n",
      "        [-2.7954, -3.8504, -0.8311,  ..., -5.3514, -5.3560, -5.3785],\n",
      "        [-2.4803, -3.8654, -0.7617,  ..., -4.6007, -4.5887, -4.5881]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2299,  1.0123, -0.3840,  0.9950, -0.3324,  0.8490,  1.0532, -0.4115],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4383,  0.9900, -0.4203,  0.9900,  0.9900, -0.3746, -0.4383],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6316e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0222e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0887, -0.0896, -0.0905,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0758, -0.0766, -0.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0887, -0.0896, -0.0905,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4275,  0.9656, -0.4099,  0.9608,  0.9656, -0.3654, -0.4275],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7790, -3.8537, -0.8260,  ..., -5.2983, -5.3015, -5.3178],\n",
      "        [-2.4649, -3.8712, -0.7665,  ..., -4.5909, -4.5814, -4.5790],\n",
      "        [-2.9384, -3.8165, -0.8513,  ..., -5.5081, -5.4994, -5.4838],\n",
      "        ...,\n",
      "        [-2.9358, -3.8187, -0.8539,  ..., -5.5797, -5.5737, -5.5629],\n",
      "        [-2.4775, -3.8723, -0.7720,  ..., -4.6337, -4.6188, -4.6103],\n",
      "        [-2.4649, -3.8712, -0.7665,  ..., -4.5909, -4.5814, -4.5790]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0014, -0.3761,  0.8540, -0.3892,  0.9284,  0.8325, -0.3655, -0.3761],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8560,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.6983e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1983, 0.2003, 0.2023,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8185,  0.9656,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7781, -3.8617, -0.8450,  ..., -5.2256, -5.2322, -5.2549],\n",
      "        [-2.7712, -3.8627, -0.8370,  ..., -5.2968, -5.2981, -5.3178],\n",
      "        [-2.7707, -3.8579, -0.8447,  ..., -5.2640, -5.2695, -5.2869],\n",
      "        ...,\n",
      "        [-2.9157, -3.8309, -0.8676,  ..., -5.5272, -5.5209, -5.5042],\n",
      "        [-2.7346, -3.8605, -0.8332,  ..., -5.1197, -5.1781, -5.2093],\n",
      "        [-2.7648, -3.8559, -0.8404,  ..., -5.2971, -5.3025, -5.3187]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9330,  0.9773,  0.9827, -0.8972,  0.9827,  0.9105,  0.9551,  0.8766],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4212, -0.4011, -0.4381,  0.9900, -0.4295,  0.9900, -0.4319],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8786e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.3294e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0852, -0.0861, -0.0869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0811, -0.0820, -0.0828,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0869, -0.0878, -0.0887,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0874, -0.0883, -0.0892,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4108, -0.3912, -0.4273,  0.9656, -0.4189,  0.9656, -0.4213],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7625, -3.8645, -0.8408,  ..., -5.2934, -5.2972, -5.3156],\n",
      "        [-2.4677, -3.8884, -0.7873,  ..., -4.5555, -4.5503, -4.5505],\n",
      "        [-2.4702, -3.8933, -0.7847,  ..., -4.5180, -4.5133, -4.5163],\n",
      "        ...,\n",
      "        [-2.4597, -3.8917, -0.7928,  ..., -4.5701, -4.5558, -4.5543],\n",
      "        [-2.7627, -3.8662, -0.8503,  ..., -5.2878, -5.2917, -5.3183],\n",
      "        [-2.4605, -3.8875, -0.7869,  ..., -4.5861, -4.5748, -4.5717]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9591, -0.4337, -0.3806, -0.4437,  0.7521, -0.4000,  0.9807, -0.4749],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4090,  0.9900, -1.2274, -0.4410,  0.9900,  0.9900, -0.4452],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5304e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0827, -0.0836, -0.0844,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0901, -0.0910, -0.0919,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3989,  0.9656, -1.2031, -0.4301,  0.9656,  0.9656, -0.4342],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9137, -3.8399, -0.8814,  ..., -5.5007, -5.4925, -5.4811],\n",
      "        [-2.4594, -3.8961, -0.7966,  ..., -4.6110, -4.5957, -4.5995],\n",
      "        [-2.7244, -3.8797, -0.8376,  ..., -5.1872, -5.2561, -5.2879],\n",
      "        ...,\n",
      "        [-2.9112, -3.8419, -0.8820,  ..., -5.5729, -5.5677, -5.5577],\n",
      "        [-2.7244, -3.8797, -0.8376,  ..., -5.1872, -5.2561, -5.2879],\n",
      "        [-2.4523, -3.8935, -0.7918,  ..., -4.6123, -4.6019, -4.5966]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7858, -0.4134,  0.9323, -0.9792, -0.4450,  0.7428,  0.9323, -0.4616],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(4.7129e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7258, -3.8714, -0.8573, -2.4716,  0.0915, -2.3964, -4.6083, -0.4976,\n",
      "         -4.0885, -2.5710, -3.9210,  0.5850, -0.2244, -3.5983, -3.0440, -2.4553,\n",
      "         -3.2951, -1.8417, -1.9970, -1.0808, -1.3792, -1.5452, -0.9692,  0.4421,\n",
      "         -2.0911, -3.4704, -3.0247, -2.6366, -3.8364,  0.9372, -1.8385, -0.3289,\n",
      "         -0.8851, -3.5084, -1.9113, -1.6304, -0.8908, -3.2003, -1.7576, -0.1578,\n",
      "         -0.7147, -0.5282, -0.4238, -4.2495, -1.7591, -3.4802, -3.5541, -3.2189,\n",
      "          0.4796,  0.5830, -3.8365, -1.5399, -7.0962, -2.9509, -3.0584, -1.0290,\n",
      "         -2.5596, -2.9567, -2.6409, -1.7629, -3.6567, -1.2384, -5.8201, -2.4498,\n",
      "         -3.5937,  1.0635, -1.4593, -0.2221, -0.3394, -3.1377, -4.0902, -5.4649,\n",
      "         -2.8385, -0.1233, -3.8099, -4.1467, -2.5206, -3.8744, -2.6291, -3.5522,\n",
      "         -4.7866, -2.5406, -0.4023, -1.4106, -1.7688, -3.1086, -1.9294, -0.7944,\n",
      "         -3.6094, -0.9032, -3.1708, -4.0221, -2.4916, -0.6823, -1.0320, -2.9722,\n",
      "         -2.8763, -3.8144, -3.3207, -3.3183, -0.1830, -1.1267, -3.7110, -2.1362,\n",
      "         -0.4221, -2.0184, -3.6413, -3.8164,  0.5897, -1.4036, -1.2322, -0.8230,\n",
      "         -4.4443, -4.6177,  0.3397, -1.1050, -2.1331, -1.4543, -0.1455, -1.0439,\n",
      "         -2.9925, -4.5854, -2.7131, -0.8719, -1.2399, -2.9181, -2.3414, -0.8749,\n",
      "         -0.7639, -4.2422,  0.0846,  0.6584, -3.3579, -4.1000, -1.2939, -2.0298,\n",
      "         -3.0809, -2.4725, -1.7931, -2.9007, -2.9853, -5.8587, -1.6778, -2.3595,\n",
      "         -1.3426, -0.7084, -0.5792, -4.0580, -3.8447, -0.1766, -2.0572, -0.5423,\n",
      "         -0.8967, -0.5442, -0.0228, -0.9658, -3.8751, -3.4242, -4.8821, -1.3552,\n",
      "          0.8439,  1.0403,  0.9764,  0.7990,  0.2712,  0.9635, -4.6355, -4.8329,\n",
      "         -5.1432, -4.9611, -5.2286, -5.3198, -5.2277, -5.1890, -5.1977, -5.1790,\n",
      "         -5.1059, -4.9963, -4.9308, -4.9105, -4.9227, -4.9445, -4.9805, -5.0304,\n",
      "         -5.0671, -5.1004, -5.1306, -5.1865, -5.2651, -5.3607, -5.4251, -5.4470,\n",
      "         -5.4393, -5.4478, -5.4678, -5.4905, -5.4783, -5.4369, -5.3929, -5.3448,\n",
      "         -5.3163, -5.2955, -5.2838, -5.2900, -5.3064],\n",
      "        [-2.7025, -3.8753, -0.8509, -2.4272,  0.1048, -2.4072, -4.6057, -0.4310,\n",
      "         -4.0472, -2.5305, -3.8994,  0.5798, -0.2215, -3.5519, -3.0811, -2.4247,\n",
      "         -3.2944, -1.8226, -2.0337, -1.0527, -1.3682, -1.5182, -0.9442,  0.4098,\n",
      "         -2.0641, -3.4211, -3.0529, -2.6142, -3.8384,  0.9343, -1.8074, -0.2858,\n",
      "         -0.8921, -3.4876, -1.9199, -1.6486, -0.8870, -3.2088, -1.7236, -0.1284,\n",
      "         -0.6992, -0.5288, -0.4100, -4.2691, -1.7400, -3.4482, -3.5071, -3.1976,\n",
      "          0.3750,  0.5069, -3.8019, -1.5358, -7.1115, -2.9315, -3.0657, -1.0074,\n",
      "         -2.5589, -2.9455, -2.6316, -1.7454, -3.6146, -1.2322, -5.8266, -2.4337,\n",
      "         -3.5944,  1.0567, -1.4292, -0.1799, -0.3513, -3.1202, -4.0647, -5.4624,\n",
      "         -2.8235, -0.1473, -3.7719, -4.1755, -2.5033, -3.8866, -2.6008, -3.5166,\n",
      "         -4.7857, -2.5253, -0.3856, -1.3721, -1.7605, -3.1013, -1.9024, -0.7563,\n",
      "         -3.5935, -0.8907, -3.1309, -4.0476, -2.4765, -0.6695, -0.9920, -2.9766,\n",
      "         -2.8598, -3.7804, -3.3092, -3.3063, -0.1679, -1.0897, -3.7256, -2.1407,\n",
      "         -0.4521, -1.9828, -3.6107, -3.8116,  0.5642, -1.6016, -1.2337, -0.9196,\n",
      "         -4.4712, -4.6793,  0.3744, -1.0414, -2.1145, -1.3642, -0.0516, -0.9858,\n",
      "         -2.8725, -4.6319, -2.6962, -0.8705, -1.0832, -2.7993, -2.3114, -0.8816,\n",
      "         -0.6564, -4.1476, -0.0631,  0.5644, -3.3398, -4.1077, -1.2866, -2.0373,\n",
      "         -3.0959, -2.4411, -1.8010, -2.8918, -2.9757, -5.8379, -1.6581, -2.3649,\n",
      "         -1.4413, -0.6872, -0.5118, -4.1428, -3.9135, -0.2301, -2.0813, -0.8595,\n",
      "         -0.9890, -0.4678, -0.1715, -0.8602, -4.0704, -5.2202, -1.5106, -2.2988,\n",
      "         -0.0187,  0.3922,  0.2403, -4.0519, -3.8796, -0.1488, -1.2350, -4.3853,\n",
      "         -6.2276, -4.7161, -2.9878, -4.8982, -1.2734,  0.8415,  1.0237,  0.9635,\n",
      "          0.8796,  0.6479,  0.9358, -4.5729, -4.8508, -5.1937, -4.9681, -5.1026,\n",
      "         -5.0663, -4.9355, -4.9772, -5.0100, -4.9495, -4.8724, -4.8265, -4.8280,\n",
      "         -4.8552, -4.8945, -4.9340, -4.9663, -4.9878, -4.9990, -5.0050, -5.0130,\n",
      "         -5.0331, -5.0603, -5.1087, -5.1682, -5.1974]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.8157, 0.8820], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3957, -0.4078,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.2275],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1754e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0801, -0.0809, -0.0817,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0825, -0.0833, -0.0842,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2508, -0.2534, -0.2559,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3860, -0.3977,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -1.2032],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4417, -3.8949, -0.8018,  ..., -4.6348, -4.6215, -4.6204],\n",
      "        [-2.4430, -3.8997, -0.8001,  ..., -4.6004, -4.5862, -4.5899],\n",
      "        [-2.7367, -3.8786, -0.8606,  ..., -5.2172, -5.2230, -5.2463],\n",
      "        ...,\n",
      "        [-2.7280, -3.8794, -0.8527,  ..., -5.2781, -5.2827, -5.3013],\n",
      "        [-2.7223, -3.8759, -0.8567,  ..., -5.2532, -5.2574, -5.2798],\n",
      "        [-2.3106, -3.9062, -0.7504,  ..., -4.4246, -4.4168, -4.4152]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4468, -0.4286,  0.8546,  0.7633,  0.8752,  0.9103,  0.9134, -1.0143],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.2243,  0.9900, -0.4342, -0.3975, -0.8697, -0.2092, -0.4488,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2830e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0386, -0.0390, -0.0394,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0878, -0.0887, -0.0896,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0360, -0.0364, -0.0368,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0908, -0.0917, -0.0926,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.2187,  0.9656, -0.4235, -0.3877, -0.8316, -0.2041, -0.4377,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5246, -3.8753, -0.8003,  ..., -4.6125, -4.6230, -4.6232],\n",
      "        [-2.7154, -3.8708, -0.8525,  ..., -5.2668, -5.2710, -5.2907],\n",
      "        [-2.4281, -3.8959, -0.7914,  ..., -4.5937, -4.5866, -4.5826],\n",
      "        ...,\n",
      "        [-2.5242, -3.8780, -0.7936,  ..., -4.5479, -4.5568, -4.5583],\n",
      "        [-2.4347, -3.8956, -0.7971,  ..., -4.5799, -4.5723, -4.5760],\n",
      "        [-2.6993, -3.8830, -0.8364,  ..., -5.1766, -5.2435, -5.2761]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2637,  0.8168, -0.4802, -0.4282, -0.8873, -0.2816, -0.4625,  0.9023],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3914,  0.9900,  0.9900,  0.9900,  0.9900, -0.4420, -0.4229, -1.0432],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3189e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0792, -0.0800, -0.0808,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0894, -0.0903, -0.0912,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0855, -0.0864, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1214, -0.1227, -0.1239,  ..., -1.0225, -1.0328, -1.0432]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3818,  0.9656,  0.9656,  0.9656,  0.9656, -0.4311, -0.4124, -0.9975],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4360, -3.9012, -0.7876,  ..., -4.4854, -4.4833, -4.4872],\n",
      "        [-2.7193, -3.8772, -0.8479,  ..., -5.2659, -5.2664, -5.2841],\n",
      "        [-2.7140, -3.8704, -0.8489,  ..., -5.2582, -5.2636, -5.2809],\n",
      "        ...,\n",
      "        [-2.4179, -3.8960, -0.7891,  ..., -4.5416, -4.5355, -4.5282],\n",
      "        [-2.4269, -3.8963, -0.7893,  ..., -4.5550, -4.5432, -4.5416],\n",
      "        [-2.5226, -3.8339, -0.8146,  ..., -0.9067, -0.9918, -0.8467]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3956,  0.9085,  0.8229,  0.8521,  0.7578, -0.4438, -0.5053, -0.9266],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4213, -0.4193,  0.9900, -0.4412,  0.9900, -0.3900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3937e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0852, -0.0861, -0.0870,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0848, -0.0857, -0.0866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0789, -0.0797, -0.0805,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4109, -0.4090,  0.9656, -0.4303,  0.9656, -0.3804,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7319, -3.8733, -0.8476,  ..., -5.2632, -5.2698, -5.2917],\n",
      "        [-2.4296, -3.8947, -0.7857,  ..., -4.5491, -4.5394, -4.5375],\n",
      "        [-2.4269, -3.8986, -0.7917,  ..., -4.5344, -4.5215, -4.5205],\n",
      "        ...,\n",
      "        [-2.7243, -3.8735, -0.8531,  ..., -5.2309, -5.2386, -5.2536],\n",
      "        [-2.4417, -3.9002, -0.7835,  ..., -4.4842, -4.4814, -4.4853],\n",
      "        [-2.8941, -3.8455, -0.8798,  ..., -5.5503, -5.5419, -5.5365]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9342, -0.4979, -0.4248,  0.8766, -0.4372,  0.9264, -0.3884,  0.6991],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.4230,  0.9900, -0.3735],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.7110e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0856, -0.0864, -0.0873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0756, -0.0763, -0.0771,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9608, -0.4126,  0.9656, -0.3643],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9016, -3.8498, -0.8786,  ..., -5.5896, -5.5853, -5.5788],\n",
      "        [-2.7132, -3.8799, -0.8286,  ..., -5.1743, -5.2389, -5.2688],\n",
      "        [-2.7442, -3.8779, -0.8510,  ..., -5.3167, -5.3227, -5.3415],\n",
      "        ...,\n",
      "        [-2.4477, -3.8980, -0.7865,  ..., -4.5579, -4.5537, -4.5504],\n",
      "        [-2.6914, -3.8688, -0.8153,  ..., -5.4249, -5.4740, -5.4803],\n",
      "        [-2.4441, -3.8946, -0.7893,  ..., -4.5789, -4.5656, -4.5588]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.6589,  0.9165,  0.9426,  0.9182,  0.8684, -0.4393,  0.8545, -0.4290],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4257, -0.4171,  0.9900,  0.9900, -0.4088, -0.4217,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5313e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0861, -0.0870, -0.0879,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0844, -0.0852, -0.0861,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0853, -0.0862, -0.0870,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4152, -0.4069,  0.9608,  0.9656, -0.3987, -0.4112,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4508, -3.8894, -0.7789,  ..., -4.5778, -4.5714, -4.5665],\n",
      "        [-2.4533, -3.8952, -0.7863,  ..., -4.5280, -4.5187, -4.5153],\n",
      "        [-2.9065, -3.8441, -0.8742,  ..., -5.4978, -5.4879, -5.4761],\n",
      "        ...,\n",
      "        [-2.4612, -3.8953, -0.7840,  ..., -4.5536, -4.5495, -4.5484],\n",
      "        [-2.9159, -3.8467, -0.8750,  ..., -5.5899, -5.5829, -5.5762],\n",
      "        [-2.7485, -3.8703, -0.8425,  ..., -5.2377, -5.2373, -5.2575]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4528, -0.4030,  0.8817,  0.9578, -0.4310, -0.4269,  0.6743,  0.9446],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4214,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.0745e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.7992e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0852, -0.0861, -0.0870, -0.0878, -0.0887, -0.0896, -0.0905, -0.0915,\n",
      "         -0.0924, -0.0933, -0.0943, -0.0952, -0.0962, -0.0971, -0.0981, -0.0991,\n",
      "         -0.1001, -0.1011, -0.1021, -0.1032, -0.1042, -0.1053, -0.1063, -0.1074,\n",
      "         -0.1085, -0.1096, -0.1107, -0.1118, -0.1129, -0.1141, -0.1152, -0.1164,\n",
      "         -0.1176, -0.1188, -0.1200, -0.1212, -0.1224, -0.1236, -0.1249, -0.1261,\n",
      "         -0.1274, -0.1287, -0.1300, -0.1313, -0.1326, -0.1340, -0.1353, -0.1367,\n",
      "         -0.1381, -0.1395, -0.1409, -0.1423, -0.1438, -0.1452, -0.1467, -0.1482,\n",
      "         -0.1496, -0.1512, -0.1527, -0.1542, -0.1558, -0.1574, -0.1590, -0.1606,\n",
      "         -0.1622, -0.1638, -0.1655, -0.1671, -0.1688, -0.1705, -0.1723, -0.1740,\n",
      "         -0.1758, -0.1775, -0.1793, -0.1811, -0.1830, -0.1848, -0.1867, -0.1886,\n",
      "         -0.1905, -0.1924, -0.1943, -0.1963, -0.1983, -0.2003, -0.2023, -0.2044,\n",
      "         -0.2064, -0.2085, -0.2106, -0.2127, -0.2149, -0.2171, -0.2192, -0.2215,\n",
      "         -0.2237, -0.2260, -0.2282, -0.2305, -0.2329, -0.2352, -0.2376, -0.2400,\n",
      "         -0.2424, -0.2449, -0.2474, -0.2499, -0.2524, -0.2549, -0.2575, -0.2601,\n",
      "         -0.2627, -0.2654, -0.2681, -0.2708, -0.2735, -0.2763, -0.2791, -0.2819,\n",
      "         -0.2847, -0.2876, -0.2905, -0.2934, -0.2964, -0.2994, -0.3024, -0.3055,\n",
      "         -0.3086, -0.3117, -0.3148, -0.3180, -0.3212, -0.3245, -0.3277, -0.3311,\n",
      "         -0.3344, -0.3378, -0.3412, -0.3446, -0.3481, -0.3516, -0.3552, -0.3588,\n",
      "         -0.3624, -0.3661, -0.3697, -0.3735, -0.3773, -0.3811, -0.3849, -0.3888,\n",
      "         -0.3927, -0.3967, -0.4007, -0.4048, -0.4088, -0.4130, -0.4171, -0.4214,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4110,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4727, -3.8881, -0.7767, -2.2567,  0.1470, -2.3431, -4.4983, -0.2164,\n",
      "         -3.8126, -2.4520, -3.8485,  0.6199, -0.1165, -3.4774, -3.0851, -2.2471,\n",
      "         -3.1880, -1.7900, -2.0029, -0.9765, -1.3233, -1.4134, -0.8622,  0.3947,\n",
      "         -1.9376, -3.3428, -3.0441, -2.4558, -3.8222,  0.9392, -1.7377, -0.1966,\n",
      "         -0.9335, -3.4303, -1.8865, -1.7339, -0.9257, -3.1220, -1.6164, -0.0172,\n",
      "         -0.7024, -0.5365, -0.3776, -4.2686, -1.6296, -3.2503, -3.4208, -3.1467,\n",
      "          0.1557,  0.1812, -3.7549, -1.4544, -7.0701, -2.8102, -2.9979, -0.9419,\n",
      "         -2.5323, -2.9119, -2.5578, -1.6398, -3.5493, -1.1579, -5.7685, -2.3310,\n",
      "         -3.5741,  1.0548, -1.3627, -0.0807, -0.4035, -3.0552, -3.9571, -5.4959,\n",
      "         -2.7049, -0.1172, -3.7140, -4.1606, -2.3818, -3.8173, -2.5229, -3.4626,\n",
      "         -4.8108, -2.4137, -0.3733, -1.3015, -1.7325, -2.9853, -1.8277, -0.6739,\n",
      "         -3.5475, -0.8735, -3.0606, -4.0256, -2.3656, -0.6735, -0.9163, -3.0120,\n",
      "         -2.7670, -3.6151, -3.2991, -3.2104, -0.1427, -1.0234, -3.6911, -2.1431,\n",
      "         -0.4686, -1.8340, -3.4587, -3.7250,  0.5989, -1.1666, -1.3897, -0.7383,\n",
      "         -4.4429, -4.5649,  0.3009, -1.0157, -2.1149, -1.3545, -0.0382, -0.9630,\n",
      "         -2.7317, -4.6261, -2.6502, -0.8793, -1.1257, -2.8019, -2.2629, -0.8217,\n",
      "         -0.6270, -4.1482, -0.3116,  0.2362, -3.3095, -4.0525, -1.2254, -1.9994,\n",
      "         -3.0708, -2.3676, -1.7156, -2.8067, -2.5356, -3.3156, -5.5783, -1.2162,\n",
      "         -0.5728, -0.4302, -0.4354, -0.4653, -0.4522, -0.4169, -4.8101, -4.8810,\n",
      "         -5.2601, -4.8517, -4.7580, -4.5031, -4.4013, -4.4706, -4.4282, -4.3940,\n",
      "         -4.4922, -4.5801, -4.6460, -4.6716, -4.6356, -4.5607, -4.5145, -4.5028,\n",
      "         -4.5131, -4.5200, -4.5220, -4.5402, -4.5685, -4.5830, -4.6003, -4.6044,\n",
      "         -4.5731, -4.5376, -4.5347, -4.5380, -4.5489, -4.5724, -4.5617, -4.5476,\n",
      "         -4.5384, -4.5318, -4.5498, -4.5684, -4.5818, -4.5898, -4.5934, -4.6026,\n",
      "         -4.6062, -4.6007, -4.5856, -4.5770, -4.5781, -4.5690, -4.5588, -4.5527,\n",
      "         -4.5440, -4.5493, -4.5409, -4.5337, -4.5327],\n",
      "        [-2.9397, -3.8379, -0.8679, -2.5427,  0.0499, -2.4201, -4.7006, -0.6525,\n",
      "         -4.3197, -2.7306, -3.9705,  0.5934, -0.2207, -3.6772, -2.9663, -2.5510,\n",
      "         -3.2984, -1.9117, -1.9312, -1.2009, -1.4195, -1.6543, -1.0381,  0.5084,\n",
      "         -2.2199, -3.5542, -2.9572, -2.7035, -3.8035,  0.9718, -1.8994, -0.4591,\n",
      "         -0.9078, -3.5339, -1.8967, -1.5922, -0.8131, -3.1723, -1.8325, -0.2335,\n",
      "         -0.7761, -0.4575, -0.4290, -4.1782, -1.8722, -3.6825, -3.7249, -3.2409,\n",
      "          0.8273,  0.7210, -3.8994, -1.5471, -7.0968, -2.9915, -3.0241, -1.0973,\n",
      "         -2.5246, -2.9448, -2.6919, -1.8637, -3.7268, -1.2491, -5.8148, -2.4702,\n",
      "         -3.5562,  1.1127, -1.5083, -0.3521, -0.3403, -3.1614, -4.1751, -5.5015,\n",
      "         -2.8796, -0.0260, -3.8714, -4.0815, -2.5524, -3.8338, -2.6903, -3.6077,\n",
      "         -4.8141, -2.5719, -0.4437, -1.5431, -1.8199, -3.1959, -2.0033, -0.9241,\n",
      "         -3.6677, -0.9754, -3.2412, -3.9590, -2.5159, -0.7194, -1.1687, -2.9335,\n",
      "         -2.9807, -3.9851, -3.3489, -3.3375, -0.2350, -1.2634, -3.6361, -2.1614,\n",
      "         -0.3767, -2.0848, -3.8103, -3.9095,  0.6988, -2.1079, -1.2733, -2.6667,\n",
      "         -4.8669, -4.6525,  0.3001, -1.2559, -2.1597, -1.5443, -0.2173, -1.0518,\n",
      "         -3.0312, -4.5844, -2.6998, -0.8614, -1.2033, -2.8937, -2.4499, -0.9580,\n",
      "         -0.9221, -4.2162,  0.6050,  0.8085, -3.3139, -4.1406, -1.3456, -1.9904,\n",
      "         -3.0778, -2.5069, -1.8626, -2.8579, -2.5418, -3.6333, -5.6886, -1.5643,\n",
      "          1.0405,  0.6023,  0.9582,  0.9550, -0.0117,  0.9278, -4.7064, -4.8726,\n",
      "         -5.1671, -4.9013, -5.0415, -4.9810, -5.1095, -5.1096, -5.1181, -5.0905,\n",
      "         -5.0777, -5.0755, -5.0877, -5.0900, -5.0842, -5.0954, -5.1295, -5.1609,\n",
      "         -5.1957, -5.2298, -5.2697, -5.3102, -5.3691, -5.4241, -5.4747, -5.4994,\n",
      "         -5.5272, -5.5637, -5.6098, -5.6061, -5.5770, -5.5490, -5.5245, -5.5178,\n",
      "         -5.5263, -5.5405, -5.5491, -5.5521, -5.5521, -5.5454, -5.5360, -5.5317,\n",
      "         -5.5371, -5.5478, -5.5463, -5.5598, -5.5808, -5.5853, -5.5920, -5.5910,\n",
      "         -5.5714, -5.5576, -5.5461, -5.5401, -5.5295]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4622,  0.7453], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4209, -0.4425, -0.3916, -0.4209,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5737e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0852, -0.0860, -0.0869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0895, -0.0904, -0.0913,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4106, -0.4315, -0.3820, -0.4106,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9561, -3.8392, -0.8630,  ..., -5.5838, -5.5807, -5.5746],\n",
      "        [-2.4929, -3.8832, -0.7715,  ..., -4.5404, -4.5314, -4.5300],\n",
      "        [-2.4821, -3.8833, -0.7722,  ..., -4.5247, -4.5156, -4.5137],\n",
      "        ...,\n",
      "        [-2.7901, -3.8652, -0.8284,  ..., -5.2629, -5.2638, -5.2802],\n",
      "        [-2.7565, -3.8627, -0.8272,  ..., -5.1034, -5.1599, -5.1906],\n",
      "        [-2.7901, -3.8652, -0.8284,  ..., -5.2629, -5.2638, -5.2802]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7262, -0.4394, -0.3830, -0.3668, -0.4394,  0.9918,  0.9530,  0.9918],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2152,  0.9900, -0.4240,  0.9900,  0.9900,  0.9900, -0.4338, -0.3960],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9481e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.6815e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2483, -0.2508, -0.2534,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0858, -0.0866, -0.0875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0878, -0.0887, -0.0895,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0801, -0.0809, -0.0817,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1912,  0.9656, -0.4136,  0.9656,  0.9656,  0.9656, -0.4231, -0.3862],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3791, -3.8871, -0.7223,  ..., -4.4029, -4.3965, -4.3968],\n",
      "        [-2.8013, -3.8522, -0.8235,  ..., -5.2593, -5.2654, -5.2859],\n",
      "        [-2.5081, -3.8821, -0.7736,  ..., -4.5177, -4.5062, -4.5045],\n",
      "        ...,\n",
      "        [-2.7604, -3.8541, -0.7977,  ..., -5.4238, -5.4708, -5.4779],\n",
      "        [-2.5068, -3.8755, -0.7650,  ..., -4.5720, -4.5638, -4.5624],\n",
      "        [-2.5153, -3.8788, -0.7719,  ..., -4.5673, -4.5563, -4.5603]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9291,  0.9593, -0.3443,  1.0356,  0.9590,  0.9349, -0.3984, -0.3475],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.4332,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.7314e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.3483e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.2003, 0.2023, 0.2043,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1983, 0.2003, 0.2023,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2003, 0.2023, 0.2043,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.4225,  0.9656,  0.9608,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7994, -3.8606, -0.8236,  ..., -5.2396, -5.2430, -5.2653],\n",
      "        [-2.8104, -3.8668, -0.8304,  ..., -5.3087, -5.3174, -5.3396],\n",
      "        [-2.7778, -3.8694, -0.8062,  ..., -5.1906, -5.2546, -5.2850],\n",
      "        ...,\n",
      "        [-2.9698, -3.8392, -0.8502,  ..., -5.5829, -5.5787, -5.5719],\n",
      "        [-2.9585, -3.8350, -0.8504,  ..., -5.4940, -5.4860, -5.4729],\n",
      "        [-2.9731, -3.8326, -0.8486,  ..., -5.5403, -5.5330, -5.5262]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0251,  1.0324,  1.0051,  1.0275, -0.3665,  0.7657,  0.9511,  0.8283],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4355,  0.9900,  0.9900, -0.4228, -0.4005, -0.8764,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6803e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0881, -0.0890, -0.0899,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1210, -0.1222, -0.1235,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4248,  0.9656,  0.9656, -0.4123, -0.3906, -0.8380,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4981, -3.8930, -0.7660,  ..., -4.5299, -4.5278, -4.5297],\n",
      "        [-2.7961, -3.8681, -0.8135,  ..., -5.2446, -5.2520, -5.2722],\n",
      "        [-2.9578, -3.8407, -0.8463,  ..., -5.5369, -5.5320, -5.5198],\n",
      "        ...,\n",
      "        [-2.5827, -3.8400, -0.7737,  ..., -4.0329, -4.0198, -4.0096],\n",
      "        [-2.7654, -3.8774, -0.8025,  ..., -5.1880, -5.2539, -5.2836],\n",
      "        [-2.7895, -3.8706, -0.8212,  ..., -5.2571, -5.2596, -5.2843]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3666,  1.0034,  0.8333, -0.3649, -0.3110, -0.8523,  0.9989,  0.9543],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4350, -0.4031,  0.9900, -0.3896, -0.3915, -0.4559,  0.9900, -0.4565],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4508e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.6369e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0880, -0.0889, -0.0898,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0815, -0.0824, -0.0832,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0922, -0.0932, -0.0941,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0924, -0.0933, -0.0942,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4242, -0.3931,  0.9608, -0.3800, -0.3819, -0.4447,  0.9656, -0.4452],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4789, -3.8946, -0.7592,  ..., -4.5259, -4.5136, -4.5143],\n",
      "        [-2.4891, -3.9008, -0.7575,  ..., -4.4494, -4.4460, -4.4498],\n",
      "        [-2.9319, -3.8494, -0.8472,  ..., -5.4844, -5.4765, -5.4668],\n",
      "        ...,\n",
      "        [-2.4702, -3.8962, -0.7589,  ..., -4.5076, -4.5003, -4.4982],\n",
      "        [-2.7692, -3.8716, -0.8186,  ..., -5.2365, -5.2453, -5.2623],\n",
      "        [-2.4801, -3.8934, -0.7665,  ..., -4.5450, -4.5383, -4.5416]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4062, -0.3133,  0.9517, -0.3470, -0.3491, -0.3522,  0.9040, -0.3641],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.2275, -0.2133,  0.9900,  0.9900, -0.4459,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3971e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0392, -0.0396, -0.0400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0367, -0.0371, -0.0375,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0902, -0.0911, -0.0920,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.2219, -0.2080,  0.9656,  0.9656, -0.4349,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7255, -3.8871, -0.8144,  ..., -5.1066, -5.1575, -5.1841],\n",
      "        [-2.5575, -3.8851, -0.7685,  ..., -4.5909, -4.5999, -4.6016],\n",
      "        [-2.5547, -3.8879, -0.7629,  ..., -4.5244, -4.5312, -4.5340],\n",
      "        ...,\n",
      "        [-2.4556, -3.9050, -0.7580,  ..., -4.5572, -4.5499, -4.5469],\n",
      "        [-2.7502, -3.8878, -0.8204,  ..., -5.2043, -5.2100, -5.2286],\n",
      "        [-2.9264, -3.8566, -0.8449,  ..., -5.4594, -5.4523, -5.4403]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9710, -0.1758, -0.2133,  0.8752,  1.0037, -0.3837,  0.9974,  0.8752],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.0565, -0.4331], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1230, -0.1242, -0.1255, -0.1267, -0.1280, -0.1293, -0.1306, -0.1319,\n",
      "         -0.1333, -0.1346, -0.1360, -0.1373, -0.1387, -0.1401, -0.1416, -0.1430,\n",
      "         -0.1444, -0.1459, -0.1474, -0.1488, -0.1504, -0.1519, -0.1534, -0.1550,\n",
      "         -0.1565, -0.1581, -0.1597, -0.1613, -0.1629, -0.1646, -0.1662, -0.1679,\n",
      "         -0.1696, -0.1713, -0.1731, -0.1748, -0.1766, -0.1784, -0.1802, -0.1820,\n",
      "         -0.1838, -0.1857, -0.1876, -0.1895, -0.1914, -0.1933, -0.1952, -0.1972,\n",
      "         -0.1992, -0.2012, -0.2033, -0.2053, -0.2074, -0.2095, -0.2116, -0.2137,\n",
      "         -0.2159, -0.2181, -0.2203, -0.2225, -0.2247, -0.2270, -0.2293, -0.2316,\n",
      "         -0.2340, -0.2363, -0.2387, -0.2411, -0.2436, -0.2460, -0.2485, -0.2510,\n",
      "         -0.2536, -0.2561, -0.2587, -0.2613, -0.2640, -0.2666, -0.2693, -0.2720,\n",
      "         -0.2748, -0.2776, -0.2804, -0.2832, -0.2861, -0.2889, -0.2919, -0.2948,\n",
      "         -0.2978, -0.3008, -0.3038, -0.3069, -0.3100, -0.3131, -0.3163, -0.3195,\n",
      "         -0.3227, -0.3260, -0.3293, -0.3326, -0.3360, -0.3394, -0.3428, -0.3462,\n",
      "         -0.3497, -0.3533, -0.3568, -0.3604, -0.3641, -0.3678, -0.3715, -0.3752,\n",
      "         -0.3790, -0.3829, -0.3867, -0.3906, -0.3946, -0.3986, -0.4026, -0.4066,\n",
      "         -0.4108, -0.4149, -0.4191, -0.4233, -0.4276, -0.4319, -0.4363, -0.4407,\n",
      "         -0.4451, -0.4496, -0.4542, -0.4588, -0.4634, -0.4681, -0.4728, -0.4776,\n",
      "         -0.4824, -0.4873, -0.4922, -0.4972, -0.5022, -0.5073, -0.5124, -0.5176,\n",
      "         -0.5228, -0.5281, -0.5334, -0.5388, -0.5443, -0.5497, -0.5553, -0.5609,\n",
      "         -0.5666, -0.5723, -0.5781, -0.5839, -0.5898, -0.5958, -0.6018, -0.6079,\n",
      "         -0.6140, -0.6202, -0.6265, -0.6328, -0.6392, -0.6457, -0.6522, -0.6588,\n",
      "         -0.6654, -0.6721, -0.6789, -0.6858, -0.6927, -0.6997, -0.7068, -0.7139,\n",
      "         -0.7211, -0.7284, -0.7358, -0.7432, -0.7507, -0.7583, -0.7660, -0.7737,\n",
      "         -0.7815, -0.7894, -0.7974, -0.8054, -0.8136, -0.8218, -0.8301, -0.8385,\n",
      "         -0.8469, -0.8555, -0.8641, -0.8729, -0.8817, -0.8906, -0.8996, -0.9087,\n",
      "         -0.9178, -0.9271, -0.9365, -0.9459, -0.9555, -0.9651, -0.9749, -0.9847,\n",
      "         -0.9947, -1.0047, -1.0149, -1.0251, -1.0355, -1.0460, -1.0565],\n",
      "        [-0.0876, -0.0885, -0.0894, -0.0903, -0.0912, -0.0921, -0.0931, -0.0940,\n",
      "         -0.0949, -0.0959, -0.0969, -0.0979, -0.0988, -0.0998, -0.1008, -0.1019,\n",
      "         -0.1029, -0.1039, -0.1050, -0.1060, -0.1071, -0.1082, -0.1093, -0.1104,\n",
      "         -0.1115, -0.1126, -0.1138, -0.1149, -0.1161, -0.1173, -0.1184, -0.1196,\n",
      "         -0.1208, -0.1221, -0.1233, -0.1245, -0.1258, -0.1271, -0.1284, -0.1297,\n",
      "         -0.1310, -0.1323, -0.1336, -0.1350, -0.1363, -0.1377, -0.1391, -0.1405,\n",
      "         -0.1419, -0.1434, -0.1448, -0.1463, -0.1477, -0.1492, -0.1507, -0.1523,\n",
      "         -0.1538, -0.1554, -0.1569, -0.1585, -0.1601, -0.1617, -0.1634, -0.1650,\n",
      "         -0.1667, -0.1684, -0.1701, -0.1718, -0.1735, -0.1753, -0.1770, -0.1788,\n",
      "         -0.1806, -0.1825, -0.1843, -0.1862, -0.1881, -0.1899, -0.1919, -0.1938,\n",
      "         -0.1958, -0.1977, -0.1997, -0.2018, -0.2038, -0.2059, -0.2079, -0.2100,\n",
      "         -0.2122, -0.2143, -0.2165, -0.2186, -0.2209, -0.2231, -0.2253, -0.2276,\n",
      "         -0.2299, -0.2322, -0.2346, -0.2370, -0.2393, -0.2418, -0.2442, -0.2467,\n",
      "         -0.2492, -0.2517, -0.2542, -0.2568, -0.2594, -0.2620, -0.2647, -0.2673,\n",
      "         -0.2700, -0.2728, -0.2755, -0.2783, -0.2811, -0.2839, -0.2868, -0.2897,\n",
      "         -0.2926, -0.2956, -0.2986, -0.3016, -0.3046, -0.3077, -0.3108, -0.3140,\n",
      "         -0.3171, -0.3203, -0.3236, -0.3268, -0.3301, -0.3335, -0.3368, -0.3402,\n",
      "         -0.3437, -0.3472, -0.3507, -0.3542, -0.3578, -0.3614, -0.3650, -0.3687,\n",
      "         -0.3725, -0.3762, -0.3800, -0.3839, -0.3877, -0.3917, -0.3956, -0.3996,\n",
      "         -0.4036, -0.4077, -0.4118, -0.4160, -0.4202, -0.4244, -0.4287, -0.4331,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0102, -0.4224], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5474e+00, -3.8554e+00, -7.8431e-01, -2.3447e+00,  1.7884e-01,\n",
      "         -2.3709e+00, -4.6216e+00, -2.6156e-01, -3.9001e+00, -2.4195e+00,\n",
      "         -3.9124e+00,  6.2656e-01, -8.9959e-02, -3.4841e+00, -3.1301e+00,\n",
      "         -2.3203e+00, -3.2013e+00, -1.7504e+00, -2.0057e+00, -9.6211e-01,\n",
      "         -1.3038e+00, -1.4070e+00, -8.2811e-01,  3.8095e-01, -1.9414e+00,\n",
      "         -3.3332e+00, -3.0787e+00, -2.5101e+00, -3.8441e+00,  9.5285e-01,\n",
      "         -1.7136e+00, -1.8485e-01, -8.5910e-01, -3.4894e+00, -1.7991e+00,\n",
      "         -1.7403e+00, -8.9082e-01, -3.1212e+00, -1.6058e+00,  4.0983e-02,\n",
      "         -6.2469e-01, -5.1042e-01, -3.6279e-01, -4.2731e+00, -1.6155e+00,\n",
      "         -3.2933e+00, -3.3818e+00, -3.1980e+00,  2.2176e-01,  4.2601e-01,\n",
      "         -3.7314e+00, -1.4942e+00, -7.1206e+00, -2.8462e+00, -2.9934e+00,\n",
      "         -9.1203e-01, -2.5135e+00, -2.9318e+00, -2.5759e+00, -1.6083e+00,\n",
      "         -3.5298e+00, -1.1880e+00, -5.7993e+00, -2.3541e+00, -3.5879e+00,\n",
      "          1.0790e+00, -1.3257e+00, -6.0406e-02, -3.2756e-01, -3.1047e+00,\n",
      "         -3.9693e+00, -5.4558e+00, -2.7412e+00, -1.2796e-01, -3.6902e+00,\n",
      "         -4.2092e+00, -2.4110e+00, -3.8124e+00, -2.5033e+00, -3.4316e+00,\n",
      "         -4.7691e+00, -2.4319e+00, -2.9730e-01, -1.2600e+00, -1.6693e+00,\n",
      "         -2.9826e+00, -1.8010e+00, -6.4834e-01, -3.5230e+00, -7.9250e-01,\n",
      "         -3.0324e+00, -4.0677e+00, -2.3834e+00, -5.9258e-01, -8.7095e-01,\n",
      "         -2.9692e+00, -2.7330e+00, -3.6560e+00, -3.2760e+00, -3.2086e+00,\n",
      "         -7.2901e-02, -9.7682e-01, -3.7319e+00, -2.0472e+00, -4.6779e-01,\n",
      "         -1.8172e+00, -3.4899e+00, -3.7476e+00,  5.9704e-01, -5.1769e+00,\n",
      "         -1.4534e+00, -2.7076e+00, -4.2560e+00, -4.5443e+00,  5.0824e-01,\n",
      "         -9.0798e-01, -2.0674e+00, -1.2990e+00, -3.8061e-02, -8.3680e-01,\n",
      "         -2.7038e+00, -4.6899e+00, -2.6457e+00, -8.6777e-01, -1.0388e+00,\n",
      "         -2.7798e+00, -2.1816e+00, -8.0095e-01, -7.0912e-01, -4.1839e+00,\n",
      "         -9.6998e-02,  5.1352e-01, -3.2425e+00, -4.1201e+00, -1.2206e+00,\n",
      "         -1.9908e+00, -3.0989e+00, -2.3783e+00, -1.6469e+00, -2.8684e+00,\n",
      "         -2.8965e+00, -5.8483e+00, -1.4678e+00, -5.9923e+00, -1.1971e+00,\n",
      "         -1.0457e+00, -9.8552e-01, -7.8272e-01, -9.7843e-01, -7.8276e-01,\n",
      "         -9.0943e-01, -4.1719e+00, -4.7517e+00, -2.3905e-01, -1.8463e+00,\n",
      "         -4.5460e+00, -5.7337e+00, -4.7037e+00, -5.3579e+00, -1.7872e+00,\n",
      "         -5.5783e+00, -4.0728e-01, -6.7250e-01, -7.3608e-01, -6.4637e-01,\n",
      "         -7.0811e-01, -5.5080e-01, -9.1483e-01, -4.0337e+00, -3.8121e+00,\n",
      "          2.4132e-01, -1.5318e+00, -3.6369e+00, -4.5936e+00, -3.6505e+00,\n",
      "         -4.8632e+00, -1.2921e+00, -5.2881e+00, -4.7818e-01, -4.4747e-01,\n",
      "         -6.0957e-01, -4.2807e-01, -5.7553e-01, -3.8220e-01, -7.3361e-01,\n",
      "         -3.8962e+00, -3.6657e+00,  1.2758e-01, -1.2860e+00, -3.6226e+00,\n",
      "         -4.6880e+00, -3.5109e+00, -3.4312e+00, -4.9467e+00, -1.4338e+00,\n",
      "         -8.1366e-01, -8.2130e-01, -8.1377e-01, -9.0938e-01, -8.7609e-01,\n",
      "         -9.2400e-01, -9.4902e-01, -8.2860e-01, -9.2209e-01, -9.1001e-01],\n",
      "        [-2.4414e+00, -3.9208e+00, -7.6475e-01, -2.2995e+00,  1.9055e-01,\n",
      "         -2.3392e+00, -4.5448e+00, -2.2063e-01, -3.7850e+00, -2.4125e+00,\n",
      "         -3.8576e+00,  6.7432e-01, -5.7753e-02, -3.4666e+00, -3.1275e+00,\n",
      "         -2.2448e+00, -3.1602e+00, -1.7461e+00, -2.0028e+00, -9.4055e-01,\n",
      "         -1.3313e+00, -1.3996e+00, -8.3628e-01,  3.8975e-01, -1.8768e+00,\n",
      "         -3.3313e+00, -3.0915e+00, -2.4494e+00, -3.8281e+00,  9.7800e-01,\n",
      "         -1.6923e+00, -1.6275e-01, -8.9966e-01, -3.4296e+00, -1.8592e+00,\n",
      "         -1.7559e+00, -9.0117e-01, -3.0886e+00, -1.5887e+00,  1.1918e-04,\n",
      "         -6.3720e-01, -5.1444e-01, -3.4450e-01, -4.2833e+00, -1.5673e+00,\n",
      "         -3.2183e+00, -3.3635e+00, -3.1478e+00,  1.2944e-01,  3.4442e-01,\n",
      "         -3.7431e+00, -1.4449e+00, -7.1083e+00, -2.8076e+00, -2.9652e+00,\n",
      "         -8.9840e-01, -2.5412e+00, -2.9283e+00, -2.5611e+00, -1.5721e+00,\n",
      "         -3.5352e+00, -1.1496e+00, -5.7961e+00, -2.3292e+00, -3.5786e+00,\n",
      "          1.0984e+00, -1.3130e+00, -3.8417e-02, -3.6700e-01, -3.0540e+00,\n",
      "         -3.9580e+00, -5.5193e+00, -2.7018e+00, -1.4846e-01, -3.6999e+00,\n",
      "         -4.2109e+00, -2.3801e+00, -3.7884e+00, -2.4934e+00, -3.4460e+00,\n",
      "         -4.8325e+00, -2.4071e+00, -3.1963e-01, -1.2559e+00, -1.7335e+00,\n",
      "         -2.9481e+00, -1.7815e+00, -6.3399e-01, -3.5646e+00, -8.0357e-01,\n",
      "         -3.0448e+00, -4.0685e+00, -2.3595e+00, -6.1406e-01, -8.7083e-01,\n",
      "         -3.0126e+00, -2.6952e+00, -3.5928e+00, -3.3140e+00, -3.2024e+00,\n",
      "         -8.5260e-02, -9.8324e-01, -3.7239e+00, -2.0628e+00, -4.6759e-01,\n",
      "         -1.8122e+00, -3.4354e+00, -3.7230e+00,  5.6704e-01, -5.6923e-01,\n",
      "         -1.2210e+00, -6.2984e-01, -4.2354e+00, -4.6129e+00,  4.1816e-01,\n",
      "         -9.0385e-01, -2.0852e+00, -1.3327e+00, -6.2025e-02, -8.8524e-01,\n",
      "         -2.7569e+00, -4.6074e+00, -2.6255e+00, -8.4073e-01, -1.1099e+00,\n",
      "         -2.7960e+00, -2.1847e+00, -7.7170e-01, -5.6391e-01, -4.1110e+00,\n",
      "         -2.3862e-01,  3.9478e-01, -3.2807e+00, -4.0702e+00, -1.1944e+00,\n",
      "         -2.0049e+00, -3.0554e+00, -2.4071e+00, -1.6387e+00, -2.8798e+00,\n",
      "         -2.5824e+00, -3.3072e+00, -5.7309e+00, -1.3035e+00, -2.5845e-01,\n",
      "         -3.5450e-01, -2.7786e-01, -4.2546e-01, -3.4586e-01, -3.6828e-01,\n",
      "         -4.7508e+00, -4.8780e+00, -5.2551e+00, -4.8486e+00, -4.7499e+00,\n",
      "         -4.4883e+00, -4.4473e+00, -4.5375e+00, -4.5121e+00, -4.4625e+00,\n",
      "         -4.5350e+00, -4.6167e+00, -4.6821e+00, -4.6926e+00, -4.6220e+00,\n",
      "         -4.5092e+00, -4.4415e+00, -4.4206e+00, -4.4253e+00, -4.4178e+00,\n",
      "         -4.4095e+00, -4.4178e+00, -4.4435e+00, -4.4689e+00, -4.4910e+00,\n",
      "         -4.4977e+00, -4.4674e+00, -4.4445e+00, -4.4511e+00, -4.4610e+00,\n",
      "         -4.4738e+00, -4.4920e+00, -4.4836e+00, -4.4650e+00, -4.4564e+00,\n",
      "         -4.4545e+00, -4.4764e+00, -4.4939e+00, -4.5045e+00, -4.5078e+00,\n",
      "         -4.5093e+00, -4.5228e+00, -4.5370e+00, -4.5488e+00, -4.5374e+00,\n",
      "         -4.5335e+00, -4.5340e+00, -4.5212e+00, -4.5171e+00, -4.5134e+00,\n",
      "         -4.5069e+00, -4.5103e+00, -4.4990e+00, -4.4872e+00, -4.4898e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8768, -0.3384], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4030,  0.9900,  0.9900, -0.4495, -1.0701, -0.4447,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4660e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0815, -0.0824, -0.0832,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0900, -0.0909, -0.0918,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3931,  0.9656,  0.9656, -0.4384, -1.0232, -0.4338,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4416, -3.9252, -0.7691,  ..., -4.5907, -4.5792, -4.5789],\n",
      "        [-2.7224, -3.9081, -0.8225,  ..., -5.2099, -5.2169, -5.2319],\n",
      "        [-2.9046, -3.8801, -0.8513,  ..., -5.4603, -5.4529, -5.4396],\n",
      "        ...,\n",
      "        [-2.4336, -3.9337, -0.7677,  ..., -4.5104, -4.4992, -4.4990],\n",
      "        [-2.7303, -3.9120, -0.8277,  ..., -5.1895, -5.1946, -5.2109],\n",
      "        [-2.7358, -3.9097, -0.8163,  ..., -5.2141, -5.2173, -5.2371]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3754,  0.8207,  0.8634, -0.4028, -0.9027, -0.3525,  0.9017,  0.9632],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4464,  0.9900,  0.9900, -0.4113, -0.4766,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1267e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0903, -0.0912, -0.0921,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0964, -0.0974, -0.0984,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4354,  0.9656,  0.9656, -0.4012, -0.4648,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9017, -3.8927, -0.8550,  ..., -5.4661, -5.4575, -5.4458],\n",
      "        [-2.4408, -3.9414, -0.7651,  ..., -4.5142, -4.5100, -4.5137],\n",
      "        [-2.8973, -3.8951, -0.8558,  ..., -5.5417, -5.5357, -5.5250],\n",
      "        ...,\n",
      "        [-2.4232, -3.9409, -0.7667,  ..., -4.5283, -4.5201, -4.5201],\n",
      "        [-2.7374, -3.9304, -0.8345,  ..., -5.2704, -5.2768, -5.2951],\n",
      "        [-2.7272, -3.9240, -0.8324,  ..., -5.1895, -5.1960, -5.2093]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8591, -0.3988,  0.8285,  0.9627, -0.3876, -0.3940,  0.9610,  0.8798],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.2368, -0.4326, -0.4632,  0.9900, -0.9148,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5768e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0408, -0.0412, -0.0416,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1263, -0.1276, -0.1289,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656, -0.2310, -0.4219, -0.4518,  0.9656, -0.8747,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8803, -3.9090, -0.8622,  ..., -5.4925, -5.4827, -5.4718],\n",
      "        [-2.6840, -3.9283, -0.8010,  ..., -5.4675, -5.5158, -5.5246],\n",
      "        [-2.5299, -3.9316, -0.7708,  ..., -4.5242, -4.5323, -4.5333],\n",
      "        ...,\n",
      "        [-2.7243, -3.9349, -0.8291,  ..., -5.2225, -5.2222, -5.2436],\n",
      "        [-2.5257, -3.9019, -0.7788,  ..., -4.0579, -4.0480, -4.0344],\n",
      "        [-2.8909, -3.9104, -0.8620,  ..., -5.5880, -5.5808, -5.5744]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9316,  0.8864, -0.2540, -0.3900, -0.4569,  0.9060, -0.9177,  0.7499],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4667,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.5127e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0944, -0.0954, -0.0963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4552,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4325, -3.9584, -0.7674,  ..., -4.5609, -4.5519, -4.5512],\n",
      "        [-2.7074, -3.9478, -0.8134,  ..., -5.2061, -5.2631, -5.2913],\n",
      "        [-2.7345, -3.9463, -0.8344,  ..., -5.1470, -5.1516, -5.1721],\n",
      "        ...,\n",
      "        [-2.7162, -3.9372, -0.8304,  ..., -5.2053, -5.2073, -5.2209],\n",
      "        [-2.7246, -3.9450, -0.8284,  ..., -5.2075, -5.2082, -5.2234],\n",
      "        [-2.8787, -3.9178, -0.8650,  ..., -5.4971, -5.4888, -5.4739]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4640,  0.9301,  0.9015,  0.9501,  0.9301,  0.7742,  0.9203,  0.9344],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4394, -1.2506,  0.9900, -0.4924, -0.4795, -0.4687, -0.4351,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.3934e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0889, -0.0898, -0.0907,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2556, -0.2581, -0.2607,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0948, -0.0958, -0.0967,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0880, -0.0889, -0.0898,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4285, -1.2259,  0.9656, -0.4803, -0.4677, -0.4571, -0.4244,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4383, -3.9648, -0.7729,  ..., -4.6033, -4.5909, -4.5975],\n",
      "        [-2.3111, -3.9732, -0.7241,  ..., -4.4309, -4.4263, -4.4304],\n",
      "        [-2.8925, -3.9237, -0.8641,  ..., -5.5921, -5.5920, -5.5817],\n",
      "        ...,\n",
      "        [-2.4321, -3.9681, -0.7751,  ..., -4.5606, -4.5499, -4.5487],\n",
      "        [-2.4424, -3.9693, -0.7667,  ..., -4.5112, -4.5080, -4.5120],\n",
      "        [-2.7258, -3.9504, -0.8286,  ..., -5.2049, -5.2052, -5.2217]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4016, -0.9616,  0.7641, -0.4248, -0.4255, -0.3859, -0.3881,  0.9219],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.4790, -0.4767, -0.4408, -0.4861, -0.2623, -0.4954],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.3979e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0969, -0.0979, -0.0989,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0983, -0.0993, -0.1003,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0452, -0.0456, -0.0461,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1002, -0.1012, -0.1023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4672, -0.4649, -0.4299, -0.4741, -0.2558, -0.4832],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6988, -3.9498, -0.8249,  ..., -5.1277, -5.1786, -5.1997],\n",
      "        [-2.6988, -3.9498, -0.8249,  ..., -5.1277, -5.1786, -5.1997],\n",
      "        [-2.4363, -3.9722, -0.7713,  ..., -4.5882, -4.5846, -4.5856],\n",
      "        ...,\n",
      "        [-2.4292, -3.9671, -0.7661,  ..., -4.6157, -4.6070, -4.6028],\n",
      "        [-2.5302, -3.9472, -0.7796,  ..., -4.6072, -4.6149, -4.6143],\n",
      "        [-2.4221, -3.9687, -0.7683,  ..., -4.5684, -4.5600, -4.5561]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9439,  0.9439, -0.4498, -0.4725, -0.3955, -0.4292, -0.2152, -0.4223],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.0235e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.1964e-09, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170,\n",
      "         0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376,\n",
      "         0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601,\n",
      "         0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847,\n",
      "         0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117,\n",
      "         0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412,\n",
      "         0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735,\n",
      "         0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088,\n",
      "         0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475,\n",
      "         0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899,\n",
      "         0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363,\n",
      "         0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870,\n",
      "         0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426,\n",
      "         0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034,\n",
      "         0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700,\n",
      "         0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429,\n",
      "         0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227,\n",
      "         0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7162, -3.9548, -0.8317, -2.5461,  0.1197, -2.4133, -4.6642, -0.5093,\n",
      "         -4.0721, -2.6231, -3.9685,  0.5545, -0.2400, -3.5780, -3.1081, -2.4382,\n",
      "         -3.2865, -1.8111, -2.0056, -1.0782, -1.3956, -1.5712, -0.9416,  0.4285,\n",
      "         -2.0751, -3.4528, -3.0864, -2.6160, -3.8493,  0.9839, -1.8096, -0.3234,\n",
      "         -0.8723, -3.5429, -1.8948, -1.6883, -0.8733, -3.1757, -1.7248, -0.1336,\n",
      "         -0.6742, -0.5101, -0.3867, -4.2803, -1.7349, -3.4699, -3.5823, -3.2612,\n",
      "          0.5079,  0.6218, -3.8178, -1.5637, -7.1584, -2.9279, -3.0334, -0.9942,\n",
      "         -2.5359, -3.0148, -2.6424, -1.7366, -3.6377, -1.2610, -5.8685, -2.4252,\n",
      "         -3.6011,  1.1132, -1.4193, -0.2037, -0.3163, -3.1711, -4.0958, -5.5163,\n",
      "         -2.8189, -0.1182, -3.7872, -4.2384, -2.4983, -3.8591, -2.5945, -3.5242,\n",
      "         -4.8334, -2.5164, -0.3339, -1.3862, -1.7916, -3.0692, -1.8888, -0.7779,\n",
      "         -3.6392, -0.8666, -3.1412, -4.1035, -2.4675, -0.6093, -1.0066, -2.9709,\n",
      "         -2.8411, -3.8093, -3.3739, -3.3056, -0.1147, -1.1096, -3.7541, -2.0980,\n",
      "         -0.4294, -1.9990, -3.6329, -3.8413,  0.5987, -1.1317, -1.1263, -0.7329,\n",
      "         -4.8149, -4.7632,  0.3566, -1.0553, -2.1417, -1.4258, -0.0990, -1.0372,\n",
      "         -2.9794, -4.6446, -2.7068, -0.8630, -1.1540, -2.8998, -2.2995, -0.8578,\n",
      "         -0.6724, -4.2021,  0.1408,  0.7158, -3.3368, -4.0898, -1.2646, -2.0154,\n",
      "         -3.1145, -2.4518, -1.7459, -2.8644, -2.9636, -5.8249, -1.6985, -2.3156,\n",
      "         -1.1691, -0.6875, -0.7219, -4.2268, -3.9716, -0.1690, -1.9562, -0.6359,\n",
      "         -0.8201, -0.2903, -0.1165, -0.5661, -4.0652, -3.3967, -4.9965, -1.3293,\n",
      "          0.9078,  0.9902,  1.0082,  0.9042,  0.9338,  0.9689, -4.6705, -4.8520,\n",
      "         -5.2012, -5.0043, -5.2154, -5.3768, -5.3037, -5.2966, -5.2698, -5.2037,\n",
      "         -5.1028, -4.9925, -4.9128, -4.8726, -4.8651, -4.8904, -4.9310, -4.9818,\n",
      "         -5.0119, -5.0334, -5.0617, -5.1141, -5.1956, -5.2927, -5.3684, -5.3925,\n",
      "         -5.3809, -5.3830, -5.3973, -5.4204, -5.4131, -5.3770, -5.3302, -5.2732,\n",
      "         -5.2337, -5.2040, -5.1813, -5.1818, -5.1938],\n",
      "        [-2.8890, -3.9282, -0.8596, -2.6251,  0.1043, -2.4328, -4.7727, -0.6831,\n",
      "         -4.2639, -2.7390, -4.0074,  0.5282, -0.2561, -3.6880, -3.0814, -2.5769,\n",
      "         -3.2970, -1.8470, -1.9834, -1.1455, -1.4371, -1.6541, -0.9895,  0.4763,\n",
      "         -2.1554, -3.5628, -3.0711, -2.7219, -3.8268,  1.0173, -1.8418, -0.4130,\n",
      "         -0.8746, -3.5691, -1.8802, -1.6435, -0.8011, -3.1611, -1.7986, -0.2015,\n",
      "         -0.6964, -0.4527, -0.3927, -4.2353, -1.8064, -3.6336, -3.6914, -3.2825,\n",
      "          0.7742,  0.8512, -3.8998, -1.5753, -7.1731, -3.0127, -3.0143, -1.0444,\n",
      "         -2.5451, -3.0098, -2.7002, -1.7986, -3.7341, -1.2766, -5.8897, -2.4954,\n",
      "         -3.5803,  1.1563, -1.4477, -0.3019, -0.3021, -3.1993, -4.2158, -5.5409,\n",
      "         -2.9060, -0.0897, -3.8734, -4.2041, -2.5806, -3.8326, -2.6505, -3.6112,\n",
      "         -4.8577, -2.6006, -0.3627, -1.4748, -1.8307, -3.1373, -1.9347, -0.8678,\n",
      "         -3.7053, -0.8951, -3.2448, -4.0758, -2.5469, -0.6266, -1.1025, -2.9440,\n",
      "         -2.9021, -3.9461, -3.3900, -3.3397, -0.1497, -1.2069, -3.7140, -2.1001,\n",
      "         -0.3897, -2.0778, -3.7751, -3.9276,  0.6265, -2.0690, -1.3401, -2.5759,\n",
      "         -4.8316, -4.6935,  0.3921, -1.1942, -2.1659, -1.5417, -0.2064, -1.0498,\n",
      "         -2.9910, -4.6382, -2.6970, -0.8385, -1.1474, -2.8987, -2.3982, -0.9360,\n",
      "         -0.8551, -4.2436,  0.5528,  0.9357, -3.2874, -4.1610, -1.3032, -2.0201,\n",
      "         -3.1133, -2.5122, -1.7885, -2.8724, -2.5666, -3.6200, -5.7171, -1.5808,\n",
      "          0.9675,  0.8116,  0.9473,  0.8986,  0.5035,  0.9895, -4.7297, -4.8895,\n",
      "         -5.1919, -4.9390, -5.0772, -5.0210, -5.1365, -5.1409, -5.1546, -5.1334,\n",
      "         -5.1051, -5.0848, -5.0848, -5.0828, -5.0794, -5.1007, -5.1371, -5.1736,\n",
      "         -5.2128, -5.2462, -5.2823, -5.3195, -5.3754, -5.4292, -5.4796, -5.5037,\n",
      "         -5.5220, -5.5549, -5.6016, -5.6053, -5.5756, -5.5426, -5.5103, -5.4970,\n",
      "         -5.5017, -5.5198, -5.5303, -5.5439, -5.5495, -5.5510, -5.5470, -5.5405,\n",
      "         -5.5370, -5.5468, -5.5467, -5.5630, -5.5866, -5.5972, -5.6040, -5.6009,\n",
      "         -5.5892, -5.5765, -5.5669, -5.5600, -5.5506]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9522, 0.8530], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5130,  0.9900, -0.4928,  0.9900, -0.4928,  0.9900, -0.2816, -0.4828],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0133e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(1.3377e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1038, -0.1048, -0.1059,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0997, -0.1007, -0.1017,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0485, -0.0490, -0.0495,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0977, -0.0986, -0.0996,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5004,  0.9656, -0.4807,  0.9656, -0.4807,  0.9656, -0.2747, -0.4708],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4245, -3.9736, -0.7785,  ..., -4.5917, -4.5837, -4.5786],\n",
      "        [-2.7196, -3.9579, -0.8412,  ..., -5.2032, -5.2033, -5.2181],\n",
      "        [-2.4292, -3.9767, -0.7833,  ..., -4.5936, -4.5823, -4.5797],\n",
      "        ...,\n",
      "        [-2.7196, -3.9579, -0.8412,  ..., -5.2032, -5.2033, -5.2181],\n",
      "        [-2.5316, -3.9524, -0.7893,  ..., -4.6228, -4.6326, -4.6317],\n",
      "        [-2.4397, -3.9731, -0.7795,  ..., -4.5809, -4.5754, -4.5783]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4527,  0.9387, -0.4202,  0.9352, -0.4202,  0.9387, -0.2286, -0.4545],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5077,  0.9900,  0.9900, -0.4725, -0.5153, -1.2900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5457e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1027, -0.1037, -0.1048,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0956, -0.0966, -0.0975,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1042, -0.1053, -0.1064,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2636, -0.2663, -0.2690,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.4951,  0.9656,  0.9656, -0.4609, -0.5026, -1.2645],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7335, -3.9602, -0.8527,  ..., -5.2571, -5.2587, -5.2805],\n",
      "        [-2.7242, -3.9548, -0.8509,  ..., -5.2458, -5.2482, -5.2630],\n",
      "        [-2.4367, -3.9742, -0.7882,  ..., -4.6161, -4.6046, -4.6062],\n",
      "        ...,\n",
      "        [-2.4448, -3.9790, -0.7867,  ..., -4.5570, -4.5563, -4.5580],\n",
      "        [-2.4337, -3.9719, -0.7886,  ..., -4.6501, -4.6408, -4.6392],\n",
      "        [-2.3169, -3.9823, -0.7443,  ..., -4.4650, -4.4606, -4.4653]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9237,  0.7355, -0.5268,  0.8015,  0.9120, -0.4456, -0.4758, -0.9865],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.4825, -0.4825,  0.9900, -0.5152,  0.9900, -0.5222,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3282e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0976, -0.0986, -0.0996,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0976, -0.0986, -0.0996,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1056, -0.1067, -0.1078,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4706, -0.4706,  0.9656, -0.5025,  0.9656, -0.5093,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7345, -3.9583, -0.8659,  ..., -5.2504, -5.2517, -5.2672],\n",
      "        [-2.4467, -3.9747, -0.8006,  ..., -4.6604, -4.6477, -4.6527],\n",
      "        [-2.4467, -3.9747, -0.8006,  ..., -4.6604, -4.6477, -4.6527],\n",
      "        ...,\n",
      "        [-2.7270, -3.9550, -0.8589,  ..., -5.2654, -5.2723, -5.2836],\n",
      "        [-2.4381, -3.9735, -0.7948,  ..., -4.6634, -4.6550, -4.6515],\n",
      "        [-2.6922, -3.9515, -0.8290,  ..., -5.5155, -5.5666, -5.5733]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7899, -0.4804, -0.4804,  0.9088, -0.5463,  0.7261, -0.4934,  0.8606],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.5157, -0.4686, -0.9787,  0.9900, -0.5157],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4508e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1351, -0.1365, -0.1379,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1043, -0.1054, -0.1064,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.5030, -0.4570, -0.9358,  0.9608, -0.5030],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7492, -3.9601, -0.8661,  ..., -5.2183, -5.2275, -5.2454],\n",
      "        [-2.7426, -3.9557, -0.8689,  ..., -5.2650, -5.2692, -5.2829],\n",
      "        [-2.9077, -3.9264, -0.8940,  ..., -5.6105, -5.6030, -5.5929],\n",
      "        ...,\n",
      "        [-2.5408, -3.9225, -0.8107,  ..., -4.1429, -4.1318, -4.1182],\n",
      "        [-2.8937, -3.9292, -0.8978,  ..., -5.5489, -5.5407, -5.5270],\n",
      "        [-2.4535, -3.9745, -0.8035,  ..., -4.6428, -4.6390, -4.6394]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8660,  0.7846,  0.8064, -0.5460, -0.5002, -1.0475,  0.8948, -0.5460],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.5311,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.1721e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1074, -0.1085, -0.1096,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9608,  0.9656, -0.5180,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7468, -3.9560, -0.8617,  ..., -5.2995, -5.2996, -5.3127],\n",
      "        [-2.7574, -3.9574, -0.8700,  ..., -5.3582, -5.3640, -5.3835],\n",
      "        [-2.9164, -3.9186, -0.8940,  ..., -5.5368, -5.5270, -5.5119],\n",
      "        ...,\n",
      "        [-2.7184, -3.9493, -0.8587,  ..., -5.1800, -5.2291, -5.2533],\n",
      "        [-2.4435, -3.9673, -0.8010,  ..., -4.6311, -4.6220, -4.6188],\n",
      "        [-2.9135, -3.9211, -0.8959,  ..., -5.6131, -5.6045, -5.5935]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8796,  0.9186,  0.7179,  0.7235,  0.8893,  0.9000, -0.5186,  0.8022],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.4761, -0.5110,  0.9900, -0.4660,  0.9900,  0.9900, -0.2857,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4245e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0963, -0.0973, -0.0983,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1034, -0.1044, -0.1055,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0492, -0.0497, -0.0502,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.4644, -0.4984,  0.9656, -0.4546,  0.9656,  0.9656, -0.2787,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4710, -3.9672, -0.7961,  ..., -4.5863, -4.5853, -4.5872],\n",
      "        [-2.4631, -3.9603, -0.7989,  ..., -4.6425, -4.6325, -4.6321],\n",
      "        [-2.7559, -3.9485, -0.8629,  ..., -5.3103, -5.3133, -5.3327],\n",
      "        ...,\n",
      "        [-2.7571, -3.9497, -0.8607,  ..., -5.3087, -5.3054, -5.3205],\n",
      "        [-2.5587, -3.9438, -0.8034,  ..., -4.5834, -4.5927, -4.5935],\n",
      "        [-2.7347, -3.9506, -0.8456,  ..., -5.2620, -5.3191, -5.3466]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4857, -0.5741,  0.8637, -0.5065,  0.7313,  0.8864, -0.2934,  0.9061],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5289, -1.1574], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1070, -0.1081, -0.1092, -0.1103, -0.1114, -0.1125, -0.1136, -0.1148,\n",
      "         -0.1159, -0.1171, -0.1183, -0.1195, -0.1207, -0.1219, -0.1232, -0.1244,\n",
      "         -0.1257, -0.1269, -0.1282, -0.1295, -0.1308, -0.1321, -0.1335, -0.1348,\n",
      "         -0.1362, -0.1376, -0.1389, -0.1403, -0.1418, -0.1432, -0.1446, -0.1461,\n",
      "         -0.1476, -0.1491, -0.1506, -0.1521, -0.1536, -0.1552, -0.1568, -0.1583,\n",
      "         -0.1599, -0.1615, -0.1632, -0.1648, -0.1665, -0.1682, -0.1699, -0.1716,\n",
      "         -0.1733, -0.1751, -0.1768, -0.1786, -0.1804, -0.1823, -0.1841, -0.1860,\n",
      "         -0.1878, -0.1897, -0.1916, -0.1936, -0.1955, -0.1975, -0.1995, -0.2015,\n",
      "         -0.2036, -0.2056, -0.2077, -0.2098, -0.2119, -0.2141, -0.2162, -0.2184,\n",
      "         -0.2206, -0.2228, -0.2251, -0.2274, -0.2297, -0.2320, -0.2343, -0.2367,\n",
      "         -0.2391, -0.2415, -0.2439, -0.2464, -0.2489, -0.2514, -0.2539, -0.2565,\n",
      "         -0.2591, -0.2617, -0.2644, -0.2670, -0.2697, -0.2724, -0.2752, -0.2780,\n",
      "         -0.2808, -0.2836, -0.2865, -0.2894, -0.2923, -0.2953, -0.2982, -0.3012,\n",
      "         -0.3043, -0.3074, -0.3105, -0.3136, -0.3168, -0.3200, -0.3232, -0.3265,\n",
      "         -0.3298, -0.3331, -0.3365, -0.3399, -0.3433, -0.3468, -0.3503, -0.3538,\n",
      "         -0.3574, -0.3610, -0.3646, -0.3683, -0.3720, -0.3758, -0.3796, -0.3834,\n",
      "         -0.3873, -0.3912, -0.3952, -0.3992, -0.4032, -0.4073, -0.4114, -0.4155,\n",
      "         -0.4197, -0.4240, -0.4282, -0.4326, -0.4369, -0.4414, -0.4458, -0.4503,\n",
      "         -0.4549, -0.4595, -0.4641, -0.4688, -0.4735, -0.4783, -0.4831, -0.4880,\n",
      "         -0.4929, -0.4979, -0.5030, -0.5080, -0.5132, -0.5183, -0.5236, -0.5289,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1347, -0.1361, -0.1374, -0.1388, -0.1402, -0.1417, -0.1431, -0.1445,\n",
      "         -0.1460, -0.1475, -0.1490, -0.1505, -0.1520, -0.1535, -0.1551, -0.1566,\n",
      "         -0.1582, -0.1598, -0.1614, -0.1631, -0.1647, -0.1664, -0.1680, -0.1697,\n",
      "         -0.1715, -0.1732, -0.1749, -0.1767, -0.1785, -0.1803, -0.1821, -0.1840,\n",
      "         -0.1858, -0.1877, -0.1896, -0.1915, -0.1934, -0.1954, -0.1974, -0.1994,\n",
      "         -0.2014, -0.2034, -0.2055, -0.2075, -0.2096, -0.2117, -0.2139, -0.2160,\n",
      "         -0.2182, -0.2204, -0.2227, -0.2249, -0.2272, -0.2295, -0.2318, -0.2341,\n",
      "         -0.2365, -0.2389, -0.2413, -0.2437, -0.2462, -0.2487, -0.2512, -0.2537,\n",
      "         -0.2563, -0.2589, -0.2615, -0.2641, -0.2668, -0.2695, -0.2722, -0.2750,\n",
      "         -0.2778, -0.2806, -0.2834, -0.2863, -0.2892, -0.2921, -0.2950, -0.2980,\n",
      "         -0.3010, -0.3041, -0.3071, -0.3102, -0.3134, -0.3165, -0.3197, -0.3230,\n",
      "         -0.3262, -0.3295, -0.3328, -0.3362, -0.3396, -0.3430, -0.3465, -0.3500,\n",
      "         -0.3535, -0.3571, -0.3607, -0.3644, -0.3680, -0.3717, -0.3755, -0.3793,\n",
      "         -0.3831, -0.3870, -0.3909, -0.3949, -0.3988, -0.4029, -0.4069, -0.4111,\n",
      "         -0.4152, -0.4194, -0.4236, -0.4279, -0.4322, -0.4366, -0.4410, -0.4455,\n",
      "         -0.4500, -0.4545, -0.4591, -0.4637, -0.4684, -0.4732, -0.4779, -0.4828,\n",
      "         -0.4876, -0.4926, -0.4975, -0.5026, -0.5076, -0.5128, -0.5180, -0.5232,\n",
      "         -0.5285, -0.5338, -0.5392, -0.5446, -0.5501, -0.5557, -0.5613, -0.5670,\n",
      "         -0.5727, -0.5785, -0.5843, -0.5902, -0.5962, -0.6022, -0.6083, -0.6145,\n",
      "         -0.6207, -0.6269, -0.6333, -0.6397, -0.6461, -0.6526, -0.6592, -0.6659,\n",
      "         -0.6726, -0.6794, -0.6863, -0.6932, -0.7002, -0.7073, -0.7144, -0.7217,\n",
      "         -0.7289, -0.7363, -0.7437, -0.7513, -0.7588, -0.7665, -0.7743, -0.7821,\n",
      "         -0.7900, -0.7980, -0.8060, -0.8142, -0.8224, -0.8307, -0.8391, -0.8475,\n",
      "         -0.8561, -0.8648, -0.8735, -0.8823, -0.8912, -0.9002, -0.9093, -0.9185,\n",
      "         -0.9278, -0.9372, -0.9466, -0.9562, -0.9658, -0.9756, -0.9855, -0.9954,\n",
      "         -1.0055, -1.0156, -1.0259, -1.0362, -1.0467, -1.0573, -1.0680, -1.0787,\n",
      "         -1.0896, -1.1007, -1.1118, -1.1230, -1.1343, -1.1458, -1.1574]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5158, -1.1067], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4717e+00, -3.9534e+00, -8.0330e-01, -2.3328e+00,  1.9701e-01,\n",
      "         -2.3892e+00, -4.5948e+00, -3.0101e-01, -3.7766e+00, -2.4707e+00,\n",
      "         -3.9217e+00,  5.4270e-01, -2.1327e-01, -3.5197e+00, -3.2540e+00,\n",
      "         -2.3199e+00, -3.2617e+00, -1.7151e+00, -2.1462e+00, -9.3330e-01,\n",
      "         -1.4308e+00, -1.4302e+00, -8.5119e-01,  3.4521e-01, -1.9213e+00,\n",
      "         -3.4039e+00, -3.2036e+00, -2.5278e+00, -3.8765e+00,  9.2106e-01,\n",
      "         -1.6821e+00, -1.6342e-01, -8.7308e-01, -3.5086e+00, -1.8769e+00,\n",
      "         -1.7719e+00, -9.0898e-01, -3.1874e+00, -1.6133e+00,  6.1032e-03,\n",
      "         -6.0402e-01, -5.3697e-01, -3.2155e-01, -4.3628e+00, -1.6063e+00,\n",
      "         -3.2099e+00, -3.3879e+00, -3.2330e+00,  9.7825e-02,  2.7361e-01,\n",
      "         -3.7863e+00, -1.5106e+00, -7.1558e+00, -2.8763e+00, -3.0570e+00,\n",
      "         -8.9833e-01, -2.5766e+00, -2.9761e+00, -2.6575e+00, -1.6427e+00,\n",
      "         -3.6062e+00, -1.2148e+00, -5.8608e+00, -2.4194e+00, -3.6371e+00,\n",
      "          1.0268e+00, -1.3228e+00, -6.7577e-02, -3.3849e-01, -3.1540e+00,\n",
      "         -4.0175e+00, -5.5535e+00, -2.7966e+00, -6.7233e-02, -3.7704e+00,\n",
      "         -4.3034e+00, -2.4832e+00, -3.8949e+00, -2.5054e+00, -3.5248e+00,\n",
      "         -4.8855e+00, -2.5142e+00, -2.8370e-01, -1.2429e+00, -1.7801e+00,\n",
      "         -2.9558e+00, -1.7644e+00, -6.4246e-01, -3.6219e+00, -7.9848e-01,\n",
      "         -3.1344e+00, -4.1702e+00, -2.4728e+00, -5.6622e-01, -8.6555e-01,\n",
      "         -3.1020e+00, -2.7405e+00, -3.5836e+00, -3.3005e+00, -3.2895e+00,\n",
      "         -5.6330e-02, -9.8057e-01, -3.7401e+00, -2.1054e+00, -4.9420e-01,\n",
      "         -1.8584e+00, -3.4323e+00, -3.7477e+00,  6.2913e-01, -1.6813e+00,\n",
      "         -1.3859e+00, -1.0722e+00, -4.2678e+00, -4.6884e+00,  4.8302e-01,\n",
      "         -9.2843e-01, -2.1930e+00, -1.3165e+00, -1.4747e-02, -9.7201e-01,\n",
      "         -2.7205e+00, -4.6598e+00, -2.6812e+00, -8.4999e-01, -1.0085e+00,\n",
      "         -2.8020e+00, -2.2013e+00, -8.0896e-01, -5.3056e-01, -4.1846e+00,\n",
      "         -3.0658e-01,  3.1545e-01, -3.3305e+00, -4.0951e+00, -1.2197e+00,\n",
      "         -2.0697e+00, -3.1195e+00, -2.4599e+00, -1.7098e+00, -2.8870e+00,\n",
      "         -2.5976e+00, -3.3547e+00, -5.7136e+00, -1.3679e+00, -4.9054e-01,\n",
      "         -4.8495e-01, -6.6467e-01, -4.6402e-01, -6.6421e-01, -4.9615e-01,\n",
      "         -4.9064e+00, -4.9548e+00, -5.2929e+00, -4.9769e+00, -4.9149e+00,\n",
      "         -4.6705e+00, -4.6393e+00, -4.7180e+00, -4.6938e+00, -4.6219e+00,\n",
      "         -4.6378e+00, -4.6809e+00, -4.7437e+00, -4.7483e+00, -4.6801e+00,\n",
      "         -4.5845e+00, -4.5421e+00, -4.5386e+00, -4.5448e+00, -4.5368e+00,\n",
      "         -4.5279e+00, -4.5403e+00, -4.5572e+00, -4.5701e+00, -4.5785e+00,\n",
      "         -4.5768e+00, -4.5476e+00, -4.5267e+00, -4.5366e+00, -4.5487e+00,\n",
      "         -4.5629e+00, -4.5733e+00, -4.5624e+00, -4.5450e+00, -4.5401e+00,\n",
      "         -4.5402e+00, -4.5643e+00, -4.5867e+00, -4.6063e+00, -4.6172e+00,\n",
      "         -4.6256e+00, -4.6459e+00, -4.6614e+00, -4.6638e+00, -4.6573e+00,\n",
      "         -4.6490e+00, -4.6517e+00, -4.6470e+00, -4.6542e+00, -4.6568e+00,\n",
      "         -4.6561e+00, -4.6628e+00, -4.6615e+00, -4.6569e+00, -4.6632e+00],\n",
      "        [-2.5728e+00, -3.8949e+00, -8.2059e-01, -2.3797e+00,  1.7855e-01,\n",
      "         -2.4207e+00, -4.6786e+00, -3.5326e-01, -3.8940e+00, -2.4808e+00,\n",
      "         -3.9808e+00,  4.9889e-01, -2.4763e-01, -3.5349e+00, -3.2550e+00,\n",
      "         -2.3944e+00, -3.3004e+00, -1.7238e+00, -2.1435e+00, -9.6245e-01,\n",
      "         -1.4023e+00, -1.4460e+00, -8.4790e-01,  3.3617e-01, -1.9936e+00,\n",
      "         -3.4063e+00, -3.1899e+00, -2.5911e+00, -3.8972e+00,  8.9712e-01,\n",
      "         -1.7068e+00, -1.9118e-01, -8.3613e-01, -3.5704e+00, -1.8183e+00,\n",
      "         -1.7522e+00, -8.9301e-01, -3.2178e+00, -1.6354e+00,  4.2933e-02,\n",
      "         -5.9454e-01, -5.2591e-01, -3.4521e-01, -4.3495e+00, -1.6597e+00,\n",
      "         -3.2874e+00, -3.4115e+00, -3.2852e+00,  2.0068e-01,  3.5335e-01,\n",
      "         -3.7773e+00, -1.5608e+00, -7.1647e+00, -2.9193e+00, -3.0820e+00,\n",
      "         -9.1681e-01, -2.5477e+00, -2.9845e+00, -2.6730e+00, -1.6863e+00,\n",
      "         -3.6059e+00, -1.2542e+00, -5.8610e+00, -2.4477e+00, -3.6506e+00,\n",
      "          1.0103e+00, -1.3392e+00, -9.6516e-02, -3.0189e-01, -3.2055e+00,\n",
      "         -4.0312e+00, -5.5022e+00, -2.8398e+00, -4.7160e-02, -3.7646e+00,\n",
      "         -4.3024e+00, -2.5170e+00, -3.9162e+00, -2.5197e+00, -3.5136e+00,\n",
      "         -4.8324e+00, -2.5386e+00, -2.6902e-01, -1.2554e+00, -1.7261e+00,\n",
      "         -2.9969e+00, -1.7888e+00, -6.6312e-01, -3.5825e+00, -7.9000e-01,\n",
      "         -3.1272e+00, -4.1713e+00, -2.4975e+00, -5.5490e-01, -8.7436e-01,\n",
      "         -3.0547e+00, -2.7834e+00, -3.6502e+00, -3.2654e+00, -3.2988e+00,\n",
      "         -5.2455e-02, -9.8346e-01, -3.7515e+00, -2.0944e+00, -4.9852e-01,\n",
      "         -1.8686e+00, -3.4869e+00, -3.7742e+00,  6.5429e-01, -5.3670e+00,\n",
      "         -1.4811e+00, -2.8527e+00, -4.2289e+00, -4.5460e+00,  5.4382e-01,\n",
      "         -9.0401e-01, -2.1877e+00, -1.3439e+00, -6.4648e-02, -9.5833e-01,\n",
      "         -2.7424e+00, -4.6751e+00, -2.6806e+00, -8.8886e-01, -1.0627e+00,\n",
      "         -2.8685e+00, -2.1998e+00, -8.2191e-01, -7.1185e-01, -4.2794e+00,\n",
      "         -7.7212e-02,  4.3611e-01, -3.3479e+00, -4.1309e+00, -1.2379e+00,\n",
      "         -2.0486e+00, -3.1362e+00, -2.4794e+00, -1.7062e+00, -2.9100e+00,\n",
      "         -2.9058e+00, -5.8196e+00, -1.5365e+00, -6.1085e+00, -1.4409e+00,\n",
      "         -1.2715e+00, -1.2577e+00, -1.0153e+00, -1.2516e+00, -1.0139e+00,\n",
      "         -1.1758e+00, -4.1568e+00, -4.7652e+00, -2.8042e-01, -1.9034e+00,\n",
      "         -4.7060e+00, -5.8239e+00, -4.7337e+00, -5.3458e+00, -1.8641e+00,\n",
      "         -5.7139e+00, -6.9535e-01, -9.0237e-01, -1.0241e+00, -8.7973e-01,\n",
      "         -9.8122e-01, -7.8920e-01, -1.1869e+00, -4.0395e+00, -3.8561e+00,\n",
      "          1.8100e-01, -1.6122e+00, -3.7844e+00, -4.7189e+00, -3.6382e+00,\n",
      "         -4.8552e+00, -1.3840e+00, -5.4323e+00, -7.5416e-01, -6.8710e-01,\n",
      "         -8.8512e-01, -6.7426e-01, -8.5057e-01, -6.3133e-01, -1.0056e+00,\n",
      "         -3.9069e+00, -3.7067e+00,  8.0040e-02, -1.3698e+00, -3.7710e+00,\n",
      "         -4.8090e+00, -3.5048e+00, -3.4521e+00, -4.9310e+00, -1.5093e+00,\n",
      "         -8.6040e-01, -1.0710e+00, -1.0894e+00, -1.1574e+00, -1.1453e+00,\n",
      "         -1.1736e+00, -1.2092e+00, -1.0761e+00, -1.1840e+00, -9.4939e-01]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5441, -1.0916], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5317, -1.3059,  0.9900,  0.9900,  0.9900,  0.9900, -0.5226],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1965e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(7.0611e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(1.6409e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1076, -0.1086, -0.1097,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2669, -0.2696, -0.2723,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1057, -0.1068, -0.1079,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5186, -1.2801,  0.9656,  0.9656,  0.9608,  0.9656, -0.5097],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7815, -3.9343, -0.8487,  ..., -5.3167, -5.3180, -5.3375],\n",
      "        [-2.4753, -3.9492, -0.7905,  ..., -4.6817, -4.6731, -4.6750],\n",
      "        [-2.3586, -3.9589, -0.7480,  ..., -4.4842, -4.4796, -4.4816],\n",
      "        ...,\n",
      "        [-2.9283, -3.9093, -0.8903,  ..., -5.5700, -5.5610, -5.5439],\n",
      "        [-2.9425, -3.9044, -0.8863,  ..., -5.6272, -5.6197, -5.6085],\n",
      "        [-2.4746, -3.9539, -0.7995,  ..., -4.6404, -4.6281, -4.6282]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9203, -0.5156, -0.9949,  0.7613,  0.7553,  0.9039,  0.8270, -0.4876],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5532, -1.0043, -0.5349, -0.5022, -0.5350,  0.9900, -0.5336,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.1039e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1119, -0.1131, -0.1142,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1387, -0.1401, -0.1415,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1082, -0.1093, -0.1104,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1080, -0.1090, -0.1101,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5396, -0.9603, -0.5217, -0.4898, -0.5218,  0.9656, -0.5204,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4553, -3.9456, -0.7808,  ..., -4.6317, -4.6256, -4.6229],\n",
      "        [-2.5618, -3.8960, -0.7913,  ..., -4.1559, -4.1460, -4.1315],\n",
      "        [-2.4636, -3.9450, -0.7781,  ..., -4.6379, -4.6304, -4.6304],\n",
      "        ...,\n",
      "        [-2.9331, -3.8974, -0.8740,  ..., -5.5535, -5.5414, -5.5255],\n",
      "        [-2.4611, -3.9492, -0.7846,  ..., -4.6338, -4.6251, -4.6237],\n",
      "        [-2.7688, -3.9302, -0.8487,  ..., -5.2910, -5.2933, -5.3109]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5210, -1.0311, -0.5724, -0.5036, -0.5596,  0.6792, -0.4872,  0.8027],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.1948, -0.3140,  0.9900,  0.9900,  0.9900, -0.5503, -0.5422,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4277e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.4155e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1391, -0.1405, -0.1419,  ..., -1.1710, -1.1828, -1.1948],\n",
      "        [-0.0541, -0.0546, -0.0552,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1113, -0.1124, -0.1136,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1097, -0.1108, -0.1119,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1424, -0.3063,  0.9656,  0.9656,  0.9656, -0.5367, -0.5289,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5645, -3.8773, -0.7909,  ..., -1.0458, -1.1264, -1.0105],\n",
      "        [-2.5562, -3.9210, -0.7713,  ..., -4.5812, -4.5869, -4.5892],\n",
      "        [-2.9248, -3.8953, -0.8622,  ..., -5.6303, -5.6247, -5.6126],\n",
      "        ...,\n",
      "        [-2.4509, -3.9387, -0.7634,  ..., -4.6703, -4.6636, -4.6626],\n",
      "        [-2.4596, -3.9417, -0.7703,  ..., -4.6424, -4.6400, -4.6409],\n",
      "        [-2.7600, -3.9229, -0.8343,  ..., -5.2880, -5.2909, -5.3100]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0631, -0.2706,  0.8029,  0.8805,  0.9128, -0.5126, -0.5571,  0.7976],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5192,  0.9900, -0.5159,  0.9900,  0.9900, -0.5519,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9614e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.4242e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1050, -0.1061, -0.1072,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1117, -0.1128, -0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5064,  0.9608, -0.5032,  0.9656,  0.9656, -0.5383,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9161, -3.8942, -0.8560,  ..., -5.6655, -5.6606, -5.6499],\n",
      "        [-2.4530, -3.9339, -0.7589,  ..., -4.6673, -4.6613, -4.6667],\n",
      "        [-2.9067, -3.8936, -0.8574,  ..., -5.5723, -5.5620, -5.5451],\n",
      "        ...,\n",
      "        [-2.7497, -3.9148, -0.8193,  ..., -5.3013, -5.3071, -5.3262],\n",
      "        [-2.4456, -3.9350, -0.7548,  ..., -4.6294, -4.6223, -4.6231],\n",
      "        [-2.7221, -3.9156, -0.8137,  ..., -5.1866, -5.2358, -5.2615]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7242, -0.5012,  0.8721, -0.4764,  0.8572,  0.7433, -0.5717,  0.9005],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.5179, -0.5427, -0.5754,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9818e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1164, -0.1176, -0.1188,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.5051, -0.5293, -0.5612,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7291, -3.9173, -0.7997,  ..., -5.2689, -5.3238, -5.3484],\n",
      "        [-2.9212, -3.8811, -0.8470,  ..., -5.5557, -5.5439, -5.5286],\n",
      "        [-2.7504, -3.9137, -0.8166,  ..., -5.2808, -5.2835, -5.3040],\n",
      "        ...,\n",
      "        [-2.4471, -3.9277, -0.7584,  ..., -4.6471, -4.6469, -4.6527],\n",
      "        [-2.7056, -3.9067, -0.7857,  ..., -5.4974, -5.5434, -5.5547],\n",
      "        [-2.7653, -3.9191, -0.8219,  ..., -5.3761, -5.3853, -5.4067]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9049,  0.6367,  0.9135, -0.4764, -0.5464, -0.5453,  0.8495,  0.9268],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5710, -0.5503,  0.9900,  0.9900, -0.5044,  0.9900, -0.3366],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5568e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1155, -0.1167, -0.1179,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1113, -0.1124, -0.1136,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1020, -0.1031, -0.1041,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0580, -0.0586, -0.0592,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5569, -0.5367,  0.9656,  0.9656, -0.4919,  0.9656, -0.3283],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7312, -3.9110, -0.7937,  ..., -5.2648, -5.3180, -5.3414],\n",
      "        [-2.4400, -3.9227, -0.7472,  ..., -4.6219, -4.6167, -4.6155],\n",
      "        [-2.4485, -3.9229, -0.7471,  ..., -4.6325, -4.6265, -4.6268],\n",
      "        ...,\n",
      "        [-2.4557, -3.9188, -0.7557,  ..., -4.6904, -4.6808, -4.6845],\n",
      "        [-2.7226, -3.9030, -0.8034,  ..., -5.1854, -5.2313, -5.2557],\n",
      "        [-2.5489, -3.9014, -0.7583,  ..., -4.6559, -4.6681, -4.6676]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9110, -0.5235, -0.5742,  0.8940,  0.9206, -0.5162,  0.9043, -0.2342],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5001], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5156e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1012, -0.1022, -0.1032, -0.1043, -0.1053, -0.1064, -0.1075, -0.1085,\n",
      "         -0.1096, -0.1108, -0.1119, -0.1130, -0.1141, -0.1153, -0.1165, -0.1176,\n",
      "         -0.1188, -0.1200, -0.1212, -0.1225, -0.1237, -0.1249, -0.1262, -0.1275,\n",
      "         -0.1288, -0.1301, -0.1314, -0.1327, -0.1341, -0.1354, -0.1368, -0.1382,\n",
      "         -0.1396, -0.1410, -0.1424, -0.1438, -0.1453, -0.1467, -0.1482, -0.1497,\n",
      "         -0.1512, -0.1528, -0.1543, -0.1559, -0.1574, -0.1590, -0.1606, -0.1623,\n",
      "         -0.1639, -0.1656, -0.1672, -0.1689, -0.1706, -0.1723, -0.1741, -0.1758,\n",
      "         -0.1776, -0.1794, -0.1812, -0.1831, -0.1849, -0.1868, -0.1887, -0.1906,\n",
      "         -0.1925, -0.1944, -0.1964, -0.1984, -0.2004, -0.2024, -0.2045, -0.2065,\n",
      "         -0.2086, -0.2107, -0.2128, -0.2150, -0.2172, -0.2194, -0.2216, -0.2238,\n",
      "         -0.2261, -0.2284, -0.2307, -0.2330, -0.2353, -0.2377, -0.2401, -0.2426,\n",
      "         -0.2450, -0.2475, -0.2500, -0.2525, -0.2551, -0.2576, -0.2602, -0.2629,\n",
      "         -0.2655, -0.2682, -0.2709, -0.2736, -0.2764, -0.2792, -0.2820, -0.2849,\n",
      "         -0.2877, -0.2907, -0.2936, -0.2966, -0.2996, -0.3026, -0.3056, -0.3087,\n",
      "         -0.3118, -0.3150, -0.3182, -0.3214, -0.3246, -0.3279, -0.3312, -0.3346,\n",
      "         -0.3379, -0.3414, -0.3448, -0.3483, -0.3518, -0.3554, -0.3590, -0.3626,\n",
      "         -0.3662, -0.3699, -0.3737, -0.3775, -0.3813, -0.3851, -0.3890, -0.3929,\n",
      "         -0.3969, -0.4009, -0.4050, -0.4091, -0.4132, -0.4174, -0.4216, -0.4258,\n",
      "         -0.4301, -0.4345, -0.4389, -0.4433, -0.4478, -0.4523, -0.4569, -0.4615,\n",
      "         -0.4661, -0.4709, -0.4756, -0.4804, -0.4853, -0.4902, -0.4951, -0.5001,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.4878], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7646, -3.9044, -0.8048, -2.4342,  0.1701, -2.4141, -4.6366, -0.5100,\n",
      "         -4.0952, -2.6028, -3.9878,  0.5166, -0.3203, -3.5906, -3.1585, -2.4382,\n",
      "         -3.3490, -1.7733, -2.0478, -1.0615, -1.4539, -1.5489, -0.9608,  0.4328,\n",
      "         -2.0732, -3.4818, -3.1321, -2.6165, -3.8491,  0.9633, -1.7775, -0.3010,\n",
      "         -0.8528, -3.5578, -1.9230, -1.6828, -0.8442, -3.2526, -1.7018, -0.1212,\n",
      "         -0.6258, -0.4817, -0.3056, -4.2834, -1.7234, -3.4786, -3.5618, -3.2808,\n",
      "          0.2778,  0.6434, -3.8391, -1.5533, -7.1970, -2.9351, -3.0937, -0.9525,\n",
      "         -2.5487, -2.9537, -2.7284, -1.7528, -3.6731, -1.2435, -5.8970, -2.4443,\n",
      "         -3.6017,  1.0932, -1.3961, -0.1952, -0.2852, -3.1956, -4.0964, -5.6169,\n",
      "         -2.8479,  0.0080, -3.8187, -4.2832, -2.5255, -3.9315, -2.5748, -3.5637,\n",
      "         -4.9366, -2.5486, -0.2956, -1.3690, -1.8387, -3.0711, -1.8541, -0.7646,\n",
      "         -3.6778, -0.8342, -3.1835, -4.1562, -2.5014, -0.5718, -0.9902, -3.0710,\n",
      "         -2.8491, -3.8152, -3.3457, -3.3628, -0.0817, -1.0966, -3.7015, -2.1132,\n",
      "         -0.4275, -2.0082, -3.6419, -3.8201,  0.7140, -0.9577, -1.2222, -0.4172,\n",
      "         -4.6051, -4.6294,  0.4509, -1.0046, -2.2291, -1.4401, -0.0576, -1.0226,\n",
      "         -2.8977, -4.5920, -2.6749, -0.8379, -1.1656, -2.9768, -2.2780, -0.8409,\n",
      "         -0.6905, -4.2840, -0.0581,  0.7063, -3.3895, -4.1110, -1.2422, -2.0451,\n",
      "         -3.0815, -2.5445, -1.7707, -2.9094, -2.9670, -5.8471, -1.7225, -2.3842,\n",
      "         -1.2038, -0.7125, -0.5244, -4.0023, -3.9272, -0.2322, -1.9697, -0.9994,\n",
      "         -0.8400, -0.5116, -0.0381, -0.5438, -4.1841, -3.3446, -4.8444, -1.4430,\n",
      "          0.7628,  0.9837,  0.9706,  0.9938,  0.7930,  0.9332, -4.7515, -4.9296,\n",
      "         -5.2341, -5.0353, -5.3253, -5.3764, -5.2857, -5.2798, -5.2611, -5.2186,\n",
      "         -5.1414, -5.0554, -4.9940, -4.9647, -4.9644, -4.9916, -5.0324, -5.0781,\n",
      "         -5.1037, -5.1281, -5.1566, -5.2099, -5.2796, -5.3667, -5.4308, -5.4607,\n",
      "         -5.4615, -5.4723, -5.4975, -5.5253, -5.5251, -5.4977, -5.4633, -5.4159,\n",
      "         -5.3801, -5.3430, -5.3255, -5.3294, -5.3505],\n",
      "        [-2.4609, -3.9182, -0.7504, -2.2415,  0.2537, -2.3464, -4.5498, -0.2457,\n",
      "         -3.7742, -2.4425, -3.9045,  0.5286, -0.2324, -3.5016, -3.2490, -2.2727,\n",
      "         -3.2633, -1.6882, -2.1000, -0.9060, -1.3969, -1.3796, -0.8275,  0.3715,\n",
      "         -1.8749, -3.3840, -3.2008, -2.4750, -3.8446,  0.9494, -1.6476, -0.1252,\n",
      "         -0.8765, -3.4836, -1.8904, -1.7968, -0.8958, -3.1946, -1.5599,  0.0338,\n",
      "         -0.5721, -0.5139, -0.2604, -4.3276, -1.5573, -3.1992, -3.3749, -3.2129,\n",
      "         -0.1383,  0.3210, -3.7820, -1.4763, -7.1844, -2.8387, -3.0577, -0.8474,\n",
      "         -2.5755, -2.9227, -2.6441, -1.5911, -3.5936, -1.1747, -5.8667, -2.3721,\n",
      "         -3.5982,  1.0638, -1.2775, -0.0173, -0.3338, -3.1187, -3.9964, -5.6276,\n",
      "         -2.7596, -0.0588, -3.7531, -4.3299, -2.4384, -3.8985, -2.4605, -3.5042,\n",
      "         -4.9500, -2.4696, -0.2521, -1.2119, -1.7854, -2.9251, -1.7282, -0.6035,\n",
      "         -3.6120, -0.7609, -3.1092, -4.1964, -2.4279, -0.5407, -0.8289, -3.1162,\n",
      "         -2.6982, -3.5766, -3.3050, -3.2675, -0.0210, -0.9472, -3.7207, -2.1028,\n",
      "         -0.4768, -1.8295, -3.4246, -3.7263,  0.6392, -0.9694, -1.3449, -0.4008,\n",
      "         -4.3595, -4.6137,  0.4854, -0.8610, -2.1835, -1.3196,  0.0199, -0.9372,\n",
      "         -2.6984, -4.6066, -2.6380, -0.8387, -1.0617, -2.8777, -2.1463, -0.7542,\n",
      "         -0.5263, -4.2191, -0.4767,  0.3720, -3.3440, -4.0785, -1.1545, -2.0649,\n",
      "         -3.0631, -2.4665, -1.6635, -2.8937, -2.6016, -3.2956, -5.7312, -1.3385,\n",
      "         -0.4375, -0.5525, -0.5189, -0.5258, -0.4005, -0.5828, -4.8916, -4.9660,\n",
      "         -5.3588, -4.9740, -4.8855, -4.6223, -4.5647, -4.6531, -4.6109, -4.5316,\n",
      "         -4.6187, -4.7308, -4.8134, -4.8223, -4.7466, -4.6358, -4.5636, -4.5307,\n",
      "         -4.5182, -4.5106, -4.5089, -4.5263, -4.5564, -4.5736, -4.5873, -4.5951,\n",
      "         -4.5709, -4.5591, -4.5712, -4.5786, -4.5844, -4.6023, -4.5944, -4.5793,\n",
      "         -4.5701, -4.5705, -4.5897, -4.6111, -4.6272, -4.6360, -4.6396, -4.6561,\n",
      "         -4.6682, -4.6761, -4.6722, -4.6696, -4.6747, -4.6716, -4.6663, -4.6625,\n",
      "         -4.6632, -4.6677, -4.6626, -4.6532, -4.6478]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9062, -0.5030], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.5590,  0.9900,  0.9900, -0.5497],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.6660e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1112, -0.1123, -0.1135,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.5452,  0.9656,  0.9608, -0.5362],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9332, -3.8649, -0.8348,  ..., -5.6378, -5.6304, -5.6174],\n",
      "        [-2.7633, -3.8937, -0.8038,  ..., -5.3000, -5.3058, -5.3275],\n",
      "        [-2.9369, -3.8619, -0.8335,  ..., -5.5648, -5.5540, -5.5359],\n",
      "        ...,\n",
      "        [-2.7175, -3.8860, -0.7725,  ..., -5.4785, -5.5181, -5.5290],\n",
      "        [-2.9209, -3.8687, -0.8395,  ..., -5.5823, -5.5677, -5.5518],\n",
      "        [-2.4604, -3.9102, -0.7413,  ..., -4.6370, -4.6288, -4.6287]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7721,  0.9471,  0.6541,  0.9250, -0.5166,  0.8797,  0.8879, -0.5660],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3221, -0.5727,  0.9900,  0.9900, -0.5190, -0.5526,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3161e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0555, -0.0561, -0.0566,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1159, -0.1170, -0.1182,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1118, -0.1129, -0.1141,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3142, -0.5586,  0.9656,  0.9656, -0.5062, -0.5390,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5619, -3.8832, -0.7401,  ..., -4.6220, -4.6288, -4.6296],\n",
      "        [-2.4571, -3.9013, -0.7385,  ..., -4.6231, -4.6162, -4.6142],\n",
      "        [-2.9377, -3.8625, -0.8333,  ..., -5.6784, -5.6729, -5.6629],\n",
      "        ...,\n",
      "        [-2.4706, -3.9048, -0.7425,  ..., -4.6399, -4.6377, -4.6407],\n",
      "        [-2.9433, -3.8539, -0.8277,  ..., -5.5689, -5.5582, -5.5420],\n",
      "        [-2.9377, -3.8625, -0.8333,  ..., -5.6784, -5.6729, -5.6629]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2545, -0.5089,  0.7696,  0.9809, -0.4842, -0.5413,  0.6763,  0.7696],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.5236,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.4571e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1059, -0.1070, -0.1081,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.5107,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7948, -3.8797, -0.7979,  ..., -5.2617, -5.2756, -5.3019],\n",
      "        [-2.7806, -3.8737, -0.7928,  ..., -5.3360, -5.3489, -5.3736],\n",
      "        [-2.7924, -3.8784, -0.7937,  ..., -5.3445, -5.3561, -5.3812],\n",
      "        ...,\n",
      "        [-2.7846, -3.8809, -0.7909,  ..., -5.3381, -5.3469, -5.3702],\n",
      "        [-2.4813, -3.8966, -0.7280,  ..., -4.5757, -4.5770, -4.5793],\n",
      "        [-2.7894, -3.8774, -0.7989,  ..., -5.3160, -5.3245, -5.3454]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9460,  0.8472,  0.9873,  0.9709,  0.8130,  0.9709, -0.4476,  0.8849],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5191, -0.5329,  0.9900, -0.5649,  0.9900, -0.5868, -0.5658, -0.3490],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.3678e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1050, -0.1061, -0.1071,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1078, -0.1089, -0.1100,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1187, -0.1199, -0.1211,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1145, -0.1156, -0.1168,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.0607, -0.0613,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5063, -0.5197,  0.9656, -0.5510,  0.9656, -0.5723, -0.5518, -0.3404],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4835, -3.8805, -0.7331,  ..., -4.6949, -4.6883, -4.6894],\n",
      "        [-2.4848, -3.8871, -0.7300,  ..., -4.6644, -4.6568, -4.6666],\n",
      "        [-2.7968, -3.8676, -0.7931,  ..., -5.3202, -5.3309, -5.3528],\n",
      "        ...,\n",
      "        [-2.4781, -3.8830, -0.7312,  ..., -4.6533, -4.6511, -4.6565],\n",
      "        [-2.4759, -3.8889, -0.7305,  ..., -4.6260, -4.6167, -4.6194],\n",
      "        [-2.5785, -3.8627, -0.7353,  ..., -4.7052, -4.7142, -4.7136]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4777, -0.4615,  0.9083, -0.5289,  0.9585, -0.5068, -0.4527, -0.2153],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5381, -0.5814, -0.5745, -0.5938, -1.2315,  0.9900,  0.9900, -1.3569],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.6232e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1089, -0.1100, -0.1111,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1176, -0.1188, -0.1200,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1162, -0.1174, -0.1186,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2773, -0.2801, -0.2829,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5248, -0.5671, -0.5603, -0.5791, -1.1775,  0.9656,  0.9656, -1.3301],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4945, -3.8829, -0.7161,  ..., -4.5817, -4.5789, -4.5866],\n",
      "        [-2.4844, -3.8769, -0.7179,  ..., -4.6715, -4.6648, -4.6629],\n",
      "        [-2.4914, -3.8817, -0.7239,  ..., -4.6396, -4.6396, -4.6411],\n",
      "        ...,\n",
      "        [-2.7748, -3.8682, -0.7647,  ..., -5.3095, -5.3600, -5.3825],\n",
      "        [-2.8045, -3.8625, -0.7817,  ..., -5.3190, -5.3274, -5.3559],\n",
      "        [-2.3667, -3.8915, -0.6743,  ..., -4.4827, -4.4791, -4.4810]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4340, -0.4789, -0.5118, -0.4827, -1.0409,  1.0110,  1.0333, -0.9218],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -1.0565, -0.5364, -0.5704,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.4970e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1085, -0.1096, -0.1107,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1154, -0.1166, -0.1177,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -1.0102, -0.5232, -0.5563,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7769, -3.8568, -0.7700,  ..., -5.2326, -5.2796, -5.2992],\n",
      "        [-2.8121, -3.8562, -0.7782,  ..., -5.3453, -5.3613, -5.3871],\n",
      "        [-2.7866, -3.8651, -0.7611,  ..., -5.3181, -5.3704, -5.3917],\n",
      "        ...,\n",
      "        [-2.4999, -3.8764, -0.7207,  ..., -4.6662, -4.6553, -4.6534],\n",
      "        [-2.5030, -3.8766, -0.7148,  ..., -4.6168, -4.6125, -4.6189],\n",
      "        [-2.9687, -3.8328, -0.8087,  ..., -5.6057, -5.5960, -5.5817]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0027,  0.8946,  1.0121,  1.0007, -1.0026, -0.4625, -0.5087,  0.9562],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5812, -0.5816], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.2218e-10, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1176, -0.1188, -0.1200, -0.1212, -0.1224, -0.1236, -0.1249, -0.1261,\n",
      "         -0.1274, -0.1287, -0.1300, -0.1313, -0.1326, -0.1340, -0.1353, -0.1367,\n",
      "         -0.1381, -0.1395, -0.1409, -0.1423, -0.1437, -0.1452, -0.1467, -0.1481,\n",
      "         -0.1496, -0.1512, -0.1527, -0.1542, -0.1558, -0.1574, -0.1589, -0.1605,\n",
      "         -0.1622, -0.1638, -0.1655, -0.1671, -0.1688, -0.1705, -0.1722, -0.1740,\n",
      "         -0.1757, -0.1775, -0.1793, -0.1811, -0.1830, -0.1848, -0.1867, -0.1886,\n",
      "         -0.1905, -0.1924, -0.1943, -0.1963, -0.1983, -0.2003, -0.2023, -0.2043,\n",
      "         -0.2064, -0.2085, -0.2106, -0.2127, -0.2149, -0.2170, -0.2192, -0.2215,\n",
      "         -0.2237, -0.2259, -0.2282, -0.2305, -0.2329, -0.2352, -0.2376, -0.2400,\n",
      "         -0.2424, -0.2449, -0.2473, -0.2498, -0.2524, -0.2549, -0.2575, -0.2601,\n",
      "         -0.2627, -0.2654, -0.2680, -0.2708, -0.2735, -0.2763, -0.2790, -0.2819,\n",
      "         -0.2847, -0.2876, -0.2905, -0.2934, -0.2964, -0.2994, -0.3024, -0.3055,\n",
      "         -0.3085, -0.3117, -0.3148, -0.3180, -0.3212, -0.3244, -0.3277, -0.3310,\n",
      "         -0.3344, -0.3378, -0.3412, -0.3446, -0.3481, -0.3516, -0.3552, -0.3587,\n",
      "         -0.3624, -0.3660, -0.3697, -0.3735, -0.3772, -0.3810, -0.3849, -0.3888,\n",
      "         -0.3927, -0.3967, -0.4007, -0.4047, -0.4088, -0.4129, -0.4171, -0.4213,\n",
      "         -0.4256, -0.4299, -0.4342, -0.4386, -0.4430, -0.4475, -0.4520, -0.4566,\n",
      "         -0.4612, -0.4659, -0.4706, -0.4753, -0.4801, -0.4850, -0.4899, -0.4948,\n",
      "         -0.4998, -0.5049, -0.5100, -0.5151, -0.5203, -0.5256, -0.5309, -0.5363,\n",
      "         -0.5417, -0.5472, -0.5527, -0.5583, -0.5639, -0.5696, -0.5754, -0.5812,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1177, -0.1188, -0.1200, -0.1213, -0.1225, -0.1237, -0.1250, -0.1262,\n",
      "         -0.1275, -0.1288, -0.1301, -0.1314, -0.1327, -0.1341, -0.1354, -0.1368,\n",
      "         -0.1382, -0.1396, -0.1410, -0.1424, -0.1438, -0.1453, -0.1468, -0.1483,\n",
      "         -0.1497, -0.1513, -0.1528, -0.1543, -0.1559, -0.1575, -0.1591, -0.1607,\n",
      "         -0.1623, -0.1639, -0.1656, -0.1673, -0.1689, -0.1706, -0.1724, -0.1741,\n",
      "         -0.1759, -0.1776, -0.1794, -0.1813, -0.1831, -0.1849, -0.1868, -0.1887,\n",
      "         -0.1906, -0.1925, -0.1945, -0.1964, -0.1984, -0.2004, -0.2024, -0.2045,\n",
      "         -0.2066, -0.2086, -0.2107, -0.2129, -0.2150, -0.2172, -0.2194, -0.2216,\n",
      "         -0.2238, -0.2261, -0.2284, -0.2307, -0.2330, -0.2354, -0.2378, -0.2402,\n",
      "         -0.2426, -0.2450, -0.2475, -0.2500, -0.2525, -0.2551, -0.2577, -0.2603,\n",
      "         -0.2629, -0.2656, -0.2682, -0.2709, -0.2737, -0.2764, -0.2792, -0.2821,\n",
      "         -0.2849, -0.2878, -0.2907, -0.2936, -0.2966, -0.2996, -0.3026, -0.3057,\n",
      "         -0.3088, -0.3119, -0.3150, -0.3182, -0.3214, -0.3247, -0.3280, -0.3313,\n",
      "         -0.3346, -0.3380, -0.3414, -0.3449, -0.3483, -0.3519, -0.3554, -0.3590,\n",
      "         -0.3626, -0.3663, -0.3700, -0.3737, -0.3775, -0.3813, -0.3852, -0.3891,\n",
      "         -0.3930, -0.3970, -0.4010, -0.4050, -0.4091, -0.4132, -0.4174, -0.4216,\n",
      "         -0.4259, -0.4302, -0.4345, -0.4389, -0.4434, -0.4478, -0.4524, -0.4569,\n",
      "         -0.4616, -0.4662, -0.4709, -0.4757, -0.4805, -0.4853, -0.4902, -0.4952,\n",
      "         -0.5002, -0.5052, -0.5104, -0.5155, -0.5207, -0.5260, -0.5313, -0.5367,\n",
      "         -0.5421, -0.5475, -0.5531, -0.5587, -0.5643, -0.5700, -0.5758, -0.5816,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5668, -0.5672], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5004e+00, -3.8746e+00, -7.0972e-01, -2.1902e+00,  2.1987e-01,\n",
      "         -2.3383e+00, -4.5405e+00, -2.5007e-01, -3.8291e+00, -2.4874e+00,\n",
      "         -3.9400e+00,  5.3903e-01, -2.1051e-01, -3.5184e+00, -3.1613e+00,\n",
      "         -2.2277e+00, -3.2642e+00, -1.7415e+00, -2.0373e+00, -9.5898e-01,\n",
      "         -1.4105e+00, -1.4113e+00, -8.5644e-01,  3.9880e-01, -1.9075e+00,\n",
      "         -3.4091e+00, -3.1198e+00, -2.4350e+00, -3.8443e+00,  9.7673e-01,\n",
      "         -1.6868e+00, -1.7198e-01, -8.8260e-01, -3.5081e+00, -1.9094e+00,\n",
      "         -1.8854e+00, -8.9201e-01, -3.2003e+00, -1.5633e+00,  6.1019e-04,\n",
      "         -6.3535e-01, -4.9625e-01, -2.5720e-01, -4.2766e+00, -1.5890e+00,\n",
      "         -3.2464e+00, -3.4385e+00, -3.2433e+00,  9.7739e-02,  3.2096e-01,\n",
      "         -3.8043e+00, -1.4654e+00, -7.1424e+00, -2.8017e+00, -3.0577e+00,\n",
      "         -8.9375e-01, -2.5382e+00, -2.8861e+00, -2.6264e+00, -1.6206e+00,\n",
      "         -3.6194e+00, -1.1634e+00, -5.8176e+00, -2.3286e+00, -3.5949e+00,\n",
      "          1.0929e+00, -1.3113e+00, -6.2686e-02, -3.3708e-01, -3.1442e+00,\n",
      "         -3.9980e+00, -5.6062e+00, -2.7205e+00, -1.2117e-01, -3.7723e+00,\n",
      "         -4.2739e+00, -2.3957e+00, -3.9020e+00, -2.4694e+00, -3.5242e+00,\n",
      "         -4.9247e+00, -2.4285e+00, -2.8287e-01, -1.2692e+00, -1.7774e+00,\n",
      "         -2.9359e+00, -1.7707e+00, -6.5153e-01, -3.6135e+00, -8.1784e-01,\n",
      "         -3.1292e+00, -4.1425e+00, -2.3826e+00, -5.7438e-01, -8.8612e-01,\n",
      "         -3.1011e+00, -2.7476e+00, -3.6244e+00, -3.3120e+00, -3.2547e+00,\n",
      "         -4.9307e-02, -1.0034e+00, -3.6810e+00, -2.1055e+00, -4.6214e-01,\n",
      "         -1.8586e+00, -3.4702e+00, -3.7493e+00,  5.8041e-01, -1.1830e+00,\n",
      "         -1.5632e+00, -6.8518e-01, -4.4679e+00, -4.5536e+00,  4.2126e-01,\n",
      "         -9.7813e-01, -2.2204e+00, -1.3564e+00, -3.9024e-02, -9.9845e-01,\n",
      "         -2.7276e+00, -4.6528e+00, -2.5835e+00, -8.0992e-01, -1.1065e+00,\n",
      "         -2.8706e+00, -2.2115e+00, -7.7582e-01, -5.8573e-01, -4.2450e+00,\n",
      "         -3.7288e-01,  3.6694e-01, -3.3930e+00, -4.0826e+00, -1.2080e+00,\n",
      "         -2.0186e+00, -3.0293e+00, -2.4335e+00, -1.6897e+00, -2.8433e+00,\n",
      "         -2.5714e+00, -3.3500e+00, -5.5384e+00, -1.2633e+00, -6.4439e-01,\n",
      "         -5.3841e-01, -4.2055e-01, -6.5114e-01, -3.9276e-01, -5.3244e-01,\n",
      "         -4.9311e+00, -4.9744e+00, -5.3596e+00, -4.9732e+00, -4.8831e+00,\n",
      "         -4.6296e+00, -4.5297e+00, -4.6116e+00, -4.5609e+00, -4.5096e+00,\n",
      "         -4.5917e+00, -4.6744e+00, -4.7447e+00, -4.7616e+00, -4.7206e+00,\n",
      "         -4.6475e+00, -4.6014e+00, -4.5848e+00, -4.5879e+00, -4.5848e+00,\n",
      "         -4.5792e+00, -4.5940e+00, -4.6214e+00, -4.6407e+00, -4.6617e+00,\n",
      "         -4.6626e+00, -4.6363e+00, -4.6123e+00, -4.6162e+00, -4.6195e+00,\n",
      "         -4.6282e+00, -4.6431e+00, -4.6307e+00, -4.6158e+00, -4.6111e+00,\n",
      "         -4.6110e+00, -4.6314e+00, -4.6511e+00, -4.6643e+00, -4.6698e+00,\n",
      "         -4.6701e+00, -4.6745e+00, -4.6768e+00, -4.6738e+00, -4.6634e+00,\n",
      "         -4.6582e+00, -4.6621e+00, -4.6562e+00, -4.6480e+00, -4.6416e+00,\n",
      "         -4.6389e+00, -4.6476e+00, -4.6423e+00, -4.6350e+00, -4.6361e+00],\n",
      "        [-2.4980e+00, -3.8789e+00, -7.1447e-01, -2.2005e+00,  2.1559e-01,\n",
      "         -2.3398e+00, -4.5556e+00, -2.5644e-01, -3.8303e+00, -2.4949e+00,\n",
      "         -3.9450e+00,  5.3828e-01, -2.1301e-01, -3.5184e+00, -3.1724e+00,\n",
      "         -2.2308e+00, -3.2643e+00, -1.7416e+00, -2.0386e+00, -9.5876e-01,\n",
      "         -1.4143e+00, -1.4130e+00, -8.5974e-01,  3.9715e-01, -1.9073e+00,\n",
      "         -3.4100e+00, -3.1299e+00, -2.4389e+00, -3.8486e+00,  9.7460e-01,\n",
      "         -1.6868e+00, -1.7275e-01, -8.7755e-01, -3.5123e+00, -1.9100e+00,\n",
      "         -1.8823e+00, -8.8829e-01, -3.1975e+00, -1.5613e+00, -7.2571e-04,\n",
      "         -6.3316e-01, -4.9485e-01, -2.6157e-01, -4.2792e+00, -1.5894e+00,\n",
      "         -3.2494e+00, -3.4429e+00, -3.2470e+00,  1.0078e-01,  3.2769e-01,\n",
      "         -3.8074e+00, -1.4739e+00, -7.1475e+00, -2.8071e+00, -3.0571e+00,\n",
      "         -8.9641e-01, -2.5375e+00, -2.8946e+00, -2.6314e+00, -1.6200e+00,\n",
      "         -3.6197e+00, -1.1723e+00, -5.8238e+00, -2.3347e+00, -3.5994e+00,\n",
      "          1.0899e+00, -1.3128e+00, -6.4618e-02, -3.3193e-01, -3.1473e+00,\n",
      "         -4.0042e+00, -5.6110e+00, -2.7245e+00, -1.1718e-01, -3.7742e+00,\n",
      "         -4.2789e+00, -2.4008e+00, -3.9034e+00, -2.4685e+00, -3.5255e+00,\n",
      "         -4.9301e+00, -2.4330e+00, -2.8412e-01, -1.2722e+00, -1.7788e+00,\n",
      "         -2.9400e+00, -1.7735e+00, -6.5429e-01, -3.6158e+00, -8.1434e-01,\n",
      "         -3.1306e+00, -4.1481e+00, -2.3879e+00, -5.7466e-01, -8.8788e-01,\n",
      "         -3.1030e+00, -2.7463e+00, -3.6301e+00, -3.3099e+00, -3.2575e+00,\n",
      "         -5.0475e-02, -1.0065e+00, -3.6857e+00, -2.1027e+00, -4.6278e-01,\n",
      "         -1.8624e+00, -3.4747e+00, -3.7575e+00,  5.8289e-01, -5.4213e-01,\n",
      "         -1.3676e+00, -7.1460e-01, -4.2698e+00, -4.5837e+00,  4.8255e-01,\n",
      "         -9.0866e-01, -2.1996e+00, -1.3528e+00, -7.1012e-02, -9.5752e-01,\n",
      "         -2.7827e+00, -4.5918e+00, -2.5799e+00, -7.9783e-01, -1.1281e+00,\n",
      "         -2.8685e+00, -2.1934e+00, -7.5382e-01, -5.7236e-01, -4.2211e+00,\n",
      "         -2.5744e-01,  3.6822e-01, -3.3998e+00, -4.0812e+00, -1.2150e+00,\n",
      "         -2.0175e+00, -3.0177e+00, -2.4818e+00, -1.6892e+00, -2.9347e+00,\n",
      "         -2.6267e+00, -3.3532e+00, -5.6895e+00, -1.3735e+00, -4.0198e-01,\n",
      "         -5.4613e-01, -3.0349e-01, -5.8163e-01, -4.3773e-01, -4.5078e-01,\n",
      "         -4.8930e+00, -4.9950e+00, -5.3739e+00, -4.9925e+00, -4.8987e+00,\n",
      "         -4.6383e+00, -4.5980e+00, -4.6856e+00, -4.6583e+00, -4.5983e+00,\n",
      "         -4.6522e+00, -4.7202e+00, -4.7771e+00, -4.7842e+00, -4.7149e+00,\n",
      "         -4.6067e+00, -4.5346e+00, -4.5046e+00, -4.5034e+00, -4.4897e+00,\n",
      "         -4.4804e+00, -4.4906e+00, -4.5193e+00, -4.5465e+00, -4.5742e+00,\n",
      "         -4.5841e+00, -4.5560e+00, -4.5344e+00, -4.5472e+00, -4.5584e+00,\n",
      "         -4.5712e+00, -4.5839e+00, -4.5710e+00, -4.5514e+00, -4.5463e+00,\n",
      "         -4.5494e+00, -4.5746e+00, -4.5970e+00, -4.6105e+00, -4.6134e+00,\n",
      "         -4.6140e+00, -4.6249e+00, -4.6374e+00, -4.6467e+00, -4.6440e+00,\n",
      "         -4.6405e+00, -4.6468e+00, -4.6378e+00, -4.6374e+00, -4.6385e+00,\n",
      "         -4.6344e+00, -4.6391e+00, -4.6329e+00, -4.6239e+00, -4.6264e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5299, -0.4536], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5733,  0.9900, -0.5832, -0.5297,  0.9900, -0.5946,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6904e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0219e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1160, -0.1171, -0.1183,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1180, -0.1192, -0.1204,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1203, -0.1215, -0.1227,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5591,  0.9656, -0.5688, -0.5166,  0.9608, -0.5800,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5159, -3.8771, -0.7113,  ..., -4.6491, -4.6413, -4.6437],\n",
      "        [-3.0083, -3.8308, -0.8008,  ..., -5.6744, -5.6673, -5.6586],\n",
      "        [-2.5133, -3.8750, -0.7095,  ..., -4.6859, -4.6799, -4.6775],\n",
      "        ...,\n",
      "        [-2.5059, -3.8763, -0.7101,  ..., -4.6337, -4.6315, -4.6292],\n",
      "        [-2.8454, -3.8605, -0.7781,  ..., -5.3304, -5.3434, -5.3677],\n",
      "        [-2.7999, -3.8573, -0.7646,  ..., -5.2386, -5.2863, -5.3069]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5419,  0.8381, -0.4993, -0.4779,  0.9461, -0.5008,  0.9297,  0.9900],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.5837, -0.5637, -0.5295, -0.5637, -0.5271],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0076e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1071, -0.1082, -0.1093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1140, -0.1152, -0.1164,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1066, -0.1077, -0.1088,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.5693, -0.5498, -0.5164, -0.5498, -0.5141],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8479, -3.8677, -0.7791,  ..., -5.3287, -5.3389, -5.3684],\n",
      "        [-2.8532, -3.8664, -0.7805,  ..., -5.3627, -5.3741, -5.4034],\n",
      "        [-2.8462, -3.8619, -0.7792,  ..., -5.3480, -5.3625, -5.3890],\n",
      "        ...,\n",
      "        [-2.5344, -3.8852, -0.7222,  ..., -4.6840, -4.6755, -4.6830],\n",
      "        [-2.5338, -3.8865, -0.7211,  ..., -4.6572, -4.6541, -4.6570],\n",
      "        [-2.5370, -3.8876, -0.7139,  ..., -4.5992, -4.5976, -4.6009]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0083,  0.9478,  0.8835, -0.5379, -0.5484, -0.4949, -0.5484, -0.4731],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.3315,  0.9900,  0.9900, -0.5509,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.9894e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0571, -0.0577, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.3233,  0.9656,  0.9656, -0.5373,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8600, -3.8714, -0.7885,  ..., -5.3299, -5.3424, -5.3653],\n",
      "        [-2.6374, -3.8669, -0.7347,  ..., -4.7317, -4.7407, -4.7405],\n",
      "        [-2.8669, -3.8773, -0.7918,  ..., -5.4062, -5.4209, -5.4503],\n",
      "        ...,\n",
      "        [-3.0107, -3.8442, -0.8162,  ..., -5.6115, -5.6010, -5.5849],\n",
      "        [-3.0271, -3.8395, -0.8124,  ..., -5.5960, -5.5899, -5.5750],\n",
      "        [-2.8247, -3.8769, -0.7661,  ..., -5.3169, -5.3642, -5.3891]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9098, -0.2714,  0.9956,  0.9225, -0.5884,  0.9223,  0.7184,  0.9701],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5383, -0.3055, -0.5073, -0.5282,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6188e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1089, -0.1100, -0.1111,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0526, -0.0531, -0.0537,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5250, -0.2979, -0.4948, -0.5152,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8573, -3.8763, -0.7888,  ..., -5.3213, -5.3310, -5.3595],\n",
      "        [-2.5437, -3.8941, -0.7264,  ..., -4.6560, -4.6467, -4.6462],\n",
      "        [-2.6415, -3.8741, -0.7293,  ..., -4.6703, -4.6782, -4.6783],\n",
      "        ...,\n",
      "        [-3.0261, -3.8535, -0.8187,  ..., -5.7052, -5.7026, -5.6941],\n",
      "        [-2.8698, -3.8776, -0.7882,  ..., -5.3552, -5.3674, -5.3942],\n",
      "        [-2.8193, -3.8725, -0.7791,  ..., -5.2276, -5.2766, -5.2970]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9770, -0.6052, -0.2990, -0.5300, -0.5786,  0.7689,  0.9748,  0.9341],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -1.3150,  0.9900, -0.4868, -0.5336, -0.5424],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5376e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0985, -0.0995, -0.1005,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1097, -0.1108, -0.1119,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -1.2890,  0.9656, -0.4748, -0.5204, -0.5290],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8642, -3.8749, -0.7813,  ..., -5.3383, -5.3492, -5.3742],\n",
      "        [-2.8593, -3.8797, -0.7838,  ..., -5.3355, -5.3484, -5.3720],\n",
      "        [-3.0284, -3.8460, -0.8165,  ..., -5.5898, -5.5826, -5.5680],\n",
      "        ...,\n",
      "        [-2.5467, -3.8919, -0.7343,  ..., -4.7091, -4.7014, -4.7040],\n",
      "        [-2.5399, -3.8985, -0.7343,  ..., -4.6407, -4.6306, -4.6317],\n",
      "        [-2.5386, -3.8935, -0.7251,  ..., -4.6851, -4.6806, -4.6771]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9368,  0.9641,  0.7067, -1.0010,  0.7554, -0.5521, -0.5439, -0.5746],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5547,  0.9900, -1.0248,  0.9900, -0.5343,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0756e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1122, -0.1134, -0.1145,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1081, -0.1092, -0.1103,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5411,  0.9656, -0.9799,  0.9656, -0.5211,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8410, -3.8731, -0.7845,  ..., -5.3309, -5.3446, -5.3725],\n",
      "        [-2.8609, -3.8805, -0.7887,  ..., -5.2642, -5.2775, -5.3080],\n",
      "        [-2.5225, -3.8949, -0.7244,  ..., -4.6271, -4.6225, -4.6193],\n",
      "        ...,\n",
      "        [-2.8167, -3.8817, -0.7651,  ..., -5.3041, -5.3545, -5.3767],\n",
      "        [-2.5286, -3.8983, -0.7304,  ..., -4.6331, -4.6266, -4.6254],\n",
      "        [-3.0158, -3.8482, -0.8134,  ..., -5.6621, -5.6562, -5.6474]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8578,  0.9065, -0.5657,  0.9573, -1.0520,  0.9409, -0.5466,  0.7658],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2157, -0.5064], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(9.6813e-10, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1415, -0.1429, -0.1444, -0.1458, -0.1473, -0.1488, -0.1503, -0.1518,\n",
      "         -0.1534, -0.1549, -0.1565, -0.1580, -0.1596, -0.1613, -0.1629, -0.1645,\n",
      "         -0.1662, -0.1679, -0.1696, -0.1713, -0.1730, -0.1748, -0.1765, -0.1783,\n",
      "         -0.1801, -0.1819, -0.1838, -0.1856, -0.1875, -0.1894, -0.1913, -0.1932,\n",
      "         -0.1952, -0.1972, -0.1991, -0.2012, -0.2032, -0.2052, -0.2073, -0.2094,\n",
      "         -0.2115, -0.2137, -0.2158, -0.2180, -0.2202, -0.2224, -0.2247, -0.2269,\n",
      "         -0.2292, -0.2316, -0.2339, -0.2363, -0.2386, -0.2410, -0.2435, -0.2459,\n",
      "         -0.2484, -0.2509, -0.2535, -0.2560, -0.2586, -0.2612, -0.2639, -0.2665,\n",
      "         -0.2692, -0.2719, -0.2747, -0.2775, -0.2803, -0.2831, -0.2860, -0.2889,\n",
      "         -0.2918, -0.2947, -0.2977, -0.3007, -0.3037, -0.3068, -0.3099, -0.3130,\n",
      "         -0.3162, -0.3194, -0.3226, -0.3259, -0.3292, -0.3325, -0.3358, -0.3392,\n",
      "         -0.3427, -0.3461, -0.3496, -0.3532, -0.3567, -0.3603, -0.3640, -0.3676,\n",
      "         -0.3714, -0.3751, -0.3789, -0.3827, -0.3866, -0.3905, -0.3944, -0.3984,\n",
      "         -0.4024, -0.4065, -0.4106, -0.4148, -0.4190, -0.4232, -0.4275, -0.4318,\n",
      "         -0.4361, -0.4405, -0.4450, -0.4495, -0.4540, -0.4586, -0.4633, -0.4679,\n",
      "         -0.4727, -0.4774, -0.4823, -0.4871, -0.4920, -0.4970, -0.5020, -0.5071,\n",
      "         -0.5122, -0.5174, -0.5226, -0.5279, -0.5332, -0.5386, -0.5441, -0.5496,\n",
      "         -0.5551, -0.5607, -0.5664, -0.5721, -0.5779, -0.5837, -0.5896, -0.5956,\n",
      "         -0.6016, -0.6077, -0.6138, -0.6200, -0.6263, -0.6326, -0.6390, -0.6454,\n",
      "         -0.6520, -0.6585, -0.6652, -0.6719, -0.6787, -0.6856, -0.6925, -0.6995,\n",
      "         -0.7065, -0.7137, -0.7209, -0.7282, -0.7355, -0.7430, -0.7505, -0.7580,\n",
      "         -0.7657, -0.7734, -0.7812, -0.7891, -0.7971, -0.8052, -0.8133, -0.8215,\n",
      "         -0.8298, -0.8382, -0.8467, -0.8552, -0.8638, -0.8726, -0.8814, -0.8903,\n",
      "         -0.8993, -0.9084, -0.9175, -0.9268, -0.9362, -0.9456, -0.9552, -0.9648,\n",
      "         -0.9746, -0.9844, -0.9944, -1.0044, -1.0145, -1.0248, -1.0351, -1.0456,\n",
      "         -1.0562, -1.0668, -1.0776, -1.0885, -1.0995, -1.1106, -1.1218, -1.1331,\n",
      "         -1.1446, -1.1562, -1.1678, -1.1796, -1.1915, -1.2036, -1.2157],\n",
      "        [-0.1025, -0.1035, -0.1045, -0.1056, -0.1067, -0.1077, -0.1088, -0.1099,\n",
      "         -0.1110, -0.1122, -0.1133, -0.1144, -0.1156, -0.1168, -0.1179, -0.1191,\n",
      "         -0.1203, -0.1215, -0.1228, -0.1240, -0.1253, -0.1265, -0.1278, -0.1291,\n",
      "         -0.1304, -0.1317, -0.1330, -0.1344, -0.1358, -0.1371, -0.1385, -0.1399,\n",
      "         -0.1413, -0.1427, -0.1442, -0.1456, -0.1471, -0.1486, -0.1501, -0.1516,\n",
      "         -0.1532, -0.1547, -0.1563, -0.1578, -0.1594, -0.1610, -0.1627, -0.1643,\n",
      "         -0.1660, -0.1676, -0.1693, -0.1711, -0.1728, -0.1745, -0.1763, -0.1781,\n",
      "         -0.1799, -0.1817, -0.1835, -0.1854, -0.1872, -0.1891, -0.1910, -0.1930,\n",
      "         -0.1949, -0.1969, -0.1989, -0.2009, -0.2029, -0.2050, -0.2070, -0.2091,\n",
      "         -0.2112, -0.2134, -0.2155, -0.2177, -0.2199, -0.2221, -0.2244, -0.2266,\n",
      "         -0.2289, -0.2312, -0.2336, -0.2359, -0.2383, -0.2407, -0.2432, -0.2456,\n",
      "         -0.2481, -0.2506, -0.2531, -0.2557, -0.2583, -0.2609, -0.2635, -0.2662,\n",
      "         -0.2689, -0.2716, -0.2743, -0.2771, -0.2799, -0.2827, -0.2856, -0.2885,\n",
      "         -0.2914, -0.2943, -0.2973, -0.3003, -0.3033, -0.3064, -0.3095, -0.3126,\n",
      "         -0.3158, -0.3190, -0.3222, -0.3254, -0.3287, -0.3321, -0.3354, -0.3388,\n",
      "         -0.3422, -0.3457, -0.3492, -0.3527, -0.3563, -0.3599, -0.3635, -0.3672,\n",
      "         -0.3709, -0.3746, -0.3784, -0.3822, -0.3861, -0.3900, -0.3939, -0.3979,\n",
      "         -0.4019, -0.4060, -0.4101, -0.4142, -0.4184, -0.4226, -0.4269, -0.4312,\n",
      "         -0.4356, -0.4400, -0.4444, -0.4489, -0.4534, -0.4580, -0.4626, -0.4673,\n",
      "         -0.4720, -0.4768, -0.4816, -0.4865, -0.4914, -0.4964, -0.5014, -0.5064,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1625, -0.4939], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6280, -3.8308, -0.7426, -2.2497,  0.2016, -2.3718, -4.5953, -0.2671,\n",
      "         -3.9627, -2.5008, -3.9738,  0.3995, -0.2560, -3.5411, -3.1883, -2.3345,\n",
      "         -3.2911, -1.7494, -2.0745, -0.9748, -1.3616, -1.4042, -0.8403,  0.3985,\n",
      "         -1.9856, -3.4080, -3.1234, -2.5272, -3.8460,  0.9228, -1.7023, -0.1835,\n",
      "         -0.8589, -3.5455, -1.8471, -1.8417, -0.8833, -3.2174, -1.5739,  0.0687,\n",
      "         -0.6141, -0.4930, -0.2851, -4.2867, -1.6533, -3.3468, -3.4676, -3.2692,\n",
      "          0.0642,  0.3330, -3.8068, -1.5226, -7.1711, -2.8768, -3.0722, -0.9015,\n",
      "         -2.5085, -2.8947, -2.6475, -1.6708, -3.6139, -1.2046, -5.8320, -2.3887,\n",
      "         -3.5860,  1.0511, -1.3158, -0.0691, -0.3129, -3.1642, -4.0220, -5.5663,\n",
      "         -2.7903, -0.0907, -3.7629, -4.2998, -2.4555, -3.9137, -2.4788, -3.5087,\n",
      "         -4.8778, -2.4800, -0.2712, -1.2666, -1.7287, -2.9933, -1.7853, -0.6567,\n",
      "         -3.5604, -0.7980, -3.1157, -4.1670, -2.4346, -0.5700, -0.8755, -3.0619,\n",
      "         -2.8003, -3.7113, -3.2793, -3.2633, -0.0458, -0.9881, -3.7061, -2.1243,\n",
      "         -0.4598, -1.8515, -3.5379, -3.7942,  0.6230, -5.2445, -1.5686, -2.7443,\n",
      "         -4.2966, -4.5346,  0.5640, -0.8915, -2.1535, -1.2982, -0.0554, -0.9146,\n",
      "         -2.7564, -4.6968, -2.6034, -0.8338, -1.0512, -2.8532, -2.1895, -0.7800,\n",
      "         -0.7057, -4.2619, -0.2520,  0.4070, -3.3519, -4.1041, -1.2255, -1.9979,\n",
      "         -3.0658, -2.4542, -1.7067, -2.9364, -2.9406, -5.8287, -1.5233, -6.0967,\n",
      "         -1.3948, -1.2894, -1.2255, -1.0183, -1.2110, -1.0125, -1.1362, -4.2528,\n",
      "         -4.8261, -0.3205, -1.8941, -4.5974, -5.7466, -4.8035, -5.3321, -1.8554,\n",
      "         -5.6767, -0.6238, -0.8912, -0.9718, -0.8681, -0.9283, -0.7760, -1.1374,\n",
      "         -4.1293, -3.8834,  0.1507, -1.5893, -3.6603, -4.6147, -3.7102, -4.8325,\n",
      "         -1.3601, -5.3906, -0.6932, -0.6763, -0.8299, -0.6593, -0.7917, -0.6116,\n",
      "         -0.9504, -3.9973, -3.7330,  0.0513, -1.3453, -3.6462, -4.7085, -3.5808,\n",
      "         -3.5166, -4.9100, -1.4965, -0.9789, -1.0769, -1.0490, -1.1511, -1.0945,\n",
      "         -1.1682, -1.1564, -1.0703, -1.1308, -1.1007],\n",
      "        [-2.5260, -3.8973, -0.7156, -2.2020,  0.2248, -2.3335, -4.5108, -0.2153,\n",
      "         -3.8484, -2.4967, -3.9029,  0.4412, -0.2252, -3.5279, -3.1902, -2.2597,\n",
      "         -3.2519, -1.7403, -2.0696, -0.9420, -1.3892, -1.3833, -0.8384,  0.4073,\n",
      "         -1.9128, -3.4108, -3.1415, -2.4622, -3.8218,  0.9565, -1.6785, -0.1523,\n",
      "         -0.8956, -3.4713, -1.9079, -1.8614, -0.8997, -3.1854, -1.5623,  0.0311,\n",
      "         -0.6274, -0.5013, -0.2534, -4.2966, -1.5989, -3.2701, -3.4529, -3.2061,\n",
      "         -0.0301,  0.2509, -3.8201, -1.4694, -7.1594, -2.8291, -3.0459, -0.8807,\n",
      "         -2.5357, -2.8851, -2.6227, -1.6298, -3.6220, -1.1613, -5.8313, -2.3586,\n",
      "         -3.5693,  1.0781, -1.2989, -0.0364, -0.3483, -3.0995, -4.0136, -5.6239,\n",
      "         -2.7460, -0.1185, -3.7729, -4.2963, -2.4163, -3.8903, -2.4729, -3.5247,\n",
      "         -4.9367, -2.4498, -0.2822, -1.2522, -1.7888, -2.9511, -1.7587, -0.6321,\n",
      "         -3.5947, -0.8111, -3.1279, -4.1619, -2.4048, -0.5793, -0.8669, -3.1008,\n",
      "         -2.7589, -3.6476, -3.3181, -3.2524, -0.0488, -0.9827, -3.6908, -2.1373,\n",
      "         -0.4571, -1.8403, -3.4853, -3.7689,  0.5859, -1.5213, -1.3966, -0.7597,\n",
      "         -4.4908, -4.7173,  0.4880, -0.8636, -2.1721, -1.2780, -0.0151, -0.9215,\n",
      "         -2.6267, -4.8178, -2.5301, -0.7875, -0.9518, -2.7792, -2.1842, -0.7513,\n",
      "         -0.5504, -4.1546, -0.3245,  0.3289, -3.3617, -4.0743, -1.1705, -2.0101,\n",
      "         -3.0328, -2.4236, -1.6979, -2.8487, -2.5909, -3.3965, -5.7244, -1.3562,\n",
      "         -0.4703, -0.5219, -0.4867, -0.4831, -0.4965, -0.5724, -4.9254, -4.9660,\n",
      "         -5.3592, -4.9846, -4.9309, -4.7124, -4.7015, -4.7855, -4.7511, -4.6657,\n",
      "         -4.6916, -4.7327, -4.7683, -4.7544, -4.6628, -4.5373, -4.4615, -4.4312,\n",
      "         -4.4397, -4.4614, -4.4746, -4.4988, -4.5334, -4.5556, -4.5826, -4.5894,\n",
      "         -4.5534, -4.5166, -4.5277, -4.5512, -4.5642, -4.5766, -4.5571, -4.5281,\n",
      "         -4.5098, -4.5029, -4.5235, -4.5421, -4.5540, -4.5585, -4.5643, -4.5812,\n",
      "         -4.5958, -4.6017, -4.5891, -4.5821, -4.5806, -4.5759, -4.5783, -4.5754,\n",
      "         -4.5729, -4.5790, -4.5773, -4.5770, -4.5828]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0977, -0.5052], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5631, -1.3466,  0.9900, -0.5270, -0.5631,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7608e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1139, -0.1151, -0.1162,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2752, -0.2779, -0.2808,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5492, -1.3199,  0.9656, -0.5140, -0.5492,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5061, -3.8966, -0.7194,  ..., -4.6291, -4.6166, -4.6187],\n",
      "        [-2.3832, -3.9044, -0.6661,  ..., -4.4971, -4.4935, -4.4948],\n",
      "        [-2.8263, -3.8773, -0.7728,  ..., -5.3247, -5.3383, -5.3655],\n",
      "        ...,\n",
      "        [-2.8259, -3.8733, -0.7764,  ..., -5.3433, -5.3565, -5.3895],\n",
      "        [-2.7903, -3.8705, -0.7677,  ..., -5.2325, -5.2758, -5.2966],\n",
      "        [-2.8343, -3.8752, -0.7763,  ..., -5.3398, -5.3532, -5.3840]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5462, -1.0300,  0.9559, -0.5034, -0.5462,  0.8970,  0.9061,  0.9567],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5438,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.5541e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1100, -0.1111, -0.1122,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5304,  0.9656,  0.9608,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5045, -3.8942, -0.7131,  ..., -4.6668, -4.6599, -4.6649],\n",
      "        [-2.8166, -3.8758, -0.7677,  ..., -5.3242, -5.3368, -5.3637],\n",
      "        [-2.9726, -3.8464, -0.8043,  ..., -5.6031, -5.5983, -5.5822],\n",
      "        ...,\n",
      "        [-2.8234, -3.8718, -0.7641,  ..., -5.3208, -5.3405, -5.3669],\n",
      "        [-2.8281, -3.8760, -0.7750,  ..., -5.2615, -5.2774, -5.3122],\n",
      "        [-2.8134, -3.8748, -0.7717,  ..., -5.3072, -5.3205, -5.3527]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5353,  0.9511,  0.9089,  0.8933,  0.8908,  0.9215,  0.8976,  0.9472],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5969, -0.5744, -0.5770, -0.5744,  0.9900,  0.9900,  0.9900, -0.5308],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8236e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1207, -0.1220, -0.1232,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1162, -0.1174, -0.1186,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1167, -0.1179, -0.1191,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1074, -0.1085, -0.1096,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5821, -0.5602, -0.5627, -0.5602,  0.9656,  0.9656,  0.9656, -0.5177],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4812, -3.8902, -0.7056,  ..., -4.6187, -4.6131, -4.6120],\n",
      "        [-2.4928, -3.8903, -0.7054,  ..., -4.6354, -4.6287, -4.6296],\n",
      "        [-2.4966, -3.8924, -0.7099,  ..., -4.6344, -4.6365, -4.6383],\n",
      "        ...,\n",
      "        [-2.8230, -3.8778, -0.7749,  ..., -5.3777, -5.3961, -5.4271],\n",
      "        [-2.7824, -3.8767, -0.7487,  ..., -5.3042, -5.3553, -5.3781],\n",
      "        [-2.4985, -3.8908, -0.7123,  ..., -4.6634, -4.6526, -4.6474]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5572, -0.6153, -0.5776, -0.6153,  0.7170,  0.9564,  0.9347, -0.5463],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5922, -0.5354, -0.3499, -1.0676, -0.5610,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9745e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1198, -0.1210, -0.1222,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1083, -0.1094, -0.1105,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1135, -0.1146, -0.1158,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5776, -0.5222, -0.3413, -1.0208, -0.5472,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8159, -3.8696, -0.7679,  ..., -5.3000, -5.3132, -5.3413],\n",
      "        [-2.4805, -3.8882, -0.7017,  ..., -4.6148, -4.6102, -4.6083],\n",
      "        [-2.4982, -3.8918, -0.6989,  ..., -4.5709, -4.5701, -4.5768],\n",
      "        ...,\n",
      "        [-2.4988, -3.8878, -0.7028,  ..., -4.6103, -4.6053, -4.6114],\n",
      "        [-2.7725, -3.8669, -0.7556,  ..., -5.2181, -5.2652, -5.2843],\n",
      "        [-2.8072, -3.8699, -0.7632,  ..., -5.2968, -5.3093, -5.3379]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9052, -0.5511, -0.4945, -0.2814, -1.0783, -0.5668,  0.9032,  0.9529],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5881, -0.5647,  0.9900,  0.9900, -0.5769,  0.9900,  0.9900, -0.3293],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3811e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1190, -0.1202, -0.1214,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1142, -0.1154, -0.1166,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0567, -0.0573, -0.0579,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5736, -0.5508,  0.9656,  0.9608, -0.5627,  0.9656,  0.9656, -0.3212],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4908, -3.8841, -0.7021,  ..., -4.6422, -4.6392, -4.6454],\n",
      "        [-2.4904, -3.8854, -0.6959,  ..., -4.6259, -4.6185, -4.6227],\n",
      "        [-2.9818, -3.8388, -0.7897,  ..., -5.6373, -5.6339, -5.6221],\n",
      "        ...,\n",
      "        [-2.9779, -3.8467, -0.7904,  ..., -5.6758, -5.6741, -5.6661],\n",
      "        [-2.9818, -3.8388, -0.7897,  ..., -5.6373, -5.6339, -5.6221],\n",
      "        [-2.5926, -3.8658, -0.7001,  ..., -4.6663, -4.6727, -4.6731]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5500, -0.6050,  0.7764,  0.9191, -0.5673,  0.7364,  0.7764, -0.2851],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.2482,  0.9900, -0.5712,  0.9900,  0.9900, -0.5272, -0.5148, -0.5607],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.7081e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.4495e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1453, -0.1468, -0.1482,  ..., -1.2234, -1.2357, -1.2482],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1156, -0.1167, -0.1179,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1066, -0.1077, -0.1088,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1041, -0.1052, -0.1063,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1134, -0.1146, -0.1157,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1935,  0.9656, -0.5571,  0.9656,  0.9656, -0.5142, -0.5021, -0.5469],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6139, -3.8211, -0.7195,  ..., -1.0934, -1.1935, -1.0891],\n",
      "        [-2.9850, -3.8438, -0.7873,  ..., -5.6695, -5.6686, -5.6610],\n",
      "        [-2.4949, -3.8812, -0.6929,  ..., -4.6625, -4.6550, -4.6575],\n",
      "        ...,\n",
      "        [-2.5031, -3.8840, -0.6973,  ..., -4.6518, -4.6449, -4.6530],\n",
      "        [-2.5024, -3.8773, -0.7014,  ..., -4.6818, -4.6734, -4.6755],\n",
      "        [-2.5029, -3.8862, -0.6983,  ..., -4.6274, -4.6277, -4.6281]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1312,  0.7502, -0.5585,  0.7557,  0.9542, -0.5145, -0.5209, -0.5559],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.0183e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8233, -3.8607, -0.7524, -2.3733,  0.1450, -2.3865, -4.5763, -0.4513,\n",
      "         -4.1819, -2.6571, -3.9924,  0.4918, -0.2781, -3.5977, -3.0420, -2.4050,\n",
      "         -3.3339, -1.8338, -1.9464, -1.0975, -1.4172, -1.5411, -0.9676,  0.4961,\n",
      "         -2.0989, -3.4914, -3.0234, -2.5841, -3.8017,  0.9800, -1.8030, -0.3241,\n",
      "         -0.8747, -3.5443, -1.9407, -1.7402, -0.8481, -3.2321, -1.6728, -0.1243,\n",
      "         -0.6855, -0.4626, -0.2951, -4.2268, -1.7587, -3.5608, -3.6569, -3.2757,\n",
      "          0.3559,  0.6197, -3.8666, -1.5260, -7.1415, -2.9156, -3.0720, -0.9853,\n",
      "         -2.4737, -2.8918, -2.6742, -1.7761, -3.6882, -1.2085, -5.8185, -2.4048,\n",
      "         -3.5477,  1.1188, -1.4067, -0.2079, -0.2979, -3.1678, -4.1098, -5.6120,\n",
      "         -2.8183, -0.0467, -3.8292, -4.2183, -2.4856, -3.9126, -2.5684, -3.5698,\n",
      "         -4.9197, -2.5076, -0.3305, -1.4117, -1.8351, -3.0873, -1.8860, -0.7930,\n",
      "         -3.6470, -0.8846, -3.1808, -4.0931, -2.4596, -0.6172, -1.0284, -3.0318,\n",
      "         -2.9050, -3.9004, -3.3620, -3.3148, -0.1082, -1.1332, -3.6624, -2.1348,\n",
      "         -0.3851, -2.0116, -3.7156, -3.8623,  0.6757, -1.4208, -1.3545, -0.7839,\n",
      "         -4.5323, -4.6414,  0.4538, -1.0806, -2.1938, -1.4494, -0.1529, -1.0486,\n",
      "         -3.0319, -4.6199, -2.6240, -0.7770, -1.2214, -2.9586, -2.3343, -0.8114,\n",
      "         -0.7433, -4.2968, -0.0574,  0.6728, -3.3943, -4.0785, -1.2791, -1.9507,\n",
      "         -3.0188, -2.5007, -1.7958, -2.9483, -3.0336, -5.8227, -1.7121, -2.4704,\n",
      "         -1.3585, -0.8042, -0.5508, -4.1704, -3.8460, -0.2206, -2.0913, -0.5676,\n",
      "         -1.0089, -0.5173,  0.0665, -0.6958, -3.9455, -3.5071, -4.8105, -1.3644,\n",
      "          0.9021,  0.9233,  0.9797,  0.9354,  0.7046,  0.9781, -4.7388, -4.9206,\n",
      "         -5.2390, -5.0705, -5.3399, -5.4358, -5.3308, -5.3415, -5.3487, -5.3222,\n",
      "         -5.2405, -5.1314, -5.0495, -5.0090, -5.0031, -5.0235, -5.0483, -5.0781,\n",
      "         -5.0961, -5.1140, -5.1398, -5.1812, -5.2441, -5.3267, -5.3942, -5.4294,\n",
      "         -5.4294, -5.4428, -5.4691, -5.5023, -5.5044, -5.4769, -5.4407, -5.3900,\n",
      "         -5.3456, -5.3116, -5.2991, -5.3111, -5.3387],\n",
      "        [-2.8233, -3.8607, -0.7524, -2.3733,  0.1450, -2.3865, -4.5763, -0.4513,\n",
      "         -4.1819, -2.6571, -3.9924,  0.4918, -0.2781, -3.5977, -3.0420, -2.4050,\n",
      "         -3.3339, -1.8338, -1.9464, -1.0975, -1.4172, -1.5411, -0.9676,  0.4961,\n",
      "         -2.0989, -3.4914, -3.0234, -2.5841, -3.8017,  0.9800, -1.8030, -0.3241,\n",
      "         -0.8747, -3.5443, -1.9407, -1.7402, -0.8481, -3.2321, -1.6728, -0.1243,\n",
      "         -0.6855, -0.4626, -0.2951, -4.2268, -1.7587, -3.5608, -3.6569, -3.2757,\n",
      "          0.3559,  0.6197, -3.8666, -1.5260, -7.1415, -2.9156, -3.0720, -0.9853,\n",
      "         -2.4737, -2.8918, -2.6742, -1.7761, -3.6882, -1.2085, -5.8185, -2.4048,\n",
      "         -3.5477,  1.1188, -1.4067, -0.2079, -0.2979, -3.1678, -4.1098, -5.6120,\n",
      "         -2.8183, -0.0467, -3.8292, -4.2183, -2.4856, -3.9126, -2.5684, -3.5698,\n",
      "         -4.9197, -2.5076, -0.3305, -1.4117, -1.8351, -3.0873, -1.8860, -0.7930,\n",
      "         -3.6470, -0.8846, -3.1808, -4.0931, -2.4596, -0.6172, -1.0284, -3.0318,\n",
      "         -2.9050, -3.9004, -3.3620, -3.3148, -0.1082, -1.1332, -3.6624, -2.1348,\n",
      "         -0.3851, -2.0116, -3.7156, -3.8623,  0.6757, -1.4208, -1.3545, -0.7839,\n",
      "         -4.5323, -4.6414,  0.4538, -1.0806, -2.1938, -1.4494, -0.1529, -1.0486,\n",
      "         -3.0319, -4.6199, -2.6240, -0.7770, -1.2214, -2.9586, -2.3343, -0.8114,\n",
      "         -0.7433, -4.2968, -0.0574,  0.6728, -3.3943, -4.0785, -1.2791, -1.9507,\n",
      "         -3.0188, -2.5007, -1.7958, -2.9483, -3.0336, -5.8227, -1.7121, -2.4704,\n",
      "         -1.3585, -0.8042, -0.5508, -4.1704, -3.8460, -0.2206, -2.0913, -0.5676,\n",
      "         -1.0089, -0.5173,  0.0665, -0.6958, -3.9455, -3.5071, -4.8105, -1.3644,\n",
      "          0.9021,  0.9233,  0.9797,  0.9354,  0.7046,  0.9781, -4.7388, -4.9206,\n",
      "         -5.2390, -5.0705, -5.3399, -5.4358, -5.3308, -5.3415, -5.3487, -5.3222,\n",
      "         -5.2405, -5.1314, -5.0495, -5.0090, -5.0031, -5.0235, -5.0483, -5.0781,\n",
      "         -5.0961, -5.1140, -5.1398, -5.1812, -5.2441, -5.3267, -5.3942, -5.4294,\n",
      "         -5.4294, -5.4428, -5.4691, -5.5023, -5.5044, -5.4769, -5.4407, -5.3900,\n",
      "         -5.3456, -5.3116, -5.2991, -5.3111, -5.3387]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9039, 0.9039], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5243, -0.5305,  0.9900,  0.9900,  0.9900, -0.5690,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.4982e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1061, -0.1071, -0.1082,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1073, -0.1084, -0.1095,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1151, -0.1163, -0.1175,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5114, -0.5174,  0.9608,  0.9656,  0.9656, -0.5550,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8070, -3.8688, -0.7357,  ..., -5.2737, -5.3267, -5.3530],\n",
      "        [-2.5173, -3.8825, -0.7004,  ..., -4.6498, -4.6375, -4.6354],\n",
      "        [-2.5210, -3.8858, -0.6917,  ..., -4.5652, -4.5648, -4.5719],\n",
      "        ...,\n",
      "        [-2.8402, -3.8664, -0.7511,  ..., -5.3031, -5.3140, -5.3376],\n",
      "        [-2.5172, -3.8849, -0.6996,  ..., -4.6264, -4.6260, -4.6301],\n",
      "        [-2.8364, -3.8640, -0.7542,  ..., -5.2865, -5.2965, -5.3206]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9780, -0.5106, -0.4733,  0.9560,  0.9508,  0.9890, -0.5419,  1.0003],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3412,  0.9900,  0.9900,  0.9900, -0.5805,  0.9900, -0.5901,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1284e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.5204e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0588, -0.0594, -0.0600,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1194, -0.1206, -0.1218,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3328,  0.9656,  0.9656,  0.9656, -0.5661,  0.9656, -0.5756,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6263, -3.8635, -0.6982,  ..., -4.6681, -4.6765, -4.6766],\n",
      "        [-2.8451, -3.8600, -0.7539,  ..., -5.3117, -5.3248, -5.3531],\n",
      "        [-2.8556, -3.8611, -0.7472,  ..., -5.3080, -5.3192, -5.3467],\n",
      "        ...,\n",
      "        [-3.0180, -3.8308, -0.7807,  ..., -5.5486, -5.5423, -5.5255],\n",
      "        [-2.5221, -3.8801, -0.6957,  ..., -4.6651, -4.6566, -4.6579],\n",
      "        [-3.0155, -3.8401, -0.7841,  ..., -5.6607, -5.6603, -5.6509]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.2786,  0.9463,  0.9902,  0.8533, -0.5032,  0.8151, -0.5411,  0.8033],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.0935, -1.2768,  0.9900,  0.9900,  0.9900,  0.9900, -0.5872,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.4600e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1510, -0.1525, -0.1541,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1486, -0.1501, -0.1516,  ..., -1.2514, -1.2640, -1.2768],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1188, -0.1200, -0.1212,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0456, -1.2209,  0.9656,  0.9608,  0.9656,  0.9656, -0.5728,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6252, -3.8300, -0.7102,  ..., -4.1433, -4.1384, -4.1326],\n",
      "        [-2.6365, -3.8181, -0.7246,  ..., -1.0923, -1.2147, -1.1125],\n",
      "        [-3.0164, -3.8300, -0.7872,  ..., -5.6221, -5.6174, -5.6095],\n",
      "        ...,\n",
      "        [-2.8441, -3.8595, -0.7616,  ..., -5.2888, -5.2987, -5.3295],\n",
      "        [-2.5262, -3.8775, -0.7011,  ..., -4.6289, -4.6215, -4.6223],\n",
      "        [-2.8156, -3.8643, -0.7414,  ..., -5.2821, -5.3364, -5.3602]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0954, -1.1461,  0.8634,  0.9730,  0.9664,  1.0144, -0.5759,  0.9965],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5642,  0.9900, -0.5599, -0.3747,  0.9900,  0.9900, -0.6198],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7316e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1141, -0.1153, -0.1165,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1254, -0.1266, -0.1279,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5503,  0.9656, -0.5461, -0.3654,  0.9656,  0.9656, -0.6045],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8445, -3.8574, -0.7664,  ..., -5.3092, -5.3208, -5.3508],\n",
      "        [-2.5262, -3.8773, -0.7099,  ..., -4.6541, -4.6460, -4.6527],\n",
      "        [-2.8535, -3.8586, -0.7667,  ..., -5.3104, -5.3240, -5.3555],\n",
      "        ...,\n",
      "        [-2.7888, -3.8513, -0.7345,  ..., -5.4913, -5.5292, -5.5332],\n",
      "        [-3.0100, -3.8324, -0.7938,  ..., -5.6632, -5.6623, -5.6564],\n",
      "        [-2.5220, -3.8740, -0.7127,  ..., -4.6412, -4.6379, -4.6446]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9699, -0.5005,  1.0256, -0.4774, -0.2758,  0.9686,  0.8227, -0.5250],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.3806, -0.6180, -0.5632, -0.5948, -0.5516, -0.5972],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.5637e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.5401e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2821, -0.2850, -0.2878,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1203, -0.1216, -0.1228,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1116, -0.1127, -0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1208, -0.1220, -0.1233,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.3532, -0.6028, -0.5493, -0.5802, -0.5380, -0.5825],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8432, -3.8560, -0.7732,  ..., -5.2742, -5.2889, -5.3134],\n",
      "        [-2.8415, -3.8585, -0.7655,  ..., -5.2873, -5.2971, -5.3231],\n",
      "        [-2.3951, -3.8865, -0.6640,  ..., -4.4842, -4.4820, -4.4852],\n",
      "        ...,\n",
      "        [-2.5212, -3.8744, -0.7100,  ..., -4.6280, -4.6207, -4.6190],\n",
      "        [-2.5225, -3.8688, -0.7192,  ..., -4.6843, -4.6759, -4.6749],\n",
      "        [-2.5225, -3.8766, -0.7159,  ..., -4.6316, -4.6291, -4.6289]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9899,  0.9896, -0.9478, -0.5367, -0.5052, -0.5838, -0.5058, -0.5455],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6158, -0.5930,  0.9900,  0.9900, -0.6058, -0.5833, -0.5947,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8987e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1246, -0.1258, -0.1271,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1200, -0.1212, -0.1224,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1180, -0.1192, -0.1204,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1203, -0.1215, -0.1227,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6006, -0.5784,  0.9656,  0.9656, -0.5908, -0.5689, -0.5800,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4896, -3.8767, -0.7139,  ..., -4.6072, -4.6006, -4.6008],\n",
      "        [-2.5010, -3.8760, -0.7126,  ..., -4.6235, -4.6167, -4.6165],\n",
      "        [-2.8219, -3.8570, -0.7776,  ..., -5.2569, -5.2689, -5.2947],\n",
      "        ...,\n",
      "        [-2.5084, -3.8765, -0.7151,  ..., -4.6004, -4.5948, -4.6009],\n",
      "        [-2.4984, -3.8801, -0.7193,  ..., -4.6122, -4.6014, -4.6037],\n",
      "        [-2.8136, -3.8538, -0.7716,  ..., -5.2750, -5.2863, -5.3139]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5505, -0.5985,  0.9715,  0.8267, -0.5713, -0.5546, -0.5313,  0.9375],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.3222e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8110, -3.8658, -0.7794, -2.4139,  0.1302, -2.4009, -4.5751, -0.4584,\n",
      "         -4.1827, -2.6376, -3.9894,  0.4936, -0.2928, -3.6083, -3.0228, -2.4138,\n",
      "         -3.3236, -1.8464, -1.9574, -1.1092, -1.4131, -1.5514, -0.9808,  0.4615,\n",
      "         -2.0970, -3.5082, -3.0032, -2.5939, -3.7969,  0.9606, -1.8334, -0.3365,\n",
      "         -0.9048, -3.5550, -1.9409, -1.7837, -0.8988, -3.2267, -1.7121, -0.1418,\n",
      "         -0.7077, -0.5181, -0.3263, -4.2288, -1.7573, -3.5561, -3.6466, -3.2918,\n",
      "          0.4461,  0.6820, -3.8755, -1.5098, -7.1165, -2.9255, -3.0675, -1.0028,\n",
      "         -2.4975, -2.9294, -2.6454, -1.7732, -3.7056, -1.1954, -5.8017, -2.4168,\n",
      "         -3.5452,  1.0980, -1.4427, -0.2228, -0.3301, -3.1840, -4.1242, -5.5751,\n",
      "         -2.8294, -0.0762, -3.8475, -4.1583, -2.4967, -3.9085, -2.5988, -3.5890,\n",
      "         -4.8863, -2.5192, -0.3487, -1.4281, -1.8396, -3.1028, -1.9241, -0.8070,\n",
      "         -3.6219, -0.9023, -3.2036, -4.0373, -2.4710, -0.6360, -1.0450, -3.0013,\n",
      "         -2.8996, -3.8884, -3.3420, -3.3276, -0.1225, -1.1471, -3.6735, -2.1051,\n",
      "         -0.4193, -2.0175, -3.7088, -3.8700,  0.6462, -1.3873, -1.3736, -0.4536,\n",
      "         -4.7262, -4.5858,  0.4701, -1.0573, -2.2214, -1.4120, -0.1472, -0.9893,\n",
      "         -2.9811, -4.6044, -2.6531, -0.8075, -1.1744, -2.8533, -2.3128, -0.8310,\n",
      "         -0.7531, -4.2641,  0.0339,  0.7560, -3.3906, -4.0730, -1.2945, -1.9737,\n",
      "         -3.0452, -2.4569, -1.7876, -2.9038, -3.0310, -5.7733, -1.7109, -2.4642,\n",
      "         -1.4545, -0.8182, -0.3660, -4.3200, -3.9279, -0.1788, -2.0839, -0.9737,\n",
      "         -1.0988, -0.2880,  0.1424, -0.6135, -4.1494, -3.2922, -4.8128, -1.4120,\n",
      "          0.9917,  1.0557,  0.9215,  0.9429,  0.9726,  0.9880, -4.7953, -4.9548,\n",
      "         -5.2896, -5.1597, -5.3688, -5.4332, -5.3636, -5.3650, -5.3647, -5.3276,\n",
      "         -5.2479, -5.1463, -5.0639, -5.0199, -5.0202, -5.0470, -5.0821, -5.1197,\n",
      "         -5.1369, -5.1573, -5.1776, -5.2183, -5.2858, -5.3709, -5.4288, -5.4470,\n",
      "         -5.4293, -5.4312, -5.4628, -5.5073, -5.5337, -5.5121, -5.4701, -5.4111,\n",
      "         -5.3630, -5.3280, -5.3179, -5.3335, -5.3626],\n",
      "        [-2.7664, -3.8555, -0.7670, -2.3551,  0.1516, -2.3969, -4.5691, -0.3768,\n",
      "         -4.1222, -2.5883, -3.9622,  0.4987, -0.2779, -3.5564, -3.0573, -2.3741,\n",
      "         -3.3175, -1.8196, -1.9801, -1.0640, -1.3926, -1.5063, -0.9434,  0.4331,\n",
      "         -2.0518, -3.4559, -3.0297, -2.5646, -3.7965,  0.9614, -1.7911, -0.2791,\n",
      "         -0.9056, -3.5310, -1.9443, -1.8068, -0.8940, -3.2327, -1.6645, -0.1012,\n",
      "         -0.6906, -0.5140, -0.3109, -4.2454, -1.7209, -3.5027, -3.5860, -3.2663,\n",
      "          0.3309,  0.6005, -3.8374, -1.4987, -7.1316, -2.9037, -3.0706, -0.9709,\n",
      "         -2.5041, -2.9070, -2.6298, -1.7362, -3.6593, -1.1811, -5.8056, -2.3969,\n",
      "         -3.5413,  1.0949, -1.4039, -0.1661, -0.3401, -3.1618, -4.0972, -5.5747,\n",
      "         -2.8090, -0.1004, -3.8056, -4.1834, -2.4736, -3.9164, -2.5593, -3.5487,\n",
      "         -4.8868, -2.5008, -0.3271, -1.3771, -1.8246, -3.0807, -1.8869, -0.7552,\n",
      "         -3.6031, -0.8823, -3.1594, -4.0599, -2.4508, -0.6182, -0.9907, -3.0085,\n",
      "         -2.8664, -3.8368, -3.3226, -3.3096, -0.1006, -1.0953, -3.6857, -2.1050,\n",
      "         -0.4442, -1.9712, -3.6618, -3.8508,  0.6200, -1.5646, -1.4118, -0.9093,\n",
      "         -4.5309, -4.6760,  0.4730, -1.0183, -2.1709, -1.3496, -0.0823, -0.9877,\n",
      "         -2.8889, -4.6480, -2.6375, -0.7895, -1.0799, -2.7973, -2.2945, -0.8226,\n",
      "         -0.6288, -4.2090, -0.1280,  0.6383, -3.3653, -4.0945, -1.2719, -1.9864,\n",
      "         -3.0618, -2.4372, -1.7883, -2.9385, -3.0045, -5.7613, -1.6877, -2.5018,\n",
      "         -1.4166, -0.8410, -0.5331, -4.2306, -3.9013, -0.2293, -2.1005, -0.8558,\n",
      "         -1.1634, -0.4635, -0.1186, -0.7937, -4.1481, -5.1107, -1.5263, -2.4060,\n",
      "         -0.0519,  0.4317,  0.3695, -4.1083, -3.8749, -0.1345, -1.2221, -4.3869,\n",
      "         -6.2614, -4.8279, -3.0676, -4.7894, -1.2996,  0.9861,  0.9848,  0.9292,\n",
      "          0.9557,  0.8143,  0.9606, -4.6307, -4.9076, -5.2632, -5.0565, -5.2115,\n",
      "         -5.1819, -5.0209, -5.1078, -5.1195, -5.0687, -4.9951, -4.9373, -4.9271,\n",
      "         -4.9524, -4.9895, -5.0126, -5.0275, -5.0409, -5.0523, -5.0660, -5.0846,\n",
      "         -5.1053, -5.1323, -5.1675, -5.2162, -5.2444]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9787, 0.9385], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5792,  0.9900, -0.5792,  0.9900,  0.9900, -0.5808, -1.2828,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5849e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1172, -0.1183, -0.1195,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1172, -0.1183, -0.1195,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1175, -0.1187, -0.1199,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1493, -0.1508, -0.1523,  ..., -1.2573, -1.2700, -1.2828],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5649,  0.9656, -0.5649,  0.9656,  0.9656, -0.5665, -1.2266,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4606, -3.8798, -0.7121,  ..., -4.6139, -4.6056, -4.6050],\n",
      "        [-2.7881, -3.8592, -0.7719,  ..., -5.2609, -5.2714, -5.3005],\n",
      "        [-2.4606, -3.8798, -0.7121,  ..., -4.6139, -4.6056, -4.6050],\n",
      "        ...,\n",
      "        [-2.4567, -3.8823, -0.7190,  ..., -4.6029, -4.5941, -4.5944],\n",
      "        [-2.5783, -3.8159, -0.7379,  ..., -1.0974, -1.2298, -1.1618],\n",
      "        [-2.7457, -3.8565, -0.7654,  ..., -5.1524, -5.2033, -5.2325]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6254,  0.9677, -0.6254,  0.8925,  0.9393, -0.5559, -1.1578,  0.9251],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5279,  0.9900, -1.3474, -0.5387,  0.9900,  0.9900, -0.5324,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8423e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1068, -0.1079, -0.1090,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2753, -0.2781, -0.2809,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1077, -0.1088, -0.1099,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5148,  0.9656, -1.3207, -0.5254,  0.9608,  0.9656, -0.5193,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4446, -3.8752, -0.7211,  ..., -4.6625, -4.6521, -4.6542],\n",
      "        [-2.7621, -3.8629, -0.7681,  ..., -5.2232, -5.2304, -5.2569],\n",
      "        [-2.3178, -3.8925, -0.6667,  ..., -4.4682, -4.4676, -4.4719],\n",
      "        ...,\n",
      "        [-2.7351, -3.8648, -0.7545,  ..., -5.2149, -5.2730, -5.3054],\n",
      "        [-2.4480, -3.8847, -0.7080,  ..., -4.5556, -4.5551, -4.5615],\n",
      "        [-2.7625, -3.8589, -0.7718,  ..., -5.2477, -5.2559, -5.2832]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5503,  0.9112, -1.0203, -0.5501,  0.9233,  0.9301, -0.5328,  0.9132],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3354, -0.5640,  0.9900,  0.9900, -0.3212, -0.5856,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5520e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0578, -0.0584, -0.0589,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1141, -0.1152, -0.1164,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1185, -0.1197, -0.1209,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3271, -0.5501,  0.9656,  0.9656, -0.3132, -0.5712,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5275, -3.8570, -0.7209,  ..., -4.6674, -4.6748, -4.6733],\n",
      "        [-2.4261, -3.8839, -0.7164,  ..., -4.6133, -4.6119, -4.6129],\n",
      "        [-2.7423, -3.8594, -0.7722,  ..., -5.1995, -5.2085, -5.2344],\n",
      "        ...,\n",
      "        [-2.4134, -3.8798, -0.7096,  ..., -4.5897, -4.5827, -4.5809],\n",
      "        [-2.7073, -3.8555, -0.7610,  ..., -5.1272, -5.1802, -5.2090],\n",
      "        [-2.7521, -3.8622, -0.7707,  ..., -5.1635, -5.1777, -5.2043]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3130, -0.5988,  0.9103,  0.8121, -0.3393, -0.5908,  0.9048,  0.8952],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5495, -0.5849, -0.5215, -0.5834,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4762e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.6056e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1112, -0.1123, -0.1134,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1183, -0.1195, -0.1207,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5359, -0.5705, -0.5086, -0.5690,  0.9656,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8876, -3.8350, -0.7979,  ..., -5.6294, -5.6277, -5.6189],\n",
      "        [-2.4119, -3.8786, -0.7085,  ..., -4.5867, -4.5810, -4.5877],\n",
      "        [-2.4055, -3.8779, -0.7137,  ..., -4.6140, -4.6122, -4.6197],\n",
      "        ...,\n",
      "        [-2.7166, -3.8528, -0.7652,  ..., -5.2063, -5.2181, -5.2425],\n",
      "        [-2.7233, -3.8565, -0.7696,  ..., -5.1936, -5.2064, -5.2283],\n",
      "        [-2.8755, -3.8331, -0.8008,  ..., -5.5283, -5.5225, -5.5087]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7568, -0.5891, -0.5769, -0.5401, -0.5922,  0.8461,  0.9032,  0.9069],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5650,  0.9900,  0.9900, -0.5237, -0.5336,  0.9900,  0.9900, -1.0900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3075e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1143, -0.1155, -0.1166,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1505, -0.1520, -0.1536,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5511,  0.9656,  0.9656, -0.5108, -0.5204,  0.9656,  0.9656, -1.0422],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3927, -3.8779, -0.7068,  ..., -4.6006, -4.5941, -4.5943],\n",
      "        [-2.7047, -3.8549, -0.7650,  ..., -5.1967, -5.2067, -5.2350],\n",
      "        [-2.6843, -3.8614, -0.7494,  ..., -5.2057, -5.2636, -5.2967],\n",
      "        ...,\n",
      "        [-2.8799, -3.8235, -0.7974,  ..., -5.5105, -5.5060, -5.4917],\n",
      "        [-2.8758, -3.8344, -0.8022,  ..., -5.6256, -5.6258, -5.6204],\n",
      "        [-2.4973, -3.8272, -0.7180,  ..., -4.0994, -4.0923, -4.0945]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6441,  0.8913,  0.9057, -0.5693, -0.5599,  0.7751,  0.7528, -1.1187],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5901,  0.9900,  0.9900, -0.5901,  0.9900, -0.5774,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9750e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1194, -0.1206, -0.1218,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1168, -0.1180, -0.1192,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5756,  0.9656,  0.9656, -0.5756,  0.9656, -0.5632,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3841, -3.8737, -0.7057,  ..., -4.6344, -4.6272, -4.6274],\n",
      "        [-2.8732, -3.8243, -0.7984,  ..., -5.5838, -5.5855, -5.5747],\n",
      "        [-2.7003, -3.8553, -0.7629,  ..., -5.2121, -5.2236, -5.2484],\n",
      "        ...,\n",
      "        [-2.3845, -3.8782, -0.7131,  ..., -4.5873, -4.5751, -4.5803],\n",
      "        [-2.8729, -3.8219, -0.7957,  ..., -5.5039, -5.5007, -5.4864],\n",
      "        [-2.7047, -3.8505, -0.7599,  ..., -5.2256, -5.2380, -5.2650]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6105,  0.8038,  0.8836, -0.6105,  0.8995, -0.5666,  0.7837,  0.9161],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5861], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.6010e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1186, -0.1198, -0.1210, -0.1222, -0.1234, -0.1247, -0.1259, -0.1272,\n",
      "         -0.1285, -0.1298, -0.1311, -0.1324, -0.1338, -0.1351, -0.1365, -0.1379,\n",
      "         -0.1392, -0.1407, -0.1421, -0.1435, -0.1450, -0.1464, -0.1479, -0.1494,\n",
      "         -0.1509, -0.1524, -0.1540, -0.1555, -0.1571, -0.1587, -0.1603, -0.1619,\n",
      "         -0.1635, -0.1652, -0.1669, -0.1685, -0.1702, -0.1720, -0.1737, -0.1755,\n",
      "         -0.1772, -0.1790, -0.1808, -0.1827, -0.1845, -0.1864, -0.1882, -0.1901,\n",
      "         -0.1921, -0.1940, -0.1960, -0.1979, -0.1999, -0.2020, -0.2040, -0.2061,\n",
      "         -0.2082, -0.2103, -0.2124, -0.2145, -0.2167, -0.2189, -0.2211, -0.2233,\n",
      "         -0.2256, -0.2279, -0.2302, -0.2325, -0.2348, -0.2372, -0.2396, -0.2420,\n",
      "         -0.2445, -0.2469, -0.2494, -0.2519, -0.2545, -0.2571, -0.2597, -0.2623,\n",
      "         -0.2649, -0.2676, -0.2703, -0.2730, -0.2758, -0.2786, -0.2814, -0.2842,\n",
      "         -0.2871, -0.2900, -0.2929, -0.2959, -0.2989, -0.3019, -0.3050, -0.3080,\n",
      "         -0.3112, -0.3143, -0.3175, -0.3207, -0.3239, -0.3272, -0.3305, -0.3338,\n",
      "         -0.3372, -0.3406, -0.3440, -0.3475, -0.3510, -0.3546, -0.3582, -0.3618,\n",
      "         -0.3654, -0.3691, -0.3729, -0.3766, -0.3804, -0.3843, -0.3881, -0.3921,\n",
      "         -0.3960, -0.4000, -0.4041, -0.4082, -0.4123, -0.4164, -0.4206, -0.4249,\n",
      "         -0.4292, -0.4335, -0.4379, -0.4423, -0.4468, -0.4513, -0.4559, -0.4605,\n",
      "         -0.4651, -0.4698, -0.4746, -0.4794, -0.4842, -0.4891, -0.4940, -0.4990,\n",
      "         -0.5041, -0.5092, -0.5143, -0.5195, -0.5247, -0.5300, -0.5354, -0.5408,\n",
      "         -0.5463, -0.5518, -0.5574, -0.5630, -0.5687, -0.5744, -0.5802, -0.5861,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5716], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7103e+00, -3.8568e+00, -7.7267e-01, -2.4248e+00,  1.5839e-01,\n",
      "         -2.3865e+00, -4.5731e+00, -4.1187e-01, -4.0814e+00, -2.6061e+00,\n",
      "         -3.9810e+00,  4.5008e-01, -3.2742e-01, -3.5858e+00, -3.0515e+00,\n",
      "         -2.3865e+00, -3.3159e+00, -1.8199e+00, -1.9668e+00, -1.0599e+00,\n",
      "         -1.3790e+00, -1.5151e+00, -9.4860e-01,  4.3199e-01, -2.0429e+00,\n",
      "         -3.4845e+00, -3.0297e+00, -2.5711e+00, -3.7791e+00,  9.6071e-01,\n",
      "         -1.8119e+00, -2.8505e-01, -9.2419e-01, -3.5566e+00, -1.9434e+00,\n",
      "         -1.7880e+00, -9.3255e-01, -3.2198e+00, -1.6883e+00, -1.0285e-01,\n",
      "         -6.8483e-01, -5.4982e-01, -3.1606e-01, -4.2482e+00, -1.7079e+00,\n",
      "         -3.4519e+00, -3.6098e+00, -3.2900e+00,  3.6612e-01,  5.7368e-01,\n",
      "         -3.8608e+00, -1.5092e+00, -7.1454e+00, -2.9123e+00, -3.0605e+00,\n",
      "         -9.6592e-01, -2.5252e+00, -2.9292e+00, -2.6112e+00, -1.7194e+00,\n",
      "         -3.6815e+00, -1.1894e+00, -5.8235e+00, -2.4043e+00, -3.5278e+00,\n",
      "          1.1017e+00, -1.4166e+00, -1.6647e-01, -3.5387e-01, -3.1820e+00,\n",
      "         -4.1135e+00, -5.5884e+00, -2.8100e+00, -8.1113e-02, -3.8260e+00,\n",
      "         -4.1795e+00, -2.4794e+00, -3.9012e+00, -2.5767e+00, -3.5689e+00,\n",
      "         -4.8938e+00, -2.5036e+00, -3.2036e-01, -1.3768e+00, -1.8308e+00,\n",
      "         -3.0785e+00, -1.9008e+00, -7.5398e-01, -3.6006e+00, -8.6897e-01,\n",
      "         -3.1817e+00, -4.0522e+00, -2.4537e+00, -6.0903e-01, -9.8936e-01,\n",
      "         -2.9953e+00, -2.8391e+00, -3.7823e+00, -3.3054e+00, -3.3179e+00,\n",
      "         -9.1888e-02, -1.0927e+00, -3.7065e+00, -2.1043e+00, -4.4590e-01,\n",
      "         -1.9807e+00, -3.6083e+00, -3.8524e+00,  6.4332e-01, -1.3936e+00,\n",
      "         -1.3756e+00, -4.5387e-01, -4.6309e+00, -4.5640e+00,  4.9828e-01,\n",
      "         -1.0054e+00, -2.1861e+00, -1.3743e+00, -1.1161e-01, -9.5261e-01,\n",
      "         -2.8767e+00, -4.5917e+00, -2.6488e+00, -8.0476e-01, -1.1506e+00,\n",
      "         -2.8243e+00, -2.2697e+00, -8.1573e-01, -6.9698e-01, -4.2638e+00,\n",
      "         -4.7582e-02,  6.4067e-01, -3.3814e+00, -4.0914e+00, -1.2588e+00,\n",
      "         -1.9980e+00, -3.0524e+00, -2.4322e+00, -1.7353e+00, -2.8664e+00,\n",
      "         -2.9346e+00, -5.7908e+00, -1.6733e+00, -2.4942e+00, -1.4639e+00,\n",
      "         -8.2319e-01, -3.7866e-01, -4.2163e+00, -3.9591e+00, -1.9232e-01,\n",
      "         -2.0561e+00, -9.8480e-01, -1.0976e+00, -2.9668e-01,  1.3607e-01,\n",
      "         -6.6320e-01, -4.0507e+00, -3.2147e+00, -4.8430e+00, -1.3771e+00,\n",
      "          8.7858e-01,  9.0453e-01,  9.6106e-01,  9.4593e-01,  8.9764e-01,\n",
      "          1.0339e+00, -4.7537e+00, -4.9245e+00, -5.2654e+00, -5.1389e+00,\n",
      "         -5.3434e+00, -5.3969e+00, -5.3253e+00, -5.3273e+00, -5.3327e+00,\n",
      "         -5.2982e+00, -5.2189e+00, -5.1192e+00, -5.0376e+00, -4.9943e+00,\n",
      "         -4.9945e+00, -5.0167e+00, -5.0551e+00, -5.0971e+00, -5.1184e+00,\n",
      "         -5.1378e+00, -5.1607e+00, -5.2047e+00, -5.2724e+00, -5.3560e+00,\n",
      "         -5.4083e+00, -5.4169e+00, -5.3898e+00, -5.3918e+00, -5.4130e+00,\n",
      "         -5.4623e+00, -5.4891e+00, -5.4690e+00, -5.4261e+00, -5.3672e+00,\n",
      "         -5.3166e+00, -5.2860e+00, -5.2796e+00, -5.2965e+00, -5.3286e+00],\n",
      "        [-2.3880e+00, -3.8750e+00, -7.0961e-01, -2.2164e+00,  2.5631e-01,\n",
      "         -2.3160e+00, -4.4733e+00, -1.3778e-01, -3.7236e+00, -2.4410e+00,\n",
      "         -3.8947e+00,  4.5409e-01, -2.3395e-01, -3.4929e+00, -3.1626e+00,\n",
      "         -2.2144e+00, -3.2232e+00, -1.7223e+00, -2.0278e+00, -8.8702e-01,\n",
      "         -1.3242e+00, -1.3341e+00, -8.0100e-01,  3.6103e-01, -1.8280e+00,\n",
      "         -3.3750e+00, -3.1128e+00, -2.4246e+00, -3.7789e+00,  9.5140e-01,\n",
      "         -1.6710e+00, -9.3419e-02, -9.4178e-01, -3.4782e+00, -1.9080e+00,\n",
      "         -1.9076e+00, -9.8538e-01, -3.1620e+00, -1.5308e+00,  6.7849e-02,\n",
      "         -6.3439e-01, -5.7916e-01, -2.6915e-01, -4.2996e+00, -1.5292e+00,\n",
      "         -3.1434e+00, -3.4124e+00, -3.2174e+00, -7.2318e-02,  2.2854e-01,\n",
      "         -3.7957e+00, -1.4284e+00, -7.1381e+00, -2.8102e+00, -3.0257e+00,\n",
      "         -8.5191e-01, -2.5543e+00, -2.8940e+00, -2.5297e+00, -1.5445e+00,\n",
      "         -3.5905e+00, -1.1161e+00, -5.8004e+00, -2.3332e+00, -3.5262e+00,\n",
      "          1.0745e+00, -1.2881e+00,  2.8567e-02, -3.9700e-01, -3.1037e+00,\n",
      "         -4.0099e+00, -5.6016e+00, -2.7205e+00, -1.5365e-01, -3.7478e+00,\n",
      "         -4.2388e+00, -2.3892e+00, -3.8696e+00, -2.4518e+00, -3.4976e+00,\n",
      "         -4.9109e+00, -2.4231e+00, -2.7044e-01, -1.2069e+00, -1.7757e+00,\n",
      "         -2.9226e+00, -1.7607e+00, -5.7699e-01, -3.5268e+00, -7.9574e-01,\n",
      "         -3.0935e+00, -4.1034e+00, -2.3767e+00, -5.7078e-01, -8.1322e-01,\n",
      "         -3.0436e+00, -2.6757e+00, -3.5224e+00, -3.2641e+00, -3.2240e+00,\n",
      "         -2.3423e-02, -9.3004e-01, -3.7257e+00, -2.0946e+00, -5.0119e-01,\n",
      "         -1.7971e+00, -3.3723e+00, -3.7470e+00,  5.5668e-01, -1.4391e+00,\n",
      "         -1.5127e+00, -5.7208e-01, -4.2693e+00, -4.5485e+00,  4.8485e-01,\n",
      "         -8.5302e-01, -2.1268e+00, -1.2404e+00, -3.3329e-03, -8.5276e-01,\n",
      "         -2.5476e+00, -4.7500e+00, -2.5664e+00, -7.7306e-01, -9.9732e-01,\n",
      "         -2.7053e+00, -2.1460e+00, -7.2759e-01, -5.2879e-01, -4.1923e+00,\n",
      "         -5.0042e-01,  2.8521e-01, -3.3445e+00, -4.0509e+00, -1.1572e+00,\n",
      "         -2.0329e+00, -3.0556e+00, -2.3313e+00, -1.6090e+00, -2.8011e+00,\n",
      "         -2.5418e+00, -3.2596e+00, -5.7120e+00, -1.2388e+00, -6.2193e-01,\n",
      "         -5.9745e-01, -5.6982e-01, -5.9708e-01, -5.9285e-01, -5.8241e-01,\n",
      "         -4.8177e+00, -4.8958e+00, -5.2668e+00, -4.9146e+00, -4.8859e+00,\n",
      "         -4.6059e+00, -4.5512e+00, -4.6426e+00, -4.5766e+00, -4.5214e+00,\n",
      "         -4.5807e+00, -4.6223e+00, -4.6605e+00, -4.6391e+00, -4.5393e+00,\n",
      "         -4.4269e+00, -4.3757e+00, -4.3634e+00, -4.3761e+00, -4.3844e+00,\n",
      "         -4.3890e+00, -4.4097e+00, -4.4388e+00, -4.4565e+00, -4.4724e+00,\n",
      "         -4.4748e+00, -4.4445e+00, -4.4284e+00, -4.4457e+00, -4.4609e+00,\n",
      "         -4.4756e+00, -4.4890e+00, -4.4758e+00, -4.4588e+00, -4.4569e+00,\n",
      "         -4.4630e+00, -4.4922e+00, -4.5164e+00, -4.5358e+00, -4.5503e+00,\n",
      "         -4.5569e+00, -4.5722e+00, -4.5836e+00, -4.5899e+00, -4.5830e+00,\n",
      "         -4.5781e+00, -4.5825e+00, -4.5791e+00, -4.5803e+00, -4.5762e+00,\n",
      "         -4.5718e+00, -4.5849e+00, -4.5936e+00, -4.5907e+00, -4.5937e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9369, -0.5936], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5927,  0.9900,  0.9900, -0.6056,  0.9900, -0.6153,  0.9900, -1.2976],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.4291e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1199, -0.1211, -0.1223,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1245, -0.1257, -0.1270,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1510, -0.1526, -0.1541,  ..., -1.2718, -1.2846, -1.2976]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5781,  0.9656,  0.9656, -0.5906,  0.9656, -0.6001,  0.9656, -1.2407],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3779, -3.8725, -0.7117,  ..., -4.5678, -4.5591, -4.5621],\n",
      "        [-2.7007, -3.8497, -0.7702,  ..., -5.2055, -5.2192, -5.2450],\n",
      "        [-2.8644, -3.8257, -0.7962,  ..., -5.6045, -5.6069, -5.6001],\n",
      "        ...,\n",
      "        [-2.3810, -3.8685, -0.7113,  ..., -4.5923, -4.5882, -4.5949],\n",
      "        [-2.8706, -3.8168, -0.7939,  ..., -5.4872, -5.4840, -5.4746],\n",
      "        [-2.4971, -3.8074, -0.7293,  ..., -1.1123, -1.2287, -1.1251]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5516,  0.9319,  0.7945, -0.5950,  0.9559, -0.5626,  0.8178, -1.1584],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5960,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.4354e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1206, -0.1218, -0.1230,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5813,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7043, -3.8463, -0.7659,  ..., -5.2357, -5.2497, -5.2800],\n",
      "        [-2.3832, -3.8683, -0.7030,  ..., -4.5764, -4.5692, -4.5723],\n",
      "        [-2.8745, -3.8142, -0.7913,  ..., -5.4784, -5.4754, -5.4668],\n",
      "        ...,\n",
      "        [-2.6992, -3.8465, -0.7643,  ..., -5.2069, -5.2186, -5.2527],\n",
      "        [-2.6966, -3.8421, -0.7622,  ..., -5.2227, -5.2366, -5.2661],\n",
      "        [-2.6766, -3.8527, -0.7464,  ..., -5.2094, -5.2707, -5.2998]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9444, -0.6164,  0.8378,  0.9196,  0.9304,  0.9299,  0.8812,  0.9477],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5926,  0.9900,  0.9900,  0.9900, -0.3624,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.9307e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1199, -0.1211, -0.1223,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5779,  0.9656,  0.9656,  0.9656, -0.3535,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3902, -3.8683, -0.7072,  ..., -4.5567, -4.5495, -4.5532],\n",
      "        [-2.8792, -3.8219, -0.7911,  ..., -5.5822, -5.5838, -5.5772],\n",
      "        [-2.6858, -3.8505, -0.7433,  ..., -5.2025, -5.2628, -5.2951],\n",
      "        ...,\n",
      "        [-2.7252, -3.8530, -0.7683,  ..., -5.2807, -5.2983, -5.3277],\n",
      "        [-2.7197, -3.8431, -0.7543,  ..., -5.2263, -5.2384, -5.2672],\n",
      "        [-2.6808, -3.8436, -0.7543,  ..., -5.1289, -5.1789, -5.2063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5334,  0.8346,  0.9625,  0.8608, -0.2909,  0.9789,  0.9716,  0.9601],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5688, -0.6030, -1.1017, -0.5808, -0.5935, -0.6030, -0.5400,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(6.2678e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1151, -0.1162, -0.1174,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1220, -0.1232, -0.1245,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1521, -0.1537, -0.1552,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1220, -0.1232, -0.1245,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1092, -0.1103, -0.1115,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5548, -0.5882, -1.0534, -0.5664, -0.5789, -0.5882, -0.5267,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4069, -3.8634, -0.6968,  ..., -4.5451, -4.5454, -4.5495],\n",
      "        [-2.3900, -3.8644, -0.6977,  ..., -4.5506, -4.5451, -4.5450],\n",
      "        [-2.5065, -3.8143, -0.7087,  ..., -4.0923, -4.0861, -4.0844],\n",
      "        ...,\n",
      "        [-2.3900, -3.8644, -0.6977,  ..., -4.5506, -4.5451, -4.5450],\n",
      "        [-2.4080, -3.8680, -0.6944,  ..., -4.5108, -4.5113, -4.5201],\n",
      "        [-2.8909, -3.8148, -0.7860,  ..., -5.5365, -5.5338, -5.5241]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5461, -0.5551, -1.1151, -0.5578, -0.5682, -0.5551, -0.5039,  0.8751],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5260, -1.3405,  0.9900,  0.9900,  0.9900, -0.5651, -0.5218, -0.3191],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.2559e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(9.9995e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1064, -0.1075, -0.1086,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2739, -0.2767, -0.2795,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1143, -0.1155, -0.1166,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1056, -0.1066, -0.1077,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0550, -0.0555, -0.0561,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5130, -1.3140,  0.9656,  0.9656,  0.9656, -0.5512, -0.5090, -0.3113],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4157, -3.8661, -0.6907,  ..., -4.5100, -4.5124, -4.5186],\n",
      "        [-2.2851, -3.8753, -0.6461,  ..., -4.4243, -4.4217, -4.4271],\n",
      "        [-2.6778, -3.8359, -0.7219,  ..., -5.4162, -5.4565, -5.4679],\n",
      "        ...,\n",
      "        [-2.4101, -3.8617, -0.6941,  ..., -4.5638, -4.5596, -4.5597],\n",
      "        [-2.4130, -3.8578, -0.7034,  ..., -4.6188, -4.6112, -4.6129],\n",
      "        [-2.5105, -3.8425, -0.6960,  ..., -4.5869, -4.5922, -4.5903]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4971, -0.9515,  0.9784,  0.9230,  0.9599, -0.5881, -0.5078, -0.3189],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5490, -0.5175, -0.5517,  0.9900,  0.9900,  0.9900, -0.5073,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.8340e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1111, -0.1122, -0.1133,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1047, -0.1058, -0.1068,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1116, -0.1127, -0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1026, -0.1037, -0.1047,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5354, -0.5047, -0.5381,  0.9656,  0.9608,  0.9656, -0.4947,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4034, -3.8644, -0.6908,  ..., -4.5666, -4.5608, -4.5613],\n",
      "        [-2.4113, -3.8669, -0.6959,  ..., -4.5952, -4.5870, -4.5949],\n",
      "        [-2.4089, -3.8672, -0.6973,  ..., -4.5696, -4.5695, -4.5724],\n",
      "        ...,\n",
      "        [-2.7250, -3.8443, -0.7494,  ..., -5.2033, -5.2113, -5.2423],\n",
      "        [-2.4107, -3.8663, -0.6981,  ..., -4.5960, -4.5866, -4.5837],\n",
      "        [-2.7282, -3.8451, -0.7551,  ..., -5.1963, -5.2053, -5.2301]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5849, -0.5056, -0.5485,  0.9828,  0.9747,  0.9672, -0.5154,  0.9803],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5084], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(3.1791e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,\n",
      "          0.2149,  0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,\n",
      "          0.2329,  0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,\n",
      "          0.2524,  0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,\n",
      "          0.2735,  0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,\n",
      "          0.2964,  0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,\n",
      "          0.3212,  0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,\n",
      "          0.3481,  0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,\n",
      "          0.3772,  0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,\n",
      "          0.4088,  0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,\n",
      "          0.4430,  0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,\n",
      "          0.4801,  0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,\n",
      "          0.5203,  0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,\n",
      "          0.5639,  0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,\n",
      "          0.6111,  0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,\n",
      "          0.6623,  0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,\n",
      "          0.7177,  0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,\n",
      "          0.7778,  0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,\n",
      "          0.8429,  0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,\n",
      "          0.9135,  0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,\n",
      "          0.9900,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1028, -0.1039, -0.1049, -0.1060, -0.1071, -0.1081, -0.1092, -0.1103,\n",
      "         -0.1115, -0.1126, -0.1137, -0.1149, -0.1160, -0.1172, -0.1184, -0.1196,\n",
      "         -0.1208, -0.1220, -0.1232, -0.1245, -0.1257, -0.1270, -0.1283, -0.1296,\n",
      "         -0.1309, -0.1322, -0.1336, -0.1349, -0.1363, -0.1377, -0.1390, -0.1404,\n",
      "         -0.1419, -0.1433, -0.1447, -0.1462, -0.1477, -0.1492, -0.1507, -0.1522,\n",
      "         -0.1537, -0.1553, -0.1569, -0.1584, -0.1600, -0.1617, -0.1633, -0.1649,\n",
      "         -0.1666, -0.1683, -0.1700, -0.1717, -0.1734, -0.1752, -0.1770, -0.1788,\n",
      "         -0.1806, -0.1824, -0.1842, -0.1861, -0.1880, -0.1899, -0.1918, -0.1937,\n",
      "         -0.1957, -0.1977, -0.1997, -0.2017, -0.2037, -0.2058, -0.2078, -0.2099,\n",
      "         -0.2121, -0.2142, -0.2164, -0.2186, -0.2208, -0.2230, -0.2252, -0.2275,\n",
      "         -0.2298, -0.2321, -0.2345, -0.2369, -0.2392, -0.2417, -0.2441, -0.2466,\n",
      "         -0.2491, -0.2516, -0.2541, -0.2567, -0.2593, -0.2619, -0.2645, -0.2672,\n",
      "         -0.2699, -0.2726, -0.2754, -0.2782, -0.2810, -0.2838, -0.2867, -0.2896,\n",
      "         -0.2925, -0.2955, -0.2984, -0.3015, -0.3045, -0.3076, -0.3107, -0.3138,\n",
      "         -0.3170, -0.3202, -0.3234, -0.3267, -0.3300, -0.3333, -0.3367, -0.3401,\n",
      "         -0.3435, -0.3470, -0.3505, -0.3541, -0.3576, -0.3612, -0.3649, -0.3686,\n",
      "         -0.3723, -0.3761, -0.3799, -0.3837, -0.3876, -0.3915, -0.3954, -0.3994,\n",
      "         -0.4035, -0.4075, -0.4117, -0.4158, -0.4200, -0.4243, -0.4285, -0.4329,\n",
      "         -0.4372, -0.4417, -0.4461, -0.4506, -0.4552, -0.4598, -0.4644, -0.4691,\n",
      "         -0.4739, -0.4786, -0.4835, -0.4884, -0.4933, -0.4983, -0.5033, -0.5084,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608, -0.4959], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8706e+00, -3.8219e+00, -7.7902e-01, -2.4954e+00,  1.6922e-01,\n",
      "         -2.3904e+00, -4.6867e+00, -5.3376e-01, -4.2789e+00, -2.6983e+00,\n",
      "         -4.0293e+00,  4.8059e-01, -2.7913e-01, -3.6622e+00, -2.9921e+00,\n",
      "         -2.4714e+00, -3.3249e+00, -1.8529e+00, -1.9007e+00, -1.1281e+00,\n",
      "         -1.3930e+00, -1.5799e+00, -9.8811e-01,  4.8556e-01, -2.0741e+00,\n",
      "         -3.5681e+00, -2.9911e+00, -2.6315e+00, -3.7367e+00,  1.0315e+00,\n",
      "         -1.8396e+00, -3.6973e-01, -9.3392e-01, -3.5788e+00, -1.9267e+00,\n",
      "         -1.7159e+00, -8.5915e-01, -3.1984e+00, -1.7408e+00, -1.6300e-01,\n",
      "         -7.0342e-01, -4.8365e-01, -3.0406e-01, -4.1780e+00, -1.7335e+00,\n",
      "         -3.6176e+00, -3.7007e+00, -3.3091e+00,  7.1339e-01,  9.1879e-01,\n",
      "         -3.9247e+00, -1.4914e+00, -7.1408e+00, -2.9541e+00, -3.0332e+00,\n",
      "         -9.9610e-01, -2.5485e+00, -2.9185e+00, -2.6480e+00, -1.7207e+00,\n",
      "         -3.7544e+00, -1.1756e+00, -5.8073e+00, -2.4269e+00, -3.4740e+00,\n",
      "          1.1858e+00, -1.4339e+00, -2.5101e-01, -3.4733e-01, -3.2002e+00,\n",
      "         -4.2181e+00, -5.5994e+00, -2.8506e+00, -4.8675e-02, -3.8898e+00,\n",
      "         -4.1187e+00, -2.5110e+00, -3.8637e+00, -2.6188e+00, -3.6279e+00,\n",
      "         -4.8973e+00, -2.5335e+00, -3.2842e-01, -1.4621e+00, -1.8520e+00,\n",
      "         -3.1282e+00, -1.9366e+00, -8.3145e-01, -3.6780e+00, -8.7966e-01,\n",
      "         -3.2551e+00, -3.9932e+00, -2.4797e+00, -6.1014e-01, -1.0788e+00,\n",
      "         -2.9641e+00, -2.8526e+00, -3.9224e+00, -3.3325e+00, -3.3498e+00,\n",
      "         -1.0360e-01, -1.1842e+00, -3.6758e+00, -2.0507e+00, -4.0922e-01,\n",
      "         -2.0278e+00, -3.7509e+00, -3.9202e+00,  6.8656e-01, -2.2190e+00,\n",
      "         -1.3125e+00, -1.2599e+00, -4.6728e+00, -4.7885e+00,  5.0734e-01,\n",
      "         -9.6666e-01, -2.1932e+00, -1.3415e+00, -1.4793e-01, -9.8357e-01,\n",
      "         -2.8182e+00, -4.7671e+00, -2.5169e+00, -6.7777e-01, -9.5509e-01,\n",
      "         -2.8010e+00, -2.2711e+00, -8.4485e-01, -7.5932e-01, -4.2315e+00,\n",
      "          3.9543e-01,  1.0265e+00, -3.3435e+00, -4.1861e+00, -1.2351e+00,\n",
      "         -2.0445e+00, -3.0375e+00, -2.4379e+00, -1.6769e+00, -2.7668e+00,\n",
      "         -2.4760e+00, -3.5808e+00, -5.6428e+00, -1.5115e+00,  9.8241e-01,\n",
      "          8.6213e-01,  9.6508e-01,  9.8132e-01,  9.8761e-01,  1.0604e+00,\n",
      "          9.6490e-01, -4.6196e+00, -4.8608e+00, -5.1352e+00, -4.8939e+00,\n",
      "         -5.0562e+00, -5.0254e+00, -5.1522e+00, -5.2481e+00, -5.2285e+00,\n",
      "         -5.1835e+00, -5.1559e+00, -5.1302e+00, -5.1211e+00, -5.1449e+00,\n",
      "         -5.2018e+00, -5.2624e+00, -5.3069e+00, -5.3466e+00, -5.3685e+00,\n",
      "         -5.3833e+00, -5.3917e+00, -5.3998e+00, -5.4035e+00, -5.4323e+00,\n",
      "         -5.4573e+00, -5.4883e+00, -5.5008e+00, -5.5180e+00, -5.5223e+00,\n",
      "         -5.5304e+00, -5.5222e+00, -5.5106e+00, -5.5038e+00, -5.5092e+00,\n",
      "         -5.5134e+00, -5.5164e+00, -5.5255e+00, -5.5348e+00, -5.5321e+00,\n",
      "         -5.5178e+00, -5.5054e+00, -5.4905e+00, -5.4838e+00, -5.4723e+00,\n",
      "         -5.4804e+00, -5.4815e+00, -5.4720e+00, -5.4784e+00, -5.4802e+00,\n",
      "         -5.4823e+00, -5.4846e+00, -5.4769e+00, -5.4680e+00, -5.4546e+00],\n",
      "        [-2.4067e+00, -3.8697e+00, -6.9118e-01, -2.2252e+00,  2.5660e-01,\n",
      "         -2.3145e+00, -4.4872e+00, -1.3614e-01, -3.7573e+00, -2.4404e+00,\n",
      "         -3.9103e+00,  5.0319e-01, -1.8311e-01, -3.4777e+00, -3.1191e+00,\n",
      "         -2.1873e+00, -3.2071e+00, -1.7325e+00, -1.9865e+00, -9.1007e-01,\n",
      "         -1.3118e+00, -1.3411e+00, -8.0991e-01,  3.8416e-01, -1.8113e+00,\n",
      "         -3.3679e+00, -3.0767e+00, -2.3973e+00, -3.7602e+00,  9.8512e-01,\n",
      "         -1.6789e+00, -1.1429e-01, -9.4099e-01, -3.4838e+00, -1.9085e+00,\n",
      "         -1.8632e+00, -9.7208e-01, -3.1381e+00, -1.5248e+00,  5.5403e-02,\n",
      "         -6.2989e-01, -5.6293e-01, -2.5147e-01, -4.2667e+00, -1.5097e+00,\n",
      "         -3.1645e+00, -3.4143e+00, -3.2208e+00,  4.6511e-02,  3.5393e-01,\n",
      "         -3.7923e+00, -1.4068e+00, -7.1313e+00, -2.7853e+00, -2.9985e+00,\n",
      "         -8.5156e-01, -2.5691e+00, -2.8979e+00, -2.5207e+00, -1.5181e+00,\n",
      "         -3.5843e+00, -1.0948e+00, -5.7814e+00, -2.3024e+00, -3.5008e+00,\n",
      "          1.1147e+00, -1.2880e+00,  1.5847e-02, -3.9141e-01, -3.1058e+00,\n",
      "         -4.0136e+00, -5.5896e+00, -2.6891e+00, -1.3266e-01, -3.7371e+00,\n",
      "         -4.2142e+00, -2.3545e+00, -3.8426e+00, -2.4475e+00, -3.4863e+00,\n",
      "         -4.8910e+00, -2.3871e+00, -2.5277e-01, -1.2263e+00, -1.7630e+00,\n",
      "         -2.9210e+00, -1.7630e+00, -5.9067e-01, -3.5430e+00, -7.8461e-01,\n",
      "         -3.0799e+00, -4.0745e+00, -2.3412e+00, -5.5725e-01, -8.3232e-01,\n",
      "         -3.0288e+00, -2.6573e+00, -3.5403e+00, -3.2711e+00, -3.2167e+00,\n",
      "         -6.7839e-03, -9.5008e-01, -3.7247e+00, -2.0473e+00, -4.8320e-01,\n",
      "         -1.7919e+00, -3.3892e+00, -3.7495e+00,  5.8823e-01, -1.2263e+00,\n",
      "         -1.3040e+00, -7.3980e-01, -4.5831e+00, -4.6889e+00,  4.5557e-01,\n",
      "         -8.7691e-01, -2.1051e+00, -1.2422e+00, -4.0288e-02, -9.0370e-01,\n",
      "         -2.6994e+00, -4.6285e+00, -2.5999e+00, -8.0136e-01, -1.0676e+00,\n",
      "         -2.7737e+00, -2.1091e+00, -7.2305e-01, -4.5962e-01, -4.1725e+00,\n",
      "         -3.4092e-01,  4.2830e-01, -3.3354e+00, -4.0596e+00, -1.1419e+00,\n",
      "         -2.0290e+00, -3.0389e+00, -2.3435e+00, -1.5946e+00, -2.8598e+00,\n",
      "         -2.5595e+00, -3.2798e+00, -5.5963e+00, -1.2640e+00, -4.9956e-01,\n",
      "         -4.8852e-01, -4.9259e-01, -4.9358e-01, -4.7966e-01, -5.5276e-01,\n",
      "         -4.8583e+00, -4.8826e+00, -5.2036e+00, -4.8672e+00, -4.7931e+00,\n",
      "         -4.5824e+00, -4.5381e+00, -4.5765e+00, -4.5587e+00, -4.5036e+00,\n",
      "         -4.5612e+00, -4.6378e+00, -4.6890e+00, -4.6568e+00, -4.5458e+00,\n",
      "         -4.4345e+00, -4.3931e+00, -4.3912e+00, -4.4057e+00, -4.4144e+00,\n",
      "         -4.4154e+00, -4.4426e+00, -4.4819e+00, -4.5050e+00, -4.5249e+00,\n",
      "         -4.5365e+00, -4.5131e+00, -4.4956e+00, -4.5083e+00, -4.5143e+00,\n",
      "         -4.5242e+00, -4.5439e+00, -4.5386e+00, -4.5226e+00, -4.5143e+00,\n",
      "         -4.5087e+00, -4.5288e+00, -4.5519e+00, -4.5702e+00, -4.5817e+00,\n",
      "         -4.5934e+00, -4.6145e+00, -4.6383e+00, -4.6499e+00, -4.6468e+00,\n",
      "         -4.6403e+00, -4.6355e+00, -4.6214e+00, -4.6089e+00, -4.6057e+00,\n",
      "         -4.6017e+00, -4.6016e+00, -4.5955e+00, -4.5879e+00, -4.5949e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9720, -0.5011], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.0801,  0.9900, -0.5092,  0.9900,  0.9900,  0.9900, -0.5443],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.4550e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1491, -0.1506, -0.1522,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1101, -0.1112, -0.1123,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.0327,  0.9656, -0.4967,  0.9656,  0.9656,  0.9608, -0.5309],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6739, -3.8440, -0.7353,  ..., -5.1047, -5.1574, -5.1868],\n",
      "        [-2.4918, -3.8197, -0.6936,  ..., -4.0632, -4.0629, -4.0663],\n",
      "        [-2.7149, -3.8493, -0.7489,  ..., -5.1981, -5.2117, -5.2391],\n",
      "        ...,\n",
      "        [-2.7100, -3.8480, -0.7444,  ..., -5.2112, -5.2219, -5.2539],\n",
      "        [-2.8627, -3.8232, -0.7774,  ..., -5.4817, -5.4713, -5.4570],\n",
      "        [-2.3988, -3.8712, -0.6918,  ..., -4.5653, -4.5664, -4.5682]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9753, -1.0927,  0.9586, -0.4969,  0.9437,  0.9581,  0.9690, -0.5410],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5539,  0.9900,  0.9900,  0.9900, -0.5586, -0.5148,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.2778e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1120, -0.1132, -0.1143,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1041, -0.1052, -0.1063,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5402,  0.9656,  0.9656,  0.9656, -0.5448, -0.5021,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3863, -3.8675, -0.6822,  ..., -4.5669, -4.5619, -4.5632],\n",
      "        [-2.7036, -3.8518, -0.7377,  ..., -5.2211, -5.2332, -5.2599],\n",
      "        [-2.8678, -3.8254, -0.7716,  ..., -5.5851, -5.5860, -5.5787],\n",
      "        ...,\n",
      "        [-2.3929, -3.8692, -0.6888,  ..., -4.5965, -4.5874, -4.5828],\n",
      "        [-2.7078, -3.8470, -0.7340,  ..., -5.2321, -5.2465, -5.2769],\n",
      "        [-2.6717, -3.8536, -0.7231,  ..., -5.1935, -5.2548, -5.2855]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5714,  0.9386,  0.8660,  0.9713, -0.5001, -0.5034,  0.9807,  0.9663],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.5623, -0.5302, -0.3319,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.4378e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1073, -0.1083, -0.1094,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0572, -0.0578, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9608, -0.5484, -0.5171, -0.3238,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8612, -3.8258, -0.7652,  ..., -5.5998, -5.5995, -5.5947],\n",
      "        [-2.6963, -3.8499, -0.7435,  ..., -5.2128, -5.2310, -5.2591],\n",
      "        [-2.7063, -3.8563, -0.7456,  ..., -5.2874, -5.3084, -5.3393],\n",
      "        ...,\n",
      "        [-2.3829, -3.8644, -0.6871,  ..., -4.6223, -4.6169, -4.6175],\n",
      "        [-2.4701, -3.8496, -0.6773,  ..., -4.6293, -4.6331, -4.6319],\n",
      "        [-2.6642, -3.8539, -0.7181,  ..., -5.2053, -5.2656, -5.2983]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8665,  0.9385,  0.9792,  0.9609, -0.5159, -0.4886, -0.3123,  0.9625],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6086, -0.5860, -0.5810,  0.9900, -0.6097, -1.3031],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8384e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.4213e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1231, -0.1244, -0.1256,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1233, -0.1246, -0.1258,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1517, -0.1532, -0.1548,  ..., -1.2772, -1.2901, -1.3031]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5936, -0.5716, -0.5666,  0.9656, -0.5946, -1.2460],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8584, -3.8178, -0.7612,  ..., -5.5655, -5.5664, -5.5599],\n",
      "        [-2.8593, -3.8149, -0.7607,  ..., -5.4894, -5.4885, -5.4804],\n",
      "        [-2.3697, -3.8666, -0.6786,  ..., -4.5745, -4.5701, -4.5774],\n",
      "        ...,\n",
      "        [-2.6786, -3.8467, -0.7341,  ..., -5.2402, -5.2586, -5.2895],\n",
      "        [-2.3581, -3.8676, -0.6723,  ..., -4.5415, -4.5370, -4.5363],\n",
      "        [-2.4787, -3.8056, -0.6933,  ..., -1.0941, -1.1467, -1.1563]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8713,  0.8909, -0.5043, -0.4896, -0.5625,  0.8846, -0.5258, -1.1208],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.3756, -0.6206, -0.5632, -0.5565, -0.5928, -0.6121,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4630e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.0483e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2811, -0.2839, -0.2868,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1255, -0.1268, -0.1281,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1139, -0.1151, -0.1162,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1238, -0.1251, -0.1263,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3484, -0.6053, -0.5493, -0.5428, -0.5781, -0.5970,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2335, -3.8819, -0.6254,  ..., -4.4242, -4.4201, -4.4264],\n",
      "        [-2.3559, -3.8683, -0.6752,  ..., -4.5493, -4.5445, -4.5443],\n",
      "        [-2.3725, -3.8727, -0.6810,  ..., -4.5902, -4.5840, -4.5926],\n",
      "        ...,\n",
      "        [-2.3623, -3.8681, -0.6740,  ..., -4.6094, -4.6035, -4.6047],\n",
      "        [-2.6877, -3.8507, -0.7397,  ..., -5.2537, -5.2678, -5.3013],\n",
      "        [-2.8603, -3.8198, -0.7653,  ..., -5.5756, -5.5782, -5.5714]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9890, -0.5302, -0.4896, -0.4856, -0.5659, -0.5323,  0.9646,  0.8719],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6173, -0.3760,  0.9900,  0.9900,  0.9900, -0.5627,  0.9900, -0.6028],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5790e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1249, -0.1261, -0.1274,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0648, -0.0654, -0.0661,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1138, -0.1150, -0.1161,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1220, -0.1232, -0.1244,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6020, -0.3667,  0.9656,  0.9656,  0.9656, -0.5488,  0.9656, -0.5880],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3557, -3.8721, -0.6778,  ..., -4.6135, -4.6099, -4.6126],\n",
      "        [-2.4600, -3.8525, -0.6901,  ..., -4.6980, -4.7027, -4.6990],\n",
      "        [-2.6917, -3.8568, -0.7448,  ..., -5.2596, -5.2754, -5.3065],\n",
      "        ...,\n",
      "        [-2.3679, -3.8788, -0.6779,  ..., -4.5040, -4.5076, -4.5180],\n",
      "        [-2.8592, -3.8226, -0.7722,  ..., -5.5091, -5.5084, -5.4995],\n",
      "        [-2.3647, -3.8769, -0.6848,  ..., -4.5726, -4.5753, -4.5798]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5459, -0.2843,  0.9640,  0.9268,  0.8654, -0.5013,  0.8712, -0.5535],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.1193e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6980, -3.8642, -0.7545, -2.4639,  0.1792, -2.3837, -4.5952, -0.4135,\n",
      "         -4.0731, -2.5839, -4.0054,  0.4860, -0.2687, -3.5843, -3.0397, -2.3777,\n",
      "         -3.2884, -1.8101, -1.9270, -1.0634, -1.3622, -1.5069, -0.9370,  0.4375,\n",
      "         -2.0075, -3.4887, -3.0198, -2.5655, -3.7559,  0.9976, -1.8050, -0.2887,\n",
      "         -0.9262, -3.5729, -1.9371, -1.7677, -0.9348, -3.1869, -1.6682, -0.0854,\n",
      "         -0.6557, -0.5443, -0.2928, -4.2506, -1.6725, -3.4404, -3.5853, -3.3050,\n",
      "          0.4089,  0.6525, -3.8644, -1.4857, -7.1346, -2.9106, -3.0252, -0.9467,\n",
      "         -2.5423, -2.9413, -2.5959, -1.6770, -3.6885, -1.1628, -5.7963, -2.4018,\n",
      "         -3.4974,  1.1431, -1.3988, -0.1600, -0.3482, -3.1974, -4.1216, -5.5636,\n",
      "         -2.8092, -0.1068, -3.8309, -4.2007, -2.4718, -3.8641, -2.5586, -3.5700,\n",
      "         -4.8636, -2.4946, -0.2858, -1.3800, -1.8211, -3.0611, -1.8849, -0.7534,\n",
      "         -3.6043, -0.8301, -3.1814, -4.0683, -2.4445, -0.5767, -0.9861, -2.9795,\n",
      "         -2.7999, -3.7721, -3.3121, -3.3247, -0.0563, -1.0957, -3.7245, -2.0543,\n",
      "         -0.4401, -1.9756, -3.5970, -3.8617,  0.6287, -1.3586, -1.4507, -1.0305,\n",
      "         -4.1418, -4.5431,  0.5175, -1.0906, -2.1340, -1.4140, -0.1585, -0.9918,\n",
      "         -2.8631, -4.5511, -2.6758, -0.8021, -1.1701, -2.8241, -2.2428, -0.7956,\n",
      "         -0.6387, -4.2851, -0.0252,  0.7127, -3.3170, -4.1166, -1.2383, -2.0193,\n",
      "         -3.0820, -2.4041, -1.6950, -2.8825, -2.9128, -5.7381, -1.6585, -2.4404,\n",
      "         -1.3240, -0.8175, -0.4673, -3.8147, -4.0102, -0.1602, -2.0300, -0.3210,\n",
      "         -0.9445, -0.4901,  0.2024, -0.4187, -4.0002, -3.9070, -4.8769, -1.3334,\n",
      "          0.8499,  0.9639,  0.9556,  0.9575,  0.8631,  0.8976, -4.6588, -4.8642,\n",
      "         -5.2214, -5.0470, -5.2454, -5.3413, -5.3024, -5.3018, -5.2775, -5.2347,\n",
      "         -5.1411, -5.0204, -4.9247, -4.8816, -4.8762, -4.8999, -4.9382, -4.9812,\n",
      "         -5.0046, -5.0204, -5.0362, -5.0733, -5.1206, -5.1887, -5.2295, -5.2382,\n",
      "         -5.2249, -5.2378, -5.2764, -5.3406, -5.3696, -5.3572, -5.3211, -5.2635,\n",
      "         -5.2183, -5.1843, -5.1782, -5.1959, -5.2256],\n",
      "        [-2.6340, -3.8526, -0.7165, -2.3987,  0.2059, -2.3753, -4.5589, -0.3254,\n",
      "         -3.9933, -2.5406, -3.9943,  0.4844, -0.2600, -3.5322, -3.0560, -2.3271,\n",
      "         -3.2880, -1.7861, -1.9350, -1.0238, -1.3252, -1.4625, -0.8917,  0.4358,\n",
      "         -1.9644, -3.4322, -3.0299, -2.5261, -3.7502,  1.0049, -1.7586, -0.2366,\n",
      "         -0.9153, -3.5626, -1.9284, -1.7973, -0.9178, -3.1955, -1.6070, -0.0343,\n",
      "         -0.6354, -0.5300, -0.2657, -4.2513, -1.6326, -3.3700, -3.5341, -3.2901,\n",
      "          0.2898,  0.5674, -3.8227, -1.4766, -7.1492, -2.8775, -3.0345, -0.9196,\n",
      "         -2.5445, -2.9284, -2.5692, -1.6369, -3.6409, -1.1513, -5.8052, -2.3798,\n",
      "         -3.4873,  1.1440, -1.3574, -0.1129, -0.3501, -3.1821, -4.0938, -5.5693,\n",
      "         -2.7850, -0.1258, -3.7874, -4.2340, -2.4440, -3.8782, -2.5118, -3.5297,\n",
      "         -4.8673, -2.4708, -0.2520, -1.3320, -1.7976, -3.0201, -1.8513, -0.7081,\n",
      "         -3.5756, -0.8119, -3.1352, -4.1008, -2.4212, -0.5544, -0.9406, -2.9657,\n",
      "         -2.7673, -3.7075, -3.2949, -3.2971, -0.0250, -1.0496, -3.7138, -2.0563,\n",
      "         -0.4433, -1.9320, -3.5389, -3.8383,  0.6079, -1.2766, -1.4623, -2.5202,\n",
      "         -4.4475, -4.5977,  0.4845, -0.9784, -2.1526, -1.3237, -0.0708, -0.9370,\n",
      "         -2.6301, -4.7754, -2.5670, -0.7387, -1.0484, -2.7785, -2.2215, -0.7698,\n",
      "         -0.5716, -4.2676, -0.1992,  0.6164, -3.3606, -4.1207, -1.1971, -2.0160,\n",
      "         -3.0821, -2.3740, -1.6722, -2.8044, -2.8451, -5.7968, -1.6231, -3.9099,\n",
      "         -0.8542, -1.1879, -4.2830, -4.4257, -0.2343, -1.5855, -4.5586, -2.9908,\n",
      "         -2.7835, -4.1909,  0.2233, -2.3373, -1.0818, -0.5369, -3.5041, -0.9413,\n",
      "          0.2943, -4.4518, -5.6465, -3.7526,  0.0883, -4.4433, -4.5288, -3.1332,\n",
      "         -3.6634, -3.7108, -2.8475, -5.0360, -1.3010,  0.8557,  0.9671,  0.9203,\n",
      "          0.9287,  0.9110,  0.9942, -4.6477, -4.8690, -5.3116, -5.2055, -5.3034,\n",
      "         -5.3520, -5.1629, -5.1040, -5.2503, -5.3001, -5.2287, -5.1415, -5.0683,\n",
      "         -5.0618, -5.1135, -5.1785, -5.2397, -5.2766, -5.2962, -5.3092, -5.3284,\n",
      "         -5.3631, -5.4069, -5.4555, -5.4936, -5.5028]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9146, 0.9295], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5316, -0.5713,  0.9900, -0.5316, -0.5969,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.3631e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1075, -0.1086, -0.1097,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1156, -0.1167, -0.1179,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5185, -0.5572,  0.9656, -0.5185, -0.5822,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3661, -3.8857, -0.6826,  ..., -4.5220, -4.5265, -4.5343],\n",
      "        [-2.3600, -3.8830, -0.6850,  ..., -4.5804, -4.5755, -4.5778],\n",
      "        [-2.6853, -3.8662, -0.7478,  ..., -5.2212, -5.2335, -5.2594],\n",
      "        ...,\n",
      "        [-2.8628, -3.8320, -0.7834,  ..., -5.5140, -5.5105, -5.4995],\n",
      "        [-2.8534, -3.8411, -0.7861,  ..., -5.6303, -5.6305, -5.6221],\n",
      "        [-2.6799, -3.8605, -0.7516,  ..., -5.2278, -5.2410, -5.2637]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5315, -0.6099,  0.8955, -0.5315, -0.5534,  0.8490,  0.8103,  0.8501],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5639, -0.5498,  0.9900,  0.9900, -0.5463, -0.5053,  0.9900, -0.5639],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.8545e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1141, -0.1152, -0.1164,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1112, -0.1123, -0.1135,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1022, -0.1033, -0.1043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1141, -0.1152, -0.1164,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5500, -0.5362,  0.9656,  0.9656, -0.5328, -0.4928,  0.9656, -0.5500],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3625, -3.8825, -0.6860,  ..., -4.6196, -4.6115, -4.6128],\n",
      "        [-2.3705, -3.8869, -0.6916,  ..., -4.5873, -4.5871, -4.5908],\n",
      "        [-2.6972, -3.8624, -0.7470,  ..., -5.2377, -5.2489, -5.2734],\n",
      "        ...,\n",
      "        [-2.3713, -3.8852, -0.6947,  ..., -4.6140, -4.6053, -4.6054],\n",
      "        [-2.6861, -3.8610, -0.7524,  ..., -5.2180, -5.2324, -5.2543],\n",
      "        [-2.3625, -3.8825, -0.6860,  ..., -4.6196, -4.6115, -4.6128]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5791, -0.5904,  0.9306,  0.8891, -0.6160, -0.5440,  0.8524, -0.5791],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5507,  0.9900,  0.9900, -0.2807,  0.9900, -1.2696, -0.4833],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6398e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1114, -0.1125, -0.1137,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1478, -0.1493, -0.1508,  ..., -1.2443, -1.2569, -1.2696],\n",
      "        [-0.0978, -0.0988, -0.0998,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5371,  0.9656,  0.9656, -0.2737,  0.9656, -1.2140, -0.4714],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6977, -3.8638, -0.7548,  ..., -5.2487, -5.2556, -5.2819],\n",
      "        [-2.3657, -3.8839, -0.6890,  ..., -4.5695, -4.5646, -4.5639],\n",
      "        [-2.7048, -3.8651, -0.7534,  ..., -5.2450, -5.2540, -5.2808],\n",
      "        ...,\n",
      "        [-2.6933, -3.8634, -0.7537,  ..., -5.2129, -5.2207, -5.2451],\n",
      "        [-2.4947, -3.8208, -0.7150,  ..., -1.1295, -1.1798, -1.1658],\n",
      "        [-2.3786, -3.8798, -0.6974,  ..., -4.6403, -4.6330, -4.6341]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9321, -0.5830,  0.9493,  0.9265, -0.3464,  0.9121, -1.1517, -0.5382],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5203, -0.5070,  0.9900,  0.9900,  0.9900,  0.9900, -0.4857,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3515e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1053, -0.1063, -0.1074,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1026, -0.1036, -0.1046,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0983, -0.0992, -0.1002,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5074, -0.4945,  0.9656,  0.9656,  0.9656,  0.9608, -0.4737,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3899, -3.8877, -0.6974,  ..., -4.6005, -4.5992, -4.6036],\n",
      "        [-2.3919, -3.8838, -0.6930,  ..., -4.5770, -4.5717, -4.5793],\n",
      "        [-2.6691, -3.8604, -0.7471,  ..., -5.1386, -5.1947, -5.2300],\n",
      "        ...,\n",
      "        [-2.8605, -3.8416, -0.7945,  ..., -5.5395, -5.5323, -5.5189],\n",
      "        [-2.3901, -3.8857, -0.6958,  ..., -4.6207, -4.6129, -4.6217],\n",
      "        [-2.8605, -3.8416, -0.7945,  ..., -5.5395, -5.5323, -5.5189]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5901, -0.5618,  0.9322,  0.9208,  0.9269,  0.8783, -0.5448,  0.8783],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5248, -0.4912,  0.9900,  0.9900, -1.0728,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1266e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.5188e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1062, -0.1072, -0.1083,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1481, -0.1496, -0.1511,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5119, -0.4791,  0.9656,  0.9656, -1.0258,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6749, -3.8600, -0.7457,  ..., -5.1518, -5.2079, -5.2428],\n",
      "        [-2.7196, -3.8705, -0.7623,  ..., -5.2890, -5.3050, -5.3329],\n",
      "        [-2.3917, -3.8870, -0.6979,  ..., -4.5796, -4.5701, -4.5748],\n",
      "        ...,\n",
      "        [-2.8875, -3.8346, -0.7893,  ..., -5.6310, -5.6287, -5.6211],\n",
      "        [-2.4921, -3.8317, -0.7013,  ..., -4.0756, -4.0759, -4.0812],\n",
      "        [-2.8875, -3.8346, -0.7893,  ..., -5.6310, -5.6287, -5.6211]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9434,  0.9567, -0.5337, -0.5390,  0.9220,  0.8410, -1.1117,  0.8410],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5459, -1.3177,  0.9900, -0.3194,  0.9900, -0.5405],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.7201e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1104, -0.1116, -0.1127,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0550, -0.0556, -0.0561,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1093, -0.1104, -0.1116,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5325, -1.2916,  0.9656, -0.3115,  0.9656, -0.5272],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6816, -3.8669, -0.7371,  ..., -5.2409, -5.3041, -5.3383],\n",
      "        [-2.8890, -3.8318, -0.7884,  ..., -5.5627, -5.5624, -5.5533],\n",
      "        [-2.3920, -3.8853, -0.7012,  ..., -4.5767, -4.5655, -4.5719],\n",
      "        ...,\n",
      "        [-2.4946, -3.8583, -0.7027,  ..., -4.7146, -4.7188, -4.7152],\n",
      "        [-2.7107, -3.8627, -0.7620,  ..., -5.2318, -5.2433, -5.2675],\n",
      "        [-2.3949, -3.8826, -0.6942,  ..., -4.5973, -4.5893, -4.5912]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9484,  0.8771, -0.5276, -1.0166,  0.8212, -0.2958,  0.9216, -0.6007],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5906,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.6376e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1195, -0.1207, -0.1219, -0.1231, -0.1244, -0.1256, -0.1269, -0.1282,\n",
      "         -0.1295, -0.1308, -0.1321, -0.1334, -0.1348, -0.1362, -0.1375, -0.1389,\n",
      "         -0.1403, -0.1417, -0.1432, -0.1446, -0.1461, -0.1476, -0.1490, -0.1506,\n",
      "         -0.1521, -0.1536, -0.1552, -0.1567, -0.1583, -0.1599, -0.1615, -0.1632,\n",
      "         -0.1648, -0.1665, -0.1682, -0.1698, -0.1716, -0.1733, -0.1750, -0.1768,\n",
      "         -0.1786, -0.1804, -0.1822, -0.1841, -0.1859, -0.1878, -0.1897, -0.1916,\n",
      "         -0.1936, -0.1955, -0.1975, -0.1995, -0.2015, -0.2035, -0.2056, -0.2077,\n",
      "         -0.2098, -0.2119, -0.2140, -0.2162, -0.2184, -0.2206, -0.2228, -0.2250,\n",
      "         -0.2273, -0.2296, -0.2319, -0.2343, -0.2366, -0.2390, -0.2415, -0.2439,\n",
      "         -0.2464, -0.2488, -0.2514, -0.2539, -0.2565, -0.2591, -0.2617, -0.2643,\n",
      "         -0.2670, -0.2697, -0.2724, -0.2752, -0.2779, -0.2807, -0.2836, -0.2864,\n",
      "         -0.2893, -0.2923, -0.2952, -0.2982, -0.3012, -0.3042, -0.3073, -0.3104,\n",
      "         -0.3136, -0.3167, -0.3199, -0.3232, -0.3264, -0.3297, -0.3330, -0.3364,\n",
      "         -0.3398, -0.3432, -0.3467, -0.3502, -0.3537, -0.3573, -0.3609, -0.3646,\n",
      "         -0.3683, -0.3720, -0.3757, -0.3795, -0.3834, -0.3872, -0.3911, -0.3951,\n",
      "         -0.3991, -0.4031, -0.4072, -0.4113, -0.4155, -0.4197, -0.4239, -0.4282,\n",
      "         -0.4325, -0.4369, -0.4413, -0.4457, -0.4502, -0.4548, -0.4594, -0.4640,\n",
      "         -0.4687, -0.4734, -0.4782, -0.4831, -0.4879, -0.4929, -0.4978, -0.5029,\n",
      "         -0.5080, -0.5131, -0.5183, -0.5235, -0.5288, -0.5341, -0.5395, -0.5450,\n",
      "         -0.5505, -0.5560, -0.5617, -0.5673, -0.5731, -0.5789, -0.5847, -0.5906,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5760,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3850e+00, -3.8818e+00, -6.9150e-01, -2.2717e+00,  2.5876e-01,\n",
      "         -2.3402e+00, -4.5042e+00, -1.4411e-01, -3.7258e+00, -2.4464e+00,\n",
      "         -3.9292e+00,  5.1366e-01, -1.9071e-01, -3.4971e+00, -3.1235e+00,\n",
      "         -2.2023e+00, -3.1934e+00, -1.7296e+00, -1.9682e+00, -9.2626e-01,\n",
      "         -1.3242e+00, -1.3569e+00, -8.1241e-01,  3.7497e-01, -1.8352e+00,\n",
      "         -3.3883e+00, -3.0777e+00, -2.4194e+00, -3.7682e+00,  9.7530e-01,\n",
      "         -1.6843e+00, -1.3272e-01, -9.4628e-01, -3.5026e+00, -1.9088e+00,\n",
      "         -1.8561e+00, -9.7662e-01, -3.1245e+00, -1.5213e+00,  6.0907e-02,\n",
      "         -6.3029e-01, -5.6543e-01, -2.5403e-01, -4.3224e+00, -1.5360e+00,\n",
      "         -3.1458e+00, -3.4294e+00, -3.2372e+00,  1.5460e-02,  2.9754e-01,\n",
      "         -3.8092e+00, -1.4228e+00, -7.1003e+00, -2.8098e+00, -2.9850e+00,\n",
      "         -8.5073e-01, -2.5456e+00, -2.9218e+00, -2.5265e+00, -1.5470e+00,\n",
      "         -3.6056e+00, -1.1077e+00, -5.7426e+00, -2.3307e+00, -3.5121e+00,\n",
      "          1.1050e+00, -1.2917e+00, -5.4302e-03, -3.9110e-01, -3.1244e+00,\n",
      "         -4.0135e+00, -5.5706e+00, -2.7149e+00, -1.5398e-01, -3.7642e+00,\n",
      "         -4.2598e+00, -2.3787e+00, -3.8294e+00, -2.4505e+00, -3.5123e+00,\n",
      "         -4.8686e+00, -2.4110e+00, -2.4698e-01, -1.2484e+00, -1.7811e+00,\n",
      "         -2.9147e+00, -1.7713e+00, -6.2098e-01, -3.5360e+00, -7.9021e-01,\n",
      "         -3.1046e+00, -4.1170e+00, -2.3638e+00, -5.5170e-01, -8.5306e-01,\n",
      "         -3.0224e+00, -2.6867e+00, -3.5318e+00, -3.2560e+00, -3.2428e+00,\n",
      "         -5.5948e-03, -9.7265e-01, -3.7355e+00, -2.0500e+00, -4.9351e-01,\n",
      "         -1.8083e+00, -3.3772e+00, -3.7550e+00,  5.7108e-01, -1.3369e+00,\n",
      "         -1.2219e+00, -1.1197e+00, -4.2562e+00, -4.5672e+00,  5.1246e-01,\n",
      "         -9.6294e-01, -2.1248e+00, -1.3130e+00, -1.1562e-01, -8.7473e-01,\n",
      "         -2.6830e+00, -4.5640e+00, -2.6373e+00, -8.0360e-01, -1.1003e+00,\n",
      "         -2.7235e+00, -2.1698e+00, -7.2289e-01, -6.2144e-01, -4.2109e+00,\n",
      "         -4.0768e-01,  3.5071e-01, -3.3076e+00, -4.0644e+00, -1.1709e+00,\n",
      "         -2.0097e+00, -3.0797e+00, -2.3324e+00, -1.6248e+00, -2.8332e+00,\n",
      "         -2.5613e+00, -3.2874e+00, -5.6643e+00, -1.2867e+00, -5.1176e-01,\n",
      "         -5.6483e-01, -6.3764e-01, -4.9212e-01, -6.0413e-01, -5.4976e-01,\n",
      "         -4.8188e+00, -4.9052e+00, -5.2813e+00, -4.9036e+00, -4.8474e+00,\n",
      "         -4.6400e+00, -4.5714e+00, -4.6690e+00, -4.6134e+00, -4.5901e+00,\n",
      "         -4.6663e+00, -4.6901e+00, -4.7030e+00, -4.6663e+00, -4.5591e+00,\n",
      "         -4.4471e+00, -4.3917e+00, -4.3721e+00, -4.3831e+00, -4.3910e+00,\n",
      "         -4.3981e+00, -4.4173e+00, -4.4421e+00, -4.4628e+00, -4.4861e+00,\n",
      "         -4.4893e+00, -4.4503e+00, -4.4241e+00, -4.4308e+00, -4.4374e+00,\n",
      "         -4.4459e+00, -4.4594e+00, -4.4460e+00, -4.4295e+00, -4.4253e+00,\n",
      "         -4.4265e+00, -4.4461e+00, -4.4640e+00, -4.4782e+00, -4.4843e+00,\n",
      "         -4.4859e+00, -4.4992e+00, -4.5122e+00, -4.5228e+00, -4.5219e+00,\n",
      "         -4.5256e+00, -4.5304e+00, -4.5328e+00, -4.5297e+00, -4.5365e+00,\n",
      "         -4.5446e+00, -4.5625e+00, -4.5691e+00, -4.5646e+00, -4.5661e+00],\n",
      "        [-2.7029e+00, -3.8619e+00, -7.5664e-01, -2.4775e+00,  1.6306e-01,\n",
      "         -2.3978e+00, -4.6037e+00, -4.0878e-01, -4.0762e+00, -2.6050e+00,\n",
      "         -4.0162e+00,  5.1558e-01, -2.7913e-01, -3.5876e+00, -3.0100e+00,\n",
      "         -2.3725e+00, -3.2869e+00, -1.8231e+00, -1.9001e+00, -1.0884e+00,\n",
      "         -1.3657e+00, -1.5227e+00, -9.4362e-01,  4.4848e-01, -2.0376e+00,\n",
      "         -3.4897e+00, -2.9906e+00, -2.5635e+00, -3.7624e+00,  9.8710e-01,\n",
      "         -1.8221e+00, -3.1451e-01, -9.2600e-01, -3.5823e+00, -1.9340e+00,\n",
      "         -1.7449e+00, -9.3382e-01, -3.1805e+00, -1.6708e+00, -9.3800e-02,\n",
      "         -6.7425e-01, -5.4201e-01, -2.9845e-01, -4.2635e+00, -1.6998e+00,\n",
      "         -3.4441e+00, -3.6151e+00, -3.3136e+00,  4.4887e-01,  6.4096e-01,\n",
      "         -3.8662e+00, -1.4958e+00, -7.1003e+00, -2.9116e+00, -3.0192e+00,\n",
      "         -9.5968e-01, -2.5199e+00, -2.9577e+00, -2.6001e+00, -1.7032e+00,\n",
      "         -3.6896e+00, -1.1728e+00, -5.7574e+00, -2.4027e+00, -3.5061e+00,\n",
      "          1.1313e+00, -1.4154e+00, -1.8635e-01, -3.4611e-01, -3.2054e+00,\n",
      "         -4.1109e+00, -5.5455e+00, -2.8074e+00, -8.5154e-02, -3.8363e+00,\n",
      "         -4.2035e+00, -2.4687e+00, -3.8599e+00, -2.5706e+00, -3.5743e+00,\n",
      "         -4.8391e+00, -2.4917e+00, -2.8899e-01, -1.4070e+00, -1.8222e+00,\n",
      "         -3.0637e+00, -1.9014e+00, -7.8470e-01, -3.6036e+00, -8.5238e-01,\n",
      "         -3.1805e+00, -4.0696e+00, -2.4414e+00, -5.8297e-01, -1.0154e+00,\n",
      "         -2.9701e+00, -2.8322e+00, -3.7816e+00, -3.3002e+00, -3.3341e+00,\n",
      "         -6.2051e-02, -1.1232e+00, -3.7129e+00, -2.0544e+00, -4.3523e-01,\n",
      "         -1.9816e+00, -3.6083e+00, -3.8518e+00,  6.5714e-01, -1.2252e+00,\n",
      "         -1.1648e+00, -9.0425e-01, -4.8348e+00, -4.7093e+00,  4.2633e-01,\n",
      "         -1.0469e+00, -2.1447e+00, -1.3792e+00, -1.3591e-01, -1.0077e+00,\n",
      "         -2.9101e+00, -4.5956e+00, -2.6583e+00, -8.0818e-01, -1.1845e+00,\n",
      "         -2.8657e+00, -2.2654e+00, -7.9090e-01, -6.5447e-01, -4.2672e+00,\n",
      "          6.9862e-02,  7.2559e-01, -3.3605e+00, -4.1057e+00, -1.2343e+00,\n",
      "         -1.9888e+00, -3.0882e+00, -2.4180e+00, -1.7245e+00, -2.8645e+00,\n",
      "         -2.9177e+00, -5.7536e+00, -1.7035e+00, -2.4422e+00, -1.2869e+00,\n",
      "         -6.9796e-01, -9.3254e-01, -4.2608e+00, -4.0179e+00, -1.5804e-01,\n",
      "         -1.9858e+00, -7.2879e-01, -8.5372e-01, -4.6293e-01, -8.0401e-02,\n",
      "         -8.6451e-01, -4.0814e+00, -3.3949e+00, -4.9114e+00, -1.3168e+00,\n",
      "          9.9906e-01,  9.3606e-01,  1.0040e+00,  9.8233e-01,  7.6752e-01,\n",
      "          1.0119e+00, -4.6653e+00, -4.8688e+00, -5.2329e+00, -5.0654e+00,\n",
      "         -5.2974e+00, -5.4357e+00, -5.3355e+00, -5.3457e+00, -5.3261e+00,\n",
      "         -5.2726e+00, -5.1738e+00, -5.0545e+00, -4.9615e+00, -4.9206e+00,\n",
      "         -4.9169e+00, -4.9409e+00, -4.9777e+00, -5.0219e+00, -5.0560e+00,\n",
      "         -5.0902e+00, -5.1258e+00, -5.1824e+00, -5.2575e+00, -5.3433e+00,\n",
      "         -5.3928e+00, -5.4020e+00, -5.3779e+00, -5.3743e+00, -5.3964e+00,\n",
      "         -5.4341e+00, -5.4507e+00, -5.4243e+00, -5.3816e+00, -5.3240e+00,\n",
      "         -5.2777e+00, -5.2489e+00, -5.2418e+00, -5.2592e+00, -5.2879e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5600,  0.9501], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.3142,  0.9900,  0.9900, -0.5797, -0.6003,  0.9900, -0.6003,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6395e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1530, -0.1545, -0.1561,  ..., -1.2881, -1.3011, -1.3142],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1214, -0.1227, -0.1239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.2566,  0.9656,  0.9656, -0.5654, -0.5855,  0.9656, -0.5855,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5025, -3.8179, -0.7117,  ..., -1.1235, -1.1904, -1.1626],\n",
      "        [-2.7043, -3.8649, -0.7521,  ..., -5.2406, -5.2546, -5.2826],\n",
      "        [-2.7027, -3.8619, -0.7548,  ..., -5.2514, -5.2665, -5.2981],\n",
      "        ...,\n",
      "        [-2.7043, -3.8649, -0.7521,  ..., -5.2406, -5.2546, -5.2826],\n",
      "        [-2.3861, -3.8787, -0.6870,  ..., -4.6226, -4.6130, -4.6163],\n",
      "        [-2.7027, -3.8619, -0.7548,  ..., -5.2514, -5.2665, -5.2981]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1546,  0.9452,  0.9564, -0.5847, -0.5535,  0.9452, -0.5535,  0.9564],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5550, -0.5651,  0.9900,  0.9900,  0.9900, -0.5582, -0.3589],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5196e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1123, -0.1134, -0.1146,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1143, -0.1155, -0.1166,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1129, -0.1141, -0.1152,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0618, -0.0624, -0.0631,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5413, -0.5512,  0.9656,  0.9656,  0.9656, -0.5445, -0.3500],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6997, -3.8592, -0.7526,  ..., -5.2682, -5.2840, -5.3114],\n",
      "        [-2.3937, -3.8764, -0.6931,  ..., -4.6387, -4.6317, -4.6363],\n",
      "        [-2.3921, -3.8821, -0.6898,  ..., -4.5986, -4.5910, -4.6024],\n",
      "        ...,\n",
      "        [-2.6997, -3.8592, -0.7526,  ..., -5.2682, -5.2840, -5.3114],\n",
      "        [-2.3944, -3.8846, -0.6849,  ..., -4.5033, -4.5075, -4.5181],\n",
      "        [-2.4863, -3.8599, -0.6884,  ..., -4.6710, -4.6782, -4.6719]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9103, -0.5044, -0.5146,  0.8871,  0.8267,  0.9103, -0.5120, -0.3148],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5981,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.7395e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1210, -0.1222, -0.1234,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9608, -0.5833,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8717, -3.8363, -0.7812,  ..., -5.5690, -5.5652, -5.5565],\n",
      "        [-2.8717, -3.8363, -0.7812,  ..., -5.5690, -5.5652, -5.5565],\n",
      "        [-2.3913, -3.8818, -0.6849,  ..., -4.5830, -4.5848, -4.5930],\n",
      "        ...,\n",
      "        [-2.7155, -3.8614, -0.7529,  ..., -5.2628, -5.2788, -5.2986],\n",
      "        [-2.6801, -3.8568, -0.7385,  ..., -5.1894, -5.2435, -5.2680],\n",
      "        [-2.7155, -3.8617, -0.7491,  ..., -5.2928, -5.3087, -5.3376]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8926,  0.8926, -0.5532,  0.9974,  0.8676,  0.9393,  0.9659,  0.9764],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6023,  0.9900,  0.9900, -0.5366,  0.9900,  0.9900, -0.5462, -0.5798],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6191e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.7919e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1218, -0.1231, -0.1243,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1105, -0.1116, -0.1127,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1173, -0.1185, -0.1197,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5874,  0.9656,  0.9656, -0.5233,  0.9656,  0.9656, -0.5327, -0.5655],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3793, -3.8779, -0.6752,  ..., -4.5655, -4.5626, -4.5637],\n",
      "        [-2.6687, -3.8513, -0.7092,  ..., -5.4466, -5.4870, -5.5004],\n",
      "        [-2.8940, -3.8398, -0.7759,  ..., -5.6724, -5.6740, -5.6707],\n",
      "        ...,\n",
      "        [-2.6858, -3.8562, -0.7319,  ..., -5.1679, -5.2228, -5.2555],\n",
      "        [-2.3938, -3.8792, -0.6798,  ..., -4.6061, -4.5975, -4.6105],\n",
      "        [-2.3935, -3.8809, -0.6804,  ..., -4.5869, -4.5870, -4.5952]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5361,  0.9740,  0.8361, -0.5011,  0.9866,  0.9687, -0.5043, -0.5475],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5566,  0.9900, -0.5121, -1.3205, -0.5523,  0.9900, -0.5566, -0.5782],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.4560e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1126, -0.1137, -0.1149,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1036, -0.1046, -0.1057,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1126, -0.1137, -0.1149,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1170, -0.1181, -0.1193,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5428,  0.9656, -0.4994, -1.2943, -0.5387,  0.9656, -0.5428, -0.5639],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3888, -3.8803, -0.6829,  ..., -4.5671, -4.5604, -4.5645],\n",
      "        [-2.9039, -3.8354, -0.7745,  ..., -5.6168, -5.6127, -5.6046],\n",
      "        [-2.4007, -3.8816, -0.6744,  ..., -4.5231, -4.5242, -4.5360],\n",
      "        ...,\n",
      "        [-2.7405, -3.8651, -0.7444,  ..., -5.1892, -5.1995, -5.2276],\n",
      "        [-2.3888, -3.8803, -0.6829,  ..., -4.5671, -4.5604, -4.5645],\n",
      "        [-2.3932, -3.8754, -0.6825,  ..., -4.5923, -4.5910, -4.6040]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5052,  0.8639, -0.5038, -1.0160, -0.5732,  0.9660, -0.5052, -0.5239],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5190, -0.5288, -0.2946,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.3363e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1050, -0.1061, -0.1071,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1070, -0.1081, -0.1091,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5062, -0.5158, -0.2874,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6963, -3.8643, -0.7203,  ..., -5.2021, -5.2618, -5.3014],\n",
      "        [-2.4028, -3.8782, -0.6784,  ..., -4.5694, -4.5696, -4.5769],\n",
      "        [-2.3945, -3.8771, -0.6758,  ..., -4.5860, -4.5816, -4.5848],\n",
      "        ...,\n",
      "        [-2.6963, -3.8643, -0.7203,  ..., -5.2021, -5.2618, -5.3014],\n",
      "        [-2.7354, -3.8572, -0.7341,  ..., -5.2572, -5.2638, -5.2875],\n",
      "        [-2.7313, -3.8608, -0.7438,  ..., -5.2334, -5.2400, -5.2631]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9560, -0.5244, -0.5774, -0.2809,  0.8911,  0.9560,  0.9743,  0.9421],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.0736, -0.5465], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.0463e-10, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1482, -0.1497, -0.1513, -0.1528, -0.1543, -0.1559, -0.1575, -0.1591,\n",
      "         -0.1607, -0.1623, -0.1639, -0.1656, -0.1672, -0.1689, -0.1706, -0.1724,\n",
      "         -0.1741, -0.1759, -0.1776, -0.1794, -0.1812, -0.1831, -0.1849, -0.1868,\n",
      "         -0.1887, -0.1906, -0.1925, -0.1945, -0.1964, -0.1984, -0.2004, -0.2024,\n",
      "         -0.2045, -0.2065, -0.2086, -0.2107, -0.2129, -0.2150, -0.2172, -0.2194,\n",
      "         -0.2216, -0.2238, -0.2261, -0.2284, -0.2307, -0.2330, -0.2354, -0.2378,\n",
      "         -0.2402, -0.2426, -0.2450, -0.2475, -0.2500, -0.2525, -0.2551, -0.2577,\n",
      "         -0.2603, -0.2629, -0.2655, -0.2682, -0.2709, -0.2737, -0.2764, -0.2792,\n",
      "         -0.2821, -0.2849, -0.2878, -0.2907, -0.2936, -0.2966, -0.2996, -0.3026,\n",
      "         -0.3057, -0.3088, -0.3119, -0.3150, -0.3182, -0.3214, -0.3247, -0.3279,\n",
      "         -0.3313, -0.3346, -0.3380, -0.3414, -0.3448, -0.3483, -0.3518, -0.3554,\n",
      "         -0.3590, -0.3626, -0.3663, -0.3700, -0.3737, -0.3775, -0.3813, -0.3852,\n",
      "         -0.3890, -0.3930, -0.3969, -0.4010, -0.4050, -0.4091, -0.4132, -0.4174,\n",
      "         -0.4216, -0.4259, -0.4302, -0.4345, -0.4389, -0.4433, -0.4478, -0.4523,\n",
      "         -0.4569, -0.4615, -0.4662, -0.4709, -0.4757, -0.4805, -0.4853, -0.4902,\n",
      "         -0.4952, -0.5002, -0.5052, -0.5103, -0.5155, -0.5207, -0.5260, -0.5313,\n",
      "         -0.5366, -0.5421, -0.5475, -0.5531, -0.5586, -0.5643, -0.5700, -0.5757,\n",
      "         -0.5816, -0.5874, -0.5934, -0.5994, -0.6054, -0.6115, -0.6177, -0.6239,\n",
      "         -0.6302, -0.6366, -0.6430, -0.6495, -0.6561, -0.6627, -0.6694, -0.6762,\n",
      "         -0.6830, -0.6899, -0.6969, -0.7039, -0.7110, -0.7182, -0.7255, -0.7328,\n",
      "         -0.7402, -0.7477, -0.7552, -0.7629, -0.7706, -0.7783, -0.7862, -0.7941,\n",
      "         -0.8022, -0.8103, -0.8185, -0.8267, -0.8351, -0.8435, -0.8520, -0.8606,\n",
      "         -0.8693, -0.8781, -0.8870, -0.8959, -0.9050, -0.9141, -0.9234, -0.9327,\n",
      "         -0.9421, -0.9516, -0.9612, -0.9710, -0.9808, -0.9907, -1.0007, -1.0108,\n",
      "         -1.0210, -1.0313, -1.0417, -1.0522, -1.0629, -1.0736,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1106, -0.1117, -0.1128, -0.1139, -0.1151, -0.1163, -0.1174, -0.1186,\n",
      "         -0.1198, -0.1210, -0.1223, -0.1235, -0.1247, -0.1260, -0.1273, -0.1286,\n",
      "         -0.1298, -0.1312, -0.1325, -0.1338, -0.1352, -0.1365, -0.1379, -0.1393,\n",
      "         -0.1407, -0.1421, -0.1436, -0.1450, -0.1465, -0.1480, -0.1495, -0.1510,\n",
      "         -0.1525, -0.1540, -0.1556, -0.1572, -0.1588, -0.1604, -0.1620, -0.1636,\n",
      "         -0.1653, -0.1669, -0.1686, -0.1703, -0.1721, -0.1738, -0.1755, -0.1773,\n",
      "         -0.1791, -0.1809, -0.1827, -0.1846, -0.1865, -0.1883, -0.1902, -0.1922,\n",
      "         -0.1941, -0.1961, -0.1980, -0.2000, -0.2021, -0.2041, -0.2062, -0.2083,\n",
      "         -0.2104, -0.2125, -0.2146, -0.2168, -0.2190, -0.2212, -0.2234, -0.2257,\n",
      "         -0.2280, -0.2303, -0.2326, -0.2349, -0.2373, -0.2397, -0.2421, -0.2446,\n",
      "         -0.2471, -0.2495, -0.2521, -0.2546, -0.2572, -0.2598, -0.2624, -0.2651,\n",
      "         -0.2677, -0.2704, -0.2732, -0.2759, -0.2787, -0.2815, -0.2844, -0.2873,\n",
      "         -0.2902, -0.2931, -0.2960, -0.2990, -0.3021, -0.3051, -0.3082, -0.3113,\n",
      "         -0.3144, -0.3176, -0.3208, -0.3241, -0.3273, -0.3306, -0.3340, -0.3374,\n",
      "         -0.3408, -0.3442, -0.3477, -0.3512, -0.3547, -0.3583, -0.3620, -0.3656,\n",
      "         -0.3693, -0.3730, -0.3768, -0.3806, -0.3845, -0.3883, -0.3923, -0.3962,\n",
      "         -0.4002, -0.4043, -0.4083, -0.4125, -0.4166, -0.4208, -0.4251, -0.4294,\n",
      "         -0.4337, -0.4381, -0.4425, -0.4470, -0.4515, -0.4561, -0.4607, -0.4653,\n",
      "         -0.4700, -0.4748, -0.4796, -0.4844, -0.4893, -0.4943, -0.4993, -0.5043,\n",
      "         -0.5094, -0.5145, -0.5197, -0.5250, -0.5303, -0.5356, -0.5411, -0.5465,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0266, -0.5330], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5042e+00, -3.8273e+00, -6.8809e-01, -2.2884e+00,  2.6298e-01,\n",
      "         -2.3468e+00, -4.5679e+00, -1.6133e-01, -3.8524e+00, -2.4429e+00,\n",
      "         -3.9636e+00,  4.8651e-01, -2.0321e-01, -3.5004e+00, -3.1102e+00,\n",
      "         -2.2599e+00, -3.2055e+00, -1.7231e+00, -1.9439e+00, -9.3765e-01,\n",
      "         -1.2765e+00, -1.3517e+00, -7.9396e-01,  3.7717e-01, -1.8860e+00,\n",
      "         -3.3786e+00, -3.0567e+00, -2.4689e+00, -3.7762e+00,  9.7640e-01,\n",
      "         -1.7015e+00, -1.4298e-01, -9.1476e-01, -3.5318e+00, -1.8468e+00,\n",
      "         -1.8610e+00, -9.6591e-01, -3.1276e+00, -1.5134e+00,  1.0647e-01,\n",
      "         -6.0945e-01, -5.5187e-01, -2.5398e-01, -4.2860e+00, -1.5693e+00,\n",
      "         -3.2324e+00, -3.4533e+00, -3.2593e+00,  4.9800e-02,  3.8198e-01,\n",
      "         -3.8012e+00, -1.4435e+00, -7.1042e+00, -2.8468e+00, -2.9843e+00,\n",
      "         -8.4245e-01, -2.5043e+00, -2.9297e+00, -2.5475e+00, -1.5660e+00,\n",
      "         -3.5898e+00, -1.1166e+00, -5.7264e+00, -2.3528e+00, -3.5087e+00,\n",
      "          1.1157e+00, -1.2930e+00, -3.3993e-03, -3.6274e-01, -3.1415e+00,\n",
      "         -4.0144e+00, -5.5495e+00, -2.7524e+00, -1.4589e-01, -3.7482e+00,\n",
      "         -4.2549e+00, -2.3984e+00, -3.8292e+00, -2.4473e+00, -3.4869e+00,\n",
      "         -4.8368e+00, -2.4263e+00, -2.2464e-01, -1.2421e+00, -1.7371e+00,\n",
      "         -2.9399e+00, -1.7811e+00, -6.1382e-01, -3.5209e+00, -7.6880e-01,\n",
      "         -3.0735e+00, -4.1074e+00, -2.3758e+00, -5.3493e-01, -8.3721e-01,\n",
      "         -2.9930e+00, -2.7101e+00, -3.6036e+00, -3.2311e+00, -3.2405e+00,\n",
      "          9.9289e-03, -9.5356e-01, -3.7448e+00, -2.0152e+00, -4.9292e-01,\n",
      "         -1.8036e+00, -3.4441e+00, -3.7670e+00,  5.9566e-01, -5.1910e+00,\n",
      "         -1.5697e+00, -2.6614e+00, -4.2317e+00, -4.5011e+00,  6.1739e-01,\n",
      "         -8.5599e-01, -2.0732e+00, -1.2517e+00, -5.1362e-02, -8.2664e-01,\n",
      "         -2.6176e+00, -4.6635e+00, -2.6058e+00, -8.0750e-01, -1.0393e+00,\n",
      "         -2.7572e+00, -2.1553e+00, -7.2714e-01, -6.5494e-01, -4.2547e+00,\n",
      "         -2.7982e-01,  4.5509e-01, -3.2625e+00, -4.1392e+00, -1.1698e+00,\n",
      "         -1.9739e+00, -3.1087e+00, -2.3578e+00, -1.6097e+00, -2.9076e+00,\n",
      "         -2.8431e+00, -5.8287e+00, -1.4869e+00, -6.1456e+00, -1.4461e+00,\n",
      "         -1.3367e+00, -1.2946e+00, -1.0659e+00, -1.2735e+00, -1.0574e+00,\n",
      "         -1.1950e+00, -4.2177e+00, -4.8409e+00, -2.3983e-01, -1.8611e+00,\n",
      "         -4.5439e+00, -5.7123e+00, -4.7344e+00, -5.3203e+00, -1.8356e+00,\n",
      "         -5.7020e+00, -6.6546e-01, -9.2898e-01, -1.0326e+00, -9.0136e-01,\n",
      "         -9.8999e-01, -8.1078e-01, -1.2044e+00, -4.0877e+00, -3.8597e+00,\n",
      "          2.3093e-01, -1.5569e+00, -3.5690e+00, -4.5770e+00, -3.6602e+00,\n",
      "         -3.4088e+00, -4.8859e+00, -1.6105e+00, -1.1627e+00, -1.1065e+00,\n",
      "         -1.0712e+00, -1.1089e+00, -1.1216e+00, -1.1517e+00, -1.1377e+00,\n",
      "         -1.0583e+00, -1.1351e+00, -1.1671e+00, -4.6437e+00, -4.7107e+00,\n",
      "         -4.9998e+00, -4.9289e+00, -5.1387e+00, -5.0481e+00, -4.9628e+00,\n",
      "         -4.6571e+00, -4.3246e+00, -4.2373e+00, -4.3212e+00, -4.2847e+00,\n",
      "         -4.1868e+00, -4.1137e+00, -4.0856e+00, -4.0825e+00, -4.0864e+00],\n",
      "        [-2.3863e+00, -3.8769e+00, -6.7617e-01, -2.2489e+00,  2.7173e-01,\n",
      "         -2.3343e+00, -4.4996e+00, -1.2126e-01, -3.7292e+00, -2.4411e+00,\n",
      "         -3.9231e+00,  5.2526e-01, -1.7281e-01, -3.4862e+00, -3.1163e+00,\n",
      "         -2.1830e+00, -3.1801e+00, -1.7235e+00, -1.9471e+00, -9.1683e-01,\n",
      "         -1.3089e+00, -1.3414e+00, -8.0081e-01,  3.7474e-01, -1.8254e+00,\n",
      "         -3.3771e+00, -3.0753e+00, -2.4034e+00, -3.7651e+00,  9.9631e-01,\n",
      "         -1.6780e+00, -1.2137e-01, -9.4201e-01, -3.4872e+00, -1.9010e+00,\n",
      "         -1.8792e+00, -9.6984e-01, -3.1112e+00, -1.5054e+00,  7.2402e-02,\n",
      "         -6.2906e-01, -5.5183e-01, -2.3815e-01, -4.3099e+00, -1.5278e+00,\n",
      "         -3.1505e+00, -3.4339e+00, -3.2203e+00, -2.7434e-02,  3.1160e-01,\n",
      "         -3.8127e+00, -1.4145e+00, -7.0930e+00, -2.8020e+00, -2.9719e+00,\n",
      "         -8.3391e-01, -2.5312e+00, -2.9231e+00, -2.5316e+00, -1.5339e+00,\n",
      "         -3.5952e+00, -1.0987e+00, -5.7242e+00, -2.3175e+00, -3.5025e+00,\n",
      "          1.1304e+00, -1.2754e+00,  1.5530e-02, -3.8983e-01, -3.1002e+00,\n",
      "         -3.9994e+00, -5.5986e+00, -2.7027e+00, -1.7416e-01, -3.7559e+00,\n",
      "         -4.2544e+00, -2.3600e+00, -3.8208e+00, -2.4463e+00, -3.5011e+00,\n",
      "         -4.8888e+00, -2.3919e+00, -2.3760e-01, -1.2365e+00, -1.7864e+00,\n",
      "         -2.9064e+00, -1.7637e+00, -6.0275e-01, -3.5469e+00, -7.8792e-01,\n",
      "         -3.0843e+00, -4.1044e+00, -2.3436e+00, -5.4718e-01, -8.3557e-01,\n",
      "         -3.0214e+00, -2.6801e+00, -3.5385e+00, -3.2560e+00, -3.2329e+00,\n",
      "          2.4658e-03, -9.5697e-01, -3.7305e+00, -2.0282e+00, -4.9949e-01,\n",
      "         -1.7984e+00, -3.3895e+00, -3.7397e+00,  5.5976e-01, -1.2646e+00,\n",
      "         -1.2366e+00, -1.0504e+00, -4.2732e+00, -4.5729e+00,  5.1971e-01,\n",
      "         -9.5001e-01, -2.1099e+00, -1.2974e+00, -9.3136e-02, -8.5360e-01,\n",
      "         -2.6815e+00, -4.5751e+00, -2.6268e+00, -8.0061e-01, -1.0846e+00,\n",
      "         -2.7270e+00, -2.1636e+00, -7.0686e-01, -6.0626e-01, -4.1900e+00,\n",
      "         -4.5833e-01,  3.6312e-01, -3.2976e+00, -4.0765e+00, -1.1615e+00,\n",
      "         -1.9905e+00, -3.0740e+00, -2.3326e+00, -1.6076e+00, -2.8432e+00,\n",
      "         -2.5716e+00, -3.3039e+00, -5.6929e+00, -1.2728e+00, -5.2110e-01,\n",
      "         -5.5010e-01, -5.5807e-01, -5.1098e-01, -5.2343e-01, -5.9338e-01,\n",
      "         -4.8242e+00, -4.9113e+00, -5.2857e+00, -4.9105e+00, -4.8418e+00,\n",
      "         -4.6115e+00, -4.5472e+00, -4.6534e+00, -4.6082e+00, -4.5806e+00,\n",
      "         -4.6287e+00, -4.6471e+00, -4.6634e+00, -4.6342e+00, -4.5340e+00,\n",
      "         -4.4234e+00, -4.3577e+00, -4.3274e+00, -4.3274e+00, -4.3321e+00,\n",
      "         -4.3424e+00, -4.3671e+00, -4.4003e+00, -4.4233e+00, -4.4489e+00,\n",
      "         -4.4544e+00, -4.4274e+00, -4.4109e+00, -4.4284e+00, -4.4423e+00,\n",
      "         -4.4504e+00, -4.4600e+00, -4.4448e+00, -4.4273e+00, -4.4244e+00,\n",
      "         -4.4287e+00, -4.4502e+00, -4.4711e+00, -4.4848e+00, -4.4888e+00,\n",
      "         -4.4887e+00, -4.4985e+00, -4.5063e+00, -4.5145e+00, -4.5137e+00,\n",
      "         -4.5178e+00, -4.5256e+00, -4.5294e+00, -4.5277e+00, -4.5304e+00,\n",
      "         -4.5366e+00, -4.5587e+00, -4.5682e+00, -4.5643e+00, -4.5673e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1221, -0.5428], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5202, -0.5234, -1.2883,  0.9900, -1.0778,  0.9900, -0.5387],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.1678e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1052, -0.1063, -0.1074,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1059, -0.1070, -0.1080,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1488, -0.1503, -0.1518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1090, -0.1101, -0.1112,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5074, -0.5105, -1.2628,  0.9656, -1.0306,  0.9656, -0.5254],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6810, -3.8528, -0.7268,  ..., -5.1220, -5.1782, -5.2117],\n",
      "        [-2.3902, -3.8738, -0.6730,  ..., -4.5895, -4.5851, -4.5871],\n",
      "        [-2.3912, -3.8766, -0.6784,  ..., -4.5896, -4.5914, -4.5974],\n",
      "        ...,\n",
      "        [-2.4958, -3.8247, -0.6840,  ..., -4.0772, -4.0767, -4.0782],\n",
      "        [-2.7244, -3.8605, -0.7325,  ..., -5.2576, -5.2636, -5.2842],\n",
      "        [-2.3866, -3.8709, -0.6721,  ..., -4.6190, -4.6167, -4.6180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9501, -0.5755, -0.5508, -1.0340,  0.9501, -1.1017,  0.9549, -0.5480],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.2917,  0.9900, -0.4895,  0.9900, -0.5604, -0.2894, -0.5368],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0611e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1504, -0.1519, -0.1534,  ..., -1.2660, -1.2788, -1.2917],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1134, -0.1145, -0.1157,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0498, -0.0503, -0.0509,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1086, -0.1097, -0.1108,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.2351,  0.9656, -0.4774,  0.9608, -0.5466, -0.2822, -0.5236],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6756, -3.8571, -0.7123,  ..., -5.1878, -5.2504, -5.2888],\n",
      "        [-2.4964, -3.8091, -0.6938,  ..., -1.0943, -1.1656, -1.1945],\n",
      "        [-2.7051, -3.8502, -0.7328,  ..., -5.2535, -5.2616, -5.2829],\n",
      "        ...,\n",
      "        [-2.3666, -3.8705, -0.6680,  ..., -4.5625, -4.5600, -4.5644],\n",
      "        [-2.4778, -3.8511, -0.6727,  ..., -4.5928, -4.6009, -4.5985],\n",
      "        [-2.3737, -3.8734, -0.6763,  ..., -4.5648, -4.5593, -4.5664]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9450, -1.1321,  0.9216, -0.5010,  0.8784, -0.5375, -0.3080, -0.5081],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5489,  0.9900,  0.9900,  0.9900, -0.5568,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.7172e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1110, -0.1122, -0.1133,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5353,  0.9656,  0.9656,  0.9656, -0.5431,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3723, -3.8666, -0.6686,  ..., -4.5714, -4.5705, -4.5828],\n",
      "        [-2.6940, -3.8472, -0.7304,  ..., -5.2622, -5.2725, -5.2948],\n",
      "        [-2.8785, -3.8237, -0.7635,  ..., -5.5414, -5.5348, -5.5276],\n",
      "        ...,\n",
      "        [-2.7079, -3.8478, -0.7243,  ..., -5.2685, -5.2811, -5.3043],\n",
      "        [-2.8730, -3.8316, -0.7658,  ..., -5.6587, -5.6545, -5.6471],\n",
      "        [-2.8760, -3.8254, -0.7635,  ..., -5.6130, -5.6077, -5.5985]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5157,  0.9226,  0.8805,  0.9521, -0.5678,  0.9661,  0.8107,  0.8236],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.5588,  0.9900,  0.9900,  0.9900, -0.6229, -0.3744,  0.9900, -0.5673],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6568e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1130, -0.1142, -0.1153,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0645, -0.0651, -0.0658,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1148, -0.1159, -0.1171,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5450,  0.9656,  0.9656,  0.9656, -0.6075, -0.3651,  0.9656, -0.5533],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3692, -3.8661, -0.6617,  ..., -4.5230, -4.5253, -4.5410],\n",
      "        [-2.8745, -3.8208, -0.7600,  ..., -5.6232, -5.6218, -5.6143],\n",
      "        [-2.6383, -3.8370, -0.6931,  ..., -5.3854, -5.4267, -5.4359],\n",
      "        ...,\n",
      "        [-2.4611, -3.8401, -0.6728,  ..., -4.6862, -4.6927, -4.6888],\n",
      "        [-2.7065, -3.8476, -0.7298,  ..., -5.2877, -5.2989, -5.3262],\n",
      "        [-2.3662, -3.8648, -0.6677,  ..., -4.6094, -4.6027, -4.6159]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.4845,  0.8299,  0.9624,  0.9548, -0.5139, -0.2738,  0.9943, -0.4935],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6504, -0.6392,  0.9900, -0.5959,  0.9900, -0.6035, -0.6612,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9127e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1316, -0.1329, -0.1342,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1293, -0.1306, -0.1319,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1221, -0.1233, -0.1246,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1338, -0.1351, -0.1365,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6343, -0.6234,  0.9656, -0.5812,  0.9656, -0.5886, -0.6449,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3521, -3.8574, -0.6597,  ..., -4.6282, -4.6237, -4.6289],\n",
      "        [-2.3515, -3.8624, -0.6686,  ..., -4.5663, -4.5616, -4.5694],\n",
      "        [-2.7011, -3.8453, -0.7318,  ..., -5.3181, -5.3321, -5.3629],\n",
      "        ...,\n",
      "        [-2.3598, -3.8620, -0.6656,  ..., -4.6088, -4.6026, -4.6138],\n",
      "        [-2.3423, -3.8583, -0.6591,  ..., -4.5680, -4.5652, -4.5696],\n",
      "        [-2.8682, -3.8234, -0.7597,  ..., -5.6801, -5.6831, -5.6799]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5334, -0.4969,  0.9749, -0.4828,  0.9559, -0.4924, -0.5248,  0.8252],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.6226,  0.9900, -0.6639],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.0861e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.8147e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1259, -0.1272, -0.1285,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1343, -0.1357, -0.1370,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656,  0.9656,  0.9656,  0.9656, -0.6072,  0.9656, -0.6475],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8556, -3.8167, -0.7582,  ..., -5.5760, -5.5723, -5.5611],\n",
      "        [-2.7028, -3.8464, -0.7346,  ..., -5.3033, -5.3169, -5.3431],\n",
      "        [-2.6967, -3.8435, -0.7301,  ..., -5.2972, -5.3127, -5.3455],\n",
      "        ...,\n",
      "        [-2.3599, -3.8586, -0.6668,  ..., -4.6283, -4.6181, -4.6165],\n",
      "        [-2.7028, -3.8464, -0.7346,  ..., -5.3033, -5.3169, -5.3431],\n",
      "        [-2.3591, -3.8627, -0.6648,  ..., -4.5922, -4.5948, -4.6014]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8907,  0.9513,  0.9826,  0.9805,  0.9826, -0.5048,  0.9513, -0.5404],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6695], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.4669e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1354, -0.1368, -0.1382, -0.1396, -0.1410, -0.1424, -0.1439, -0.1453,\n",
      "         -0.1468, -0.1483, -0.1498, -0.1513, -0.1528, -0.1543, -0.1559, -0.1575,\n",
      "         -0.1591, -0.1607, -0.1623, -0.1639, -0.1656, -0.1673, -0.1690, -0.1707,\n",
      "         -0.1724, -0.1741, -0.1759, -0.1777, -0.1795, -0.1813, -0.1831, -0.1850,\n",
      "         -0.1868, -0.1887, -0.1906, -0.1925, -0.1945, -0.1964, -0.1984, -0.2004,\n",
      "         -0.2025, -0.2045, -0.2066, -0.2087, -0.2108, -0.2129, -0.2150, -0.2172,\n",
      "         -0.2194, -0.2216, -0.2239, -0.2261, -0.2284, -0.2307, -0.2330, -0.2354,\n",
      "         -0.2378, -0.2402, -0.2426, -0.2451, -0.2475, -0.2500, -0.2526, -0.2551,\n",
      "         -0.2577, -0.2603, -0.2629, -0.2656, -0.2683, -0.2710, -0.2737, -0.2765,\n",
      "         -0.2793, -0.2821, -0.2849, -0.2878, -0.2907, -0.2937, -0.2966, -0.2996,\n",
      "         -0.3026, -0.3057, -0.3088, -0.3119, -0.3151, -0.3182, -0.3215, -0.3247,\n",
      "         -0.3280, -0.3313, -0.3346, -0.3380, -0.3414, -0.3449, -0.3484, -0.3519,\n",
      "         -0.3554, -0.3590, -0.3627, -0.3663, -0.3700, -0.3738, -0.3775, -0.3813,\n",
      "         -0.3852, -0.3891, -0.3930, -0.3970, -0.4010, -0.4051, -0.4091, -0.4133,\n",
      "         -0.4174, -0.4217, -0.4259, -0.4302, -0.4346, -0.4390, -0.4434, -0.4479,\n",
      "         -0.4524, -0.4570, -0.4616, -0.4662, -0.4710, -0.4757, -0.4805, -0.4854,\n",
      "         -0.4903, -0.4952, -0.5002, -0.5053, -0.5104, -0.5155, -0.5207, -0.5260,\n",
      "         -0.5313, -0.5367, -0.5421, -0.5476, -0.5531, -0.5587, -0.5643, -0.5700,\n",
      "         -0.5758, -0.5816, -0.5875, -0.5934, -0.5994, -0.6055, -0.6116, -0.6178,\n",
      "         -0.6240, -0.6303, -0.6367, -0.6431, -0.6496, -0.6562, -0.6628, -0.6695,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6530], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8646, -3.8224, -0.7607, -2.5330,  0.1770, -2.4064, -4.7195, -0.5354,\n",
      "         -4.2739, -2.6721, -4.0260,  0.5316, -0.2679, -3.6942, -2.9807, -2.4974,\n",
      "         -3.2809, -1.8433, -1.8666, -1.1338, -1.3985, -1.5817, -0.9744,  0.4923,\n",
      "         -2.0856, -3.5959, -2.9822, -2.6570, -3.7418,  1.0462, -1.8333, -0.3826,\n",
      "         -0.9289, -3.5737, -1.9035, -1.7424, -0.8520, -3.1487, -1.7341, -0.1514,\n",
      "         -0.6916, -0.4721, -0.2864, -4.2077, -1.7453, -3.6022, -3.6916, -3.3051,\n",
      "          0.6884,  0.9046, -3.9497, -1.4934, -7.0840, -2.9881, -2.9863, -0.9877,\n",
      "         -2.5326, -2.9363, -2.6656, -1.7304, -3.7794, -1.1758, -5.7356, -2.4548,\n",
      "         -3.4781,  1.2023, -1.4203, -0.2587, -0.3313, -3.1974, -4.2108, -5.5855,\n",
      "         -2.8907, -0.0919, -3.9284, -4.1669, -2.5418, -3.8172, -2.6214, -3.6628,\n",
      "         -4.8750, -2.5646, -0.3077, -1.4783, -1.8719, -3.1331, -1.9339, -0.8475,\n",
      "         -3.6908, -0.8786, -3.2810, -4.0361, -2.5076, -0.5924, -1.0920, -2.9403,\n",
      "         -2.8720, -3.9148, -3.3227, -3.3696, -0.0870, -1.1970, -3.6668, -2.0253,\n",
      "         -0.4099, -2.0330, -3.7482, -3.9321,  0.6523, -1.5021, -1.5143, -1.4734,\n",
      "         -4.8200, -4.6246,  0.4979, -1.1413, -2.1854, -1.4213, -0.1901, -0.9520,\n",
      "         -2.9142, -4.7461, -2.6071, -0.7367, -1.1256, -2.8551, -2.2860, -0.8307,\n",
      "         -0.7559, -4.2294,  0.3587,  0.9828, -3.2906, -4.2265, -1.2468, -2.0072,\n",
      "         -3.0622, -2.4609, -1.7377, -2.7801, -2.5180, -3.6135, -5.7267, -1.4754,\n",
      "          0.7883,  0.8416,  0.9209,  0.9765,  0.8007,  1.0039, -4.6482, -4.8441,\n",
      "         -5.1774, -4.9661, -5.1136, -5.0544, -5.1313, -5.1968, -5.2382, -5.2293,\n",
      "         -5.1926, -5.1630, -5.1546, -5.1548, -5.1794, -5.2332, -5.2964, -5.3375,\n",
      "         -5.3748, -5.4043, -5.4379, -5.4643, -5.5005, -5.5264, -5.5522, -5.5615,\n",
      "         -5.5863, -5.6190, -5.6678, -5.6664, -5.6385, -5.6062, -5.5866, -5.5718,\n",
      "         -5.5795, -5.5878, -5.5925, -5.5936, -5.5881, -5.5733, -5.5619, -5.5571,\n",
      "         -5.5633, -5.5637, -5.5599, -5.5697, -5.5865, -5.5941, -5.5963, -5.5923,\n",
      "         -5.5790, -5.5671, -5.5569, -5.5585, -5.5501],\n",
      "        [-2.3424, -3.8662, -0.6623, -2.2420,  0.2929, -2.3254, -4.4846, -0.0926,\n",
      "         -3.6892, -2.3885, -3.8981,  0.5395, -0.1669, -3.4871, -3.1208, -2.1741,\n",
      "         -3.1559, -1.7030, -1.9527, -0.8898, -1.2944, -1.3248, -0.7861,  0.3659,\n",
      "         -1.7914, -3.3727, -3.0807, -2.3918, -3.7585,  1.0074, -1.6596, -0.0985,\n",
      "         -0.9528, -3.4688, -1.8886, -1.9007, -0.9732, -3.0890, -1.5029,  0.0766,\n",
      "         -0.6143, -0.5560, -0.2402, -4.3196, -1.4981, -3.1061, -3.3841, -3.2076,\n",
      "         -0.0331,  0.2904, -3.8030, -1.4002, -7.0730, -2.7963, -2.9529, -0.8210,\n",
      "         -2.5391, -2.9087, -2.5302, -1.5006, -3.5919, -1.0837, -5.7064, -2.3088,\n",
      "         -3.5003,  1.1361, -1.2639,  0.0324, -0.4008, -3.0882, -3.9836, -5.5962,\n",
      "         -2.7032, -0.1886, -3.7615, -4.2686, -2.3628, -3.7951, -2.4392, -3.5020,\n",
      "         -4.8892, -2.3963, -0.2323, -1.2200, -1.7958, -2.9059, -1.7472, -0.5846,\n",
      "         -3.5557, -0.7768, -3.0877, -4.1224, -2.3461, -0.5393, -0.8219, -3.0239,\n",
      "         -2.6503, -3.4944, -3.2573, -3.2338,  0.0127, -0.9420, -3.7446, -2.0269,\n",
      "         -0.5090, -1.7811, -3.3490, -3.7401,  0.5415, -1.2371, -1.5654, -0.7315,\n",
      "         -4.3778, -4.5477,  0.4758, -0.9113, -2.1159, -1.2759, -0.0655, -0.9193,\n",
      "         -2.5715, -4.6451, -2.5895, -0.7907, -1.0766, -2.7850, -2.1475, -0.7143,\n",
      "         -0.5004, -4.2061, -0.5360,  0.3185, -3.2826, -4.0910, -1.1320, -2.0003,\n",
      "         -3.0728, -2.3507, -1.5914, -2.7965, -2.5284, -3.2245, -5.5959, -1.1833,\n",
      "         -0.6863, -0.5439, -0.5360, -0.6509, -0.5312, -0.5631, -4.8377, -4.9016,\n",
      "         -5.3087, -4.9010, -4.7936, -4.5334, -4.4059, -4.4849, -4.4373, -4.4252,\n",
      "         -4.5446, -4.6293, -4.6943, -4.7169, -4.6720, -4.5904, -4.5349, -4.5138,\n",
      "         -4.5163, -4.5115, -4.5045, -4.5209, -4.5512, -4.5795, -4.5963, -4.5978,\n",
      "         -4.5648, -4.5325, -4.5401, -4.5480, -4.5625, -4.5835, -4.5676, -4.5494,\n",
      "         -4.5458, -4.5432, -4.5634, -4.5857, -4.5971, -4.6001, -4.5988, -4.6014,\n",
      "         -4.6068, -4.6099, -4.6028, -4.6040, -4.6050, -4.5938, -4.5853, -4.5850,\n",
      "         -4.5820, -4.5925, -4.5923, -4.5869, -4.5887]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8886, -0.5852], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6295, -0.6752,  0.9900, -0.6295,  0.9900,  0.9900,  0.9900, -0.6641],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5676e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1273, -0.1286, -0.1299,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1366, -0.1380, -0.1394,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1344, -0.1357, -0.1371,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6139, -0.6586,  0.9656, -0.6139,  0.9656,  0.9656,  0.9656, -0.6478],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3423, -3.8767, -0.6713,  ..., -4.6053, -4.5978, -4.6103],\n",
      "        [-2.3352, -3.8727, -0.6654,  ..., -4.6235, -4.6206, -4.6231],\n",
      "        [-2.6883, -3.8645, -0.7357,  ..., -5.2702, -5.2803, -5.3079],\n",
      "        ...,\n",
      "        [-2.6843, -3.8616, -0.7388,  ..., -5.2624, -5.2765, -5.3077],\n",
      "        [-2.8623, -3.8344, -0.7663,  ..., -5.6304, -5.6314, -5.6229],\n",
      "        [-2.3348, -3.8760, -0.6738,  ..., -4.5656, -4.5592, -4.5694]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5366, -0.5760,  0.9464, -0.5366,  0.9631,  0.9630,  0.8272, -0.5420],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6018,  0.9900, -0.6019, -1.1841,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.0919e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1218, -0.1230, -0.1242,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5870,  0.9656, -0.5870, -1.1322,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8560, -3.8498, -0.7733,  ..., -5.6720, -5.6741, -5.6689],\n",
      "        [-2.3422, -3.8859, -0.6694,  ..., -4.5282, -4.5352, -4.5474],\n",
      "        [-2.6389, -3.8625, -0.7277,  ..., -5.1469, -5.2082, -5.2425],\n",
      "        ...,\n",
      "        [-2.8583, -3.8435, -0.7698,  ..., -5.6282, -5.6268, -5.6186],\n",
      "        [-2.6800, -3.8690, -0.7409,  ..., -5.2451, -5.2535, -5.2824],\n",
      "        [-2.6480, -3.8698, -0.7182,  ..., -5.2144, -5.2804, -5.3242]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8147, -0.5476,  0.9347, -0.5652, -1.0163,  0.8230,  0.9551,  0.9278],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6209,  0.9900, -0.6478,  0.9900,  0.9900, -0.6370,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.2250e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1256, -0.1269, -0.1282,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1310, -0.1324, -0.1337,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1289, -0.1302, -0.1315,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6056,  0.9656, -0.6318,  0.9656,  0.9656, -0.6213,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3382, -3.8886, -0.6719,  ..., -4.5886, -4.5850, -4.5875],\n",
      "        [-2.6842, -3.8796, -0.7395,  ..., -5.2384, -5.2448, -5.2668],\n",
      "        [-2.3280, -3.8882, -0.6713,  ..., -4.5686, -4.5676, -4.5678],\n",
      "        ...,\n",
      "        [-2.3345, -3.8872, -0.6708,  ..., -4.6189, -4.6153, -4.6223],\n",
      "        [-2.6377, -3.8688, -0.7302,  ..., -5.1299, -5.1907, -5.2287],\n",
      "        [-2.6236, -3.8662, -0.7075,  ..., -5.3713, -5.4154, -5.4311]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6486,  0.9341, -0.6081,  0.8860,  0.9519, -0.6140,  0.9320,  0.9428],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.3750, -0.6298,  0.9900,  0.9900, -0.5927,  0.9900, -0.5611, -0.6049],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9518e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.0646, -0.0652, -0.0659,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.1287, -0.1300,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1135, -0.1147, -0.1158,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1224, -0.1236, -0.1249,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.3658, -0.6143,  0.9656,  0.9656, -0.5781,  0.9656, -0.5473, -0.5900],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4447, -3.8706, -0.6902,  ..., -4.6026, -4.6106, -4.6074],\n",
      "        [-2.3253, -3.8932, -0.6731,  ..., -4.5703, -4.5653, -4.5676],\n",
      "        [-2.6864, -3.8809, -0.7442,  ..., -5.2340, -5.2441, -5.2668],\n",
      "        ...,\n",
      "        [-2.6441, -3.8813, -0.7226,  ..., -5.1800, -5.2489, -5.2907],\n",
      "        [-2.3404, -3.8893, -0.6833,  ..., -4.6457, -4.6391, -4.6438],\n",
      "        [-2.3400, -3.8960, -0.6788,  ..., -4.5974, -4.5999, -4.6051]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.3438, -0.6224,  0.9598,  0.8400, -0.6026,  0.9227, -0.5877, -0.6289],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5921,  0.9900, -0.5921, -0.5519,  0.9900, -0.3486],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6439e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1198, -0.1210, -0.1222,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1116, -0.1128, -0.1139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0600, -0.0607, -0.0613,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5775,  0.9656, -0.5775, -0.5383,  0.9608, -0.3400],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8466, -3.8677, -0.7816,  ..., -5.6700, -5.6686, -5.6621],\n",
      "        [-2.6873, -3.8870, -0.7484,  ..., -5.1506, -5.1593, -5.1842],\n",
      "        [-2.3330, -3.8963, -0.6739,  ..., -4.5832, -4.5786, -4.5832],\n",
      "        ...,\n",
      "        [-2.3404, -3.9015, -0.6733,  ..., -4.5432, -4.5447, -4.5564],\n",
      "        [-2.8301, -3.8637, -0.7814,  ..., -5.5488, -5.5438, -5.5280],\n",
      "        [-2.4423, -3.8770, -0.6830,  ..., -4.5362, -4.5413, -4.5403]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8282,  0.9379, -0.6749,  0.9045, -0.6749, -0.5891,  0.8951, -0.3854],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6005,  0.9900, -0.5975,  0.9900, -1.3606,  0.9900, -0.6217, -1.3476],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.1195e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1215, -0.1227, -0.1239,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1209, -0.1221, -0.1233,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1258, -0.1270, -0.1283,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1569, -0.1584, -0.1600,  ..., -1.3208, -1.3341, -1.3476]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5857,  0.9656, -0.5828,  0.9608, -1.3336,  0.9656, -0.6064, -1.2886],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3283, -3.9012, -0.6812,  ..., -4.5783, -4.5716, -4.5776],\n",
      "        [-2.6721, -3.8858, -0.7484,  ..., -5.2068, -5.2157, -5.2337],\n",
      "        [-2.3350, -3.9015, -0.6793,  ..., -4.6038, -4.6058, -4.6121],\n",
      "        ...,\n",
      "        [-2.6621, -3.8805, -0.7429,  ..., -5.2130, -5.2196, -5.2371],\n",
      "        [-2.3319, -3.8974, -0.6798,  ..., -4.6003, -4.5998, -4.6097],\n",
      "        [-2.4575, -3.8372, -0.7033,  ..., -1.0312, -1.1224, -1.2173]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6029,  0.9088, -0.6380,  0.9072, -1.0338,  0.8315, -0.6224, -1.0848],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(5.1824e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170,\n",
      "         0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376,\n",
      "         0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601,\n",
      "         0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847,\n",
      "         0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117,\n",
      "         0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412,\n",
      "         0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735,\n",
      "         0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088,\n",
      "         0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475,\n",
      "         0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899,\n",
      "         0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363,\n",
      "         0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870,\n",
      "         0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426,\n",
      "         0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034,\n",
      "         0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700,\n",
      "         0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429,\n",
      "         0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227,\n",
      "         0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6774, -3.8925, -0.7504, -2.5311,  0.1972, -2.4065, -4.6486, -0.3969,\n",
      "         -4.0531, -2.5779, -3.9945,  0.4109, -0.3690, -3.5777, -3.0733, -2.3819,\n",
      "         -3.2648, -1.7804, -1.9657, -1.0437, -1.3539, -1.5216, -0.9153,  0.4317,\n",
      "         -2.0074, -3.4802, -3.0564, -2.5631, -3.7754,  1.0306, -1.7818, -0.2740,\n",
      "         -0.9190, -3.5512, -1.9211, -1.7931, -0.9089, -3.1576, -1.6523, -0.0824,\n",
      "         -0.6229, -0.5245, -0.2767, -4.2906, -1.6683, -3.4156, -3.5753, -3.2820,\n",
      "          0.4639,  0.7210, -3.8594, -1.5216, -7.1455, -2.9159, -2.9935, -0.9180,\n",
      "         -2.5472, -2.9790, -2.6408, -1.6744, -3.6764, -1.1955, -5.7983, -2.4045,\n",
      "         -3.5168,  1.1770, -1.3714, -0.1446, -0.3385, -3.1730, -4.1198, -5.5737,\n",
      "         -2.8179, -0.1047, -3.8286, -4.2603, -2.4726, -3.8424, -2.5554, -3.5616,\n",
      "         -4.8661, -2.4954, -0.2427, -1.3590, -1.8354, -3.0657, -1.8634, -0.7405,\n",
      "         -3.6668, -0.8164, -3.1711, -4.1201, -2.4471, -0.5294, -0.9682, -2.9917,\n",
      "         -2.7982, -3.7527, -3.3094, -3.3398, -0.0197, -1.0761, -3.7695, -2.0275,\n",
      "         -0.4545, -1.9585, -3.5809, -3.8720,  0.6365, -1.3884, -1.3843, -0.4531,\n",
      "         -4.6270, -4.5931,  0.5805, -0.9908, -2.1668, -1.3781, -0.1355, -0.9334,\n",
      "         -2.8223, -4.5982, -2.6476, -0.7885, -1.1288, -2.8527, -2.2244, -0.8090,\n",
      "         -0.6788, -4.2568,  0.0524,  0.7847, -3.3181, -4.1494, -1.2287, -2.0125,\n",
      "         -3.0863, -2.4532, -1.6811, -2.8242, -2.9046, -5.8568, -1.6733, -2.5015,\n",
      "         -1.4753, -0.8376, -0.3979, -4.2604, -3.9538, -0.1843, -2.0494, -1.0002,\n",
      "         -1.1137, -0.2996,  0.1177, -0.7537, -4.0485, -3.2046, -4.9219, -1.3635,\n",
      "          0.9812,  1.0392,  0.9445,  0.9398,  0.8315,  0.9754, -4.7450, -4.9047,\n",
      "         -5.2507, -5.1500, -5.3349, -5.3731, -5.2910, -5.2809, -5.3187, -5.3020,\n",
      "         -5.2279, -5.1127, -5.0123, -4.9562, -4.9491, -4.9724, -5.0052, -5.0438,\n",
      "         -5.0581, -5.0774, -5.1051, -5.1557, -5.2215, -5.3160, -5.3822, -5.4100,\n",
      "         -5.3970, -5.4017, -5.4254, -5.4698, -5.4970, -5.4819, -5.4482, -5.3866,\n",
      "         -5.3365, -5.2926, -5.2780, -5.2887, -5.3132],\n",
      "        [-2.8420, -3.8595, -0.7743, -2.6010,  0.1895, -2.4215, -4.7613, -0.5482,\n",
      "         -4.2393, -2.6797, -4.0344,  0.4120, -0.3709, -3.6853, -3.0360, -2.5047,\n",
      "         -3.2849, -1.8156, -1.9328, -1.1037, -1.3921, -1.5851, -0.9539,  0.4889,\n",
      "         -2.0707, -3.5887, -3.0321, -2.6616, -3.7503,  1.0628, -1.8054, -0.3531,\n",
      "         -0.9208, -3.5777, -1.9043, -1.7544, -0.8432, -3.1493, -1.7169, -0.1387,\n",
      "         -0.6481, -0.4707, -0.2757, -4.2356, -1.7259, -3.5656, -3.6716, -3.3058,\n",
      "          0.7200,  0.9561, -3.9401, -1.5265, -7.1465, -2.9911, -2.9813, -0.9621,\n",
      "         -2.5685, -2.9610, -2.6892, -1.7176, -3.7698, -1.2073, -5.8042, -2.4671,\n",
      "         -3.4913,  1.2190, -1.3921, -0.2313, -0.3221, -3.2011, -4.2275, -5.5831,\n",
      "         -2.8957, -0.0817, -3.9102, -4.2232, -2.5445, -3.8224, -2.6023, -3.6459,\n",
      "         -4.8736, -2.5698, -0.2629, -1.4384, -1.8567, -3.1260, -1.9000, -0.8158,\n",
      "         -3.7272, -0.8418, -3.2727, -4.0884, -2.5150, -0.5417, -1.0532, -2.9635,\n",
      "         -2.8448, -3.8782, -3.3275, -3.3775, -0.0444, -1.1602, -3.7184, -2.0218,\n",
      "         -0.4089, -2.0197, -3.7108, -3.9442,  0.6577, -1.5529, -1.5321, -1.5209,\n",
      "         -4.7860, -4.6303,  0.5457, -1.1046, -2.1788, -1.4184, -0.1762, -0.9526,\n",
      "         -2.8810, -4.7410, -2.6048, -0.7358, -1.1073, -2.8784, -2.2549, -0.8477,\n",
      "         -0.7255, -4.2283,  0.4018,  1.0396, -3.2927, -4.2451, -1.2261, -2.0400,\n",
      "         -3.0694, -2.4813, -1.7150, -2.7645, -2.5047, -3.5781, -5.7436, -1.4837,\n",
      "          0.8847,  0.9108,  0.9290,  0.9640,  0.8301,  0.9708, -4.6417, -4.8264,\n",
      "         -5.1591, -4.9768, -5.1135, -5.0558, -5.1249, -5.1773, -5.2197, -5.2117,\n",
      "         -5.1620, -5.1137, -5.0951, -5.0964, -5.1219, -5.1768, -5.2322, -5.2816,\n",
      "         -5.3276, -5.3677, -5.4090, -5.4429, -5.4860, -5.5239, -5.5581, -5.5679,\n",
      "         -5.5867, -5.6158, -5.6709, -5.6797, -5.6565, -5.6306, -5.6031, -5.5831,\n",
      "         -5.5820, -5.5866, -5.5893, -5.5962, -5.5946, -5.5916, -5.5833, -5.5817,\n",
      "         -5.5825, -5.5900, -5.5857, -5.5962, -5.6099, -5.6140, -5.6189, -5.6132,\n",
      "         -5.5995, -5.5897, -5.5793, -5.5768, -5.5639]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9519, 0.9149], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.3884,  0.9900, -0.6515, -0.6773,  0.9900,  0.9900,  0.9900, -0.6762],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.5066e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1616, -0.1632, -0.1649,  ..., -1.3608, -1.3746, -1.3884],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1318, -0.1331, -0.1345,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1368, -0.1382, -0.1396,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3276,  0.9656, -0.6354, -0.6606,  0.9656,  0.9656,  0.9656, -0.6596],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4470, -3.8386, -0.6977,  ..., -1.0707, -1.1715, -1.1937],\n",
      "        [-2.8392, -3.8610, -0.7722,  ..., -5.6697, -5.6706, -5.6653],\n",
      "        [-2.3171, -3.8991, -0.6690,  ..., -4.6122, -4.6074, -4.6107],\n",
      "        ...,\n",
      "        [-2.8392, -3.8610, -0.7722,  ..., -5.6697, -5.6706, -5.6653],\n",
      "        [-2.6177, -3.8806, -0.7294,  ..., -5.1650, -5.2270, -5.2603],\n",
      "        [-2.3176, -3.8975, -0.6743,  ..., -4.6170, -4.6161, -4.6275]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1179,  0.8660, -0.6769, -0.6309,  0.9247,  0.8660,  0.9531, -0.6226],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -1.4668, -0.6926, -0.6604,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.6190e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.6140e-12, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1401, -0.1415, -0.1429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1336, -0.1349, -0.1363,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -1.4377, -0.6755, -0.6441,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6615, -3.8902, -0.7409,  ..., -5.2715, -5.2833, -5.3106],\n",
      "        [-2.6491, -3.8890, -0.7392,  ..., -5.2570, -5.2663, -5.2924],\n",
      "        [-2.6241, -3.8887, -0.7175,  ..., -5.2644, -5.3340, -5.3743],\n",
      "        ...,\n",
      "        [-2.3161, -3.9039, -0.6705,  ..., -4.6303, -4.6303, -4.6354],\n",
      "        [-2.3192, -3.9033, -0.6706,  ..., -4.6427, -4.6372, -4.6483],\n",
      "        [-2.8154, -3.8647, -0.7717,  ..., -5.6006, -5.6022, -5.5884]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9756,  0.9710,  0.9491,  0.9725, -1.0327, -0.6366, -0.6053,  0.9286],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.7018, -0.7264, -0.7264, -0.7109,  0.9900, -0.7109,  0.9900, -0.7149],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.2407e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1420, -0.1434, -0.1449,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1469, -0.1484, -0.1499,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1469, -0.1484, -0.1499,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1438, -0.1453, -0.1467,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1446, -0.1461, -0.1476,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6845, -0.7085, -0.7085, -0.6933,  0.9656, -0.6933,  0.9656, -0.6973],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3143, -3.9011, -0.6688,  ..., -4.6168, -4.6154, -4.6256],\n",
      "        [-2.3029, -3.8992, -0.6648,  ..., -4.6660, -4.6601, -4.6645],\n",
      "        [-2.3029, -3.8992, -0.6648,  ..., -4.6660, -4.6601, -4.6645],\n",
      "        ...,\n",
      "        [-2.3063, -3.9010, -0.6647,  ..., -4.6327, -4.6252, -4.6297],\n",
      "        [-2.8307, -3.8592, -0.7687,  ..., -5.6126, -5.6146, -5.6092],\n",
      "        [-2.3057, -3.9043, -0.6714,  ..., -4.6129, -4.6059, -4.6104]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6159, -0.6408, -0.6408, -0.6841,  0.8644, -0.6841,  0.9233, -0.6055],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.6808,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(4.3299e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1377, -0.1391, -0.1405,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.6640,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6647, -3.8948, -0.7459,  ..., -5.2383, -5.2526, -5.2830],\n",
      "        [-2.6143, -3.8852, -0.7313,  ..., -5.2332, -5.2937, -5.3204],\n",
      "        [-2.6519, -3.8924, -0.7480,  ..., -5.3147, -5.3223, -5.3457],\n",
      "        ...,\n",
      "        [-2.6449, -3.8882, -0.7417,  ..., -5.3236, -5.3358, -5.3567],\n",
      "        [-2.3108, -3.8991, -0.6738,  ..., -4.6948, -4.6832, -4.6892],\n",
      "        [-2.6519, -3.8924, -0.7480,  ..., -5.3147, -5.3223, -5.3457]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9456,  0.9437,  0.8989,  0.7992,  0.9535,  0.7992, -0.6132,  0.8989],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6583, -0.4617,  0.9900, -0.6588,  0.9900, -0.6588, -0.6674,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0353e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1332, -0.1345, -0.1359,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0795, -0.0803, -0.0811,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1333, -0.1346, -0.1360,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1350, -0.1364, -0.1378,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6421, -0.4503,  0.9656, -0.6425,  0.9608, -0.6425, -0.6509,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3162, -3.9014, -0.6724,  ..., -4.6692, -4.6606, -4.6656],\n",
      "        [-2.4217, -3.8830, -0.6755,  ..., -4.5909, -4.5967, -4.5952],\n",
      "        [-2.8325, -3.8700, -0.7729,  ..., -5.7097, -5.7136, -5.7126],\n",
      "        ...,\n",
      "        [-2.3176, -3.9045, -0.6635,  ..., -4.5887, -4.5908, -4.6017],\n",
      "        [-2.3144, -3.9028, -0.6697,  ..., -4.6564, -4.6515, -4.6615],\n",
      "        [-2.6271, -3.8899, -0.7182,  ..., -5.2858, -5.3565, -5.3943]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6318, -0.3822,  0.8561, -0.6129,  0.9126, -0.6129, -0.6305,  0.9323],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6621,  0.9900, -0.4347, -1.2092, -0.6872,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.6691e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1339, -0.1353, -0.1367,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1390, -0.1404, -0.1418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6457,  0.9656, -0.4240, -1.1562, -0.6702,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6839, -3.8861, -0.7308,  ..., -5.3058, -5.3159, -5.3409],\n",
      "        [-2.3254, -3.9023, -0.6700,  ..., -4.6420, -4.6418, -4.6457],\n",
      "        [-2.6885, -3.8961, -0.7466,  ..., -5.3555, -5.3690, -5.3960],\n",
      "        ...,\n",
      "        [-2.3106, -3.8994, -0.6650,  ..., -4.6131, -4.6076, -4.6113],\n",
      "        [-2.6764, -3.8922, -0.7348,  ..., -5.3054, -5.3132, -5.3337],\n",
      "        [-2.6140, -3.8771, -0.7039,  ..., -5.4449, -5.4917, -5.5052]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9469, -0.6694,  0.9465, -0.3545, -1.1890, -0.6578,  0.9278,  0.9433],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6336,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(2.5704e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1282, -0.1295, -0.1308, -0.1321, -0.1334, -0.1348, -0.1361, -0.1375,\n",
      "         -0.1389, -0.1403, -0.1417, -0.1432, -0.1446, -0.1461, -0.1475, -0.1490,\n",
      "         -0.1505, -0.1521, -0.1536, -0.1551, -0.1567, -0.1583, -0.1599, -0.1615,\n",
      "         -0.1631, -0.1648, -0.1664, -0.1681, -0.1698, -0.1715, -0.1733, -0.1750,\n",
      "         -0.1768, -0.1786, -0.1804, -0.1822, -0.1840, -0.1859, -0.1878, -0.1897,\n",
      "         -0.1916, -0.1935, -0.1955, -0.1975, -0.1995, -0.2015, -0.2035, -0.2056,\n",
      "         -0.2076, -0.2097, -0.2119, -0.2140, -0.2162, -0.2183, -0.2205, -0.2228,\n",
      "         -0.2250, -0.2273, -0.2296, -0.2319, -0.2343, -0.2366, -0.2390, -0.2414,\n",
      "         -0.2439, -0.2463, -0.2488, -0.2513, -0.2539, -0.2564, -0.2590, -0.2616,\n",
      "         -0.2643, -0.2669, -0.2696, -0.2724, -0.2751, -0.2779, -0.2807, -0.2835,\n",
      "         -0.2864, -0.2893, -0.2922, -0.2952, -0.2982, -0.3012, -0.3042, -0.3073,\n",
      "         -0.3104, -0.3135, -0.3167, -0.3199, -0.3231, -0.3264, -0.3297, -0.3330,\n",
      "         -0.3364, -0.3398, -0.3432, -0.3467, -0.3502, -0.3537, -0.3573, -0.3609,\n",
      "         -0.3645, -0.3682, -0.3719, -0.3757, -0.3795, -0.3833, -0.3872, -0.3911,\n",
      "         -0.3951, -0.3990, -0.4031, -0.4071, -0.4113, -0.4154, -0.4196, -0.4238,\n",
      "         -0.4281, -0.4325, -0.4368, -0.4412, -0.4457, -0.4502, -0.4547, -0.4593,\n",
      "         -0.4640, -0.4687, -0.4734, -0.4782, -0.4830, -0.4879, -0.4928, -0.4978,\n",
      "         -0.5028, -0.5079, -0.5130, -0.5182, -0.5234, -0.5287, -0.5341, -0.5395,\n",
      "         -0.5449, -0.5504, -0.5560, -0.5616, -0.5673, -0.5730, -0.5788, -0.5846,\n",
      "         -0.5905, -0.5965, -0.6025, -0.6086, -0.6148, -0.6210, -0.6272, -0.6336,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6180,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3267, -3.9003, -0.6722, -2.2788,  0.3083, -2.3286, -4.5184, -0.1171,\n",
      "         -3.6691, -2.4514, -3.9177,  0.3986, -0.2982, -3.4768, -3.2064, -2.1847,\n",
      "         -3.1930, -1.6769, -2.0237, -0.8348, -1.2864, -1.3239, -0.7557,  0.3562,\n",
      "         -1.7926, -3.3731, -3.1620, -2.3950, -3.7661,  1.0239, -1.6291, -0.0445,\n",
      "         -0.9338, -3.4827, -1.9016, -1.9429, -0.9632, -3.1198, -1.4685,  0.1021,\n",
      "         -0.5859, -0.5524, -0.2124, -4.3304, -1.4945, -3.0883, -3.4261, -3.2204,\n",
      "         -0.0586,  0.2587, -3.8001, -1.4563, -7.1594, -2.8040, -2.9819, -0.7968,\n",
      "         -2.5619, -2.9330, -2.5551, -1.5041, -3.5866, -1.1388, -5.7918, -2.3230,\n",
      "         -3.5098,  1.1504, -1.2321,  0.0831, -0.3825, -3.1019, -4.0290, -5.6278,\n",
      "         -2.7154, -0.1722, -3.7529, -4.3318, -2.3715, -3.8444, -2.4027, -3.4958,\n",
      "         -4.9239, -2.4071, -0.1866, -1.1534, -1.7787, -2.8928, -1.7162, -0.5286,\n",
      "         -3.5918, -0.7566, -3.0889, -4.1826, -2.3607, -0.4817, -0.7558, -3.0557,\n",
      "         -2.6471, -3.4772, -3.2547, -3.2385,  0.0574, -0.8762, -3.7870, -2.0342,\n",
      "         -0.5118, -1.7863, -3.3363, -3.7653,  0.5494, -0.6672, -1.4298, -0.7989,\n",
      "         -4.1762, -4.5835,  0.5923, -0.7740, -2.0868, -1.2644, -0.0616, -0.8832,\n",
      "         -2.6117, -4.5812, -2.5691, -0.7784, -1.0715, -2.8052, -2.1157, -0.6994,\n",
      "         -0.4281, -4.1886, -0.4341,  0.2815, -3.3324, -4.1055, -1.1276, -2.0220,\n",
      "         -3.0522, -2.4161, -1.5872, -2.9127, -2.6112, -3.2118, -5.7545, -1.3044,\n",
      "         -0.5712, -0.6786, -0.5868, -0.6639, -0.6490, -0.6410, -4.8382, -4.9410,\n",
      "         -5.3434, -4.9742, -4.8694, -4.5927, -4.4957, -4.5964, -4.5648, -4.5264,\n",
      "         -4.6015, -4.6626, -4.7184, -4.7372, -4.6781, -4.5663, -4.4884, -4.4456,\n",
      "         -4.4316, -4.4127, -4.4022, -4.4137, -4.4435, -4.4784, -4.5051, -4.5095,\n",
      "         -4.4853, -4.4665, -4.4849, -4.5024, -4.5142, -4.5243, -4.5030, -4.4813,\n",
      "         -4.4728, -4.4767, -4.5033, -4.5266, -4.5391, -4.5427, -4.5395, -4.5473,\n",
      "         -4.5593, -4.5753, -4.5802, -4.5873, -4.5996, -4.5969, -4.5993, -4.6035,\n",
      "         -4.6045, -4.6186, -4.6153, -4.6104, -4.6153],\n",
      "        [-2.6867, -3.8891, -0.7328, -2.4872,  0.2071, -2.3960, -4.6266, -0.4083,\n",
      "         -4.0619, -2.6388, -4.0076,  0.3853, -0.3920, -3.5766, -3.0972, -2.3773,\n",
      "         -3.2955, -1.7809, -1.9558, -1.0223, -1.3515, -1.5165, -0.9052,  0.4324,\n",
      "         -2.0171, -3.4820, -3.0786, -2.5593, -3.7676,  1.0451, -1.7729, -0.2496,\n",
      "         -0.9165, -3.5603, -1.9361, -1.8104, -0.9030, -3.1874, -1.6261, -0.0678,\n",
      "         -0.6397, -0.5166, -0.2522, -4.2735, -1.6782, -3.4238, -3.6341, -3.2953,\n",
      "          0.4108,  0.6454, -3.8615, -1.5368, -7.1659, -2.9173, -3.0216, -0.9176,\n",
      "         -2.5352, -2.9672, -2.6474, -1.6820, -3.6801, -1.2093, -5.8129, -2.4060,\n",
      "         -3.5079,  1.1898, -1.3647, -0.1225, -0.3347, -3.1826, -4.1514, -5.6084,\n",
      "         -2.8203, -0.0961, -3.8291, -4.2723, -2.4730, -3.8774, -2.5304, -3.5637,\n",
      "         -4.9016, -2.5005, -0.2334, -1.3347, -1.8325, -3.0575, -1.8563, -0.7157,\n",
      "         -3.6714, -0.8345, -3.1764, -4.1309, -2.4499, -0.5142, -0.9415, -3.0054,\n",
      "         -2.8133, -3.7616, -3.3068, -3.3310, -0.0091, -1.0507, -3.7552, -2.0405,\n",
      "         -0.4529, -1.9775, -3.5937, -3.8724,  0.6397, -0.8633, -1.3542, -0.5009,\n",
      "         -4.6113, -4.6173,  0.5432, -0.9442, -2.1521, -1.4037, -0.0993, -0.9547,\n",
      "         -2.7868, -4.5735, -2.6411, -0.7903, -1.1467, -2.9126, -2.2589, -0.8005,\n",
      "         -0.6078, -4.2945,  0.0532,  0.6890, -3.3378, -4.1531, -1.2108, -2.0116,\n",
      "         -3.0896, -2.4691, -1.7149, -2.8884, -2.8853, -5.8821, -1.7359, -2.4870,\n",
      "         -1.1707, -0.8430, -0.7011, -4.0527, -3.9198, -0.2306, -1.9846, -0.9173,\n",
      "         -0.9413, -0.6098, -0.1215, -0.6083, -4.1976, -3.3176, -4.8694, -1.4295,\n",
      "          0.9306,  0.9077,  0.9635,  0.9651,  0.8337,  0.9549, -4.7126, -4.9005,\n",
      "         -5.2268, -5.0719, -5.3375, -5.3656, -5.2633, -5.2772, -5.2837, -5.2434,\n",
      "         -5.1665, -5.0647, -4.9839, -4.9411, -4.9317, -4.9496, -4.9843, -5.0230,\n",
      "         -5.0418, -5.0639, -5.0938, -5.1442, -5.2156, -5.3031, -5.3700, -5.4021,\n",
      "         -5.4069, -5.4212, -5.4535, -5.4954, -5.5095, -5.4957, -5.4685, -5.4207,\n",
      "         -5.3723, -5.3281, -5.3065, -5.3098, -5.3287]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6318,  0.9259], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6168,  0.9900,  0.9900,  0.9900,  0.9900, -0.6024, -0.5697,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1068e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1248, -0.1260, -0.1273,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1219, -0.1231, -0.1243,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1152, -0.1164, -0.1176,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6016,  0.9656,  0.9656,  0.9656,  0.9656, -0.5876, -0.5556,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3361, -3.8986, -0.6715,  ..., -4.6149, -4.6100, -4.6159],\n",
      "        [-2.7041, -3.8847, -0.7368,  ..., -5.3001, -5.3065, -5.3292],\n",
      "        [-2.8547, -3.8671, -0.7665,  ..., -5.6617, -5.6623, -5.6543],\n",
      "        ...,\n",
      "        [-2.3431, -3.8937, -0.6662,  ..., -4.6232, -4.6194, -4.6265],\n",
      "        [-2.3464, -3.8982, -0.6651,  ..., -4.5894, -4.5923, -4.5995],\n",
      "        [-2.6915, -3.8791, -0.7338,  ..., -5.3087, -5.3115, -5.3316]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6350,  0.9473,  0.8586,  0.9435,  0.9311, -0.6535, -0.6260,  0.7960],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.6148, -0.6346, -0.6076,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.1257e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(6.8211e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1284, -0.1297, -0.1310,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1229, -0.1242, -0.1254,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.5997, -0.6189, -0.5926,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6629, -3.8815, -0.7111,  ..., -5.2541, -5.3155, -5.3542],\n",
      "        [-2.8706, -3.8546, -0.7614,  ..., -5.6211, -5.6202, -5.6111],\n",
      "        [-2.7021, -3.8760, -0.7316,  ..., -5.3165, -5.3239, -5.3409],\n",
      "        ...,\n",
      "        [-2.3367, -3.8909, -0.6616,  ..., -4.6096, -4.6082, -4.6108],\n",
      "        [-2.3520, -3.8948, -0.6690,  ..., -4.6408, -4.6441, -4.6478],\n",
      "        [-2.6629, -3.8815, -0.7111,  ..., -5.2541, -5.3155, -5.3542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9365,  0.8709,  0.8053,  0.9445, -0.6316, -0.6635, -0.6744,  0.9365],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-1.1670,  0.9900,  0.9900,  0.9900, -1.3763, -0.3774,  0.9900, -0.3656],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.9507e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1611, -0.1628, -0.1644,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0650, -0.0657, -0.0663,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0630, -0.0636, -0.0643,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1159,  0.9656,  0.9656,  0.9656, -1.3490, -0.3681,  0.9608, -0.3566],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4737, -3.8432, -0.6779,  ..., -4.1457, -4.1359, -4.1284],\n",
      "        [-2.7212, -3.8791, -0.7371,  ..., -5.3078, -5.3141, -5.3371],\n",
      "        [-2.7221, -3.8817, -0.7312,  ..., -5.3263, -5.3319, -5.3536],\n",
      "        ...,\n",
      "        [-2.4673, -3.8690, -0.6800,  ..., -4.6220, -4.6313, -4.6317],\n",
      "        [-2.8621, -3.8568, -0.7637,  ..., -5.5603, -5.5545, -5.5389],\n",
      "        [-2.4683, -3.8723, -0.6735,  ..., -4.5719, -4.5795, -4.5796]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.2375,  0.9224,  0.9453,  0.8930, -1.0803, -0.3627,  0.9152, -0.3862],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6247,  0.9900, -0.6427,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(3.8516e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1264, -0.1276, -0.1289,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1300, -0.1313, -0.1327,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6093,  0.9656, -0.6269,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3648, -3.8886, -0.6686,  ..., -4.6403, -4.6422, -4.6465],\n",
      "        [-2.8870, -3.8481, -0.7600,  ..., -5.5618, -5.5587, -5.5484],\n",
      "        [-2.3584, -3.8851, -0.6624,  ..., -4.6713, -4.6698, -4.6727],\n",
      "        ...,\n",
      "        [-2.7257, -3.8780, -0.7322,  ..., -5.3383, -5.3440, -5.3694],\n",
      "        [-2.8808, -3.8568, -0.7642,  ..., -5.6739, -5.6727, -5.6690],\n",
      "        [-2.7269, -3.8754, -0.7362,  ..., -5.3211, -5.3290, -5.3516]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6692,  0.8902, -0.6734,  0.9559,  0.9651,  0.9504,  0.8595,  0.9287],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.6529, -0.6221,  0.9900, -0.6154,  0.9900, -0.6126,  0.9900, -0.6701],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(2.0735e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1321, -0.1334, -0.1348,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1258, -0.1271, -0.1284,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1239, -0.1252, -0.1264,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1356, -0.1369, -0.1383,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6368, -0.6067,  0.9608, -0.6002,  0.9656, -0.5975,  0.9656, -0.6536],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3629, -3.8823, -0.6596,  ..., -4.6432, -4.6370, -4.6420],\n",
      "        [-2.3684, -3.8834, -0.6642,  ..., -4.6592, -4.6546, -4.6677],\n",
      "        [-2.8710, -3.8470, -0.7590,  ..., -5.5825, -5.5790, -5.5634],\n",
      "        ...,\n",
      "        [-2.3714, -3.8852, -0.6600,  ..., -4.5956, -4.5993, -4.6076],\n",
      "        [-2.7443, -3.8790, -0.7405,  ..., -5.3976, -5.4113, -5.4433],\n",
      "        [-2.3622, -3.8805, -0.6596,  ..., -4.6779, -4.6758, -4.6796]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7121, -0.6381,  0.9177, -0.6373,  0.9455, -0.6116,  0.9704, -0.6694],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6445, -0.6325, -0.6767, -0.7013, -0.6767,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 215])\n",
      "prune_penalty\n",
      "tensor(1.9565e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1304, -0.1317, -0.1330,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1279, -0.1292, -0.1305,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1369, -0.1383, -0.1397,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6286, -0.6169, -0.6600, -0.6840, -0.6600,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7332, -3.8682, -0.7322,  ..., -5.3614, -5.3747, -5.4048],\n",
      "        [-2.3685, -3.8820, -0.6616,  ..., -4.6562, -4.6545, -4.6665],\n",
      "        [-2.3694, -3.8724, -0.6658,  ..., -4.6943, -4.6891, -4.6940],\n",
      "        ...,\n",
      "        [-2.3631, -3.8782, -0.6571,  ..., -4.6428, -4.6371, -4.6412],\n",
      "        [-2.8926, -3.8404, -0.7551,  ..., -5.6558, -5.6595, -5.6548],\n",
      "        [-2.7406, -3.8673, -0.7342,  ..., -5.3934, -5.4087, -5.4380]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9775, -0.6308, -0.6185, -0.7040, -0.6455, -0.7040,  0.8787,  0.9629],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "turn_level_rewards\n",
      "tensor([-0.7135, -1.4185], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 215])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1444, -0.1458, -0.1473, -0.1488, -0.1503, -0.1518, -0.1533, -0.1549,\n",
      "         -0.1564, -0.1580, -0.1596, -0.1612, -0.1629, -0.1645, -0.1662, -0.1678,\n",
      "         -0.1695, -0.1712, -0.1730, -0.1747, -0.1765, -0.1783, -0.1801, -0.1819,\n",
      "         -0.1837, -0.1856, -0.1875, -0.1894, -0.1913, -0.1932, -0.1951, -0.1971,\n",
      "         -0.1991, -0.2011, -0.2032, -0.2052, -0.2073, -0.2094, -0.2115, -0.2136,\n",
      "         -0.2158, -0.2180, -0.2202, -0.2224, -0.2246, -0.2269, -0.2292, -0.2315,\n",
      "         -0.2338, -0.2362, -0.2386, -0.2410, -0.2434, -0.2459, -0.2484, -0.2509,\n",
      "         -0.2534, -0.2560, -0.2586, -0.2612, -0.2638, -0.2665, -0.2692, -0.2719,\n",
      "         -0.2746, -0.2774, -0.2802, -0.2830, -0.2859, -0.2888, -0.2917, -0.2947,\n",
      "         -0.2976, -0.3006, -0.3037, -0.3067, -0.3098, -0.3130, -0.3161, -0.3193,\n",
      "         -0.3226, -0.3258, -0.3291, -0.3324, -0.3358, -0.3392, -0.3426, -0.3461,\n",
      "         -0.3496, -0.3531, -0.3567, -0.3603, -0.3639, -0.3676, -0.3713, -0.3750,\n",
      "         -0.3788, -0.3827, -0.3865, -0.3904, -0.3944, -0.3984, -0.4024, -0.4064,\n",
      "         -0.4105, -0.4147, -0.4189, -0.4231, -0.4274, -0.4317, -0.4361, -0.4405,\n",
      "         -0.4449, -0.4494, -0.4539, -0.4585, -0.4632, -0.4678, -0.4726, -0.4773,\n",
      "         -0.4822, -0.4870, -0.4920, -0.4969, -0.5019, -0.5070, -0.5121, -0.5173,\n",
      "         -0.5225, -0.5278, -0.5331, -0.5385, -0.5440, -0.5495, -0.5550, -0.5606,\n",
      "         -0.5663, -0.5720, -0.5778, -0.5836, -0.5895, -0.5955, -0.6015, -0.6076,\n",
      "         -0.6137, -0.6199, -0.6262, -0.6325, -0.6389, -0.6453, -0.6518, -0.6584,\n",
      "         -0.6651, -0.6718, -0.6786, -0.6854, -0.6924, -0.6993, -0.7064, -0.7135,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1651, -0.1668, -0.1685, -0.1702, -0.1719, -0.1736, -0.1754, -0.1771,\n",
      "         -0.1789, -0.1807, -0.1826, -0.1844, -0.1863, -0.1882, -0.1901, -0.1920,\n",
      "         -0.1939, -0.1959, -0.1978, -0.1998, -0.2019, -0.2039, -0.2060, -0.2080,\n",
      "         -0.2101, -0.2123, -0.2144, -0.2166, -0.2188, -0.2210, -0.2232, -0.2255,\n",
      "         -0.2277, -0.2300, -0.2324, -0.2347, -0.2371, -0.2395, -0.2419, -0.2443,\n",
      "         -0.2468, -0.2493, -0.2518, -0.2544, -0.2569, -0.2595, -0.2621, -0.2648,\n",
      "         -0.2675, -0.2702, -0.2729, -0.2757, -0.2784, -0.2813, -0.2841, -0.2870,\n",
      "         -0.2899, -0.2928, -0.2958, -0.2987, -0.3018, -0.3048, -0.3079, -0.3110,\n",
      "         -0.3141, -0.3173, -0.3205, -0.3237, -0.3270, -0.3303, -0.3337, -0.3370,\n",
      "         -0.3404, -0.3439, -0.3473, -0.3509, -0.3544, -0.3580, -0.3616, -0.3652,\n",
      "         -0.3689, -0.3727, -0.3764, -0.3802, -0.3841, -0.3879, -0.3919, -0.3958,\n",
      "         -0.3998, -0.4039, -0.4079, -0.4121, -0.4162, -0.4204, -0.4247, -0.4290,\n",
      "         -0.4333, -0.4377, -0.4421, -0.4466, -0.4511, -0.4556, -0.4602, -0.4649,\n",
      "         -0.4696, -0.4743, -0.4791, -0.4839, -0.4888, -0.4938, -0.4988, -0.5038,\n",
      "         -0.5089, -0.5140, -0.5192, -0.5245, -0.5298, -0.5351, -0.5405, -0.5460,\n",
      "         -0.5515, -0.5571, -0.5627, -0.5684, -0.5741, -0.5799, -0.5858, -0.5917,\n",
      "         -0.5977, -0.6037, -0.6098, -0.6160, -0.6222, -0.6285, -0.6348, -0.6412,\n",
      "         -0.6477, -0.6543, -0.6609, -0.6675, -0.6743, -0.6811, -0.6880, -0.6949,\n",
      "         -0.7019, -0.7090, -0.7162, -0.7234, -0.7307, -0.7381, -0.7456, -0.7531,\n",
      "         -0.7607, -0.7684, -0.7762, -0.7840, -0.7919, -0.7999, -0.8080, -0.8161,\n",
      "         -0.8244, -0.8327, -0.8411, -0.8496, -0.8582, -0.8669, -0.8756, -0.8845,\n",
      "         -0.8934, -0.9024, -0.9116, -0.9208, -0.9301, -0.9395, -0.9489, -0.9585,\n",
      "         -0.9682, -0.9780, -0.9879, -0.9979, -1.0079, -1.0181, -1.0284, -1.0388,\n",
      "         -1.0493, -1.0599, -1.0706, -1.0814, -1.0923, -1.1034, -1.1145, -1.1258,\n",
      "         -1.1371, -1.1486, -1.1602, -1.1719, -1.1838, -1.1957, -1.2078, -1.2200,\n",
      "         -1.2323, -1.2448, -1.2574, -1.2701, -1.2829, -1.2958, -1.3089, -1.3221,\n",
      "         -1.3355, -1.3490, -1.3626, -1.3764, -1.3903, -1.4043, -1.4185]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 215])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6959, -1.3564], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3653, -3.8718, -0.6593, -2.2537,  0.3085, -2.3323, -4.5002, -0.1195,\n",
      "         -3.7114, -2.4659, -3.9184,  0.4171, -0.2398, -3.4905, -3.1871, -2.1845,\n",
      "         -3.1931, -1.6878, -2.0014, -0.8509, -1.3058, -1.3332, -0.7704,  0.3708,\n",
      "         -1.8130, -3.3866, -3.1360, -2.3926, -3.7519,  1.0289, -1.6339, -0.0589,\n",
      "         -0.9291, -3.4758, -1.8986, -1.9152, -0.9522, -3.1208, -1.4619,  0.0910,\n",
      "         -0.6021, -0.5417, -0.1944, -4.3183, -1.5105, -3.1259, -3.4507, -3.2190,\n",
      "         -0.0371,  0.3216, -3.8114, -1.4452, -7.1417, -2.7997, -2.9791, -0.8080,\n",
      "         -2.5252, -2.8987, -2.5622, -1.5249, -3.6011, -1.1270, -5.7655, -2.3168,\n",
      "         -3.4932,  1.1574, -1.2393,  0.0694, -0.3697, -3.0951, -4.0406, -5.6198,\n",
      "         -2.7140, -0.1719, -3.7622, -4.3041, -2.3692, -3.8399, -2.3990, -3.5053,\n",
      "         -4.9142, -2.4047, -0.1862, -1.1718, -1.7769, -2.9002, -1.7242, -0.5443,\n",
      "         -3.5948, -0.7817, -3.0986, -4.1569, -2.3580, -0.4832, -0.7742, -3.0648,\n",
      "         -2.6754, -3.5103, -3.2549, -3.2307,  0.0554, -0.8940, -3.7569, -2.0220,\n",
      "         -0.4989, -1.7978, -3.3719, -3.7875,  0.5546, -1.5250, -1.5403, -1.0184,\n",
      "         -4.2723, -4.6581,  0.6142, -0.8185, -2.0968, -1.2200, -0.0219, -0.8499,\n",
      "         -2.5577, -4.6447, -2.5763, -0.7521, -0.9591, -2.7209, -2.1390, -0.7123,\n",
      "         -0.3957, -4.1583, -0.5026,  0.3367, -3.2735, -4.1084, -1.1526, -1.9972,\n",
      "         -3.0537, -2.3714, -1.6193, -2.8753, -2.5867, -3.2951, -5.7477, -1.3155,\n",
      "         -0.6511, -0.5858, -0.6089, -0.6405, -0.6926, -0.6736, -4.9113, -4.9502,\n",
      "         -5.3237, -4.9842, -4.8976, -4.6187, -4.5513, -4.6653, -4.6438, -4.5980,\n",
      "         -4.6403, -4.6818, -4.7506, -4.7620, -4.7018, -4.6087, -4.5636, -4.5495,\n",
      "         -4.5469, -4.5334, -4.5185, -4.5284, -4.5544, -4.5846, -4.6035, -4.6050,\n",
      "         -4.5763, -4.5581, -4.5684, -4.5771, -4.5923, -4.5939, -4.5650, -4.5363,\n",
      "         -4.5329, -4.5343, -4.5603, -4.5902, -4.6051, -4.6125, -4.6136, -4.6205,\n",
      "         -4.6291, -4.6334, -4.6312, -4.6305, -4.6372, -4.6346, -4.6409, -4.6391,\n",
      "         -4.6349, -4.6424, -4.6437, -4.6394, -4.6471],\n",
      "        [-2.4978, -3.8158, -0.6808, -2.3063,  0.2799, -2.3679, -4.5888, -0.1740,\n",
      "         -3.8563, -2.4872, -3.9827,  0.3696, -0.2734, -3.5121, -3.1857, -2.2712,\n",
      "         -3.2361, -1.7050, -2.0011, -0.8965, -1.2887, -1.3614, -0.7756,  0.3637,\n",
      "         -1.9013, -3.3950, -3.1200, -2.4675, -3.7773,  0.9971, -1.6720, -0.1002,\n",
      "         -0.8926, -3.5474, -1.8392, -1.8951, -0.9381, -3.1546, -1.4889,  0.1272,\n",
      "         -0.5977, -0.5337, -0.2191, -4.2971, -1.5799, -3.2249, -3.4834, -3.2805,\n",
      "          0.0778,  0.4113, -3.8049, -1.4980, -7.1482, -2.8506, -3.0050, -0.8341,\n",
      "         -2.5013, -2.9124, -2.5837, -1.5784, -3.6062, -1.1651, -5.7602, -2.3516,\n",
      "         -3.5106,  1.1352, -1.2682,  0.0284, -0.3331, -3.1583, -4.0587, -5.5623,\n",
      "         -2.7655, -0.1440, -3.7617, -4.2969, -2.4105, -3.8644, -2.4175, -3.4988,\n",
      "         -4.8532, -2.4380, -0.1774, -1.1945, -1.7191, -2.9479, -1.7583, -0.5771,\n",
      "         -3.5623, -0.7791, -3.0970, -4.1519, -2.3904, -0.4770, -0.7924, -3.0188,\n",
      "         -2.7280, -3.5960, -3.2209, -3.2404,  0.0546, -0.9073, -3.7602, -2.0037,\n",
      "         -0.5024, -1.8124, -3.4416, -3.8175,  0.5897, -5.2138, -1.6259, -2.7321,\n",
      "         -4.2304, -4.5098,  0.6835, -0.8008, -2.1019, -1.2567, -0.0805, -0.8338,\n",
      "         -2.5883, -4.6508, -2.5804, -0.7933, -1.0206, -2.7965, -2.1464, -0.7330,\n",
      "         -0.6002, -4.2697, -0.2506,  0.4719, -3.2991, -4.1512, -1.1798, -1.9797,\n",
      "         -3.0785, -2.3943, -1.6206, -2.9089, -2.8220, -5.8543, -1.5135, -6.1834,\n",
      "         -1.5912, -1.4774, -1.4633, -1.2063, -1.4332, -1.1958, -1.3507, -4.2109,\n",
      "         -4.8253, -0.2696, -1.8798, -4.6395, -5.7540, -4.7760, -5.3564, -1.8569,\n",
      "         -5.7347, -0.8356, -1.0617, -1.1966, -1.0352, -1.1473, -0.9504, -1.3623,\n",
      "         -4.0794, -3.8509,  0.1985, -1.5710, -3.6374, -4.6381, -3.6376, -4.8417,\n",
      "         -1.3625, -5.4562, -0.9057, -0.8514, -1.0544, -0.8330, -1.0130, -0.7914,\n",
      "         -1.1794, -3.9500, -3.6989,  0.1039, -1.3377, -3.6227, -4.7341, -3.5098,\n",
      "         -3.4609, -4.9186, -1.4870, -1.0199, -1.2754, -1.2911, -1.3471, -1.3141,\n",
      "         -1.3691, -1.3809, -1.2863, -1.3603, -1.2158]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6421, -1.2860], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9263, -0.8707, -1.7467,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.3705e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.3307e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1874, -0.1893, -0.1912,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1761, -0.1779, -0.1797,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3569, -0.3605, -0.3642,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9034, -0.8492, -1.7121,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3162, -3.8711, -0.6605,  ..., -4.1184, -4.1082, -4.1024],\n",
      "        [-2.3180, -3.8622, -0.6653,  ..., -4.1924, -4.1795, -4.1742],\n",
      "        [-2.1768, -3.8825, -0.6030,  ..., -4.1649, -4.1669, -4.1666],\n",
      "        ...,\n",
      "        [-2.8314, -3.8563, -0.7545,  ..., -5.4872, -5.4790, -5.4623],\n",
      "        [-2.9386, -3.8205, -0.7735,  ..., -4.8528, -4.7767, -4.7325],\n",
      "        [-2.8221, -3.8602, -0.7507,  ..., -5.5972, -5.5994, -5.5770]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6505, -0.6099, -1.1031,  1.0161,  1.0165,  1.0243,  0.9008,  1.0212],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9105,  0.9900, -0.9602, -0.9412, -0.8994, -0.7867,  0.9900, -0.9720],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.3208e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(7.0018e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1842, -0.1861, -0.1879,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1943, -0.1962, -0.1982,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1355, -0.1369, -0.1383,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1966, -0.1986, -0.2006,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8881,  0.9608, -0.9365, -0.9180, -0.8772, -0.7673,  0.9656, -0.9481],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3273, -3.8746, -0.6588,  ..., -4.1228, -4.1154, -4.1134],\n",
      "        [-2.9310, -3.8171, -0.7719,  ..., -4.9458, -4.8376, -4.7601],\n",
      "        [-2.3107, -3.8687, -0.6557,  ..., -4.1946, -4.1837, -4.1758],\n",
      "        ...,\n",
      "        [-2.4125, -3.8609, -0.6540,  ..., -4.5595, -4.5334, -4.5047],\n",
      "        [-2.9453, -3.8156, -0.7705,  ..., -4.8299, -4.7618, -4.7249],\n",
      "        [-2.3000, -3.8691, -0.6545,  ..., -4.0978, -4.0869, -4.0835]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6054,  0.9278, -0.6714, -0.7147, -0.6378, -0.3754,  0.9054, -0.6524],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.9395,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(4.1683e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1901, -0.1920, -0.1939,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.9163,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7628, -3.8633, -0.7373,  ..., -5.5774, -5.5967, -5.5950],\n",
      "        [-2.8272, -3.8647, -0.7626,  ..., -5.6443, -5.6396, -5.6207],\n",
      "        [-2.7628, -3.8633, -0.7373,  ..., -5.5774, -5.5967, -5.5950],\n",
      "        ...,\n",
      "        [-2.3314, -3.8822, -0.6652,  ..., -4.1271, -4.1222, -4.1203],\n",
      "        [-2.9480, -3.8279, -0.7783,  ..., -4.8985, -4.8292, -4.7909],\n",
      "        [-2.8226, -3.8674, -0.7644,  ..., -5.5973, -5.5981, -5.5825]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9455,  0.9432,  0.9455,  0.9515,  0.9347, -0.6920,  0.8219,  0.9388],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8030,  0.9900, -0.9258, -0.9153, -1.7696, -0.8703, -0.8788],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2710e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1383, -0.1397, -0.1411,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3616, -0.3653, -0.3689,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1761, -0.1778, -0.1796,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1778, -0.1796, -0.1814,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7832,  0.9656, -0.9029, -0.8928, -1.7345, -0.8489, -0.8572],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8290, -3.8740, -0.7661,  ..., -5.6527, -5.6519, -5.6291],\n",
      "        [-2.4058, -3.8717, -0.6644,  ..., -4.6695, -4.6545, -4.6425],\n",
      "        [-2.9432, -3.8416, -0.7850,  ..., -4.9633, -4.8933, -4.8497],\n",
      "        ...,\n",
      "        [-2.1669, -3.9060, -0.6045,  ..., -4.1522, -4.1566, -4.1615],\n",
      "        [-2.3281, -3.8858, -0.6733,  ..., -4.1932, -4.1828, -4.1819],\n",
      "        [-2.3300, -3.8938, -0.6715,  ..., -4.1911, -4.1847, -4.1836]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9181, -0.4226,  0.7463, -0.7135, -0.8023, -1.2740, -0.7166, -0.7221],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7768,  0.9900, -0.9267, -0.9026, -1.6200,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2296e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1338, -0.1351, -0.1365,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1826, -0.1844, -0.1863,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3148, -0.3180, -0.3212,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7576,  0.9656, -0.9039, -0.8803, -1.5490,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7706, -3.8938, -0.7530,  ..., -5.4862, -5.5004, -5.5047],\n",
      "        [-2.7596, -3.8846, -0.7576,  ..., -5.3156, -5.3342, -5.3321],\n",
      "        [-2.4115, -3.8836, -0.6568,  ..., -4.6307, -4.6105, -4.5928],\n",
      "        ...,\n",
      "        [-2.3388, -3.9012, -0.6700,  ..., -4.1349, -4.1304, -4.1327],\n",
      "        [-2.2499, -3.8986, -0.6219,  ..., -4.3650, -4.3274, -4.3048],\n",
      "        [-2.8243, -3.8824, -0.7668,  ..., -5.6335, -5.6343, -5.6249]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8577,  0.8439, -0.4709,  0.8668, -0.7984, -0.7866, -1.3730,  0.8661],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8619, -1.1000,  0.9900,  0.9900, -0.9300,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4568e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1744, -0.1761, -0.1779,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [-0.1881, -0.1900, -0.1919,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8406, -1.0518,  0.9656,  0.9656, -0.9070,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8479, -3.8937, -0.7767,  ..., -5.7302, -5.7331, -5.7218],\n",
      "        [-2.3383, -3.9064, -0.6766,  ..., -4.1854, -4.1793, -4.1803],\n",
      "        [-2.5012, -3.8438, -0.6940,  ..., -0.9853, -1.0319, -1.2828],\n",
      "        ...,\n",
      "        [-2.3347, -3.9070, -0.6760,  ..., -4.1274, -4.1183, -4.1183],\n",
      "        [-2.8334, -3.8839, -0.7694,  ..., -5.6379, -5.6384, -5.6225],\n",
      "        [-2.8363, -3.8854, -0.7590,  ..., -5.6692, -5.6745, -5.6662]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8302, -0.8275, -1.0495,  0.6431,  0.8170, -0.8506,  0.7153,  0.8144],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.4661, -0.9074], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1706, -0.1724, -0.1741, -0.1759, -0.1776, -0.1794, -0.1812, -0.1831,\n",
      "         -0.1849, -0.1868, -0.1887, -0.1906, -0.1925, -0.1945, -0.1964, -0.1984,\n",
      "         -0.2004, -0.2024, -0.2045, -0.2065, -0.2086, -0.2107, -0.2129, -0.2150,\n",
      "         -0.2172, -0.2194, -0.2216, -0.2238, -0.2261, -0.2284, -0.2307, -0.2330,\n",
      "         -0.2354, -0.2378, -0.2402, -0.2426, -0.2450, -0.2475, -0.2500, -0.2525,\n",
      "         -0.2551, -0.2577, -0.2603, -0.2629, -0.2655, -0.2682, -0.2709, -0.2737,\n",
      "         -0.2764, -0.2792, -0.2821, -0.2849, -0.2878, -0.2907, -0.2936, -0.2966,\n",
      "         -0.2996, -0.3026, -0.3057, -0.3088, -0.3119, -0.3150, -0.3182, -0.3214,\n",
      "         -0.3247, -0.3279, -0.3313, -0.3346, -0.3380, -0.3414, -0.3448, -0.3483,\n",
      "         -0.3518, -0.3554, -0.3590, -0.3626, -0.3663, -0.3700, -0.3737, -0.3775,\n",
      "         -0.3813, -0.3852, -0.3890, -0.3930, -0.3969, -0.4010, -0.4050, -0.4091,\n",
      "         -0.4132, -0.4174, -0.4216, -0.4259, -0.4302, -0.4345, -0.4389, -0.4433,\n",
      "         -0.4478, -0.4523, -0.4569, -0.4615, -0.4662, -0.4709, -0.4757, -0.4805,\n",
      "         -0.4853, -0.4902, -0.4952, -0.5002, -0.5052, -0.5103, -0.5155, -0.5207,\n",
      "         -0.5260, -0.5313, -0.5366, -0.5421, -0.5475, -0.5531, -0.5586, -0.5643,\n",
      "         -0.5700, -0.5757, -0.5816, -0.5874, -0.5934, -0.5994, -0.6054, -0.6115,\n",
      "         -0.6177, -0.6239, -0.6302, -0.6366, -0.6430, -0.6495, -0.6561, -0.6627,\n",
      "         -0.6694, -0.6762, -0.6830, -0.6899, -0.6969, -0.7039, -0.7110, -0.7182,\n",
      "         -0.7255, -0.7328, -0.7402, -0.7477, -0.7552, -0.7629, -0.7706, -0.7783,\n",
      "         -0.7862, -0.7941, -0.8022, -0.8103, -0.8185, -0.8267, -0.8351, -0.8435,\n",
      "         -0.8520, -0.8606, -0.8693, -0.8781, -0.8870, -0.8959, -0.9050, -0.9141,\n",
      "         -0.9234, -0.9327, -0.9421, -0.9516, -0.9612, -0.9710, -0.9808, -0.9907,\n",
      "         -1.0007, -1.0108, -1.0210, -1.0313, -1.0417, -1.0522, -1.0629, -1.0736,\n",
      "         -1.0845, -1.0954, -1.1065, -1.1176, -1.1289, -1.1403, -1.1519, -1.1635,\n",
      "         -1.1752, -1.1871, -1.1991, -1.2112, -1.2235, -1.2358, -1.2483, -1.2609,\n",
      "         -1.2736, -1.2865, -1.2995, -1.3126, -1.3259, -1.3393, -1.3528, -1.3665,\n",
      "         -1.3803, -1.3942, -1.4083, -1.4225, -1.4369, -1.4514, -1.4661,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1836, -0.1854, -0.1873, -0.1892, -0.1911, -0.1930, -0.1950, -0.1969,\n",
      "         -0.1989, -0.2009, -0.2030, -0.2050, -0.2071, -0.2092, -0.2113, -0.2134,\n",
      "         -0.2156, -0.2178, -0.2200, -0.2222, -0.2244, -0.2267, -0.2290, -0.2313,\n",
      "         -0.2336, -0.2360, -0.2384, -0.2408, -0.2432, -0.2457, -0.2482, -0.2507,\n",
      "         -0.2532, -0.2558, -0.2583, -0.2609, -0.2636, -0.2662, -0.2689, -0.2716,\n",
      "         -0.2744, -0.2772, -0.2800, -0.2828, -0.2856, -0.2885, -0.2914, -0.2944,\n",
      "         -0.2974, -0.3004, -0.3034, -0.3065, -0.3096, -0.3127, -0.3158, -0.3190,\n",
      "         -0.3223, -0.3255, -0.3288, -0.3321, -0.3355, -0.3389, -0.3423, -0.3457,\n",
      "         -0.3492, -0.3528, -0.3563, -0.3599, -0.3636, -0.3672, -0.3709, -0.3747,\n",
      "         -0.3785, -0.3823, -0.3862, -0.3901, -0.3940, -0.3980, -0.4020, -0.4061,\n",
      "         -0.4102, -0.4143, -0.4185, -0.4227, -0.4270, -0.4313, -0.4357, -0.4401,\n",
      "         -0.4445, -0.4490, -0.4535, -0.4581, -0.4627, -0.4674, -0.4721, -0.4769,\n",
      "         -0.4817, -0.4866, -0.4915, -0.4965, -0.5015, -0.5066, -0.5117, -0.5168,\n",
      "         -0.5221, -0.5273, -0.5327, -0.5380, -0.5435, -0.5490, -0.5545, -0.5601,\n",
      "         -0.5658, -0.5715, -0.5773, -0.5831, -0.5890, -0.5949, -0.6009, -0.6070,\n",
      "         -0.6131, -0.6193, -0.6256, -0.6319, -0.6383, -0.6447, -0.6512, -0.6578,\n",
      "         -0.6645, -0.6712, -0.6780, -0.6848, -0.6917, -0.6987, -0.7058, -0.7129,\n",
      "         -0.7201, -0.7274, -0.7347, -0.7421, -0.7496, -0.7572, -0.7649, -0.7726,\n",
      "         -0.7804, -0.7883, -0.7962, -0.8043, -0.8124, -0.8206, -0.8289, -0.8373,\n",
      "         -0.8457, -0.8543, -0.8629, -0.8716, -0.8804, -0.8893, -0.8983, -0.9074,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4018, -0.8850], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4864e+00, -3.8552e+00, -6.7265e-01, -2.2438e+00,  2.9603e-01,\n",
      "         -2.3337e+00, -4.4885e+00, -1.2203e-01, -3.8158e+00, -2.5083e+00,\n",
      "         -3.9459e+00,  2.3137e-01, -4.8057e-01, -3.5401e+00, -3.2553e+00,\n",
      "         -2.2922e+00, -3.2449e+00, -1.6925e+00, -2.0672e+00, -8.2965e-01,\n",
      "         -1.2359e+00, -1.3015e+00, -7.1886e-01,  3.4070e-01, -1.8934e+00,\n",
      "         -3.4094e+00, -3.1804e+00, -2.4912e+00, -3.7818e+00,  9.4925e-01,\n",
      "         -1.6550e+00, -3.4903e-02, -9.3328e-01, -3.5193e+00, -1.8783e+00,\n",
      "         -2.0831e+00, -1.0061e+00, -3.1765e+00, -1.4600e+00,  1.7508e-01,\n",
      "         -6.0995e-01, -5.9154e-01, -2.4069e-01, -4.3376e+00, -1.5886e+00,\n",
      "         -3.2067e+00, -3.5076e+00, -3.2643e+00, -1.9463e-01, -8.6466e-02,\n",
      "         -3.8436e+00, -1.5137e+00, -7.1994e+00, -2.8955e+00, -3.0332e+00,\n",
      "         -8.0974e-01, -2.5032e+00, -2.9240e+00, -2.5366e+00, -1.5884e+00,\n",
      "         -3.6290e+00, -1.1758e+00, -5.8070e+00, -2.4070e+00, -3.5157e+00,\n",
      "          1.0848e+00, -1.2532e+00,  9.8546e-02, -3.7905e-01, -3.1289e+00,\n",
      "         -4.0595e+00, -5.6058e+00, -2.8178e+00, -2.7622e-01, -3.7858e+00,\n",
      "         -4.3546e+00, -2.4614e+00, -3.8929e+00, -2.3871e+00, -3.5231e+00,\n",
      "         -4.8966e+00, -2.4946e+00, -2.0884e-01, -1.1344e+00, -1.7350e+00,\n",
      "         -2.9168e+00, -1.7354e+00, -5.1141e-01, -3.5087e+00, -7.8442e-01,\n",
      "         -3.1183e+00, -4.2116e+00, -2.4446e+00, -5.0921e-01, -7.2641e-01,\n",
      "         -3.0430e+00, -2.7360e+00, -3.5837e+00, -3.2475e+00, -3.2378e+00,\n",
      "          2.9516e-02, -8.4732e-01, -3.7812e+00, -2.1371e+00, -5.2914e-01,\n",
      "         -1.7985e+00, -3.4409e+00, -3.8316e+00,  4.3872e-01, -5.2654e+00,\n",
      "         -1.8141e+00, -2.7971e+00, -4.2358e+00, -4.5374e+00,  6.5556e-01,\n",
      "         -7.3733e-01, -2.0602e+00, -1.2050e+00, -6.7853e-02, -8.6184e-01,\n",
      "         -2.5904e+00, -4.6942e+00, -2.6010e+00, -8.0683e-01, -9.8841e-01,\n",
      "         -2.7569e+00, -2.1284e+00, -7.3083e-01, -5.3402e-01, -4.2479e+00,\n",
      "         -5.3749e-01, -4.0156e-02, -3.3188e+00, -4.1008e+00, -1.1613e+00,\n",
      "         -1.9864e+00, -3.0980e+00, -2.3661e+00, -1.6496e+00, -2.9898e+00,\n",
      "         -2.8498e+00, -5.8966e+00, -1.4790e+00, -6.2722e+00, -1.7070e+00,\n",
      "         -1.5579e+00, -1.5849e+00, -1.2965e+00, -1.5475e+00, -1.2780e+00,\n",
      "         -1.4661e+00, -4.2227e+00, -4.9347e+00, -3.7926e-01, -1.8774e+00,\n",
      "         -4.6101e+00, -5.7394e+00, -4.7616e+00, -5.3813e+00, -1.8560e+00,\n",
      "         -5.7984e+00, -9.3558e-01, -1.1276e+00, -1.3044e+00, -1.1055e+00,\n",
      "         -1.2596e+00, -1.0220e+00, -1.4748e+00, -4.0907e+00, -3.9446e+00,\n",
      "          8.7447e-02, -1.5583e+00, -3.6026e+00, -4.6320e+00, -3.6490e+00,\n",
      "         -4.8709e+00, -1.3635e+00, -5.5203e+00, -1.0066e+00, -9.1747e-01,\n",
      "         -1.1694e+00, -9.0083e-01, -1.1316e+00, -8.5844e-01, -1.2994e+00,\n",
      "         -3.9718e+00, -3.7958e+00, -7.7150e-03, -1.3270e+00, -3.5932e+00,\n",
      "         -4.7285e+00, -3.5264e+00, -3.4699e+00, -4.9545e+00, -1.4900e+00,\n",
      "         -1.0566e+00, -1.3470e+00, -1.3948e+00, -1.4143e+00, -1.4159e+00,\n",
      "         -1.4406e+00, -1.4883e+00, -1.3578e+00, -1.4684e+00, -1.4873e+00,\n",
      "         -4.7213e+00, -4.7246e+00, -5.0806e+00, -5.0074e+00, -5.1912e+00,\n",
      "         -5.0976e+00, -5.0114e+00, -4.6872e+00, -4.3802e+00, -4.3172e+00,\n",
      "         -4.3886e+00, -4.3460e+00, -4.2372e+00, -4.1735e+00, -4.1433e+00,\n",
      "         -4.1435e+00, -4.1335e+00],\n",
      "        [-2.3345e+00, -3.9150e+00, -6.7385e-01, -2.2032e+00,  3.1288e-01,\n",
      "         -2.3226e+00, -4.4129e+00, -8.5090e-02, -3.6684e+00, -2.5078e+00,\n",
      "         -3.8818e+00,  2.6736e-01, -4.5065e-01, -3.5358e+00, -3.2645e+00,\n",
      "         -2.2059e+00, -3.2161e+00, -1.6890e+00, -2.0794e+00, -7.9793e-01,\n",
      "         -1.2744e+00, -1.2876e+00, -7.2070e-01,  3.3898e-01, -1.8268e+00,\n",
      "         -3.4231e+00, -3.2061e+00, -2.4190e+00, -3.7611e+00,  9.7493e-01,\n",
      "         -1.6245e+00, -6.1450e-03, -9.7732e-01, -3.4526e+00, -1.9340e+00,\n",
      "         -2.1091e+00, -9.9346e-01, -3.1559e+00, -1.4667e+00,  1.2986e-01,\n",
      "         -6.3132e-01, -5.8048e-01, -2.3313e-01, -4.3692e+00, -1.5413e+00,\n",
      "         -3.1137e+00, -3.4831e+00, -3.2056e+00, -2.7551e-01, -1.6806e-01,\n",
      "         -3.8682e+00, -1.4838e+00, -7.1997e+00, -2.8486e+00, -3.0229e+00,\n",
      "         -7.9849e-01, -2.5296e+00, -2.9236e+00, -2.5238e+00, -1.5575e+00,\n",
      "         -3.6442e+00, -1.1629e+00, -5.8285e+00, -2.3750e+00, -3.5058e+00,\n",
      "          1.1008e+00, -1.2302e+00,  1.2503e-01, -4.2495e-01, -3.0720e+00,\n",
      "         -4.0576e+00, -5.6725e+00, -2.7676e+00, -3.0947e-01, -3.8113e+00,\n",
      "         -4.3639e+00, -2.4197e+00, -3.8873e+00, -2.3998e+00, -3.5500e+00,\n",
      "         -4.9699e+00, -2.4599e+00, -2.2834e-01, -1.1270e+00, -1.7954e+00,\n",
      "         -2.8899e+00, -1.7234e+00, -4.9634e-01, -3.5397e+00, -8.1045e-01,\n",
      "         -3.1474e+00, -4.2165e+00, -2.4121e+00, -5.2692e-01, -7.2679e-01,\n",
      "         -3.0824e+00, -2.7057e+00, -3.5038e+00, -3.2726e+00, -3.2420e+00,\n",
      "          1.7276e-02, -8.4871e-01, -3.7696e+00, -2.1590e+00, -5.3568e-01,\n",
      "         -1.7978e+00, -3.3745e+00, -3.8103e+00,  3.9284e-01, -8.0944e-01,\n",
      "         -1.6167e+00, -9.0429e-01, -4.2218e+00, -4.6101e+00,  5.5848e-01,\n",
      "         -7.3972e-01, -2.0863e+00, -1.2288e+00, -9.0683e-02, -9.2724e-01,\n",
      "         -2.6315e+00, -4.6148e+00, -2.5820e+00, -8.0697e-01, -1.0559e+00,\n",
      "         -2.7833e+00, -2.1500e+00, -7.0442e-01, -3.9191e-01, -4.1674e+00,\n",
      "         -6.7465e-01, -1.6421e-01, -3.3682e+00, -4.0371e+00, -1.1377e+00,\n",
      "         -2.0009e+00, -3.0593e+00, -2.4040e+00, -1.6561e+00, -2.9893e+00,\n",
      "         -2.7021e+00, -3.2723e+00, -5.8036e+00, -1.2862e+00, -8.5332e-01,\n",
      "         -9.3930e-01, -7.8124e-01, -8.6206e-01, -8.0118e-01, -9.0732e-01,\n",
      "         -4.9219e+00, -5.0144e+00, -5.4211e+00, -5.0659e+00, -4.9562e+00,\n",
      "         -4.6846e+00, -4.5678e+00, -4.6549e+00, -4.6485e+00, -4.6338e+00,\n",
      "         -4.7276e+00, -4.7897e+00, -4.8463e+00, -4.8622e+00, -4.8123e+00,\n",
      "         -4.7087e+00, -4.6307e+00, -4.5981e+00, -4.5983e+00, -4.5893e+00,\n",
      "         -4.5728e+00, -4.5756e+00, -4.6069e+00, -4.6506e+00, -4.6817e+00,\n",
      "         -4.6878e+00, -4.6466e+00, -4.5994e+00, -4.5966e+00, -4.6104e+00,\n",
      "         -4.6234e+00, -4.6250e+00, -4.5838e+00, -4.5494e+00, -4.5356e+00,\n",
      "         -4.5347e+00, -4.5569e+00, -4.5864e+00, -4.6027e+00, -4.6043e+00,\n",
      "         -4.6018e+00, -4.6080e+00, -4.6209e+00, -4.6438e+00, -4.6554e+00,\n",
      "         -4.6711e+00, -4.6909e+00, -4.6915e+00, -4.7014e+00, -4.7080e+00,\n",
      "         -4.7056e+00, -4.7122e+00, -4.7056e+00, -4.6918e+00, -4.6981e+00,\n",
      "         -4.6970e+00, -4.6814e+00, -4.6622e+00, -4.6338e+00, -4.5778e+00,\n",
      "         -4.5558e+00, -4.5021e+00, -4.4713e+00, -4.4136e+00, -4.3842e+00,\n",
      "         -4.2979e+00, -4.2239e+00, -4.1965e+00, -4.1736e+00, -4.1562e+00,\n",
      "         -4.1523e+00, -4.1513e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3871, -0.8574], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.9181, -0.8798, -0.7935, -0.9241],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0599e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1780, -0.1798, -0.1816,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1367, -0.1381, -0.1395,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1870, -0.1888, -0.1907,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.8955, -0.8581, -0.7740, -0.9013],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7990, -3.9048, -0.7586,  ..., -5.4983, -5.5178, -5.5208],\n",
      "        [-2.8589, -3.8971, -0.7677,  ..., -5.6890, -5.6899, -5.6734],\n",
      "        [-2.7986, -3.8894, -0.7474,  ..., -5.6172, -5.6363, -5.6382],\n",
      "        ...,\n",
      "        [-2.3428, -3.9194, -0.6704,  ..., -4.1964, -4.1906, -4.1904],\n",
      "        [-2.4316, -3.8962, -0.6611,  ..., -4.6438, -4.6268, -4.6084],\n",
      "        [-2.3320, -3.9179, -0.6710,  ..., -4.1559, -4.1521, -4.1513]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7958,  0.8063,  0.7880,  0.7966, -0.9181, -0.9001, -0.5642, -0.8948],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.4870, -0.9523, -1.8025,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4400e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1731, -0.1748, -0.1766,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1926, -0.1946, -0.1966,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.4218, -0.9288, -1.7668,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8698, -3.8900, -0.7724,  ..., -5.6694, -5.6571, -5.6463],\n",
      "        [-2.5009, -3.8586, -0.6772,  ..., -4.1745, -4.1675, -4.1571],\n",
      "        [-2.3307, -3.9188, -0.6698,  ..., -4.1546, -4.1495, -4.1514],\n",
      "        ...,\n",
      "        [-2.8025, -3.8946, -0.7659,  ..., -5.3501, -5.3683, -5.3697],\n",
      "        [-2.9738, -3.8598, -0.7932,  ..., -4.9970, -4.9352, -4.9022],\n",
      "        [-2.9691, -3.8674, -0.7968,  ..., -5.0276, -4.9582, -4.9209]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7829, -1.4244, -0.9241, -1.4949,  0.8017,  0.7812,  0.5883,  0.5761],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9464,  0.9900, -1.1000,  0.9900, -1.0052, -1.0003, -0.9464,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.5824e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1915, -0.1934, -0.1953,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [-0.2024, -0.2044, -0.2065,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1915, -0.1934, -0.1953,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9230,  0.9608, -1.0518,  0.9656, -0.9804, -0.9756, -0.9230,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3388, -3.9101, -0.6721,  ..., -4.1804, -4.1772, -4.1779],\n",
      "        [-2.9646, -3.8508, -0.7902,  ..., -5.0756, -4.9903, -4.9230],\n",
      "        [-2.5302, -3.8474, -0.7008,  ..., -1.0667, -1.1140, -1.3417],\n",
      "        ...,\n",
      "        [-2.3329, -3.9080, -0.6602,  ..., -4.1998, -4.1924, -4.1906],\n",
      "        [-2.3388, -3.9101, -0.6721,  ..., -4.1804, -4.1772, -4.1779],\n",
      "        [-2.8849, -3.8865, -0.7726,  ..., -5.5727, -5.5636, -5.5540]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9510,  0.7104, -1.1293,  0.8130, -0.9807, -0.9891, -0.9510,  0.8186],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.9602,  0.9900, -0.8702,  0.9900, -0.8827,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6628e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1943, -0.1962, -0.1982,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1520, -0.1536, -0.1551,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.9365,  0.9656, -0.8488,  0.9656, -0.8609,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8999, -3.8867, -0.7798,  ..., -5.7367, -5.7338, -5.7233],\n",
      "        [-2.8909, -3.8776, -0.7729,  ..., -5.6738, -5.6622, -5.6501],\n",
      "        [-2.3439, -3.8991, -0.6714,  ..., -4.1900, -4.1845, -4.1843],\n",
      "        ...,\n",
      "        [-2.8315, -3.8945, -0.7637,  ..., -5.5141, -5.5289, -5.5317],\n",
      "        [-2.4485, -3.8833, -0.6746,  ..., -4.6964, -4.6925, -4.6824],\n",
      "        [-2.9861, -3.8476, -0.7905,  ..., -5.0127, -4.9519, -4.9187]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8638,  0.8379, -0.9570,  0.8626, -0.6028,  0.8603, -0.5970,  0.6467],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9991,  0.9900,  0.9900, -1.0447,  0.9900,  0.9900, -1.0271, -1.0264],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0469e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2021, -0.2042, -0.2062,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2078, -0.2099, -0.2120,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2076, -0.2097, -0.2118,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9744,  0.9656,  0.9656, -1.0189,  0.9656,  0.9656, -1.0018, -1.0010],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3607, -3.9002, -0.6647,  ..., -4.1127, -4.1103, -4.1122],\n",
      "        [-2.9953, -3.8457, -0.7937,  ..., -5.0498, -4.9796, -4.9429],\n",
      "        [-2.9028, -3.8788, -0.7698,  ..., -5.7020, -5.6950, -5.6839],\n",
      "        ...,\n",
      "        [-2.9959, -3.8360, -0.7899,  ..., -4.9963, -4.9349, -4.9059],\n",
      "        [-2.3473, -3.8980, -0.6603,  ..., -4.1455, -4.1418, -4.1432],\n",
      "        [-2.3536, -3.9020, -0.6669,  ..., -4.1099, -4.1047, -4.1054]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9300,  0.6728,  0.9128, -0.9999,  0.8801,  0.7169, -1.0511, -0.9980],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9905,  0.9900,  0.9900, -1.8963, -1.0659,  0.9900, -1.0360],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5346e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2004, -0.2024, -0.2045,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2156, -0.2178, -0.2200,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2096, -0.2117, -0.2138,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9661,  0.9656,  0.9656, -1.8587, -1.0396,  0.9656, -1.0105],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9230, -3.8641, -0.7725,  ..., -5.6329, -5.6221, -5.6106],\n",
      "        [-2.3754, -3.8850, -0.6771,  ..., -4.1975, -4.1913, -4.1931],\n",
      "        [-2.9268, -3.8647, -0.7735,  ..., -5.5761, -5.5640, -5.5565],\n",
      "        ...,\n",
      "        [-2.3563, -3.8870, -0.6665,  ..., -4.1041, -4.0975, -4.0989],\n",
      "        [-2.9205, -3.8614, -0.7722,  ..., -5.6236, -5.6189, -5.6040],\n",
      "        [-2.3785, -3.8894, -0.6697,  ..., -4.1210, -4.1182, -4.1228]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9572, -0.9623,  0.9431,  0.9369, -1.3880, -0.9854,  0.8563, -0.9854],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.7139, -1.0400], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3331, -0.3364, -0.3398, -0.3432, -0.3467, -0.3502, -0.3538, -0.3573,\n",
      "         -0.3609, -0.3646, -0.3683, -0.3720, -0.3757, -0.3795, -0.3834, -0.3872,\n",
      "         -0.3912, -0.3951, -0.3991, -0.4031, -0.4072, -0.4113, -0.4155, -0.4197,\n",
      "         -0.4239, -0.4282, -0.4325, -0.4369, -0.4413, -0.4458, -0.4503, -0.4548,\n",
      "         -0.4594, -0.4640, -0.4687, -0.4735, -0.4782, -0.4831, -0.4880, -0.4929,\n",
      "         -0.4979, -0.5029, -0.5080, -0.5131, -0.5183, -0.5235, -0.5288, -0.5341,\n",
      "         -0.5395, -0.5450, -0.5505, -0.5561, -0.5617, -0.5673, -0.5731, -0.5789,\n",
      "         -0.5847, -0.5906, -0.5966, -0.6026, -0.6087, -0.6148, -0.6211, -0.6273,\n",
      "         -0.6337, -0.6401, -0.6465, -0.6531, -0.6597, -0.6663, -0.6731, -0.6799,\n",
      "         -0.6867, -0.6937, -0.7007, -0.7077, -0.7149, -0.7221, -0.7294, -0.7368,\n",
      "         -0.7442, -0.7517, -0.7593, -0.7670, -0.7747, -0.7826, -0.7905, -0.7985,\n",
      "         -0.8065, -0.8147, -0.8229, -0.8312, -0.8396, -0.8481, -0.8567, -0.8653,\n",
      "         -0.8740, -0.8829, -0.8918, -0.9008, -0.9099, -0.9191, -0.9284, -0.9378,\n",
      "         -0.9472, -0.9568, -0.9665, -0.9762, -0.9861, -0.9960, -1.0061, -1.0163,\n",
      "         -1.0265, -1.0369, -1.0474, -1.0580, -1.0686, -1.0794, -1.0903, -1.1014,\n",
      "         -1.1125, -1.1237, -1.1351, -1.1465, -1.1581, -1.1698, -1.1816, -1.1936,\n",
      "         -1.2056, -1.2178, -1.2301, -1.2425, -1.2551, -1.2677, -1.2806, -1.2935,\n",
      "         -1.3066, -1.3198, -1.3331, -1.3465, -1.3601, -1.3739, -1.3878, -1.4018,\n",
      "         -1.4159, -1.4302, -1.4447, -1.4593, -1.4740, -1.4889, -1.5040, -1.5191,\n",
      "         -1.5345, -1.5500, -1.5656, -1.5815, -1.5974, -1.6136, -1.6299, -1.6463,\n",
      "         -1.6630, -1.6798, -1.6967, -1.7139,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2104, -0.2125, -0.2147, -0.2168, -0.2190, -0.2212, -0.2235, -0.2257,\n",
      "         -0.2280, -0.2303, -0.2326, -0.2350, -0.2374, -0.2398, -0.2422, -0.2446,\n",
      "         -0.2471, -0.2496, -0.2521, -0.2547, -0.2572, -0.2598, -0.2625, -0.2651,\n",
      "         -0.2678, -0.2705, -0.2732, -0.2760, -0.2788, -0.2816, -0.2844, -0.2873,\n",
      "         -0.2902, -0.2931, -0.2961, -0.2991, -0.3021, -0.3052, -0.3083, -0.3114,\n",
      "         -0.3145, -0.3177, -0.3209, -0.3241, -0.3274, -0.3307, -0.3341, -0.3374,\n",
      "         -0.3408, -0.3443, -0.3478, -0.3513, -0.3548, -0.3584, -0.3620, -0.3657,\n",
      "         -0.3694, -0.3731, -0.3769, -0.3807, -0.3845, -0.3884, -0.3923, -0.3963,\n",
      "         -0.4003, -0.4043, -0.4084, -0.4126, -0.4167, -0.4209, -0.4252, -0.4295,\n",
      "         -0.4338, -0.4382, -0.4426, -0.4471, -0.4516, -0.4562, -0.4608, -0.4654,\n",
      "         -0.4701, -0.4749, -0.4797, -0.4845, -0.4894, -0.4944, -0.4994, -0.5044,\n",
      "         -0.5095, -0.5146, -0.5198, -0.5251, -0.5304, -0.5358, -0.5412, -0.5466,\n",
      "         -0.5522, -0.5577, -0.5634, -0.5691, -0.5748, -0.5806, -0.5865, -0.5924,\n",
      "         -0.5984, -0.6044, -0.6105, -0.6167, -0.6229, -0.6292, -0.6356, -0.6420,\n",
      "         -0.6485, -0.6550, -0.6617, -0.6683, -0.6751, -0.6819, -0.6888, -0.6957,\n",
      "         -0.7028, -0.7099, -0.7170, -0.7243, -0.7316, -0.7390, -0.7465, -0.7540,\n",
      "         -0.7616, -0.7693, -0.7771, -0.7849, -0.7929, -0.8009, -0.8090, -0.8171,\n",
      "         -0.8254, -0.8337, -0.8421, -0.8506, -0.8592, -0.8679, -0.8767, -0.8855,\n",
      "         -0.8945, -0.9035, -0.9126, -0.9219, -0.9312, -0.9406, -0.9501, -0.9597,\n",
      "         -0.9694, -0.9792, -0.9891, -0.9990, -1.0091, -1.0193, -1.0296, -1.0400,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6388, -1.0144], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3238e+00, -3.8844e+00, -6.3234e-01, -2.1736e+00,  2.9289e-01,\n",
      "         -2.3255e+00, -4.3453e+00, -9.3793e-02, -3.6599e+00, -2.4844e+00,\n",
      "         -3.9230e+00,  2.4087e-01, -5.3424e-01, -3.5379e+00, -3.1879e+00,\n",
      "         -2.1896e+00, -3.2185e+00, -1.7009e+00, -2.0627e+00, -7.9638e-01,\n",
      "         -1.2225e+00, -1.2709e+00, -7.0991e-01,  3.6885e-01, -1.8324e+00,\n",
      "         -3.4307e+00, -3.1257e+00, -2.4157e+00, -3.7666e+00,  9.5993e-01,\n",
      "         -1.6608e+00, -3.3176e-03, -9.5951e-01, -3.5072e+00, -1.9136e+00,\n",
      "         -2.2548e+00, -1.0625e+00, -3.1628e+00, -1.4484e+00,  1.5399e-01,\n",
      "         -6.7982e-01, -6.3303e-01, -2.4575e-01, -4.3339e+00, -1.5533e+00,\n",
      "         -3.0980e+00, -3.4896e+00, -3.2691e+00, -2.3476e-01, -9.5067e-02,\n",
      "         -3.8550e+00, -1.4585e+00, -7.1415e+00, -2.8324e+00, -3.0306e+00,\n",
      "         -8.3184e-01, -2.4819e+00, -2.9209e+00, -2.4906e+00, -1.5712e+00,\n",
      "         -3.6549e+00, -1.1315e+00, -5.7692e+00, -2.3665e+00, -3.5085e+00,\n",
      "          1.0749e+00, -1.2880e+00,  1.0295e-01, -4.1511e-01, -3.1343e+00,\n",
      "         -4.0154e+00, -5.5736e+00, -2.7579e+00, -3.0076e-01, -3.8118e+00,\n",
      "         -4.2904e+00, -2.4141e+00, -3.8981e+00, -2.3730e+00, -3.5562e+00,\n",
      "         -4.8726e+00, -2.4582e+00, -2.3851e-01, -1.1355e+00, -1.7590e+00,\n",
      "         -2.8732e+00, -1.7620e+00, -5.0787e-01, -3.4802e+00, -8.5475e-01,\n",
      "         -3.1598e+00, -4.1536e+00, -2.4148e+00, -5.4018e-01, -7.3719e-01,\n",
      "         -3.0696e+00, -2.7236e+00, -3.4915e+00, -3.2589e+00, -3.2224e+00,\n",
      "          1.1339e-02, -8.6148e-01, -3.7437e+00, -2.1751e+00, -5.0990e-01,\n",
      "         -1.7895e+00, -3.3772e+00, -3.7949e+00,  4.0156e-01, -5.2843e+00,\n",
      "         -2.0046e+00, -2.8085e+00, -4.1613e+00, -4.5052e+00,  6.3505e-01,\n",
      "         -7.5621e-01, -2.0744e+00, -1.2041e+00, -6.5612e-02, -8.7893e-01,\n",
      "         -2.5440e+00, -4.6685e+00, -2.6058e+00, -8.0692e-01, -1.0097e+00,\n",
      "         -2.7201e+00, -2.1602e+00, -7.3056e-01, -5.3032e-01, -4.2662e+00,\n",
      "         -5.8912e-01, -4.0905e-02, -3.3241e+00, -3.9819e+00, -1.1872e+00,\n",
      "         -1.9697e+00, -3.1013e+00, -2.3260e+00, -1.6647e+00, -2.9843e+00,\n",
      "         -2.7273e+00, -3.3645e+00, -5.8353e+00, -1.1904e+00, -1.4813e+00,\n",
      "         -1.5653e+00, -1.5191e+00, -1.5562e+00, -1.6056e+00, -1.5450e+00,\n",
      "         -1.5346e+00, -1.4505e+00, -1.4873e+00, -1.3885e+00, -4.8981e+00,\n",
      "         -4.9691e+00, -5.2955e+00, -5.0622e+00, -5.1754e+00, -5.0736e+00,\n",
      "         -5.0127e+00, -4.9990e+00, -4.8978e+00, -4.9158e+00, -4.9814e+00,\n",
      "         -4.9665e+00, -4.8876e+00, -4.7855e+00, -4.6928e+00, -4.6231e+00,\n",
      "         -4.5641e+00, -4.4985e+00, -4.4306e+00, -4.3770e+00, -4.3518e+00,\n",
      "         -4.3509e+00, -4.3699e+00, -4.3950e+00, -4.4251e+00, -4.4538e+00,\n",
      "         -4.4698e+00, -4.4593e+00, -4.4281e+00, -4.4187e+00, -4.4184e+00,\n",
      "         -4.4358e+00, -4.4707e+00, -4.5089e+00, -4.5382e+00, -4.5530e+00,\n",
      "         -4.5571e+00, -4.5558e+00, -4.5527e+00, -4.5449e+00, -4.5390e+00,\n",
      "         -4.5417e+00, -4.5526e+00, -4.5573e+00, -4.5722e+00, -4.5726e+00,\n",
      "         -4.5786e+00, -4.5899e+00, -4.5908e+00, -4.5963e+00, -4.6087e+00,\n",
      "         -4.6182e+00, -4.6196e+00, -4.6258e+00, -4.6058e+00, -4.5602e+00,\n",
      "         -4.5409e+00, -4.5239e+00, -4.5059e+00, -4.4879e+00, -4.4770e+00,\n",
      "         -4.4494e+00, -4.4363e+00, -4.4056e+00, -4.3924e+00, -4.3684e+00,\n",
      "         -4.3367e+00, -4.3157e+00],\n",
      "        [-2.3856e+00, -3.8827e+00, -6.6893e-01, -2.2060e+00,  2.7522e-01,\n",
      "         -2.3479e+00, -4.3879e+00, -1.2945e-01, -3.7407e+00, -2.5461e+00,\n",
      "         -3.9335e+00,  2.1115e-01, -5.4790e-01, -3.5651e+00, -3.1855e+00,\n",
      "         -2.2153e+00, -3.2510e+00, -1.7257e+00, -2.0661e+00, -8.4920e-01,\n",
      "         -1.2527e+00, -1.3232e+00, -7.5569e-01,  3.6657e-01, -1.8649e+00,\n",
      "         -3.4747e+00, -3.1348e+00, -2.4335e+00, -3.7610e+00,  9.7405e-01,\n",
      "         -1.6735e+00, -6.0907e-02, -9.8674e-01, -3.5091e+00, -1.9450e+00,\n",
      "         -2.2232e+00, -1.0142e+00, -3.1894e+00, -1.4893e+00,  8.9473e-02,\n",
      "         -7.0361e-01, -6.0672e-01, -2.5253e-01, -4.3301e+00, -1.5828e+00,\n",
      "         -3.1731e+00, -3.5415e+00, -3.2729e+00, -1.2006e-01,  2.6333e-02,\n",
      "         -3.8917e+00, -1.4816e+00, -7.1433e+00, -2.8522e+00, -3.0563e+00,\n",
      "         -8.5298e-01, -2.5075e+00, -2.9120e+00, -2.5275e+00, -1.6028e+00,\n",
      "         -3.6920e+00, -1.1592e+00, -5.7820e+00, -2.3768e+00, -3.5075e+00,\n",
      "          1.0921e+00, -1.2987e+00,  4.2638e-02, -4.3549e-01, -3.1429e+00,\n",
      "         -4.0704e+00, -5.6014e+00, -2.7780e+00, -2.9215e-01, -3.8552e+00,\n",
      "         -4.2843e+00, -2.4327e+00, -3.9247e+00, -2.4125e+00, -3.5965e+00,\n",
      "         -4.9039e+00, -2.4727e+00, -2.5239e-01, -1.1940e+00, -1.7859e+00,\n",
      "         -2.9170e+00, -1.7832e+00, -5.6508e-01, -3.5515e+00, -8.9125e-01,\n",
      "         -3.2055e+00, -4.1520e+00, -2.4277e+00, -5.5277e-01, -8.0121e-01,\n",
      "         -3.0813e+00, -2.7598e+00, -3.5562e+00, -3.2848e+00, -3.2642e+00,\n",
      "         -1.3280e-02, -9.1828e-01, -3.7433e+00, -2.1832e+00, -5.1588e-01,\n",
      "         -1.8385e+00, -3.4338e+00, -3.8305e+00,  4.1015e-01, -1.5118e+00,\n",
      "         -1.9318e+00, -1.0268e+00, -4.4736e+00, -4.5584e+00,  4.7662e-01,\n",
      "         -8.8308e-01, -2.1118e+00, -1.2733e+00, -1.0265e-01, -1.0019e+00,\n",
      "         -2.6300e+00, -4.6309e+00, -2.5990e+00, -8.1446e-01, -1.0835e+00,\n",
      "         -2.7954e+00, -2.2197e+00, -7.4842e-01, -4.5445e-01, -4.2647e+00,\n",
      "         -6.3445e-01,  4.5859e-02, -3.3985e+00, -3.9932e+00, -1.1888e+00,\n",
      "         -1.9853e+00, -3.0616e+00, -2.3634e+00, -1.7059e+00, -2.9080e+00,\n",
      "         -2.6631e+00, -3.3159e+00, -5.6396e+00, -1.2232e+00, -1.1837e+00,\n",
      "         -1.0785e+00, -9.6120e-01, -1.1050e+00, -1.0004e+00, -9.7137e-01,\n",
      "         -4.9985e+00, -5.0140e+00, -5.4404e+00, -5.0701e+00, -4.9484e+00,\n",
      "         -4.6975e+00, -4.5498e+00, -4.5982e+00, -4.5727e+00, -4.6024e+00,\n",
      "         -4.7390e+00, -4.8280e+00, -4.8895e+00, -4.9002e+00, -4.8550e+00,\n",
      "         -4.7704e+00, -4.7063e+00, -4.6818e+00, -4.6909e+00, -4.6953e+00,\n",
      "         -4.6944e+00, -4.7143e+00, -4.7474e+00, -4.7875e+00, -4.8139e+00,\n",
      "         -4.8211e+00, -4.7830e+00, -4.7425e+00, -4.7425e+00, -4.7457e+00,\n",
      "         -4.7538e+00, -4.7494e+00, -4.7102e+00, -4.6810e+00, -4.6812e+00,\n",
      "         -4.6831e+00, -4.7040e+00, -4.7290e+00, -4.7391e+00, -4.7358e+00,\n",
      "         -4.7254e+00, -4.7256e+00, -4.7291e+00, -4.7383e+00, -4.7369e+00,\n",
      "         -4.7423e+00, -4.7476e+00, -4.7431e+00, -4.7482e+00, -4.7463e+00,\n",
      "         -4.7342e+00, -4.7399e+00, -4.7293e+00, -4.7177e+00, -4.7156e+00,\n",
      "         -4.7065e+00, -4.6907e+00, -4.6746e+00, -4.6491e+00, -4.5893e+00,\n",
      "         -4.5772e+00, -4.5336e+00, -4.5071e+00, -4.4334e+00, -4.3907e+00,\n",
      "         -4.3021e+00, -4.2325e+00, -4.1911e+00, -4.1653e+00, -4.1512e+00,\n",
      "         -4.1478e+00, -4.1518e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5133, -1.0500], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.0156,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.0171],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.7460e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(3.1187e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2055, -0.2075, -0.2096,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2057, -0.2078, -0.2099,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9905,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.9920],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8920, -3.8648, -0.7713,  ..., -5.4054, -5.4229, -5.4219],\n",
      "        [-2.4149, -3.8804, -0.6794,  ..., -4.1890, -4.1884, -4.1897],\n",
      "        [-2.8977, -3.8563, -0.7541,  ..., -5.6720, -5.6870, -5.6930],\n",
      "        ...,\n",
      "        [-2.9637, -3.8523, -0.7717,  ..., -5.6787, -5.6607, -5.6501],\n",
      "        [-2.9586, -3.8532, -0.7605,  ..., -5.6876, -5.6776, -5.6724],\n",
      "        [-2.4160, -3.8838, -0.6757,  ..., -4.1962, -4.1930, -4.1959]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9865, -0.9667,  0.9953,  1.0220,  0.8345,  0.9827,  0.9985, -0.9641],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0543, -1.7237,  0.9900, -1.9164, -1.0835, -1.0595,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.1248e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2133, -0.2154, -0.2176,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3350, -0.3383, -0.3418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2143, -0.2165, -0.2187,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0282, -1.6481,  0.9656, -1.8784, -1.0567, -1.0333,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4201, -3.8852, -0.6696,  ..., -4.1284, -4.1299, -4.1326],\n",
      "        [-2.3433, -3.8872, -0.6303,  ..., -4.3743, -4.3428, -4.3238],\n",
      "        [-2.9645, -3.8556, -0.7678,  ..., -5.6778, -5.6603, -5.6457],\n",
      "        ...,\n",
      "        [-2.4090, -3.8879, -0.6726,  ..., -4.1615, -4.1577, -4.1602],\n",
      "        [-2.9631, -3.8640, -0.7646,  ..., -5.7098, -5.6981, -5.6870],\n",
      "        [-3.0400, -3.8206, -0.7773,  ..., -5.1484, -5.0599, -4.9941]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9925, -1.5772,  0.9928, -1.3487, -0.9908, -0.9598,  1.0360,  0.8776],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0597,  0.9900, -1.0767,  0.9900,  0.9900, -1.5485, -1.0344, -1.0141],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8214e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2144, -0.2166, -0.2187,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2178, -0.2200, -0.2222,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1802, -0.1821, -0.1839,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2093, -0.2114, -0.2135,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2052, -0.2072, -0.2093,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0336,  0.9656, -1.0501,  0.9656,  0.9656, -1.4807, -1.0088, -0.9891],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4164, -3.8916, -0.6654,  ..., -4.1291, -4.1280, -4.1297],\n",
      "        [-2.9630, -3.8640, -0.7631,  ..., -5.5861, -5.5709, -5.5627],\n",
      "        [-2.4103, -3.8907, -0.6702,  ..., -4.1338, -4.1290, -4.1295],\n",
      "        ...,\n",
      "        [-2.5807, -3.8403, -0.6758,  ..., -4.2314, -4.2209, -4.2044],\n",
      "        [-2.4216, -3.8955, -0.6638,  ..., -4.1226, -4.1198, -4.1219],\n",
      "        [-2.4150, -3.8875, -0.6725,  ..., -4.2066, -4.2026, -4.2025]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0008,  1.0123, -1.0167,  1.0165,  0.8032, -1.5439, -0.9505, -0.9734],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -1.0825, -1.9210, -1.0647,  0.9900, -0.9351],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5042e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.2218e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2154, -0.2176, -0.2198,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1611, -0.1627, -0.1643,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -1.0558, -1.8830, -1.0384,  0.9656, -0.9120],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0480, -3.8363, -0.7717,  ..., -5.0841, -5.0199, -4.9851],\n",
      "        [-2.9597, -3.8687, -0.7578,  ..., -5.6208, -5.6132, -5.5994],\n",
      "        [-3.0480, -3.8363, -0.7717,  ..., -5.0841, -5.0199, -4.9851],\n",
      "        ...,\n",
      "        [-2.4037, -3.8976, -0.6557,  ..., -4.1540, -4.1512, -4.1519],\n",
      "        [-2.8898, -3.8794, -0.7588,  ..., -5.4048, -5.4222, -5.4212],\n",
      "        [-2.5105, -3.8775, -0.6703,  ..., -4.7376, -4.7294, -4.7184]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8229,  0.9249,  0.8229, -1.0239, -1.3915, -1.0704,  0.9911, -0.6217],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000,  0.9900,  0.9900,  0.9900, -1.0717,  0.9900, -1.0771,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0464e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2179, -0.2201, -0.2223,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518,  0.9656,  0.9656,  0.9656, -1.0453,  0.9656, -1.0505,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5934, -3.8481, -0.6854,  ..., -1.0943, -1.1404, -1.3765],\n",
      "        [-3.0430, -3.8537, -0.7700,  ..., -5.1202, -5.0525, -5.0151],\n",
      "        [-2.9545, -3.8820, -0.7521,  ..., -5.5925, -5.5797, -5.5689],\n",
      "        ...,\n",
      "        [-2.8914, -3.8993, -0.7464,  ..., -5.5336, -5.5537, -5.5546],\n",
      "        [-2.3939, -3.9064, -0.6473,  ..., -4.2138, -4.2058, -4.2024],\n",
      "        [-2.9510, -3.8879, -0.7492,  ..., -5.7111, -5.6987, -5.6877]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1687,  0.7793,  0.9846,  0.9827, -0.9874,  0.9868, -1.0351,  1.0031],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0574,  0.9900,  0.9900,  0.9900, -0.9117,  0.9900, -1.0110, -1.0202],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4767e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2139, -0.2161, -0.2183,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2045, -0.2066, -0.2087,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2064, -0.2085, -0.2106,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0314,  0.9656,  0.9656,  0.9656, -0.8892,  0.9656, -0.9861, -0.9950],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3938, -3.9219, -0.6511,  ..., -4.1196, -4.1142, -4.1140],\n",
      "        [-2.8762, -3.8915, -0.7275,  ..., -5.6488, -5.6681, -5.6752],\n",
      "        [-2.9488, -3.8971, -0.7521,  ..., -5.7506, -5.7429, -5.7314],\n",
      "        ...,\n",
      "        [-2.9375, -3.8901, -0.7471,  ..., -5.6327, -5.6215, -5.6095],\n",
      "        [-2.3948, -3.9121, -0.6560,  ..., -4.2061, -4.2015, -4.2019],\n",
      "        [-2.3906, -3.9170, -0.6533,  ..., -4.1915, -4.1873, -4.1890]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0458,  0.9485,  0.9612,  0.9625, -0.6548,  0.9563, -1.0054, -1.0066],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0434, -0.9000], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(3.4838e-09, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(3.7194e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2111, -0.2132, -0.2154, -0.2175, -0.2197, -0.2220, -0.2242, -0.2265,\n",
      "         -0.2288, -0.2311, -0.2334, -0.2358, -0.2381, -0.2405, -0.2430, -0.2454,\n",
      "         -0.2479, -0.2504, -0.2529, -0.2555, -0.2581, -0.2607, -0.2633, -0.2660,\n",
      "         -0.2687, -0.2714, -0.2741, -0.2769, -0.2797, -0.2825, -0.2854, -0.2882,\n",
      "         -0.2912, -0.2941, -0.2971, -0.3001, -0.3031, -0.3062, -0.3093, -0.3124,\n",
      "         -0.3155, -0.3187, -0.3219, -0.3252, -0.3285, -0.3318, -0.3351, -0.3385,\n",
      "         -0.3420, -0.3454, -0.3489, -0.3524, -0.3560, -0.3596, -0.3632, -0.3669,\n",
      "         -0.3706, -0.3743, -0.3781, -0.3819, -0.3858, -0.3897, -0.3936, -0.3976,\n",
      "         -0.4016, -0.4057, -0.4098, -0.4139, -0.4181, -0.4223, -0.4266, -0.4309,\n",
      "         -0.4352, -0.4396, -0.4441, -0.4486, -0.4531, -0.4577, -0.4623, -0.4670,\n",
      "         -0.4717, -0.4764, -0.4812, -0.4861, -0.4910, -0.4960, -0.5010, -0.5060,\n",
      "         -0.5112, -0.5163, -0.5215, -0.5268, -0.5321, -0.5375, -0.5429, -0.5484,\n",
      "         -0.5540, -0.5596, -0.5652, -0.5709, -0.5767, -0.5825, -0.5884, -0.5943,\n",
      "         -0.6003, -0.6064, -0.6125, -0.6187, -0.6250, -0.6313, -0.6377, -0.6441,\n",
      "         -0.6506, -0.6572, -0.6638, -0.6705, -0.6773, -0.6841, -0.6910, -0.6980,\n",
      "         -0.7051, -0.7122, -0.7194, -0.7267, -0.7340, -0.7414, -0.7489, -0.7565,\n",
      "         -0.7641, -0.7718, -0.7796, -0.7875, -0.7954, -0.8035, -0.8116, -0.8198,\n",
      "         -0.8281, -0.8364, -0.8449, -0.8534, -0.8620, -0.8707, -0.8795, -0.8884,\n",
      "         -0.8974, -0.9065, -0.9156, -0.9249, -0.9342, -0.9437, -0.9532, -0.9628,\n",
      "         -0.9725, -0.9824, -0.9923, -1.0023, -1.0124, -1.0227, -1.0330, -1.0434,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1550, -0.1566, -0.1582, -0.1598, -0.1614, -0.1630, -0.1647, -0.1663,\n",
      "         -0.1680, -0.1697, -0.1714, -0.1731, -0.1749, -0.1767, -0.1784, -0.1802,\n",
      "         -0.1821, -0.1839, -0.1858, -0.1876, -0.1895, -0.1914, -0.1934, -0.1953,\n",
      "         -0.1973, -0.1993, -0.2013, -0.2033, -0.2054, -0.2075, -0.2096, -0.2117,\n",
      "         -0.2138, -0.2160, -0.2182, -0.2204, -0.2226, -0.2248, -0.2271, -0.2294,\n",
      "         -0.2317, -0.2341, -0.2364, -0.2388, -0.2412, -0.2437, -0.2461, -0.2486,\n",
      "         -0.2511, -0.2537, -0.2562, -0.2588, -0.2614, -0.2641, -0.2667, -0.2694,\n",
      "         -0.2721, -0.2749, -0.2777, -0.2805, -0.2833, -0.2862, -0.2891, -0.2920,\n",
      "         -0.2949, -0.2979, -0.3009, -0.3040, -0.3070, -0.3101, -0.3133, -0.3164,\n",
      "         -0.3196, -0.3229, -0.3261, -0.3294, -0.3327, -0.3361, -0.3395, -0.3429,\n",
      "         -0.3464, -0.3499, -0.3534, -0.3570, -0.3606, -0.3642, -0.3679, -0.3716,\n",
      "         -0.3754, -0.3792, -0.3830, -0.3869, -0.3908, -0.3947, -0.3987, -0.4027,\n",
      "         -0.4068, -0.4109, -0.4151, -0.4193, -0.4235, -0.4278, -0.4321, -0.4365,\n",
      "         -0.4409, -0.4453, -0.4498, -0.4544, -0.4590, -0.4636, -0.4683, -0.4730,\n",
      "         -0.4778, -0.4826, -0.4875, -0.4924, -0.4974, -0.5024, -0.5075, -0.5126,\n",
      "         -0.5178, -0.5230, -0.5283, -0.5336, -0.5390, -0.5445, -0.5500, -0.5555,\n",
      "         -0.5611, -0.5668, -0.5725, -0.5783, -0.5842, -0.5901, -0.5960, -0.6020,\n",
      "         -0.6081, -0.6143, -0.6205, -0.6267, -0.6331, -0.6395, -0.6459, -0.6524,\n",
      "         -0.6590, -0.6657, -0.6724, -0.6792, -0.6861, -0.6930, -0.7000, -0.7071,\n",
      "         -0.7142, -0.7214, -0.7287, -0.7361, -0.7435, -0.7510, -0.7586, -0.7663,\n",
      "         -0.7740, -0.7818, -0.7897, -0.7977, -0.8058, -0.8139, -0.8221, -0.8304,\n",
      "         -0.8388, -0.8473, -0.8558, -0.8645, -0.8732, -0.8820, -0.8910, -0.9000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0177, -0.8778], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3759e+00, -3.9244e+00, -6.3689e-01, -2.1500e+00,  3.1304e-01,\n",
      "         -2.3282e+00, -4.3397e+00, -9.8728e-02, -3.7244e+00, -2.5188e+00,\n",
      "         -3.9297e+00,  2.5117e-01, -4.9313e-01, -3.5444e+00, -3.2282e+00,\n",
      "         -2.2050e+00, -3.2247e+00, -1.7033e+00, -2.0598e+00, -7.8423e-01,\n",
      "         -1.2155e+00, -1.2816e+00, -7.0375e-01,  3.6930e-01, -1.8071e+00,\n",
      "         -3.4540e+00, -3.1955e+00, -2.4232e+00, -3.7506e+00,  1.0057e+00,\n",
      "         -1.6226e+00,  1.1616e-02, -9.9066e-01, -3.4925e+00, -1.9511e+00,\n",
      "         -2.2768e+00, -9.6627e-01, -3.1682e+00, -1.4524e+00,  1.5452e-01,\n",
      "         -6.4872e-01, -5.5566e-01, -2.1915e-01, -4.3243e+00, -1.5221e+00,\n",
      "         -3.1594e+00, -3.5071e+00, -3.2564e+00, -2.7013e-01, -6.3197e-02,\n",
      "         -3.8941e+00, -1.5066e+00, -7.2368e+00, -2.8670e+00, -3.0380e+00,\n",
      "         -8.0700e-01, -2.5425e+00, -2.9133e+00, -2.5475e+00, -1.5425e+00,\n",
      "         -3.6828e+00, -1.1811e+00, -5.8448e+00, -2.3869e+00, -3.4932e+00,\n",
      "          1.1291e+00, -1.2327e+00,  1.3564e-01, -4.4259e-01, -3.1096e+00,\n",
      "         -4.0831e+00, -5.6753e+00, -2.7897e+00, -3.5690e-01, -3.8394e+00,\n",
      "         -4.3470e+00, -2.4324e+00, -3.9119e+00, -2.3715e+00, -3.5768e+00,\n",
      "         -4.9660e+00, -2.4808e+00, -2.2846e-01, -1.1248e+00, -1.8043e+00,\n",
      "         -2.8875e+00, -1.7257e+00, -4.8615e-01, -3.5916e+00, -8.3321e-01,\n",
      "         -3.1827e+00, -4.2018e+00, -2.4333e+00, -5.3397e-01, -7.2251e-01,\n",
      "         -3.1334e+00, -2.7077e+00, -3.5431e+00, -3.3278e+00, -3.2639e+00,\n",
      "          1.9296e-02, -8.3970e-01, -3.7641e+00, -2.1933e+00, -5.1944e-01,\n",
      "         -1.8191e+00, -3.4150e+00, -3.7892e+00,  3.5147e-01, -1.5201e+00,\n",
      "         -1.9765e+00, -1.0403e+00, -4.4786e+00, -4.6187e+00,  5.1424e-01,\n",
      "         -8.0106e-01, -2.0854e+00, -1.2249e+00, -1.7521e-02, -9.8549e-01,\n",
      "         -2.5842e+00, -4.7152e+00, -2.5758e+00, -8.0688e-01, -1.0251e+00,\n",
      "         -2.8279e+00, -2.1554e+00, -7.1902e-01, -3.6685e-01, -4.2361e+00,\n",
      "         -8.0922e-01, -4.6772e-02, -3.3757e+00, -3.9868e+00, -1.1536e+00,\n",
      "         -2.0174e+00, -3.0675e+00, -2.3888e+00, -1.6483e+00, -2.9604e+00,\n",
      "         -2.7276e+00, -3.3193e+00, -5.6926e+00, -1.2002e+00, -1.1524e+00,\n",
      "         -1.0270e+00, -1.0336e+00, -1.1508e+00, -1.0713e+00, -1.1871e+00,\n",
      "         -5.0288e+00, -5.0386e+00, -5.4723e+00, -5.0870e+00, -4.9858e+00,\n",
      "         -4.7173e+00, -4.5680e+00, -4.6267e+00, -4.6045e+00, -4.6248e+00,\n",
      "         -4.7596e+00, -4.8438e+00, -4.8957e+00, -4.8951e+00, -4.8385e+00,\n",
      "         -4.7408e+00, -4.6615e+00, -4.6302e+00, -4.6420e+00, -4.6555e+00,\n",
      "         -4.6684e+00, -4.6922e+00, -4.7238e+00, -4.7561e+00, -4.7832e+00,\n",
      "         -4.7924e+00, -4.7610e+00, -4.7268e+00, -4.7280e+00, -4.7305e+00,\n",
      "         -4.7298e+00, -4.7259e+00, -4.6889e+00, -4.6636e+00, -4.6635e+00,\n",
      "         -4.6645e+00, -4.6878e+00, -4.7149e+00, -4.7292e+00, -4.7301e+00,\n",
      "         -4.7267e+00, -4.7307e+00, -4.7395e+00, -4.7503e+00, -4.7462e+00,\n",
      "         -4.7529e+00, -4.7592e+00, -4.7591e+00, -4.7703e+00, -4.7679e+00,\n",
      "         -4.7585e+00, -4.7661e+00, -4.7565e+00, -4.7455e+00, -4.7412e+00,\n",
      "         -4.7401e+00, -4.7289e+00, -4.7194e+00, -4.6967e+00, -4.6365e+00,\n",
      "         -4.6247e+00, -4.5783e+00, -4.5419e+00, -4.4627e+00, -4.4127e+00,\n",
      "         -4.3162e+00, -4.2369e+00, -4.1915e+00, -4.1700e+00, -4.1561e+00,\n",
      "         -4.1489e+00, -4.1520e+00],\n",
      "        [-2.4845e+00, -3.9056e+00, -6.4354e-01, -2.2174e+00,  3.2089e-01,\n",
      "         -2.3255e+00, -4.3676e+00, -1.4747e-01, -3.8313e+00, -2.5414e+00,\n",
      "         -3.9342e+00,  2.3208e-01, -5.4307e-01, -3.5815e+00, -3.2329e+00,\n",
      "         -2.2612e+00, -3.2555e+00, -1.6871e+00, -2.0792e+00, -8.1375e-01,\n",
      "         -1.2298e+00, -1.3018e+00, -7.2005e-01,  3.6840e-01, -1.8608e+00,\n",
      "         -3.4760e+00, -3.1843e+00, -2.4708e+00, -3.7725e+00,  9.8160e-01,\n",
      "         -1.6703e+00, -1.8965e-02, -1.0038e+00, -3.5053e+00, -1.9608e+00,\n",
      "         -2.2500e+00, -1.0223e+00, -3.1882e+00, -1.4796e+00,  1.3306e-01,\n",
      "         -6.2821e-01, -6.0947e-01, -2.1796e-01, -4.3328e+00, -1.5646e+00,\n",
      "         -3.2337e+00, -3.5457e+00, -3.2689e+00, -2.2609e-01, -3.8405e-03,\n",
      "         -3.9036e+00, -1.5166e+00, -7.2432e+00, -2.8898e+00, -3.0415e+00,\n",
      "         -8.0752e-01, -2.5629e+00, -2.9500e+00, -2.5525e+00, -1.5811e+00,\n",
      "         -3.6922e+00, -1.1788e+00, -5.8472e+00, -2.4019e+00, -3.5063e+00,\n",
      "          1.1182e+00, -1.2726e+00,  1.1172e-01, -4.4373e-01, -3.1207e+00,\n",
      "         -4.0877e+00, -5.6288e+00, -2.8047e+00, -3.3034e-01, -3.8459e+00,\n",
      "         -4.3527e+00, -2.4470e+00, -3.9101e+00, -2.3836e+00, -3.5805e+00,\n",
      "         -4.9159e+00, -2.4907e+00, -2.2709e-01, -1.1371e+00, -1.7694e+00,\n",
      "         -2.9162e+00, -1.7488e+00, -5.0425e-01, -3.5742e+00, -8.2304e-01,\n",
      "         -3.1841e+00, -4.2070e+00, -2.4420e+00, -5.2753e-01, -7.3025e-01,\n",
      "         -3.1382e+00, -2.7276e+00, -3.5998e+00, -3.3128e+00, -3.2755e+00,\n",
      "          2.1985e-02, -8.4869e-01, -3.7682e+00, -2.1833e+00, -5.1633e-01,\n",
      "         -1.8355e+00, -3.4632e+00, -3.7893e+00,  3.8249e-01, -1.5072e+00,\n",
      "         -1.9537e+00, -1.0351e+00, -4.5334e+00, -4.6034e+00,  5.3210e-01,\n",
      "         -8.0279e-01, -2.0874e+00, -1.2343e+00, -1.9084e-02, -9.7947e-01,\n",
      "         -2.6329e+00, -4.7075e+00, -2.5854e+00, -7.7495e-01, -1.0298e+00,\n",
      "         -2.8295e+00, -2.1501e+00, -7.3803e-01, -3.7921e-01, -4.2504e+00,\n",
      "         -7.3239e-01,  2.1642e-02, -3.3842e+00, -3.9973e+00, -1.1434e+00,\n",
      "         -2.0326e+00, -3.0944e+00, -2.3889e+00, -1.6575e+00, -2.9698e+00,\n",
      "         -2.7783e+00, -5.8811e+00, -1.5039e+00, -2.7217e+00, -1.5242e+00,\n",
      "         -1.4155e+00, -9.2149e-01, -4.1688e+00, -4.1370e+00, -4.2100e-01,\n",
      "         -2.0094e+00, -6.6813e-01, -1.6554e+00, -9.0958e-01,  2.0776e-02,\n",
      "         -9.6136e-01, -4.1371e+00, -3.0318e+00, -4.9626e+00, -1.3071e+00,\n",
      "         -8.2178e-01, -7.6930e-01, -4.9118e-01, -6.1653e-01, -5.3131e-01,\n",
      "         -7.7877e-01, -5.0355e+00, -5.0447e+00, -5.4000e+00, -5.0366e+00,\n",
      "         -5.3368e+00, -5.3318e+00, -5.1883e+00, -5.1653e+00, -5.1965e+00,\n",
      "         -5.2114e+00, -5.2517e+00, -5.1998e+00, -4.9780e+00, -4.7764e+00,\n",
      "         -4.6492e+00, -4.5671e+00, -4.5424e+00, -4.5444e+00, -4.5351e+00,\n",
      "         -4.5264e+00, -4.5322e+00, -4.5569e+00, -4.5969e+00, -4.6404e+00,\n",
      "         -4.6690e+00, -4.6974e+00, -4.7230e+00, -4.7524e+00, -4.7759e+00,\n",
      "         -4.7931e+00, -4.7959e+00, -4.7829e+00, -4.7676e+00, -4.7544e+00,\n",
      "         -4.7576e+00, -4.7747e+00, -4.7815e+00, -4.7893e+00, -4.7899e+00,\n",
      "         -4.7783e+00, -4.7551e+00, -4.7211e+00, -4.6800e+00, -4.6513e+00,\n",
      "         -4.6508e+00, -4.6569e+00, -4.6708e+00, -4.6934e+00, -4.7243e+00,\n",
      "         -4.7191e+00, -4.7128e+00, -4.7112e+00, -4.6998e+00, -4.6853e+00,\n",
      "         -4.6648e+00, -4.6451e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1037, -0.6681], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.0310,  0.9900, -1.0589,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1182e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2086, -0.2107, -0.2128,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.0056,  0.9656, -1.0328,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9238, -3.9102, -0.7357,  ..., -5.7158, -5.7075, -5.6923],\n",
      "        [-3.0275, -3.8680, -0.7542,  ..., -5.1019, -5.0416, -5.0117],\n",
      "        [-2.3836, -3.9300, -0.6391,  ..., -4.1307, -4.1293, -4.1318],\n",
      "        ...,\n",
      "        [-2.9261, -3.9031, -0.7390,  ..., -5.5980, -5.5849, -5.5748],\n",
      "        [-3.0233, -3.8750, -0.7595,  ..., -5.1494, -5.0842, -5.0473],\n",
      "        [-3.0235, -3.8694, -0.7559,  ..., -5.1214, -5.0608, -5.0268]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9322,  0.7720, -1.0465,  0.7485, -1.0438,  0.9176,  0.7295,  0.7485],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.9887,  0.9900,  0.9900,  0.9900, -1.0471, -0.8924, -1.0080],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3347e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2000, -0.2020, -0.2041,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2118, -0.2140, -0.2161,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1537, -0.1553, -0.1568,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2039, -0.2060, -0.2081,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9643,  0.9656,  0.9656,  0.9656, -1.0212, -0.8704, -0.9831],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9314, -3.9133, -0.7414,  ..., -5.7612, -5.7552, -5.7436],\n",
      "        [-2.3881, -3.9290, -0.6429,  ..., -4.2114, -4.2062, -4.2053],\n",
      "        [-2.9173, -3.9088, -0.7349,  ..., -5.6596, -5.6486, -5.6430],\n",
      "        ...,\n",
      "        [-2.3735, -3.9307, -0.6316,  ..., -4.2179, -4.2108, -4.2082],\n",
      "        [-2.4820, -3.9152, -0.6355,  ..., -4.7027, -4.6813, -4.6557],\n",
      "        [-2.3931, -3.9383, -0.6378,  ..., -4.1208, -4.1178, -4.1192]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9021, -1.0174,  0.9083,  0.9146,  0.8978, -1.0522, -0.7143, -0.9992],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0443, -1.0456,  0.9900,  0.9900,  0.9900,  0.9900, -1.0061, -1.0061],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3401e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2113, -0.2134, -0.2156,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2115, -0.2137, -0.2158,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2035, -0.2056, -0.2077,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2035, -0.2056, -0.2077,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0186, -1.0199,  0.9608,  0.9656,  0.9656,  0.9656, -0.9812, -0.9812],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3788, -3.9362, -0.6309,  ..., -4.1613, -4.1578, -4.1615],\n",
      "        [-2.3860, -3.9401, -0.6385,  ..., -4.1272, -4.1227, -4.1223],\n",
      "        [-3.0157, -3.8707, -0.7467,  ..., -5.2158, -5.1326, -5.0734],\n",
      "        ...,\n",
      "        [-2.8660, -3.9258, -0.7258,  ..., -5.5523, -5.5719, -5.5781],\n",
      "        [-2.3879, -3.9350, -0.6404,  ..., -4.1986, -4.1938, -4.1950],\n",
      "        [-2.3879, -3.9350, -0.6404,  ..., -4.1986, -4.1938, -4.1950]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1007, -1.0579,  0.8201,  0.8987,  0.8782,  0.9048, -1.0142, -1.0142],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.8943,  0.9900,  0.9900,  0.9900, -1.0535, -1.1000,  0.9900, -1.0735],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4436e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3871, -0.3910, -0.3949,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2172, -0.2194, -0.2216,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.8568,  0.9656,  0.9656,  0.9656, -1.0275, -1.0518,  0.9656, -1.0471],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2367, -3.9553, -0.5778,  ..., -4.2459, -4.2473, -4.2468],\n",
      "        [-2.9239, -3.9116, -0.7317,  ..., -5.7104, -5.7004, -5.6802],\n",
      "        [-2.8637, -3.9259, -0.7241,  ..., -5.5593, -5.5784, -5.5832],\n",
      "        ...,\n",
      "        [-2.5678, -3.8738, -0.6617,  ..., -1.0561, -1.1040, -1.4122],\n",
      "        [-2.9195, -3.9067, -0.7198,  ..., -5.7089, -5.7053, -5.7002],\n",
      "        [-2.3807, -3.9351, -0.6392,  ..., -4.1397, -4.1362, -4.1351]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5257,  0.9156,  0.9104,  0.8202, -1.0301, -1.1413,  0.9115, -1.0438],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9134,  0.9900,  0.9900,  0.9900, -0.9016, -1.7082, -1.0141, -1.5172],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8051e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1573, -0.1589, -0.1605,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3320, -0.3353, -0.3387,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2052, -0.2072, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1766, -0.1784, -0.1802,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8908,  0.9656,  0.9656,  0.9656, -0.8793, -1.6334, -0.9891, -1.4507],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4690, -3.9115, -0.6341,  ..., -4.7575, -4.7424, -4.7235],\n",
      "        [-2.9160, -3.9045, -0.7257,  ..., -5.7134, -5.6998, -5.6857],\n",
      "        [-2.8452, -3.9143, -0.7216,  ..., -5.4189, -5.4388, -5.4418],\n",
      "        ...,\n",
      "        [-2.2993, -3.9399, -0.5841,  ..., -4.4258, -4.3990, -4.3806],\n",
      "        [-2.3791, -3.9360, -0.6262,  ..., -4.2069, -4.2000, -4.2019],\n",
      "        [-2.5404, -3.8800, -0.6347,  ..., -4.2339, -4.2242, -4.2020]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7177,  0.9124,  0.9189,  0.9189, -0.7504, -1.6889, -0.9882, -1.5943],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9951, -1.0445,  0.9900, -1.0410,  0.9900,  0.9900, -1.8760, -1.0445],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8194e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2013, -0.2033, -0.2054,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2113, -0.2134, -0.2156,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3833, -0.3872, -0.3911,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2113, -0.2134, -0.2156,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9706, -1.0187,  0.9656, -1.0154,  0.9656,  0.9656, -1.8388, -1.0187],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3748, -3.9248, -0.6252,  ..., -4.2204, -4.2129, -4.2134],\n",
      "        [-2.3620, -3.9313, -0.6223,  ..., -4.1809, -4.1768, -4.1766],\n",
      "        [-2.9087, -3.9042, -0.7180,  ..., -5.6576, -5.6480, -5.6345],\n",
      "        ...,\n",
      "        [-3.0254, -3.8711, -0.7329,  ..., -5.1726, -5.1058, -5.0728],\n",
      "        [-2.2244, -3.9502, -0.5641,  ..., -4.2507, -4.2545, -4.2533],\n",
      "        [-2.3620, -3.9313, -0.6223,  ..., -4.1809, -4.1768, -4.1766]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9841, -0.9714,  0.8898, -1.0701,  0.9208,  0.7885, -1.5159, -0.9714],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.0413,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4463e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2107, -0.2128, -0.2149, -0.2171, -0.2193, -0.2215, -0.2238, -0.2260,\n",
      "         -0.2283, -0.2306, -0.2329, -0.2353, -0.2377, -0.2401, -0.2425, -0.2449,\n",
      "         -0.2474, -0.2499, -0.2524, -0.2550, -0.2576, -0.2602, -0.2628, -0.2654,\n",
      "         -0.2681, -0.2708, -0.2736, -0.2763, -0.2791, -0.2819, -0.2848, -0.2877,\n",
      "         -0.2906, -0.2935, -0.2965, -0.2995, -0.3025, -0.3056, -0.3086, -0.3118,\n",
      "         -0.3149, -0.3181, -0.3213, -0.3245, -0.3278, -0.3311, -0.3345, -0.3379,\n",
      "         -0.3413, -0.3447, -0.3482, -0.3517, -0.3553, -0.3589, -0.3625, -0.3661,\n",
      "         -0.3698, -0.3736, -0.3773, -0.3812, -0.3850, -0.3889, -0.3928, -0.3968,\n",
      "         -0.4008, -0.4049, -0.4089, -0.4131, -0.4172, -0.4215, -0.4257, -0.4300,\n",
      "         -0.4344, -0.4387, -0.4432, -0.4477, -0.4522, -0.4567, -0.4614, -0.4660,\n",
      "         -0.4707, -0.4755, -0.4803, -0.4851, -0.4900, -0.4950, -0.5000, -0.5050,\n",
      "         -0.5101, -0.5153, -0.5205, -0.5258, -0.5311, -0.5364, -0.5418, -0.5473,\n",
      "         -0.5528, -0.5584, -0.5641, -0.5698, -0.5755, -0.5813, -0.5872, -0.5931,\n",
      "         -0.5991, -0.6052, -0.6113, -0.6175, -0.6237, -0.6300, -0.6364, -0.6428,\n",
      "         -0.6493, -0.6559, -0.6625, -0.6692, -0.6759, -0.6828, -0.6897, -0.6966,\n",
      "         -0.7037, -0.7108, -0.7179, -0.7252, -0.7325, -0.7399, -0.7474, -0.7549,\n",
      "         -0.7626, -0.7703, -0.7781, -0.7859, -0.7939, -0.8019, -0.8100, -0.8181,\n",
      "         -0.8264, -0.8348, -0.8432, -0.8517, -0.8603, -0.8690, -0.8778, -0.8866,\n",
      "         -0.8956, -0.9047, -0.9138, -0.9230, -0.9323, -0.9418, -0.9513, -0.9609,\n",
      "         -0.9706, -0.9804, -0.9903, -1.0003, -1.0104, -1.0206, -1.0309, -1.0413,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0156,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3629, -3.9258, -0.6174, -2.1306,  0.3223, -2.3315, -4.3575, -0.0857,\n",
      "         -3.7091, -2.4918, -3.9525,  0.2867, -0.4668, -3.5591, -3.2303, -2.2035,\n",
      "         -3.1914, -1.7011, -2.0380, -0.7790, -1.2415, -1.2875, -0.6932,  0.3853,\n",
      "         -1.7806, -3.4677, -3.2051, -2.4250, -3.7628,  1.0344, -1.6043,  0.0218,\n",
      "         -0.9852, -3.5067, -1.9574, -2.2384, -0.9391, -3.1305, -1.4436,  0.1831,\n",
      "         -0.6223, -0.5281, -0.1940, -4.3358, -1.4926, -3.1424, -3.4712, -3.2667,\n",
      "         -0.1746, -0.1155, -3.9147, -1.5330, -7.2690, -2.8781, -2.9956, -0.7940,\n",
      "         -2.5475, -2.9120, -2.5706, -1.5149, -3.6994, -1.2068, -5.8570, -2.3975,\n",
      "         -3.5014,  1.1635, -1.2045,  0.1555, -0.4305, -3.1168, -4.0959, -5.6828,\n",
      "         -2.7991, -0.4477, -3.8521, -4.3702, -2.4367, -3.8784, -2.3610, -3.5898,\n",
      "         -4.9661, -2.4844, -0.2129, -1.1181, -1.7993, -2.8852, -1.7057, -0.4787,\n",
      "         -3.6395, -0.8087, -3.1980, -4.2209, -2.4378, -0.5235, -0.7113, -3.1572,\n",
      "         -2.6950, -3.5316, -3.3387, -3.2572,  0.0331, -0.8307, -3.7797, -2.1958,\n",
      "         -0.5088, -1.8120, -3.3926, -3.8746,  0.2713, -1.6782, -1.8769, -1.2181,\n",
      "         -4.3223, -4.7423,  0.6180, -0.7470, -2.0782, -1.1711, -0.1048, -0.9118,\n",
      "         -2.5329, -4.7424, -2.5792, -0.7748, -0.9123, -2.7619, -2.1152, -0.6993,\n",
      "         -0.2982, -4.1942, -0.6826, -0.1133, -3.2959, -4.0271, -1.1657, -2.0267,\n",
      "         -3.0800, -2.3995, -1.6356, -2.9710, -2.7211, -3.3451, -5.8297, -1.3149,\n",
      "         -0.9392, -0.9825, -0.9687, -1.0303, -1.0652, -1.1245, -5.0421, -5.0564,\n",
      "         -5.4555, -5.0994, -5.0221, -4.7376, -4.6171, -4.7452, -4.7330, -4.7074,\n",
      "         -4.7842, -4.8479, -4.8943, -4.8760, -4.7789, -4.6445, -4.5800, -4.5818,\n",
      "         -4.6131, -4.6296, -4.6306, -4.6413, -4.6667, -4.6851, -4.6987, -4.6991,\n",
      "         -4.6648, -4.6369, -4.6455, -4.6491, -4.6543, -4.6469, -4.6127, -4.5849,\n",
      "         -4.5821, -4.5875, -4.6194, -4.6573, -4.6798, -4.6909, -4.7016, -4.7246,\n",
      "         -4.7471, -4.7602, -4.7583, -4.7622, -4.7649, -4.7725, -4.7885, -4.7934,\n",
      "         -4.7806, -4.7858, -4.7863, -4.7822, -4.7823, -4.7792, -4.7538, -4.7298,\n",
      "         -4.6977, -4.6369, -4.6269, -4.5768, -4.5371, -4.4601, -4.3976, -4.3091,\n",
      "         -4.2254, -4.1879, -4.1640, -4.1423, -4.1377, -4.1365],\n",
      "        [-2.9051, -3.9075, -0.7068, -2.4481,  0.1703, -2.4254, -4.5663, -0.5359,\n",
      "         -4.3044, -2.7915, -4.1216,  0.2846, -0.6019, -3.7268, -3.0897, -2.4982,\n",
      "         -3.3270, -1.8606, -1.9361, -1.0592, -1.3741, -1.5834, -0.9212,  0.4962,\n",
      "         -2.0879, -3.6554, -3.1019, -2.6569, -3.7538,  1.0848, -1.7971, -0.2935,\n",
      "         -0.9609, -3.6432, -2.0203, -2.0436, -0.8332, -3.1971, -1.6619, -0.0745,\n",
      "         -0.6880, -0.4537, -0.2329, -4.2317, -1.7416, -3.6390, -3.7892, -3.4048,\n",
      "          0.5780,  0.5013, -4.0235, -1.6488, -7.2609, -3.0379, -3.0216, -0.9764,\n",
      "         -2.5223, -2.9361, -2.7140, -1.7563, -3.8581, -1.3096, -5.8678, -2.5046,\n",
      "         -3.4883,  1.2466, -1.3766, -0.1650, -0.3592, -3.2571, -4.2902, -5.6829,\n",
      "         -2.9500, -0.3304, -3.9869, -4.2932, -2.5820, -3.8966, -2.5372, -3.7167,\n",
      "         -4.9582, -2.6157, -0.2800, -1.4006, -1.8999, -3.1214, -1.8983, -0.7685,\n",
      "         -3.7913, -0.9105, -3.3511, -4.1570, -2.5633, -0.5719, -1.0062, -3.0645,\n",
      "         -2.9171, -3.9492, -3.4194, -3.3932, -0.0637, -1.1072, -3.7330, -2.2057,\n",
      "         -0.4229, -2.0921, -3.7767, -4.0490,  0.4106, -1.0673, -1.5922, -0.6291,\n",
      "         -4.8501, -4.7313,  0.5497, -0.9794, -2.2169, -1.4456, -0.2905, -1.0377,\n",
      "         -2.8960, -4.6775, -2.6293, -0.7704, -1.1801, -3.0169, -2.3083, -0.8248,\n",
      "         -0.6256, -4.3794,  0.1858,  0.5338, -3.3538, -4.1091, -1.2786, -2.0048,\n",
      "         -3.0735, -2.5356, -1.7795, -2.9724, -3.0344, -5.9257, -1.8515, -2.5644,\n",
      "         -1.4638, -1.0748, -0.9547, -4.3558, -3.8888, -0.3933, -2.1050, -1.0768,\n",
      "         -1.1390, -0.6943, -0.3446, -0.5422, -4.4578, -3.4093, -4.8675, -1.5555,\n",
      "          0.9457,  0.9068,  0.8705,  0.8996,  1.0434,  1.0394, -4.8312, -5.0075,\n",
      "         -5.3562, -5.1814, -5.4570, -5.4884, -5.4598, -5.4532, -5.4502, -5.4194,\n",
      "         -5.3346, -5.2181, -5.1346, -5.0852, -5.0778, -5.0960, -5.1259, -5.1557,\n",
      "         -5.1704, -5.1805, -5.1941, -5.2302, -5.2814, -5.3431, -5.3847, -5.4052,\n",
      "         -5.4148, -5.4326, -5.4693, -5.5157, -5.5370, -5.5339, -5.5149, -5.4787,\n",
      "         -5.4494, -5.4338, -5.4309, -5.4529, -5.4837, -5.5245, -5.5614, -5.5876,\n",
      "         -5.5917, -5.5833, -5.5906, -5.6081, -5.6338, -5.6790, -5.7181, -5.7334,\n",
      "         -5.7372, -5.7401, -5.7371, -5.7385, -5.7302, -5.7154]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0184,  0.9509], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.0089,  0.9900, -1.0282, -1.4785, -0.8610,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4145e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2041, -0.2062, -0.2082,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1721, -0.1738, -0.1756,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1483, -0.1498, -0.1513,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.9840,  0.9656, -1.0028, -1.4137, -0.8398,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8990, -3.8951, -0.7038,  ..., -5.7083, -5.6963, -5.6802],\n",
      "        [-2.8945, -3.8977, -0.7041,  ..., -5.6570, -5.6488, -5.6343],\n",
      "        [-2.3504, -3.9225, -0.6101,  ..., -4.1822, -4.1754, -4.1759],\n",
      "        ...,\n",
      "        [-2.5195, -3.8703, -0.6101,  ..., -4.2162, -4.2055, -4.1867],\n",
      "        [-2.4497, -3.9041, -0.6025,  ..., -4.7020, -4.6741, -4.6442],\n",
      "        [-2.9019, -3.8983, -0.7034,  ..., -5.6992, -5.6934, -5.6732]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9336,  0.8969, -0.9578,  0.9569, -1.0121, -1.5447, -0.7740,  0.9389],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.0112, -1.8313,  0.9900, -0.9580, -1.0231,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4407e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(7.6672e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2046, -0.2066, -0.2087,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3742, -0.3780, -0.3818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2070, -0.2091, -0.2112,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.9863, -1.7950,  0.9656, -0.9344, -0.9979,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0084, -3.8528, -0.7057,  ..., -5.1333, -5.0734, -5.0415],\n",
      "        [-2.3433, -3.9148, -0.5939,  ..., -4.2220, -4.2121, -4.2077],\n",
      "        [-2.1914, -3.9401, -0.5425,  ..., -4.2344, -4.2382, -4.2392],\n",
      "        ...,\n",
      "        [-2.3340, -3.9171, -0.5956,  ..., -4.1312, -4.1238, -4.1196],\n",
      "        [-2.8260, -3.9096, -0.6877,  ..., -5.5577, -5.5754, -5.5815],\n",
      "        [-2.8873, -3.8907, -0.6958,  ..., -5.6540, -5.6490, -5.6323]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8341, -0.9929, -1.5288,  0.9382, -0.9676, -0.9942,  0.9330,  0.8521],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.9553, -0.9470,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5008e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.5255e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1933, -0.1952, -0.1972,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.9317, -0.9236,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8755, -3.8954, -0.6904,  ..., -5.6667, -5.6587, -5.6484],\n",
      "        [-2.8823, -3.9009, -0.6898,  ..., -5.7325, -5.7268, -5.7098],\n",
      "        [-2.3459, -3.9224, -0.5971,  ..., -4.1992, -4.1925, -4.1938],\n",
      "        ...,\n",
      "        [-2.8059, -3.8918, -0.6677,  ..., -5.6215, -5.6463, -5.6551],\n",
      "        [-3.0025, -3.8587, -0.7045,  ..., -5.1577, -5.0955, -5.0608],\n",
      "        [-2.8183, -3.9083, -0.6835,  ..., -5.5552, -5.5768, -5.5789]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9549,  0.9634, -0.9626, -0.9689,  0.8450,  0.9378,  0.8086,  0.9311],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9850, -1.6480, -0.9853, -0.9410, -1.1000,  0.9900, -0.9850,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.1697e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1993, -0.2013, -0.2033,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3203, -0.3235, -0.3268,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1993, -0.2013, -0.2034,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1993, -0.2013, -0.2033,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9607, -1.5758, -0.9610, -0.9178, -1.0518,  0.9656, -0.9607,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3443, -3.9204, -0.5941,  ..., -4.1268, -4.1207, -4.1236],\n",
      "        [-2.2550, -3.9282, -0.5500,  ..., -4.4283, -4.3989, -4.3777],\n",
      "        [-2.3312, -3.9234, -0.6000,  ..., -4.1668, -4.1608, -4.1607],\n",
      "        ...,\n",
      "        [-3.0034, -3.8616, -0.7055,  ..., -5.1560, -5.0912, -5.0609],\n",
      "        [-2.3443, -3.9204, -0.5941,  ..., -4.1268, -4.1207, -4.1236],\n",
      "        [-2.9928, -3.8565, -0.7033,  ..., -5.2036, -5.1144, -5.0502]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0042, -1.5890, -0.9627, -0.9763, -1.0429,  0.8002, -1.0042,  0.8729],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9448, -0.9294,  0.9900, -0.8399,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0886e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1911, -0.1931, -0.1950,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1880, -0.1899, -0.1918,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9215, -0.9065,  0.9656, -0.8192,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3512, -3.9259, -0.5958,  ..., -4.1134, -4.1073, -4.1088],\n",
      "        [-2.3439, -3.9189, -0.6014,  ..., -4.1806, -4.1740, -4.1756],\n",
      "        [-2.8085, -3.9056, -0.6896,  ..., -5.3996, -5.4239, -5.4207],\n",
      "        ...,\n",
      "        [-2.8888, -3.9000, -0.6959,  ..., -5.5993, -5.5872, -5.5709],\n",
      "        [-2.8891, -3.9059, -0.7006,  ..., -5.7572, -5.7522, -5.7380],\n",
      "        [-2.8826, -3.8951, -0.6844,  ..., -5.6930, -5.6872, -5.6799]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9594, -0.9792,  0.9500, -0.7537,  0.9500,  0.9665,  0.9342,  0.9553],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8183, -1.7916,  0.9900, -0.9615,  0.9900,  0.9900, -0.9832],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5326e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1410, -0.1424, -0.1438,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3661, -0.3698, -0.3735,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1989, -0.2009, -0.2029,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7981, -1.7561,  0.9656, -0.9377,  0.9656,  0.9656, -0.9590],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8911, -3.9014, -0.6968,  ..., -5.6016, -5.5887, -5.5746],\n",
      "        [-2.4290, -3.9053, -0.5913,  ..., -4.6617, -4.6319, -4.5996],\n",
      "        [-2.1774, -3.9447, -0.5369,  ..., -4.2155, -4.2190, -4.2199],\n",
      "        ...,\n",
      "        [-2.8791, -3.9005, -0.6979,  ..., -5.6450, -5.6397, -5.6243],\n",
      "        [-2.8851, -3.8983, -0.6958,  ..., -5.6928, -5.6799, -5.6655],\n",
      "        [-2.3379, -3.9236, -0.6020,  ..., -4.1156, -4.1090, -4.1063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9701, -0.8007, -1.5503,  0.8224, -1.0252,  0.9190,  0.9581, -1.0147],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9483, -0.9483], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(5.2963e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1918, -0.1938, -0.1957, -0.1977, -0.1997, -0.2017, -0.2038, -0.2058,\n",
      "         -0.2079, -0.2100, -0.2121, -0.2143, -0.2164, -0.2186, -0.2208, -0.2231,\n",
      "         -0.2253, -0.2276, -0.2299, -0.2322, -0.2346, -0.2369, -0.2393, -0.2417,\n",
      "         -0.2442, -0.2466, -0.2491, -0.2517, -0.2542, -0.2568, -0.2594, -0.2620,\n",
      "         -0.2646, -0.2673, -0.2700, -0.2727, -0.2755, -0.2783, -0.2811, -0.2839,\n",
      "         -0.2868, -0.2897, -0.2926, -0.2956, -0.2985, -0.3016, -0.3046, -0.3077,\n",
      "         -0.3108, -0.3139, -0.3171, -0.3203, -0.3235, -0.3268, -0.3301, -0.3334,\n",
      "         -0.3368, -0.3402, -0.3437, -0.3471, -0.3506, -0.3542, -0.3577, -0.3614,\n",
      "         -0.3650, -0.3687, -0.3724, -0.3762, -0.3800, -0.3838, -0.3877, -0.3916,\n",
      "         -0.3956, -0.3996, -0.4036, -0.4077, -0.4118, -0.4160, -0.4202, -0.4244,\n",
      "         -0.4287, -0.4330, -0.4374, -0.4418, -0.4463, -0.4508, -0.4553, -0.4599,\n",
      "         -0.4646, -0.4693, -0.4740, -0.4788, -0.4836, -0.4885, -0.4935, -0.4984,\n",
      "         -0.5035, -0.5086, -0.5137, -0.5189, -0.5241, -0.5294, -0.5348, -0.5402,\n",
      "         -0.5456, -0.5511, -0.5567, -0.5623, -0.5680, -0.5737, -0.5795, -0.5854,\n",
      "         -0.5913, -0.5973, -0.6033, -0.6094, -0.6156, -0.6218, -0.6281, -0.6344,\n",
      "         -0.6408, -0.6473, -0.6538, -0.6604, -0.6671, -0.6738, -0.6806, -0.6875,\n",
      "         -0.6945, -0.7015, -0.7086, -0.7157, -0.7230, -0.7303, -0.7376, -0.7451,\n",
      "         -0.7526, -0.7602, -0.7679, -0.7756, -0.7835, -0.7914, -0.7994, -0.8075,\n",
      "         -0.8156, -0.8239, -0.8322, -0.8406, -0.8491, -0.8577, -0.8663, -0.8751,\n",
      "         -0.8839, -0.8928, -0.9019, -0.9110, -0.9202, -0.9295, -0.9389, -0.9483,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1918, -0.1938, -0.1957, -0.1977, -0.1997, -0.2017, -0.2038, -0.2058,\n",
      "         -0.2079, -0.2100, -0.2121, -0.2143, -0.2164, -0.2186, -0.2208, -0.2231,\n",
      "         -0.2253, -0.2276, -0.2299, -0.2322, -0.2346, -0.2369, -0.2393, -0.2417,\n",
      "         -0.2442, -0.2466, -0.2491, -0.2517, -0.2542, -0.2568, -0.2594, -0.2620,\n",
      "         -0.2646, -0.2673, -0.2700, -0.2727, -0.2755, -0.2783, -0.2811, -0.2839,\n",
      "         -0.2868, -0.2897, -0.2926, -0.2956, -0.2985, -0.3016, -0.3046, -0.3077,\n",
      "         -0.3108, -0.3139, -0.3171, -0.3203, -0.3235, -0.3268, -0.3301, -0.3334,\n",
      "         -0.3368, -0.3402, -0.3437, -0.3471, -0.3506, -0.3542, -0.3577, -0.3614,\n",
      "         -0.3650, -0.3687, -0.3724, -0.3762, -0.3800, -0.3838, -0.3877, -0.3916,\n",
      "         -0.3956, -0.3996, -0.4036, -0.4077, -0.4118, -0.4160, -0.4202, -0.4244,\n",
      "         -0.4287, -0.4330, -0.4374, -0.4418, -0.4463, -0.4508, -0.4553, -0.4599,\n",
      "         -0.4646, -0.4693, -0.4740, -0.4788, -0.4836, -0.4885, -0.4935, -0.4984,\n",
      "         -0.5035, -0.5086, -0.5137, -0.5189, -0.5241, -0.5294, -0.5348, -0.5402,\n",
      "         -0.5456, -0.5511, -0.5567, -0.5623, -0.5680, -0.5737, -0.5795, -0.5854,\n",
      "         -0.5913, -0.5973, -0.6033, -0.6094, -0.6156, -0.6218, -0.6281, -0.6344,\n",
      "         -0.6408, -0.6473, -0.6538, -0.6604, -0.6671, -0.6738, -0.6806, -0.6875,\n",
      "         -0.6945, -0.7015, -0.7086, -0.7157, -0.7230, -0.7303, -0.7376, -0.7451,\n",
      "         -0.7526, -0.7602, -0.7679, -0.7756, -0.7835, -0.7914, -0.7994, -0.8075,\n",
      "         -0.8156, -0.8239, -0.8322, -0.8406, -0.8491, -0.8577, -0.8663, -0.8751,\n",
      "         -0.8839, -0.8928, -0.9019, -0.9110, -0.9202, -0.9295, -0.9389, -0.9483,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.9249, -0.9249], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3328, -3.9224, -0.5878, -2.1269,  0.3523, -2.3303, -4.3422, -0.0213,\n",
      "         -3.6737, -2.4133, -3.9480,  0.2791, -0.4272, -3.5275, -3.2154, -2.1762,\n",
      "         -3.1541, -1.6760, -2.0297, -0.7729, -1.2074, -1.2760, -0.6823,  0.4069,\n",
      "         -1.7432, -3.4443, -3.1963, -2.4022, -3.7440,  1.0594, -1.5795,  0.0264,\n",
      "         -0.9932, -3.4972, -1.9497, -2.1610, -0.9025, -3.0940, -1.4375,  0.2204,\n",
      "         -0.5672, -0.4817, -0.1700, -4.3472, -1.4536, -3.1028, -3.3962, -3.2510,\n",
      "         -0.1418, -0.0768, -3.9011, -1.5138, -7.2728, -2.8646, -2.9544, -0.7547,\n",
      "         -2.5675, -2.8846, -2.5943, -1.4738, -3.6767, -1.1840, -5.8442, -2.3772,\n",
      "         -3.4781,  1.1966, -1.1672,  0.1718, -0.4333, -3.0948, -4.0628, -5.7051,\n",
      "         -2.7782, -0.4158, -3.8307, -4.3905, -2.4078, -3.8376, -2.3627, -3.5599,\n",
      "         -4.9751, -2.4598, -0.1865, -1.1137, -1.8073, -2.8880, -1.6696, -0.4691,\n",
      "         -3.6645, -0.7567, -3.1664, -4.2344, -2.4104, -0.5060, -0.7036, -3.1635,\n",
      "         -2.6549, -3.4989, -3.3588, -3.2621,  0.0545, -0.8249, -3.7933, -2.1898,\n",
      "         -0.4961, -1.7613, -3.3498, -3.8434,  0.3204, -1.5280, -1.8402, -1.0618,\n",
      "         -4.4353, -4.6657,  0.5650, -0.7859, -2.0933, -1.2121, -0.1241, -0.9408,\n",
      "         -2.4923, -4.7796, -2.5435, -0.7747, -1.0003, -2.8836, -2.0666, -0.7027,\n",
      "         -0.3384, -4.2285, -0.6833, -0.0651, -3.3044, -4.0192, -1.1120, -2.0350,\n",
      "         -3.0549, -2.4256, -1.5690, -2.8818, -2.6549, -3.2533, -5.7136, -1.1461,\n",
      "         -1.1697, -0.9735, -1.0636, -0.9802, -1.1166, -1.0539, -4.9577, -4.9846,\n",
      "         -5.4321, -5.0265, -4.9155, -4.6264, -4.4585, -4.5152, -4.4951, -4.5583,\n",
      "         -4.7336, -4.8402, -4.9044, -4.9086, -4.8378, -4.7264, -4.6569, -4.6366,\n",
      "         -4.6518, -4.6584, -4.6594, -4.6724, -4.6903, -4.7142, -4.7276, -4.7349,\n",
      "         -4.7055, -4.6763, -4.6816, -4.6778, -4.6812, -4.6934, -4.6808, -4.6627,\n",
      "         -4.6657, -4.6627, -4.6863, -4.7094, -4.7233, -4.7312, -4.7377, -4.7597,\n",
      "         -4.7814, -4.7973, -4.7956, -4.7995, -4.8010, -4.7989, -4.8037, -4.8058,\n",
      "         -4.7981, -4.8065, -4.7997, -4.7915, -4.7904, -4.7959, -4.7864, -4.7802,\n",
      "         -4.7457, -4.6849, -4.6732, -4.6244, -4.5896, -4.5061, -4.4597, -4.3450,\n",
      "         -4.2534, -4.2077, -4.1753, -4.1503, -4.1439, -4.1453],\n",
      "        [-2.3328, -3.9224, -0.5878, -2.1269,  0.3523, -2.3303, -4.3422, -0.0213,\n",
      "         -3.6737, -2.4133, -3.9480,  0.2791, -0.4272, -3.5275, -3.2154, -2.1762,\n",
      "         -3.1541, -1.6760, -2.0297, -0.7729, -1.2074, -1.2760, -0.6823,  0.4069,\n",
      "         -1.7432, -3.4443, -3.1963, -2.4022, -3.7440,  1.0594, -1.5795,  0.0264,\n",
      "         -0.9932, -3.4972, -1.9497, -2.1610, -0.9025, -3.0940, -1.4375,  0.2204,\n",
      "         -0.5672, -0.4817, -0.1700, -4.3472, -1.4536, -3.1028, -3.3962, -3.2510,\n",
      "         -0.1418, -0.0768, -3.9011, -1.5138, -7.2728, -2.8646, -2.9544, -0.7547,\n",
      "         -2.5675, -2.8846, -2.5943, -1.4738, -3.6767, -1.1840, -5.8442, -2.3772,\n",
      "         -3.4781,  1.1966, -1.1672,  0.1718, -0.4333, -3.0948, -4.0628, -5.7051,\n",
      "         -2.7782, -0.4158, -3.8307, -4.3905, -2.4078, -3.8376, -2.3627, -3.5599,\n",
      "         -4.9751, -2.4598, -0.1865, -1.1137, -1.8073, -2.8880, -1.6696, -0.4691,\n",
      "         -3.6645, -0.7567, -3.1664, -4.2344, -2.4104, -0.5060, -0.7036, -3.1635,\n",
      "         -2.6549, -3.4989, -3.3588, -3.2621,  0.0545, -0.8249, -3.7933, -2.1898,\n",
      "         -0.4961, -1.7613, -3.3498, -3.8434,  0.3204, -1.5280, -1.8402, -1.0618,\n",
      "         -4.4353, -4.6657,  0.5650, -0.7859, -2.0933, -1.2121, -0.1241, -0.9408,\n",
      "         -2.4923, -4.7796, -2.5435, -0.7747, -1.0003, -2.8836, -2.0666, -0.7027,\n",
      "         -0.3384, -4.2285, -0.6833, -0.0651, -3.3044, -4.0192, -1.1120, -2.0350,\n",
      "         -3.0549, -2.4256, -1.5690, -2.8818, -2.6549, -3.2533, -5.7136, -1.1461,\n",
      "         -1.1697, -0.9735, -1.0636, -0.9802, -1.1166, -1.0539, -4.9577, -4.9846,\n",
      "         -5.4321, -5.0265, -4.9155, -4.6264, -4.4585, -4.5152, -4.4951, -4.5583,\n",
      "         -4.7336, -4.8402, -4.9044, -4.9086, -4.8378, -4.7264, -4.6569, -4.6366,\n",
      "         -4.6518, -4.6584, -4.6594, -4.6724, -4.6903, -4.7142, -4.7276, -4.7349,\n",
      "         -4.7055, -4.6763, -4.6816, -4.6778, -4.6812, -4.6934, -4.6808, -4.6627,\n",
      "         -4.6657, -4.6627, -4.6863, -4.7094, -4.7233, -4.7312, -4.7377, -4.7597,\n",
      "         -4.7814, -4.7973, -4.7956, -4.7995, -4.8010, -4.7989, -4.8037, -4.8058,\n",
      "         -4.7981, -4.8065, -4.7997, -4.7915, -4.7904, -4.7959, -4.7864, -4.7802,\n",
      "         -4.7457, -4.6849, -4.6732, -4.6244, -4.5896, -4.5061, -4.4597, -4.3450,\n",
      "         -4.2534, -4.2077, -4.1753, -4.1503, -4.1439, -4.1453]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0596, -1.0596], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.4147,  0.9900, -0.9086, -0.8978,  0.9900, -1.6021, -0.8910],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8129e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1647, -0.1663, -0.1680,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3113, -0.3145, -0.3176,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1802, -0.1821, -0.1839,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.3528,  0.9656, -0.8862, -0.8757,  0.9656, -1.5319, -0.8690],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8724, -3.9052, -0.6860,  ..., -5.7341, -5.7308, -5.7162],\n",
      "        [-2.4928, -3.8665, -0.5928,  ..., -4.1543, -4.1460, -4.1309],\n",
      "        [-2.8712, -3.8964, -0.6890,  ..., -5.6589, -5.6601, -5.6406],\n",
      "        ...,\n",
      "        [-2.8837, -3.9050, -0.6963,  ..., -5.7527, -5.7507, -5.7378],\n",
      "        [-2.2478, -3.9241, -0.5439,  ..., -4.4331, -4.4019, -4.3759],\n",
      "        [-2.3404, -3.9170, -0.5948,  ..., -4.1653, -4.1582, -4.1585]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9855, -1.4191,  0.8878, -0.9292, -0.9415,  0.9454, -1.5347, -0.9567],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.8748,  0.9900, -0.7848,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6264e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1352, -0.1366, -0.1379,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.8532,  0.9656, -0.7655,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-3.0109, -3.8578, -0.6981,  ..., -5.1758, -5.1179, -5.0852],\n",
      "        [-2.8780, -3.8947, -0.6848,  ..., -5.6097, -5.5987, -5.5850],\n",
      "        [-2.7896, -3.8993, -0.6812,  ..., -5.4229, -5.4448, -5.4457],\n",
      "        ...,\n",
      "        [-2.4129, -3.8965, -0.5785,  ..., -4.6608, -4.6292, -4.5965],\n",
      "        [-2.7895, -3.8891, -0.6588,  ..., -5.6272, -5.6548, -5.6613],\n",
      "        [-2.9953, -3.8525, -0.6958,  ..., -5.2217, -5.1307, -5.0676]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8111,  0.9839,  0.9677, -0.9318,  0.8309, -0.7730,  0.9589,  0.9041],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9017,  0.9900,  0.9900, -0.9179, -0.8626, -0.9039, -1.7435,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9542e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1824, -0.1843, -0.1861,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1829, -0.1847, -0.1866,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3563, -0.3599, -0.3635,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8795,  0.9656,  0.9656, -0.8953, -0.8413, -0.8816, -1.7090,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3281, -3.9077, -0.5795,  ..., -4.1395, -4.1302, -4.1339],\n",
      "        [-3.0098, -3.8464, -0.6871,  ..., -5.1558, -5.0990, -5.0701],\n",
      "        [-2.8602, -3.8890, -0.6834,  ..., -5.6555, -5.6514, -5.6375],\n",
      "        ...,\n",
      "        [-2.3251, -3.9102, -0.5874,  ..., -4.1357, -4.1282, -4.1272],\n",
      "        [-2.1514, -3.9292, -0.5196,  ..., -4.1856, -4.1951, -4.1980],\n",
      "        [-2.8655, -3.8869, -0.6706,  ..., -5.7049, -5.6989, -5.6920]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9701,  0.8442,  0.9466, -0.9327, -0.9033, -0.8962, -1.5332,  0.9896],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8947,  0.9900,  0.9900,  0.9900, -0.8959,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.7690e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1810, -0.1828, -0.1847,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8726,  0.9656,  0.9656,  0.9656, -0.8738,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3288, -3.9047, -0.5801,  ..., -4.0954, -4.0865, -4.0872],\n",
      "        [-2.7808, -3.8807, -0.6469,  ..., -5.6329, -5.6607, -5.6688],\n",
      "        [-2.8692, -3.8822, -0.6742,  ..., -5.6131, -5.6026, -5.5917],\n",
      "        ...,\n",
      "        [-2.8515, -3.8853, -0.6735,  ..., -5.6866, -5.6825, -5.6700],\n",
      "        [-3.0059, -3.8365, -0.6816,  ..., -5.1436, -5.0896, -5.0650],\n",
      "        [-2.7803, -3.8890, -0.6682,  ..., -5.4350, -5.4543, -5.4573]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.9199,  0.9751,  1.0008,  1.0095, -0.9046,  1.0014,  0.8884,  0.9849],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.9134, -0.9134,  0.9900,  0.9900, -1.7281,  0.9900,  0.9900, -0.7754],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6379e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1848, -0.1866, -0.1885,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1848, -0.1866, -0.1885,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1336, -0.1349, -0.1363,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8908, -0.8908,  0.9656,  0.9656, -1.6939,  0.9656,  0.9656, -0.7563],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3159, -3.8940, -0.5737,  ..., -4.0832, -4.0719, -4.0713],\n",
      "        [-2.3159, -3.8940, -0.5737,  ..., -4.0832, -4.0719, -4.0713],\n",
      "        [-2.7814, -3.8936, -0.6561,  ..., -5.5957, -5.6164, -5.6237],\n",
      "        ...,\n",
      "        [-2.7814, -3.8936, -0.6561,  ..., -5.5957, -5.6164, -5.6237],\n",
      "        [-2.8576, -3.8783, -0.6663,  ..., -5.6999, -5.6966, -5.6775],\n",
      "        [-2.3906, -3.8789, -0.5642,  ..., -4.7268, -4.7041, -4.6734]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.8838, -0.8838,  0.9762,  0.8384, -1.5263,  0.9762,  1.0027, -0.6884],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.8810, -0.8864, -0.8419,  0.9900, -0.8803, -0.9099, -0.7586],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2718e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1782, -0.1800, -0.1818,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1793, -0.1811, -0.1830,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1781, -0.1799, -0.1817,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1841, -0.1859, -0.1878,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1307, -0.1320, -0.1333,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.8593, -0.8646, -0.8212,  0.9656, -0.8586, -0.8874, -0.7399],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8427, -3.8702, -0.6600,  ..., -5.7171, -5.7082, -5.6933],\n",
      "        [-2.3024, -3.8920, -0.5658,  ..., -4.1262, -4.1172, -4.1166],\n",
      "        [-2.3159, -3.8894, -0.5594,  ..., -4.0871, -4.0800, -4.0842],\n",
      "        ...,\n",
      "        [-2.3054, -3.8888, -0.5566,  ..., -4.1310, -4.1213, -4.1248],\n",
      "        [-2.2907, -3.8889, -0.5583,  ..., -4.0780, -4.0676, -4.0678],\n",
      "        [-2.3864, -3.8744, -0.5481,  ..., -4.6645, -4.6332, -4.5978]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9987, -0.8267, -0.8606, -0.8316,  0.9531, -0.8892, -0.8641, -0.7250],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.5615e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0260, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101, -0.1112, -0.1124, -0.1135, -0.1146, -0.1158,\n",
      "         -0.1170, -0.1181, -0.1193, -0.1205, -0.1218, -0.1230, -0.1242, -0.1255,\n",
      "         -0.1268, -0.1280, -0.1293, -0.1306, -0.1320, -0.1333, -0.1346, -0.1360,\n",
      "         -0.1374, -0.1388, -0.1402, -0.1416, -0.1430, -0.1444, -0.1459, -0.1474,\n",
      "         -0.1489, -0.1504, -0.1519, -0.1534, -0.1550, -0.1565, -0.1581, -0.1597,\n",
      "         -0.1613, -0.1630, -0.1646, -0.1663, -0.1679, -0.1696, -0.1714, -0.1731,\n",
      "         -0.1748, -0.1766, -0.1784, -0.1802, -0.1820, -0.1838, -0.1857, -0.1876,\n",
      "         -0.1895, -0.1914, -0.1933, -0.1953, -0.1972, -0.1992, -0.2013, -0.2033,\n",
      "         -0.2053, -0.2074, -0.2095, -0.2116, -0.2138, -0.2159, -0.2181, -0.2203,\n",
      "         -0.2225, -0.2248, -0.2270, -0.2293, -0.2317, -0.2340, -0.2364, -0.2387,\n",
      "         -0.2412, -0.2436, -0.2461, -0.2485, -0.2511, -0.2536, -0.2562, -0.2587,\n",
      "         -0.2614, -0.2640, -0.2667, -0.2694, -0.2721, -0.2748, -0.2776, -0.2804,\n",
      "         -0.2832, -0.2861, -0.2890, -0.2919, -0.2949, -0.2978, -0.3008, -0.3039,\n",
      "         -0.3069, -0.3100, -0.3132, -0.3163, -0.3195, -0.3228, -0.3260, -0.3293,\n",
      "         -0.3326, -0.3360, -0.3394, -0.3428, -0.3463, -0.3498, -0.3533, -0.3569,\n",
      "         -0.3605, -0.3641, -0.3678, -0.3715, -0.3753, -0.3791, -0.3829, -0.3868,\n",
      "         -0.3907, -0.3946, -0.3986, -0.4026, -0.4067, -0.4108, -0.4150, -0.4192,\n",
      "         -0.4234, -0.4277, -0.4320, -0.4363, -0.4408, -0.4452, -0.4497, -0.4542,\n",
      "         -0.4588, -0.4635, -0.4681, -0.4729, -0.4777, -0.4825, -0.4874, -0.4923,\n",
      "         -0.4972, -0.5023, -0.5073, -0.5125, -0.5176, -0.5229, -0.5282, -0.5335,\n",
      "         -0.5389, -0.5443, -0.5498, -0.5554, -0.5610, -0.5667, -0.5724, -0.5782,\n",
      "         -0.5840, -0.5899, -0.5959, -0.6019, -0.6080, -0.6141, -0.6203, -0.6266,\n",
      "         -0.6329, -0.6393, -0.6457, -0.6523, -0.6589, -0.6655, -0.6722, -0.6790,\n",
      "         -0.6859, -0.6928, -0.6998, -0.7069, -0.7140, -0.7212, -0.7285, -0.7359,\n",
      "         -0.7433, -0.7508, -0.7584, -0.7661, -0.7738, -0.7816, -0.7895, -0.7975,\n",
      "         -0.8055, -0.8137, -0.8219, -0.8302, -0.8386, -0.8470, -0.8556, -0.8642,\n",
      "         -0.8730, -0.8818, -0.8907, -0.8997, -0.9088, -0.9180, -0.9272, -0.9366,\n",
      "         -0.9461, -0.9556, -0.9653, -0.9750, -0.9849, -0.9948, -1.0049, -1.0150,\n",
      "         -1.0253, -1.0356, -1.0461, -1.0567, -1.0673, -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4786e+00, -3.8270e+00, -5.7393e-01, -2.1622e+00,  3.4541e-01,\n",
      "         -2.3412e+00, -4.4619e+00, -4.5855e-02, -3.8315e+00, -2.4011e+00,\n",
      "         -4.0298e+00,  4.3685e-01, -3.5334e-01, -3.5064e+00, -3.1656e+00,\n",
      "         -2.2207e+00, -3.1707e+00, -1.6875e+00, -1.9336e+00, -8.2448e-01,\n",
      "         -1.1914e+00, -1.2924e+00, -6.8792e-01,  4.2881e-01, -1.8027e+00,\n",
      "         -3.4020e+00, -3.1402e+00, -2.4296e+00, -3.7459e+00,  1.0664e+00,\n",
      "         -1.5947e+00, -1.5878e-02, -9.1244e-01, -3.5725e+00, -1.8912e+00,\n",
      "         -2.0898e+00, -8.5729e-01, -3.1017e+00, -1.4122e+00,  2.5514e-01,\n",
      "         -5.2105e-01, -4.3561e-01, -1.5412e-01, -4.2897e+00, -1.4851e+00,\n",
      "         -3.2006e+00, -3.4146e+00, -3.3109e+00, -1.2051e-02,  6.9303e-02,\n",
      "         -3.8542e+00, -1.5414e+00, -7.2739e+00, -2.8725e+00, -2.9403e+00,\n",
      "         -7.5383e-01, -2.5101e+00, -2.8450e+00, -2.6096e+00, -1.4808e+00,\n",
      "         -3.6316e+00, -1.1962e+00, -5.8043e+00, -2.3590e+00, -3.4643e+00,\n",
      "          1.2250e+00, -1.1586e+00,  1.4482e-01, -3.4421e-01, -3.1488e+00,\n",
      "         -4.0376e+00, -5.6632e+00, -2.7826e+00, -3.9736e-01, -3.7802e+00,\n",
      "         -4.3623e+00, -2.3965e+00, -3.8237e+00, -2.3394e+00, -3.5019e+00,\n",
      "         -4.9165e+00, -2.4333e+00, -1.4188e-01, -1.1337e+00, -1.7346e+00,\n",
      "         -2.9120e+00, -1.6761e+00, -4.9465e-01, -3.6271e+00, -7.0846e-01,\n",
      "         -3.1015e+00, -4.2023e+00, -2.3845e+00, -4.7313e-01, -7.1359e-01,\n",
      "         -3.0882e+00, -2.6721e+00, -3.5813e+00, -3.3263e+00, -3.2382e+00,\n",
      "          8.5282e-02, -8.3604e-01, -3.7641e+00, -2.1157e+00, -4.7625e-01,\n",
      "         -1.7577e+00, -3.4193e+00, -3.8407e+00,  3.6767e-01, -5.2814e+00,\n",
      "         -1.8086e+00, -2.7293e+00, -4.2508e+00, -4.6213e+00,  7.5403e-01,\n",
      "         -7.2155e-01, -2.0546e+00, -1.1825e+00, -8.5699e-02, -7.8916e-01,\n",
      "         -2.4776e+00, -4.8052e+00, -2.5339e+00, -7.5023e-01, -9.5024e-01,\n",
      "         -2.8591e+00, -2.0192e+00, -6.7990e-01, -4.9449e-01, -4.2804e+00,\n",
      "         -3.8513e-01,  1.1458e-01, -3.2486e+00, -4.1196e+00, -1.1307e+00,\n",
      "         -1.9822e+00, -3.0465e+00, -2.4278e+00, -1.5394e+00, -2.9518e+00,\n",
      "         -2.8129e+00, -5.9363e+00, -1.4420e+00, -6.4327e+00, -1.6278e+00,\n",
      "         -1.5516e+00, -1.5373e+00, -1.2763e+00, -1.4953e+00, -1.2646e+00,\n",
      "         -1.4028e+00, -4.3090e+00, -4.9287e+00, -4.2573e-01, -1.8451e+00,\n",
      "         -4.6214e+00, -5.7794e+00, -4.7778e+00, -5.3913e+00, -1.7960e+00,\n",
      "         -5.8955e+00, -7.8932e-01, -1.0639e+00, -1.1964e+00, -1.0468e+00,\n",
      "         -1.1498e+00, -9.6413e-01, -1.3711e+00, -4.1087e+00, -3.8872e+00,\n",
      "          9.5296e-02, -1.4943e+00, -3.5852e+00, -4.6192e+00, -3.6677e+00,\n",
      "         -4.8665e+00, -1.2893e+00, -5.6070e+00, -8.7589e-01, -8.4106e-01,\n",
      "         -1.0515e+00, -8.2586e-01, -1.0079e+00, -7.8640e-01, -1.1783e+00,\n",
      "         -3.9981e+00, -3.7315e+00, -1.3516e-03, -1.2618e+00, -3.5791e+00,\n",
      "         -4.7249e+00, -3.5304e+00, -4.9135e+00, -1.1651e+00, -5.4722e+00,\n",
      "         -9.6436e-01, -8.6784e-01, -1.0890e+00, -8.8544e-01, -1.0692e+00,\n",
      "         -8.5667e-01, -1.1883e+00, -3.9267e+00, -3.8173e+00, -9.2492e-02,\n",
      "         -1.2972e+00, -3.6082e+00, -4.8203e+00, -3.4601e+00, -3.4908e+00,\n",
      "         -5.0048e+00, -1.4045e+00, -1.1802e+00, -1.0017e+00, -9.7569e-01,\n",
      "         -9.6530e-01, -9.7926e-01, -9.1232e-01, -8.9539e-01, -9.0913e-01,\n",
      "         -9.5091e-01, -1.2909e+00],\n",
      "        [-2.8385e+00, -3.8661e+00, -6.5297e-01, -2.4323e+00,  2.4273e-01,\n",
      "         -2.3987e+00, -4.5530e+00, -4.2881e-01, -4.2423e+00, -2.6601e+00,\n",
      "         -4.0934e+00,  4.7710e-01, -4.4274e-01, -3.6707e+00, -3.0276e+00,\n",
      "         -2.4159e+00, -3.2442e+00, -1.8157e+00, -1.8482e+00, -1.0408e+00,\n",
      "         -1.3360e+00, -1.5477e+00, -8.9625e-01,  5.5172e-01, -2.0091e+00,\n",
      "         -3.5982e+00, -3.0569e+00, -2.5820e+00, -3.7026e+00,  1.1387e+00,\n",
      "         -1.7446e+00, -2.7017e-01, -9.3300e-01, -3.5996e+00, -1.9941e+00,\n",
      "         -1.9387e+00, -7.6839e-01, -3.1177e+00, -1.6200e+00, -2.1069e-02,\n",
      "         -6.0605e-01, -3.7023e-01, -1.6894e-01, -4.2297e+00, -1.6571e+00,\n",
      "         -3.5762e+00, -3.6712e+00, -3.3487e+00,  5.9524e-01,  5.6751e-01,\n",
      "         -3.9842e+00, -1.5951e+00, -7.2870e+00, -2.9750e+00, -2.9269e+00,\n",
      "         -8.9536e-01, -2.5074e+00, -2.8556e+00, -2.7177e+00, -1.6631e+00,\n",
      "         -3.8017e+00, -1.2560e+00, -5.8511e+00, -2.4339e+00, -3.4266e+00,\n",
      "          1.3184e+00, -1.2990e+00, -1.1525e-01, -3.1573e-01, -3.1919e+00,\n",
      "         -4.2103e+00, -5.7188e+00, -2.8766e+00, -3.0754e-01, -3.9319e+00,\n",
      "         -4.2858e+00, -2.4973e+00, -3.8060e+00, -2.5100e+00, -3.6532e+00,\n",
      "         -4.9695e+00, -2.5296e+00, -2.1676e-01, -1.3805e+00, -1.8935e+00,\n",
      "         -3.0983e+00, -1.8326e+00, -7.3402e-01, -3.8004e+00, -8.2076e-01,\n",
      "         -3.2757e+00, -4.1350e+00, -2.4733e+00, -5.3136e-01, -9.7399e-01,\n",
      "         -3.0593e+00, -2.8373e+00, -3.8943e+00, -3.4350e+00, -3.3573e+00,\n",
      "         -8.1667e-03, -1.0824e+00, -3.7123e+00, -2.1470e+00, -3.8700e-01,\n",
      "         -2.0032e+00, -3.7086e+00, -3.9786e+00,  4.7063e-01, -1.5692e+00,\n",
      "         -1.5329e+00, -5.8324e-01, -4.7189e+00, -4.7182e+00,  6.1030e-01,\n",
      "         -9.7915e-01, -2.1893e+00, -1.3732e+00, -1.8939e-01, -8.9010e-01,\n",
      "         -2.7326e+00, -4.8628e+00, -2.5205e+00, -6.7018e-01, -1.0690e+00,\n",
      "         -2.9516e+00, -2.2000e+00, -7.8167e-01, -6.5141e-01, -4.2950e+00,\n",
      "          1.2782e-01,  6.1575e-01, -3.2916e+00, -4.1169e+00, -1.2266e+00,\n",
      "         -1.9842e+00, -3.0200e+00, -2.5014e+00, -1.6447e+00, -2.8188e+00,\n",
      "         -2.9668e+00, -5.9488e+00, -1.6970e+00, -2.5270e+00, -1.4225e+00,\n",
      "         -9.3589e-01, -5.5265e-01, -4.2818e+00, -4.1183e+00, -3.1488e-01,\n",
      "         -1.9800e+00, -7.7885e-01, -1.1120e+00, -3.3156e-01, -2.7497e-01,\n",
      "         -5.7513e-01, -4.6680e+00, -3.4427e+00, -4.9947e+00, -1.2729e+00,\n",
      "          8.9525e-01,  9.6590e-01,  1.0047e+00,  1.0047e+00,  1.0888e+00,\n",
      "          1.0148e+00, -4.6827e+00, -4.9027e+00, -5.2783e+00, -5.1356e+00,\n",
      "         -5.3106e+00, -5.3561e+00, -5.4323e+00, -5.4420e+00, -5.4543e+00,\n",
      "         -5.4208e+00, -5.3205e+00, -5.1776e+00, -5.0803e+00, -5.0430e+00,\n",
      "         -5.0549e+00, -5.0858e+00, -5.1358e+00, -5.1907e+00, -5.2255e+00,\n",
      "         -5.2486e+00, -5.2751e+00, -5.3114e+00, -5.3675e+00, -5.4314e+00,\n",
      "         -5.4712e+00, -5.4892e+00, -5.4850e+00, -5.4970e+00, -5.5306e+00,\n",
      "         -5.5834e+00, -5.6103e+00, -5.6064e+00, -5.5794e+00, -5.5386e+00,\n",
      "         -5.5129e+00, -5.4923e+00, -5.4955e+00, -5.5140e+00, -5.5432e+00,\n",
      "         -5.5830e+00, -5.6183e+00, -5.6435e+00, -5.6501e+00, -5.6316e+00,\n",
      "         -5.6377e+00, -5.6527e+00, -5.6733e+00, -5.7112e+00, -5.7358e+00,\n",
      "         -5.7404e+00, -5.7359e+00, -5.7271e+00, -5.7239e+00, -5.7207e+00,\n",
      "         -5.7107e+00, -5.6969e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0061,  0.9957], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.8231,  0.9900, -0.8118,  0.9900, -0.7370,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0764e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1665, -0.1682, -0.1699,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1642, -0.1659, -0.1676,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.8028,  0.9656, -0.7917,  0.9656, -0.7188,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3066, -3.8844, -0.5481,  ..., -4.0846, -4.0769, -4.0813],\n",
      "        [-2.8351, -3.8634, -0.6523,  ..., -5.6673, -5.6638, -5.6506],\n",
      "        [-2.3029, -3.8849, -0.5512,  ..., -4.1499, -4.1440, -4.1463],\n",
      "        ...,\n",
      "        [-2.8347, -3.8718, -0.6459,  ..., -5.7535, -5.7568, -5.7395],\n",
      "        [-2.7618, -3.8695, -0.6434,  ..., -5.4544, -5.4779, -5.4770],\n",
      "        [-2.9904, -3.8260, -0.6596,  ..., -5.2030, -5.1476, -5.1189]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7761,  0.9465, -0.7847,  0.9937, -0.6740,  0.9980,  0.9784,  0.8332],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7796, -1.6550,  0.9900,  0.9900,  0.9900, -0.8473],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.2181e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1577, -0.1593, -0.1609,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1714, -0.1731, -0.1749,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7604, -1.6222,  0.9656,  0.9656,  0.9656, -0.8264],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7613, -3.8539, -0.6173,  ..., -5.6488, -5.6752, -5.6796],\n",
      "        [-2.7747, -3.8745, -0.6355,  ..., -5.6180, -5.6369, -5.6446],\n",
      "        [-2.3038, -3.8706, -0.5526,  ..., -4.1635, -4.1523, -4.1545],\n",
      "        ...,\n",
      "        [-2.8526, -3.8591, -0.6454,  ..., -5.6311, -5.6241, -5.6118],\n",
      "        [-2.8421, -3.8577, -0.6471,  ..., -5.7264, -5.7179, -5.7053],\n",
      "        [-2.2782, -3.8744, -0.5438,  ..., -4.0815, -4.0688, -4.0709]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9643,  0.9602, -0.7940, -1.5389,  0.8314,  0.9903,  0.9869, -0.8300],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7490, -0.8256, -0.7947, -1.6342, -0.7490,  0.9900, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.4869e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1515, -0.1530, -0.1546,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1670, -0.1687, -0.1704,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1515, -0.1530, -0.1546,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7305, -0.8052, -0.7751, -1.6019, -0.7305,  0.9608, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8446, -3.8543, -0.6337,  ..., -5.7293, -5.7244, -5.7192],\n",
      "        [-2.3044, -3.8687, -0.5514,  ..., -4.1457, -4.1360, -4.1395],\n",
      "        [-2.2976, -3.8695, -0.5504,  ..., -4.0908, -4.0794, -4.0796],\n",
      "        ...,\n",
      "        [-2.3044, -3.8687, -0.5514,  ..., -4.1457, -4.1360, -4.1395],\n",
      "        [-2.9824, -3.8126, -0.6528,  ..., -5.2654, -5.1727, -5.1112],\n",
      "        [-2.4782, -3.8153, -0.5654,  ..., -1.0531, -1.0929, -1.2649]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9819, -0.7857, -0.8158, -0.8342, -1.5380, -0.7857,  0.9103, -1.1131],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7729, -0.7650,  0.9900,  0.9900,  0.9900, -0.6490],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0576e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.1348e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1564, -0.1579, -0.1595,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1118, -0.1129, -0.1141,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7538, -0.7461,  0.9656,  0.9656,  0.9656, -0.6329],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8354, -3.8498, -0.6449,  ..., -5.6805, -5.6799, -5.6646],\n",
      "        [-2.7599, -3.8443, -0.6148,  ..., -5.6675, -5.6929, -5.6994],\n",
      "        [-2.3061, -3.8647, -0.5428,  ..., -4.0902, -4.0838, -4.0891],\n",
      "        ...,\n",
      "        [-2.8459, -3.8576, -0.6501,  ..., -5.7757, -5.7728, -5.7603],\n",
      "        [-2.9954, -3.8031, -0.6486,  ..., -5.1794, -5.1295, -5.1049],\n",
      "        [-2.3764, -3.8518, -0.5297,  ..., -4.6961, -4.6613, -4.6282]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9434,  0.9621, -0.8020, -0.7666,  0.9885,  0.9560,  0.8785, -0.7141],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.2546, -0.7462, -0.7555,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0149e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.6946e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1460, -0.1475, -0.1490,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.1996, -0.7278, -0.7369,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8300, -3.8464, -0.6499,  ..., -5.7148, -5.7127, -5.6976],\n",
      "        [-2.8325, -3.8555, -0.6492,  ..., -5.7844, -5.7840, -5.7707],\n",
      "        [-2.4456, -3.8170, -0.5465,  ..., -4.1340, -4.1366, -4.1294],\n",
      "        ...,\n",
      "        [-2.8351, -3.8461, -0.6488,  ..., -5.7528, -5.7477, -5.7341],\n",
      "        [-2.7584, -3.8533, -0.6445,  ..., -5.5018, -5.5256, -5.5256],\n",
      "        [-2.9929, -3.8037, -0.6550,  ..., -5.2115, -5.1601, -5.1339]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9109,  0.9874, -1.3045, -0.8089, -0.7936,  0.9789,  0.9617,  0.8412],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7730,  0.9900, -1.4346, -0.7627, -0.7446,  0.9900,  0.9900, -0.7405],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7836e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1564, -0.1580, -0.1596,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2788, -0.2816, -0.2844,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1498, -0.1513, -0.1528,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7539,  0.9656, -1.3717, -0.7439, -0.7262,  0.9656,  0.9656, -0.7222],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3028, -3.8615, -0.5786,  ..., -4.0896, -4.0787, -4.0786],\n",
      "        [-2.7658, -3.8656, -0.6574,  ..., -5.6712, -5.6885, -5.6977],\n",
      "        [-2.1978, -3.8654, -0.5184,  ..., -4.4562, -4.4142, -4.3801],\n",
      "        ...,\n",
      "        [-2.9908, -3.8044, -0.6710,  ..., -5.2184, -5.1693, -5.1443],\n",
      "        [-2.8282, -3.8519, -0.6664,  ..., -5.7414, -5.7357, -5.7279],\n",
      "        [-2.3000, -3.8649, -0.5780,  ..., -4.1202, -4.1095, -4.1135]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7943,  0.9503, -1.4211, -0.8006, -0.8091,  0.8391,  0.9838, -0.7758],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6300, -0.7064], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(8.2052e-09, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1085, -0.1096, -0.1107, -0.1118, -0.1130, -0.1141, -0.1153, -0.1164,\n",
      "         -0.1176, -0.1188, -0.1200, -0.1212, -0.1224, -0.1237, -0.1249, -0.1262,\n",
      "         -0.1274, -0.1287, -0.1300, -0.1313, -0.1327, -0.1340, -0.1354, -0.1367,\n",
      "         -0.1381, -0.1395, -0.1409, -0.1423, -0.1438, -0.1452, -0.1467, -0.1482,\n",
      "         -0.1497, -0.1512, -0.1527, -0.1543, -0.1558, -0.1574, -0.1590, -0.1606,\n",
      "         -0.1622, -0.1638, -0.1655, -0.1672, -0.1689, -0.1706, -0.1723, -0.1740,\n",
      "         -0.1758, -0.1776, -0.1794, -0.1812, -0.1830, -0.1848, -0.1867, -0.1886,\n",
      "         -0.1905, -0.1924, -0.1944, -0.1963, -0.1983, -0.2003, -0.2023, -0.2044,\n",
      "         -0.2065, -0.2085, -0.2106, -0.2128, -0.2149, -0.2171, -0.2193, -0.2215,\n",
      "         -0.2237, -0.2260, -0.2283, -0.2306, -0.2329, -0.2353, -0.2376, -0.2400,\n",
      "         -0.2425, -0.2449, -0.2474, -0.2499, -0.2524, -0.2550, -0.2575, -0.2601,\n",
      "         -0.2628, -0.2654, -0.2681, -0.2708, -0.2735, -0.2763, -0.2791, -0.2819,\n",
      "         -0.2848, -0.2876, -0.2906, -0.2935, -0.2965, -0.2994, -0.3025, -0.3055,\n",
      "         -0.3086, -0.3117, -0.3149, -0.3181, -0.3213, -0.3245, -0.3278, -0.3311,\n",
      "         -0.3344, -0.3378, -0.3412, -0.3447, -0.3482, -0.3517, -0.3552, -0.3588,\n",
      "         -0.3624, -0.3661, -0.3698, -0.3735, -0.3773, -0.3811, -0.3850, -0.3889,\n",
      "         -0.3928, -0.3968, -0.4008, -0.4048, -0.4089, -0.4130, -0.4172, -0.4214,\n",
      "         -0.4257, -0.4300, -0.4343, -0.4387, -0.4431, -0.4476, -0.4521, -0.4567,\n",
      "         -0.4613, -0.4660, -0.4707, -0.4754, -0.4802, -0.4851, -0.4900, -0.4949,\n",
      "         -0.4999, -0.5050, -0.5101, -0.5152, -0.5205, -0.5257, -0.5310, -0.5364,\n",
      "         -0.5418, -0.5473, -0.5528, -0.5584, -0.5640, -0.5697, -0.5755, -0.5813,\n",
      "         -0.5872, -0.5931, -0.5991, -0.6051, -0.6112, -0.6174, -0.6237, -0.6300,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1429, -0.1444, -0.1458, -0.1473, -0.1488, -0.1503, -0.1518, -0.1533,\n",
      "         -0.1549, -0.1564, -0.1580, -0.1596, -0.1612, -0.1629, -0.1645, -0.1662,\n",
      "         -0.1678, -0.1695, -0.1712, -0.1730, -0.1747, -0.1765, -0.1783, -0.1801,\n",
      "         -0.1819, -0.1837, -0.1856, -0.1875, -0.1894, -0.1913, -0.1932, -0.1952,\n",
      "         -0.1971, -0.1991, -0.2011, -0.2032, -0.2052, -0.2073, -0.2094, -0.2115,\n",
      "         -0.2136, -0.2158, -0.2180, -0.2202, -0.2224, -0.2246, -0.2269, -0.2292,\n",
      "         -0.2315, -0.2339, -0.2362, -0.2386, -0.2410, -0.2434, -0.2459, -0.2484,\n",
      "         -0.2509, -0.2534, -0.2560, -0.2586, -0.2612, -0.2638, -0.2665, -0.2692,\n",
      "         -0.2719, -0.2746, -0.2774, -0.2802, -0.2831, -0.2859, -0.2888, -0.2917,\n",
      "         -0.2947, -0.2976, -0.3006, -0.3037, -0.3068, -0.3099, -0.3130, -0.3161,\n",
      "         -0.3193, -0.3226, -0.3258, -0.3291, -0.3324, -0.3358, -0.3392, -0.3426,\n",
      "         -0.3461, -0.3496, -0.3531, -0.3567, -0.3603, -0.3639, -0.3676, -0.3713,\n",
      "         -0.3750, -0.3788, -0.3827, -0.3865, -0.3904, -0.3944, -0.3984, -0.4024,\n",
      "         -0.4064, -0.4106, -0.4147, -0.4189, -0.4231, -0.4274, -0.4317, -0.4361,\n",
      "         -0.4405, -0.4449, -0.4494, -0.4540, -0.4585, -0.4632, -0.4679, -0.4726,\n",
      "         -0.4774, -0.4822, -0.4870, -0.4920, -0.4969, -0.5020, -0.5070, -0.5121,\n",
      "         -0.5173, -0.5225, -0.5278, -0.5332, -0.5385, -0.5440, -0.5495, -0.5550,\n",
      "         -0.5606, -0.5663, -0.5720, -0.5778, -0.5836, -0.5895, -0.5955, -0.6015,\n",
      "         -0.6076, -0.6137, -0.6199, -0.6262, -0.6325, -0.6389, -0.6453, -0.6519,\n",
      "         -0.6584, -0.6651, -0.6718, -0.6786, -0.6854, -0.6924, -0.6994, -0.7064,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6144, -0.6890], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3727, -3.8490, -0.5653, -2.1428,  0.3555, -2.3073, -4.3655, -0.0354,\n",
      "         -3.7203, -2.4058, -3.9687,  0.4776, -0.3057, -3.5429, -3.1312, -2.1355,\n",
      "         -3.1515, -1.6862, -1.9421, -0.8285, -1.2169, -1.2927, -0.7274,  0.4418,\n",
      "         -1.7911, -3.4392, -3.0982, -2.3652, -3.7445,  1.0558, -1.6233, -0.0259,\n",
      "         -0.9945, -3.5155, -1.9736, -2.1463, -0.9536, -3.0910, -1.4473,  0.2026,\n",
      "         -0.5337, -0.5100, -0.1586, -4.3707, -1.4829, -3.1261, -3.4153, -3.2531,\n",
      "         -0.0735, -0.0579, -3.9097, -1.4849, -7.3051, -2.8147, -2.9239, -0.7496,\n",
      "         -2.5749, -2.8863, -2.5916, -1.4903, -3.6713, -1.1476, -5.8443, -2.3200,\n",
      "         -3.4704,  1.2168, -1.1847,  0.1488, -0.4164, -3.0916, -3.9894, -5.6623,\n",
      "         -2.7080, -0.4291, -3.8218, -4.3348, -2.3312, -3.8028, -2.3738, -3.5477,\n",
      "         -4.9120, -2.3718, -0.2002, -1.1487, -1.7449, -2.9083, -1.6880, -0.4968,\n",
      "         -3.6349, -0.7145, -3.1381, -4.1629, -2.3265, -0.5402, -0.7241, -3.1795,\n",
      "         -2.6604, -3.5001, -3.3345, -3.2344,  0.0348, -0.8560, -3.7736, -2.1748,\n",
      "         -0.4704, -1.7624, -3.3390, -3.8200,  0.3424, -1.2991, -1.8024, -0.8317,\n",
      "         -4.4321, -4.6362,  0.5529, -0.8180, -2.0864, -1.2208, -0.1315, -0.8788,\n",
      "         -2.4584, -4.7866, -2.5488, -0.7341, -1.0366, -2.8819, -2.0379, -0.7065,\n",
      "         -0.3738, -4.2389, -0.5956, -0.0366, -3.2879, -4.0701, -1.1058, -2.0332,\n",
      "         -3.0698, -2.4150, -1.5471, -2.8146, -2.6686, -5.9076, -1.4219, -2.6333,\n",
      "         -1.2862, -1.2328, -0.6688, -3.9734, -4.0798, -0.4157, -1.9556, -0.4454,\n",
      "         -1.5032, -0.7027, -0.0244, -1.0406, -4.0734, -2.9280, -4.9914, -1.2033,\n",
      "         -0.6607, -0.6057, -0.8712, -0.7992, -0.6899, -0.6657, -4.9484, -4.9728,\n",
      "         -5.3418, -4.9358, -5.2844, -5.2813, -5.1242, -5.0790, -5.0740, -5.1333,\n",
      "         -5.2361, -5.1960, -4.9507, -4.6779, -4.5762, -4.5734, -4.6041, -4.6435,\n",
      "         -4.6562, -4.6698, -4.6933, -4.7283, -4.7766, -4.8177, -4.8321, -4.8402,\n",
      "         -4.8532, -4.8710, -4.8882, -4.9012, -4.8965, -4.8787, -4.8604, -4.8498,\n",
      "         -4.8563, -4.8743, -4.8844, -4.8873, -4.8858, -4.8690, -4.8538, -4.8224,\n",
      "         -4.7773, -4.7426, -4.7354, -4.7428, -4.7538, -4.7773, -4.8169, -4.8127,\n",
      "         -4.8062, -4.7945, -4.7692, -4.7364, -4.7019, -4.6701],\n",
      "        [-2.3148, -3.8586, -0.5935, -2.0994,  0.3354, -2.3151, -4.3647, -0.0165,\n",
      "         -3.6805, -2.4118, -3.9744,  0.5108, -0.2644, -3.5291, -3.1171, -2.1195,\n",
      "         -3.1468, -1.7181, -1.9328, -0.8181, -1.2191, -1.3026, -0.7194,  0.4485,\n",
      "         -1.7678, -3.4404, -3.1060, -2.3529, -3.7366,  1.0685, -1.5986, -0.0144,\n",
      "         -0.9762, -3.5112, -1.9456, -2.1459, -0.8874, -3.0902, -1.4450,  0.2105,\n",
      "         -0.5711, -0.4522, -0.1718, -4.3660, -1.4661, -3.1001, -3.4004, -3.2510,\n",
      "         -0.0645, -0.0734, -3.9162, -1.4929, -7.3027, -2.8216, -2.9328, -0.7678,\n",
      "         -2.5604, -2.8595, -2.6147, -1.4775, -3.6747, -1.1713, -5.8554, -2.3313,\n",
      "         -3.4672,  1.2246, -1.1658,  0.1512, -0.4045, -3.0934, -4.0083, -5.7154,\n",
      "         -2.7194, -0.4316, -3.8313, -4.3260, -2.3457, -3.8136, -2.3816, -3.5592,\n",
      "         -4.9734, -2.3891, -0.2095, -1.1500, -1.8012, -2.9013, -1.6882, -0.4962,\n",
      "         -3.6696, -0.7455, -3.1560, -4.1551, -2.3401, -0.5451, -0.7345, -3.1722,\n",
      "         -2.6621, -3.4852, -3.3374, -3.2332,  0.0194, -0.8654, -3.7630, -2.1887,\n",
      "         -0.4649, -1.7651, -3.3344, -3.8373,  0.3380, -1.5367, -1.7384, -0.5853,\n",
      "         -4.4250, -4.6645,  0.6347, -0.7830, -2.0952, -1.1987, -0.1318, -0.8261,\n",
      "         -2.4991, -4.7392, -2.5525, -0.7707, -0.9887, -2.8460, -2.0336, -0.6885,\n",
      "         -0.4069, -4.2163, -0.5345, -0.0422, -3.2766, -4.0623, -1.1425, -2.0172,\n",
      "         -3.0275, -2.4419, -1.5506, -2.8223, -2.5774, -3.2453, -5.7877, -1.1798,\n",
      "         -0.7351, -0.7777, -0.8103, -0.7814, -0.7540, -0.7254, -4.8701, -4.9497,\n",
      "         -5.3839, -5.0004, -4.9274, -4.6429, -4.5194, -4.6271, -4.6064, -4.6802,\n",
      "         -4.8730, -4.9723, -5.0138, -4.9740, -4.8639, -4.7698, -4.7494, -4.7668,\n",
      "         -4.7701, -4.7435, -4.7067, -4.6955, -4.7160, -4.7399, -4.7525, -4.7462,\n",
      "         -4.7029, -4.6741, -4.6762, -4.6642, -4.6957, -4.7520, -4.7781, -4.7716,\n",
      "         -4.7696, -4.7512, -4.7580, -4.7665, -4.7663, -4.7689, -4.7695, -4.8146,\n",
      "         -4.8695, -4.8977, -4.9046, -4.8970, -4.8978, -4.8850, -4.8815, -4.8808,\n",
      "         -4.8752, -4.8787, -4.8673, -4.8440, -4.8410, -4.8357, -4.8029, -4.7782,\n",
      "         -4.7365, -4.6800, -4.6753, -4.6162, -4.5847, -4.5235, -4.4895, -4.3705,\n",
      "         -4.2785, -4.2341, -4.1956, -4.1614, -4.1507, -4.1528]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7154, -0.7640], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7770,  0.9900, -0.7550, -0.7455,  0.9900, -0.7492, -0.7455, -0.6319],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2239e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1572, -0.1588, -0.1604,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1527, -0.1543, -0.1558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1516, -0.1531, -0.1546,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1508, -0.1523, -0.1539,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1088, -0.1099, -0.1111,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7578,  0.9656, -0.7364, -0.7271,  0.9656, -0.7307, -0.7271, -0.6163],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3039, -3.8596, -0.6039,  ..., -4.0840, -4.0754, -4.0761],\n",
      "        [-2.8263, -3.8476, -0.6918,  ..., -5.7647, -5.7601, -5.7502],\n",
      "        [-2.3112, -3.8605, -0.5965,  ..., -4.0908, -4.0847, -4.0889],\n",
      "        ...,\n",
      "        [-2.3106, -3.8659, -0.6020,  ..., -4.0909, -4.0793, -4.0838],\n",
      "        [-2.2978, -3.8633, -0.6036,  ..., -4.1132, -4.1030, -4.1058],\n",
      "        [-2.3676, -3.8484, -0.5753,  ..., -4.7391, -4.7052, -4.6733]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7815,  0.9507, -0.7772, -0.7768,  0.9174, -0.7918, -0.7768, -0.7013],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.7568,  0.9900,  0.9900, -0.7766, -0.7671],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1382e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1571, -0.1587, -0.1603,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1552, -0.1568, -0.1583,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656,  0.9656, -0.7381,  0.9656,  0.9656, -0.7574, -0.7482],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9687, -3.8032, -0.6999,  ..., -5.2985, -5.2063, -5.1535],\n",
      "        [-2.9799, -3.8077, -0.7008,  ..., -5.2469, -5.2003, -5.1758],\n",
      "        [-2.8185, -3.8523, -0.6988,  ..., -5.7472, -5.7426, -5.7356],\n",
      "        ...,\n",
      "        [-2.8222, -3.8579, -0.6974,  ..., -5.7945, -5.7941, -5.7850],\n",
      "        [-2.3051, -3.8586, -0.6116,  ..., -4.0827, -4.0728, -4.0739],\n",
      "        [-2.2992, -3.8580, -0.6017,  ..., -4.1400, -4.1296, -4.1328]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9228,  0.8441,  0.9875, -0.7640,  0.9789,  0.9923, -0.7692, -0.7848],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6218,  0.9900,  0.9900, -1.2307, -1.4246,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0238e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1071, -0.1082, -0.1093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1432, -0.1447, -0.1462,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2768, -0.2796, -0.2825,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6064,  0.9656,  0.9656, -1.1768, -1.3622,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7589, -3.8565, -0.7030,  ..., -5.5289, -5.5454, -5.5456],\n",
      "        [-2.9851, -3.8104, -0.7076,  ..., -5.2385, -5.1890, -5.1617],\n",
      "        [-2.3713, -3.8493, -0.5901,  ..., -4.7260, -4.6908, -4.6588],\n",
      "        ...,\n",
      "        [-2.4460, -3.8170, -0.6024,  ..., -4.1337, -4.1354, -4.1285],\n",
      "        [-2.1973, -3.8617, -0.5554,  ..., -4.4588, -4.4181, -4.3833],\n",
      "        [-2.8492, -3.8504, -0.7047,  ..., -5.6565, -5.6490, -5.6456]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9338,  0.8537, -0.6756,  0.9616,  0.9574, -1.2588, -1.3807,  0.9574],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7636, -0.6988, -0.6227, -1.5846, -1.5846,  0.9900, -0.6868, -0.7041],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(9.8509e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1545, -0.1560, -0.1576,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1414, -0.1428, -0.1442,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1073, -0.1084, -0.1094,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1389, -0.1403, -0.1418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1424, -0.1439, -0.1453,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7447, -0.6816, -0.6074, -1.5533, -1.5533,  0.9656, -0.6699, -0.6867],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2818, -3.8581, -0.6113,  ..., -4.0636, -4.0544, -4.0546],\n",
      "        [-2.3111, -3.8556, -0.6199,  ..., -4.1543, -4.1441, -4.1429],\n",
      "        [-2.3698, -3.8486, -0.6044,  ..., -4.7657, -4.7405, -4.7134],\n",
      "        ...,\n",
      "        [-2.8380, -3.8486, -0.7092,  ..., -5.7411, -5.7341, -5.7269],\n",
      "        [-2.3101, -3.8569, -0.6195,  ..., -4.1282, -4.1195, -4.1227],\n",
      "        [-2.3088, -3.8635, -0.6176,  ..., -4.1418, -4.1330, -4.1375]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7586, -0.7166, -0.6676, -1.4856, -1.4856,  0.9548, -0.7323, -0.6881],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -0.6899,  0.9900, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.7238e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1396, -0.1410, -0.1424,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -0.6729,  0.9656, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8322, -3.8578, -0.7072,  ..., -5.7556, -5.7550, -5.7456],\n",
      "        [-2.7637, -3.8462, -0.6849,  ..., -5.6730, -5.6982, -5.7006],\n",
      "        [-2.8450, -3.8512, -0.7083,  ..., -5.7151, -5.7103, -5.6953],\n",
      "        ...,\n",
      "        [-2.3017, -3.8523, -0.6175,  ..., -4.1522, -4.1408, -4.1406],\n",
      "        [-2.9933, -3.8021, -0.7117,  ..., -5.1658, -5.1183, -5.0939],\n",
      "        [-2.4720, -3.8057, -0.6298,  ..., -1.1597, -1.1909, -1.1504]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0070,  0.9563,  1.0072,  0.9320,  0.9763, -0.7050,  0.9113, -1.1756],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7015, -0.7245,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6215e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1419, -0.1434, -0.1448,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6842, -0.7067,  0.9656,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7642, -3.8657, -0.6992,  ..., -5.6454, -5.6569, -5.6585],\n",
      "        [-2.9856, -3.8056, -0.7113,  ..., -5.1733, -5.1235, -5.0928],\n",
      "        [-2.2955, -3.8585, -0.6089,  ..., -4.0631, -4.0562, -4.0592],\n",
      "        ...,\n",
      "        [-2.8406, -3.8559, -0.7133,  ..., -5.7510, -5.7465, -5.7424],\n",
      "        [-2.7622, -3.8437, -0.6821,  ..., -5.6660, -5.6913, -5.6959],\n",
      "        [-2.9856, -3.8056, -0.7113,  ..., -5.1733, -5.1235, -5.0928]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9758,  0.8750, -0.6538, -0.7263,  0.9708,  0.9805,  0.9538,  0.8750],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6807, -0.7281], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1377, -0.1391, -0.1405, -0.1419, -0.1433, -0.1448, -0.1463, -0.1477,\n",
      "         -0.1492, -0.1507, -0.1523, -0.1538, -0.1554, -0.1569, -0.1585, -0.1601,\n",
      "         -0.1617, -0.1634, -0.1650, -0.1667, -0.1684, -0.1701, -0.1718, -0.1735,\n",
      "         -0.1753, -0.1770, -0.1788, -0.1806, -0.1825, -0.1843, -0.1862, -0.1880,\n",
      "         -0.1899, -0.1919, -0.1938, -0.1958, -0.1977, -0.1997, -0.2017, -0.2038,\n",
      "         -0.2058, -0.2079, -0.2100, -0.2121, -0.2143, -0.2164, -0.2186, -0.2208,\n",
      "         -0.2231, -0.2253, -0.2276, -0.2299, -0.2322, -0.2346, -0.2369, -0.2393,\n",
      "         -0.2417, -0.2442, -0.2467, -0.2491, -0.2517, -0.2542, -0.2568, -0.2594,\n",
      "         -0.2620, -0.2646, -0.2673, -0.2700, -0.2727, -0.2755, -0.2783, -0.2811,\n",
      "         -0.2839, -0.2868, -0.2897, -0.2926, -0.2956, -0.2986, -0.3016, -0.3046,\n",
      "         -0.3077, -0.3108, -0.3139, -0.3171, -0.3203, -0.3236, -0.3268, -0.3301,\n",
      "         -0.3335, -0.3368, -0.3402, -0.3437, -0.3471, -0.3506, -0.3542, -0.3578,\n",
      "         -0.3614, -0.3650, -0.3687, -0.3724, -0.3762, -0.3800, -0.3838, -0.3877,\n",
      "         -0.3916, -0.3956, -0.3996, -0.4036, -0.4077, -0.4118, -0.4160, -0.4202,\n",
      "         -0.4244, -0.4287, -0.4330, -0.4374, -0.4418, -0.4463, -0.4508, -0.4554,\n",
      "         -0.4600, -0.4646, -0.4693, -0.4740, -0.4788, -0.4837, -0.4885, -0.4935,\n",
      "         -0.4985, -0.5035, -0.5086, -0.5137, -0.5189, -0.5241, -0.5294, -0.5348,\n",
      "         -0.5402, -0.5456, -0.5512, -0.5567, -0.5623, -0.5680, -0.5738, -0.5796,\n",
      "         -0.5854, -0.5913, -0.5973, -0.6033, -0.6094, -0.6156, -0.6218, -0.6281,\n",
      "         -0.6344, -0.6408, -0.6473, -0.6539, -0.6605, -0.6671, -0.6739, -0.6807,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1473, -0.1488, -0.1503, -0.1518, -0.1533, -0.1549, -0.1565, -0.1580,\n",
      "         -0.1596, -0.1612, -0.1629, -0.1645, -0.1662, -0.1679, -0.1696, -0.1713,\n",
      "         -0.1730, -0.1747, -0.1765, -0.1783, -0.1801, -0.1819, -0.1838, -0.1856,\n",
      "         -0.1875, -0.1894, -0.1913, -0.1932, -0.1952, -0.1971, -0.1991, -0.2012,\n",
      "         -0.2032, -0.2052, -0.2073, -0.2094, -0.2115, -0.2137, -0.2158, -0.2180,\n",
      "         -0.2202, -0.2224, -0.2247, -0.2269, -0.2292, -0.2315, -0.2339, -0.2362,\n",
      "         -0.2386, -0.2410, -0.2435, -0.2459, -0.2484, -0.2509, -0.2535, -0.2560,\n",
      "         -0.2586, -0.2612, -0.2639, -0.2665, -0.2692, -0.2719, -0.2747, -0.2775,\n",
      "         -0.2803, -0.2831, -0.2860, -0.2888, -0.2918, -0.2947, -0.2977, -0.3007,\n",
      "         -0.3037, -0.3068, -0.3099, -0.3130, -0.3162, -0.3194, -0.3226, -0.3259,\n",
      "         -0.3292, -0.3325, -0.3358, -0.3392, -0.3427, -0.3461, -0.3496, -0.3531,\n",
      "         -0.3567, -0.3603, -0.3640, -0.3676, -0.3713, -0.3751, -0.3789, -0.3827,\n",
      "         -0.3866, -0.3905, -0.3944, -0.3984, -0.4024, -0.4065, -0.4106, -0.4148,\n",
      "         -0.4189, -0.4232, -0.4274, -0.4318, -0.4361, -0.4405, -0.4450, -0.4495,\n",
      "         -0.4540, -0.4586, -0.4632, -0.4679, -0.4726, -0.4774, -0.4822, -0.4871,\n",
      "         -0.4920, -0.4970, -0.5020, -0.5071, -0.5122, -0.5174, -0.5226, -0.5279,\n",
      "         -0.5332, -0.5386, -0.5440, -0.5495, -0.5551, -0.5607, -0.5664, -0.5721,\n",
      "         -0.5779, -0.5837, -0.5896, -0.5956, -0.6016, -0.6076, -0.6138, -0.6200,\n",
      "         -0.6262, -0.6326, -0.6390, -0.6454, -0.6519, -0.6585, -0.6652, -0.6719,\n",
      "         -0.6787, -0.6855, -0.6925, -0.6995, -0.7065, -0.7137, -0.7209, -0.7281,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6639, -0.7102], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2881e+00, -3.8505e+00, -6.1024e-01, -2.0963e+00,  3.4076e-01,\n",
      "         -2.3211e+00, -4.3711e+00, -1.8861e-03, -3.6513e+00, -2.3728e+00,\n",
      "         -3.9450e+00,  5.3039e-01, -1.9063e-01, -3.4804e+00, -3.0892e+00,\n",
      "         -2.0722e+00, -3.1301e+00, -1.7171e+00, -1.9347e+00, -8.2828e-01,\n",
      "         -1.1991e+00, -1.2990e+00, -7.5415e-01,  4.6298e-01, -1.7796e+00,\n",
      "         -3.4090e+00, -3.0800e+00, -2.3143e+00, -3.7256e+00,  1.0563e+00,\n",
      "         -1.5941e+00, -3.0968e-02, -9.7994e-01, -3.4843e+00, -1.9332e+00,\n",
      "         -2.0024e+00, -8.9363e-01, -3.0787e+00, -1.4575e+00,  2.0818e-01,\n",
      "         -5.5847e-01, -4.5584e-01, -1.9115e-01, -4.4088e+00, -1.4726e+00,\n",
      "         -3.0750e+00, -3.3678e+00, -3.2192e+00, -1.0959e-01, -5.1453e-02,\n",
      "         -3.8732e+00, -1.4751e+00, -7.3207e+00, -2.7778e+00, -2.9194e+00,\n",
      "         -7.7362e-01, -2.5906e+00, -2.8828e+00, -2.6390e+00, -1.4816e+00,\n",
      "         -3.6438e+00, -1.1592e+00, -5.8701e+00, -2.2877e+00, -3.4659e+00,\n",
      "          1.2082e+00, -1.1682e+00,  1.2832e-01, -4.1028e-01, -3.0705e+00,\n",
      "         -3.9589e+00, -5.7163e+00, -2.6790e+00, -3.2554e-01, -3.8042e+00,\n",
      "         -4.2956e+00, -2.3070e+00, -3.7999e+00, -2.3964e+00, -3.5289e+00,\n",
      "         -4.9739e+00, -2.3478e+00, -2.3512e-01, -1.1667e+00, -1.7880e+00,\n",
      "         -2.9038e+00, -1.6829e+00, -5.0999e-01, -3.6701e+00, -7.2711e-01,\n",
      "         -3.1237e+00, -4.1248e+00, -2.3010e+00, -5.7307e-01, -7.5073e-01,\n",
      "         -3.1941e+00, -2.6602e+00, -3.4610e+00, -3.3191e+00, -3.2423e+00,\n",
      "         -8.4812e-03, -8.8242e-01, -3.7962e+00, -2.2203e+00, -4.5800e-01,\n",
      "         -1.7494e+00, -3.3146e+00, -3.7709e+00,  4.4843e-01, -1.0681e+00,\n",
      "         -1.5205e+00, -5.9452e-01, -4.3202e+00, -4.6765e+00,  5.4170e-01,\n",
      "         -7.8395e-01, -2.0629e+00, -1.2391e+00, -5.4740e-02, -8.5144e-01,\n",
      "         -2.4475e+00, -4.7012e+00, -2.5914e+00, -8.0551e-01, -1.0604e+00,\n",
      "         -2.9185e+00, -2.0262e+00, -7.0226e-01, -3.9226e-01, -4.2221e+00,\n",
      "         -5.1680e-01, -3.2614e-02, -3.2482e+00, -4.0642e+00, -1.1196e+00,\n",
      "         -2.0608e+00, -3.0807e+00, -2.4811e+00, -1.5625e+00, -2.8362e+00,\n",
      "         -2.5627e+00, -3.1738e+00, -5.8098e+00, -1.2169e+00, -5.7982e-01,\n",
      "         -6.5760e-01, -7.8781e-01, -6.7033e-01, -8.2614e-01, -6.7994e-01,\n",
      "         -4.8475e+00, -4.9580e+00, -5.4046e+00, -4.9728e+00, -4.8594e+00,\n",
      "         -4.5894e+00, -4.4628e+00, -4.5393e+00, -4.4920e+00, -4.5603e+00,\n",
      "         -4.8088e+00, -4.9598e+00, -5.0201e+00, -4.9862e+00, -4.8661e+00,\n",
      "         -4.7350e+00, -4.6736e+00, -4.6565e+00, -4.6446e+00, -4.6221e+00,\n",
      "         -4.5879e+00, -4.5823e+00, -4.6004e+00, -4.6273e+00, -4.6514e+00,\n",
      "         -4.6607e+00, -4.6308e+00, -4.6127e+00, -4.6265e+00, -4.6202e+00,\n",
      "         -4.6392e+00, -4.6848e+00, -4.6968e+00, -4.6800e+00, -4.6824e+00,\n",
      "         -4.6801e+00, -4.6994e+00, -4.7153e+00, -4.7184e+00, -4.7155e+00,\n",
      "         -4.7176e+00, -4.7471e+00, -4.7851e+00, -4.8091e+00, -4.8157e+00,\n",
      "         -4.8181e+00, -4.8231e+00, -4.8087e+00, -4.7999e+00, -4.7973e+00,\n",
      "         -4.7944e+00, -4.8102e+00, -4.8059e+00, -4.7843e+00, -4.7866e+00,\n",
      "         -4.7833e+00, -4.7603e+00, -4.7475e+00, -4.7178e+00, -4.6708e+00,\n",
      "         -4.6602e+00, -4.5986e+00, -4.5554e+00, -4.4769e+00, -4.4410e+00,\n",
      "         -4.3089e+00, -4.2271e+00, -4.1846e+00, -4.1491e+00, -4.1156e+00,\n",
      "         -4.1051e+00, -4.1064e+00],\n",
      "        [-2.2795e+00, -3.8518e+00, -6.0084e-01, -2.0859e+00,  3.4265e-01,\n",
      "         -2.3198e+00, -4.3525e+00,  5.0359e-03, -3.6367e+00, -2.3605e+00,\n",
      "         -3.9451e+00,  5.3780e-01, -1.8594e-01, -3.4752e+00, -3.0826e+00,\n",
      "         -2.0588e+00, -3.1257e+00, -1.7160e+00, -1.9328e+00, -8.2745e-01,\n",
      "         -1.1958e+00, -1.2977e+00, -7.5270e-01,  4.5818e-01, -1.7770e+00,\n",
      "         -3.4039e+00, -3.0734e+00, -2.3037e+00, -3.7230e+00,  1.0588e+00,\n",
      "         -1.5948e+00, -3.1533e-02, -9.8508e-01, -3.4818e+00, -1.9326e+00,\n",
      "         -2.0059e+00, -8.9550e-01, -3.0760e+00, -1.4588e+00,  2.0826e-01,\n",
      "         -5.6381e-01, -4.5652e-01, -1.9338e-01, -4.4122e+00, -1.4727e+00,\n",
      "         -3.0630e+00, -3.3575e+00, -3.2169e+00, -1.1991e-01, -6.8692e-02,\n",
      "         -3.8671e+00, -1.4631e+00, -7.3131e+00, -2.7699e+00, -2.9191e+00,\n",
      "         -7.7072e-01, -2.5850e+00, -2.8758e+00, -2.6334e+00, -1.4797e+00,\n",
      "         -3.6384e+00, -1.1473e+00, -5.8656e+00, -2.2789e+00, -3.4622e+00,\n",
      "          1.2103e+00, -1.1670e+00,  1.3075e-01, -4.1635e-01, -3.0673e+00,\n",
      "         -3.9431e+00, -5.7159e+00, -2.6691e+00, -3.3017e-01, -3.8004e+00,\n",
      "         -4.2901e+00, -2.2993e+00, -3.7966e+00, -2.3954e+00, -3.5251e+00,\n",
      "         -4.9740e+00, -2.3395e+00, -2.3773e-01, -1.1664e+00, -1.7895e+00,\n",
      "         -2.8999e+00, -1.6797e+00, -5.0754e-01, -3.6678e+00, -7.3215e-01,\n",
      "         -3.1186e+00, -4.1194e+00, -2.2923e+00, -5.7662e-01, -7.5024e-01,\n",
      "         -3.1953e+00, -2.6595e+00, -3.4487e+00, -3.3210e+00, -3.2409e+00,\n",
      "         -8.7723e-03, -8.8207e-01, -3.7930e+00, -2.2205e+00, -4.6314e-01,\n",
      "         -1.7453e+00, -3.3030e+00, -3.7582e+00,  4.4611e-01, -1.2921e+00,\n",
      "         -1.6547e+00, -8.1666e-01, -4.3756e+00, -4.6344e+00,  4.9166e-01,\n",
      "         -8.5699e-01, -2.0760e+00, -1.2418e+00, -7.3212e-02, -8.9836e-01,\n",
      "         -2.4366e+00, -4.7661e+00, -2.5623e+00, -7.8976e-01, -1.0704e+00,\n",
      "         -2.9196e+00, -2.0358e+00, -7.2123e-01, -4.0381e-01, -4.2100e+00,\n",
      "         -6.7103e-01, -5.8302e-02, -3.2795e+00, -4.0599e+00, -1.1288e+00,\n",
      "         -2.0516e+00, -3.0761e+00, -2.4626e+00, -1.5463e+00, -2.7829e+00,\n",
      "         -2.5402e+00, -3.1876e+00, -5.6519e+00, -1.0898e+00, -8.4901e-01,\n",
      "         -6.5893e-01, -6.7892e-01, -7.1610e-01, -7.0983e-01, -6.7856e-01,\n",
      "         -4.8538e+00, -4.9321e+00, -5.3754e+00, -4.9466e+00, -4.8316e+00,\n",
      "         -4.5743e+00, -4.4458e+00, -4.4869e+00, -4.4424e+00, -4.5418e+00,\n",
      "         -4.7500e+00, -4.8639e+00, -4.9375e+00, -4.9451e+00, -4.8778e+00,\n",
      "         -4.7775e+00, -4.7329e+00, -4.7286e+00, -4.7280e+00, -4.7104e+00,\n",
      "         -4.6848e+00, -4.6834e+00, -4.7089e+00, -4.7406e+00, -4.7628e+00,\n",
      "         -4.7683e+00, -4.7382e+00, -4.7030e+00, -4.7044e+00, -4.6916e+00,\n",
      "         -4.7151e+00, -4.7554e+00, -4.7642e+00, -4.7534e+00, -4.7543e+00,\n",
      "         -4.7467e+00, -4.7633e+00, -4.7734e+00, -4.7721e+00, -4.7709e+00,\n",
      "         -4.7702e+00, -4.7951e+00, -4.8256e+00, -4.8385e+00, -4.8294e+00,\n",
      "         -4.8206e+00, -4.8176e+00, -4.7945e+00, -4.7836e+00, -4.7766e+00,\n",
      "         -4.7654e+00, -4.7718e+00, -4.7615e+00, -4.7522e+00, -4.7616e+00,\n",
      "         -4.7656e+00, -4.7498e+00, -4.7332e+00, -4.6935e+00, -4.6320e+00,\n",
      "         -4.6197e+00, -4.5689e+00, -4.5461e+00, -4.4759e+00, -4.4452e+00,\n",
      "         -4.3169e+00, -4.2304e+00, -4.1812e+00, -4.1452e+00, -4.1137e+00,\n",
      "         -4.1019e+00, -4.1042e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7003, -0.7152], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6856, -1.5696, -0.7016, -0.7498, -0.6934,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0747e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1387, -0.1401, -0.1415,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3207, -0.3240, -0.3273,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1419, -0.1434, -0.1448,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6687, -1.5385, -0.6843, -0.7313, -0.6763,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2916, -3.8464, -0.6106,  ..., -4.1376, -4.1251, -4.1242],\n",
      "        [-2.0801, -3.8744, -0.5301,  ..., -4.1825, -4.1899, -4.1919],\n",
      "        [-2.2947, -3.8551, -0.6048,  ..., -4.0521, -4.0439, -4.0462],\n",
      "        ...,\n",
      "        [-2.8394, -3.8475, -0.7020,  ..., -5.7036, -5.7001, -5.6843],\n",
      "        [-2.8282, -3.8535, -0.6985,  ..., -5.7466, -5.7467, -5.7358],\n",
      "        [-2.7609, -3.8410, -0.6778,  ..., -5.6683, -5.6948, -5.6961]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6684, -1.4911, -0.6312, -0.7139, -0.6420,  0.9981,  0.9947,  0.9466],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.1872,  0.9900,  0.9900,  0.9900,  0.9900, -0.7331],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5364e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1382, -0.1396, -0.1410,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1483, -0.1498, -0.1513,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.1352,  0.9656,  0.9656,  0.9656,  0.9656, -0.7150],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9813, -3.8061, -0.7035,  ..., -5.2190, -5.1710, -5.1386],\n",
      "        [-2.8282, -3.8447, -0.7006,  ..., -5.6539, -5.6504, -5.6421],\n",
      "        [-2.4366, -3.8082, -0.5965,  ..., -4.0955, -4.0909, -4.0804],\n",
      "        ...,\n",
      "        [-2.7633, -3.8600, -0.6910,  ..., -5.6425, -5.6536, -5.6585],\n",
      "        [-2.7618, -3.8399, -0.6738,  ..., -5.6750, -5.7008, -5.7030],\n",
      "        [-2.2905, -3.8497, -0.5997,  ..., -4.0607, -4.0503, -4.0536]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8586,  0.9586, -1.1979,  0.8655,  0.9122,  0.9634,  0.9402, -0.6814],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6009,  0.9900,  0.9900, -0.7317, -0.7317, -0.6846, -1.1000,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8942e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1035, -0.1046, -0.1056,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1385, -0.1399, -0.1413,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5861,  0.9656,  0.9608, -0.7136, -0.7136, -0.6678, -1.0518,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3584, -3.8374, -0.5773,  ..., -4.6819, -4.6487, -4.6158],\n",
      "        [-2.8226, -3.8450, -0.6943,  ..., -5.6991, -5.6949, -5.6916],\n",
      "        [-2.9726, -3.7965, -0.6969,  ..., -5.2628, -5.1823, -5.1271],\n",
      "        ...,\n",
      "        [-2.2916, -3.8445, -0.6026,  ..., -4.1007, -4.0876, -4.0883],\n",
      "        [-2.4608, -3.7967, -0.6133,  ..., -1.0856, -1.1166, -1.1612],\n",
      "        [-2.9859, -3.7964, -0.6972,  ..., -5.1924, -5.1443, -5.1189]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6175,  0.9858,  0.9187, -0.6852, -0.6852, -0.6653, -1.1108,  0.8648],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6954,  0.9900,  0.9900, -0.7676,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6660e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1407, -0.1421, -0.1435,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1553, -0.1569, -0.1584,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6782,  0.9656,  0.9656, -0.7487,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7535, -3.8439, -0.6881,  ..., -5.4990, -5.5142, -5.5153],\n",
      "        [-2.8289, -3.8373, -0.6811,  ..., -5.7115, -5.7096, -5.7080],\n",
      "        [-2.2862, -3.8393, -0.5978,  ..., -4.0974, -4.0844, -4.0837],\n",
      "        ...,\n",
      "        [-2.2757, -3.8417, -0.5975,  ..., -4.0519, -4.0392, -4.0383],\n",
      "        [-2.7612, -3.8534, -0.6834,  ..., -5.6430, -5.6557, -5.6589],\n",
      "        [-2.8250, -3.8451, -0.6901,  ..., -5.7554, -5.7534, -5.7466]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9096,  0.9709, -0.6540,  0.9568,  0.9204, -0.6745,  0.9617,  0.9807],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7657,  0.9900, -0.6254, -0.7754, -0.7501,  0.9900, -0.7501,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7654e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1549, -0.1565, -0.1580,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1077, -0.1088, -0.1099,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1517, -0.1533, -0.1548,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7468,  0.9656, -0.6100, -0.7562, -0.7316,  0.9656, -0.7316,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2639, -3.8340, -0.5812,  ..., -4.1135, -4.1005, -4.0981],\n",
      "        [-2.8402, -3.8335, -0.6876,  ..., -5.6122, -5.6039, -5.6022],\n",
      "        [-2.3497, -3.8282, -0.5801,  ..., -4.7366, -4.7128, -4.6856],\n",
      "        ...,\n",
      "        [-2.8293, -3.8322, -0.6862,  ..., -5.7388, -5.7328, -5.7269],\n",
      "        [-2.2644, -3.8392, -0.5907,  ..., -4.0707, -4.0579, -4.0556],\n",
      "        [-2.9774, -3.7967, -0.6907,  ..., -5.2226, -5.1745, -5.1470]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6910,  0.9282, -0.6271, -0.6680, -0.6837,  0.9202, -0.6837,  0.8646],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7166, -0.7592, -0.6164,  0.9900, -0.7637,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4181e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.1388e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1450, -0.1464, -0.1479,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1536, -0.1551, -0.1567,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1545, -0.1561, -0.1576,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6989, -0.7405, -0.6012,  0.9656, -0.7448,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8231, -3.8287, -0.6802,  ..., -5.6845, -5.6844, -5.6729],\n",
      "        [-2.2731, -3.8283, -0.5898,  ..., -4.1249, -4.1112, -4.1090],\n",
      "        [-2.2692, -3.8375, -0.5821,  ..., -4.0480, -4.0347, -4.0363],\n",
      "        ...,\n",
      "        [-2.2712, -3.8335, -0.5788,  ..., -4.0464, -4.0370, -4.0385],\n",
      "        [-2.8413, -3.8296, -0.6842,  ..., -5.6070, -5.6008, -5.5994],\n",
      "        [-2.9813, -3.7847, -0.6858,  ..., -5.1702, -5.1256, -5.1047]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9350, -0.6296, -0.6733, -0.6113,  0.9697, -0.6551,  0.9297,  0.8948],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.6054, -1.4135], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(7.2722e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3281, -0.3314, -0.3347, -0.3381, -0.3415, -0.3450, -0.3485, -0.3520,\n",
      "         -0.3555, -0.3591, -0.3627, -0.3664, -0.3701, -0.3739, -0.3776, -0.3814,\n",
      "         -0.3853, -0.3892, -0.3931, -0.3971, -0.4011, -0.4052, -0.4092, -0.4134,\n",
      "         -0.4176, -0.4218, -0.4260, -0.4303, -0.4347, -0.4391, -0.4435, -0.4480,\n",
      "         -0.4525, -0.4571, -0.4617, -0.4664, -0.4711, -0.4758, -0.4806, -0.4855,\n",
      "         -0.4904, -0.4954, -0.5004, -0.5054, -0.5105, -0.5157, -0.5209, -0.5261,\n",
      "         -0.5315, -0.5368, -0.5422, -0.5477, -0.5533, -0.5588, -0.5645, -0.5702,\n",
      "         -0.5760, -0.5818, -0.5876, -0.5936, -0.5996, -0.6056, -0.6118, -0.6179,\n",
      "         -0.6242, -0.6305, -0.6368, -0.6433, -0.6498, -0.6563, -0.6630, -0.6697,\n",
      "         -0.6764, -0.6833, -0.6902, -0.6971, -0.7042, -0.7113, -0.7185, -0.7257,\n",
      "         -0.7331, -0.7405, -0.7479, -0.7555, -0.7631, -0.7708, -0.7786, -0.7865,\n",
      "         -0.7944, -0.8025, -0.8106, -0.8188, -0.8270, -0.8354, -0.8438, -0.8523,\n",
      "         -0.8610, -0.8696, -0.8784, -0.8873, -0.8963, -0.9053, -0.9145, -0.9237,\n",
      "         -0.9330, -0.9425, -0.9520, -0.9616, -0.9713, -0.9811, -0.9910, -1.0010,\n",
      "         -1.0111, -1.0214, -1.0317, -1.0421, -1.0526, -1.0633, -1.0740, -1.0848,\n",
      "         -1.0958, -1.1069, -1.1181, -1.1293, -1.1408, -1.1523, -1.1639, -1.1757,\n",
      "         -1.1875, -1.1995, -1.2117, -1.2239, -1.2363, -1.2488, -1.2614, -1.2741,\n",
      "         -1.2870, -1.3000, -1.3131, -1.3264, -1.3398, -1.3533, -1.3670, -1.3808,\n",
      "         -1.3947, -1.4088, -1.4230, -1.4374, -1.4519, -1.4666, -1.4814, -1.4964,\n",
      "         -1.5115, -1.5268, -1.5422, -1.5578, -1.5735, -1.5894, -1.6054,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2747, -0.2775, -0.2803, -0.2831, -0.2859, -0.2888, -0.2918, -0.2947,\n",
      "         -0.2977, -0.3007, -0.3037, -0.3068, -0.3099, -0.3130, -0.3162, -0.3194,\n",
      "         -0.3226, -0.3259, -0.3291, -0.3325, -0.3358, -0.3392, -0.3427, -0.3461,\n",
      "         -0.3496, -0.3531, -0.3567, -0.3603, -0.3639, -0.3676, -0.3713, -0.3751,\n",
      "         -0.3789, -0.3827, -0.3866, -0.3905, -0.3944, -0.3984, -0.4024, -0.4065,\n",
      "         -0.4106, -0.4147, -0.4189, -0.4232, -0.4274, -0.4318, -0.4361, -0.4405,\n",
      "         -0.4450, -0.4495, -0.4540, -0.4586, -0.4632, -0.4679, -0.4726, -0.4774,\n",
      "         -0.4822, -0.4871, -0.4920, -0.4970, -0.5020, -0.5071, -0.5122, -0.5174,\n",
      "         -0.5226, -0.5279, -0.5332, -0.5386, -0.5440, -0.5495, -0.5551, -0.5607,\n",
      "         -0.5664, -0.5721, -0.5779, -0.5837, -0.5896, -0.5955, -0.6016, -0.6076,\n",
      "         -0.6138, -0.6200, -0.6262, -0.6326, -0.6390, -0.6454, -0.6519, -0.6585,\n",
      "         -0.6652, -0.6719, -0.6787, -0.6855, -0.6925, -0.6994, -0.7065, -0.7136,\n",
      "         -0.7209, -0.7281, -0.7355, -0.7429, -0.7504, -0.7580, -0.7657, -0.7734,\n",
      "         -0.7812, -0.7891, -0.7971, -0.8051, -0.8133, -0.8215, -0.8298, -0.8381,\n",
      "         -0.8466, -0.8552, -0.8638, -0.8725, -0.8813, -0.8902, -0.8992, -0.9083,\n",
      "         -0.9175, -0.9268, -0.9361, -0.9456, -0.9551, -0.9648, -0.9745, -0.9844,\n",
      "         -0.9943, -1.0044, -1.0145, -1.0247, -1.0351, -1.0456, -1.0561, -1.0668,\n",
      "         -1.0776, -1.0884, -1.0994, -1.1105, -1.1218, -1.1331, -1.1445, -1.1561,\n",
      "         -1.1678, -1.1796, -1.1915, -1.2035, -1.2157, -1.2280, -1.2404, -1.2529,\n",
      "         -1.2655, -1.2783, -1.2912, -1.3043, -1.3175, -1.3308, -1.3442, -1.3578,\n",
      "         -1.3715, -1.3854, -1.3993, -1.4135,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5737, -1.3515], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.0715, -3.8591, -0.5210, -2.0024,  0.4239, -2.2575, -4.2325,  0.1061,\n",
      "         -3.4037, -2.2313, -3.8575,  0.5585, -0.1935, -3.4021, -3.1014, -1.9360,\n",
      "         -3.0449, -1.6404, -1.9576, -0.7194, -1.1617, -1.1875, -0.6692,  0.4363,\n",
      "         -1.7199, -3.3052, -3.0587, -2.1960, -3.7397,  1.0452, -1.5583,  0.0877,\n",
      "         -0.9983, -3.4036, -1.9170, -2.0704, -0.9848, -3.0140, -1.3971,  0.3196,\n",
      "         -0.5407, -0.5118, -0.1717, -4.4596, -1.4153, -2.8587, -3.2361, -3.1415,\n",
      "         -0.4042, -0.3706, -3.7778, -1.4020, -7.3210, -2.6693, -2.8513, -0.6914,\n",
      "         -2.5753, -2.8951, -2.5530, -1.4301, -3.5461, -1.0830, -5.8693, -2.2085,\n",
      "         -3.4766,  1.1956, -1.1340,  0.2611, -0.4385, -2.9799, -3.8201, -5.6808,\n",
      "         -2.5724, -0.4366, -3.6973, -4.3326, -2.2061, -3.7296, -2.3389, -3.4332,\n",
      "         -4.9298, -2.2541, -0.2207, -1.0385, -1.7078, -2.7778, -1.6330, -0.3897,\n",
      "         -3.5154, -0.6844, -3.0100, -4.1495, -2.2125, -0.5633, -0.6160, -3.1891,\n",
      "         -2.5921, -3.2662, -3.2525, -3.1561,  0.0284, -0.7501, -3.7895, -2.2082,\n",
      "         -0.4752, -1.6535, -3.1328, -3.6668,  0.3258, -1.2564, -1.7359, -2.3815,\n",
      "         -4.0805, -4.6571,  0.5355, -0.6993, -2.0421, -1.1073,  0.0834, -0.7994,\n",
      "         -2.1579, -4.9003, -2.5200, -0.7007, -0.9049, -2.7654, -1.9508, -0.6610,\n",
      "         -0.2381, -4.1073, -0.9441, -0.3498, -3.2040, -3.9992, -1.0597, -2.0440,\n",
      "         -3.0945, -2.3668, -1.4841, -2.7652, -2.5637, -3.1017, -5.7040, -0.9911,\n",
      "         -1.5700, -0.9631, -1.6660, -1.5521, -1.6324, -4.8800, -4.9077, -5.4344,\n",
      "         -4.8857, -4.8138, -4.8214, -4.8777, -4.9266, -4.8263, -4.7594, -4.7169,\n",
      "         -4.6751, -4.6212, -4.5656, -4.5164, -4.4737, -4.4226, -4.3730, -4.3305,\n",
      "         -4.3092, -4.3029, -4.3077, -4.3273, -4.3550, -4.3805, -4.4151, -4.4633,\n",
      "         -4.5065, -4.5322, -4.5410, -4.5356, -4.5369, -4.5458, -4.5604, -4.5820,\n",
      "         -4.6068, -4.6146, -4.6271, -4.6293, -4.6154, -4.5914, -4.5636, -4.5546,\n",
      "         -4.5607, -4.5574, -4.5501, -4.5444, -4.5413, -4.5340, -4.5378, -4.5325,\n",
      "         -4.5268, -4.5291, -4.5259, -4.5195, -4.5190, -4.5209, -4.5170, -4.5187,\n",
      "         -4.4931, -4.4593, -4.4509, -4.4189, -4.3921, -4.4004, -4.3420, -4.2768,\n",
      "         -4.2391, -4.2115, -4.1883, -4.1814, -4.1864, -4.1875],\n",
      "        [-2.1602, -3.8441, -0.5389, -2.0039,  0.3950, -2.2778, -4.2913,  0.0873,\n",
      "         -3.4878, -2.2498, -3.8788,  0.5703, -0.1966, -3.4130, -3.0925, -1.9845,\n",
      "         -3.0563, -1.6662, -1.9204, -0.7418, -1.1551, -1.2087, -0.6867,  0.4554,\n",
      "         -1.7131, -3.3148, -3.0553, -2.2388, -3.7231,  1.0533, -1.5542,  0.0565,\n",
      "         -0.9565, -3.4215, -1.9044, -2.0523, -0.9607, -3.0214, -1.3832,  0.2963,\n",
      "         -0.5308, -0.4888, -0.1839, -4.4348, -1.4205, -2.9381, -3.2644, -3.1591,\n",
      "         -0.3119, -0.2498, -3.7937, -1.4247, -7.3180, -2.7080, -2.8658, -0.7200,\n",
      "         -2.5398, -2.8684, -2.5594, -1.4219, -3.5576, -1.1012, -5.8568, -2.2351,\n",
      "         -3.4589,  1.2001, -1.1267,  0.2245, -0.3946, -2.9989, -3.8409, -5.6789,\n",
      "         -2.6103, -0.4142, -3.7164, -4.3011, -2.2432, -3.7416, -2.3273, -3.4452,\n",
      "         -4.9262, -2.2859, -0.2185, -1.0780, -1.7215, -2.8109, -1.6324, -0.4177,\n",
      "         -3.5437, -0.6729, -3.0244, -4.1218, -2.2431, -0.5619, -0.6566, -3.1695,\n",
      "         -2.5982, -3.3387, -3.2848, -3.1723,  0.0210, -0.7965, -3.8040, -2.2248,\n",
      "         -0.4636, -1.6606, -3.2029, -3.7153,  0.3509, -5.2498, -1.7540, -2.7370,\n",
      "         -4.0311, -4.5676,  0.6521, -0.6998, -2.0272, -1.1467, -0.0175, -0.7494,\n",
      "         -2.2877, -4.8004, -2.5622, -0.7770, -0.9882, -2.8063, -1.9500, -0.6836,\n",
      "         -0.4620, -4.1511, -0.6921, -0.2011, -3.1707, -4.0455, -1.1086, -2.0188,\n",
      "         -3.1080, -2.3919, -1.4831, -2.8687, -2.6160, -3.2253, -5.9028, -1.0146,\n",
      "         -1.3464, -1.2400, -1.2235, -1.2564, -1.2891, -1.2741, -1.2384, -1.1805,\n",
      "         -1.1960, -1.5320, -4.7846, -4.8900, -5.2228, -4.9501, -5.0827, -4.9364,\n",
      "         -4.8552, -4.8737, -4.7842, -4.8091, -4.8704, -4.8603, -4.7919, -4.7005,\n",
      "         -4.6073, -4.5299, -4.4613, -4.3890, -4.3282, -4.2822, -4.2695, -4.2835,\n",
      "         -4.3099, -4.3428, -4.3842, -4.4246, -4.4472, -4.4356, -4.3961, -4.3803,\n",
      "         -4.3893, -4.4189, -4.4743, -4.5285, -4.5679, -4.5888, -4.5987, -4.6076,\n",
      "         -4.6144, -4.6139, -4.6155, -4.6251, -4.6379, -4.6405, -4.6511, -4.6421,\n",
      "         -4.6347, -4.6399, -4.6426, -4.6531, -4.6672, -4.6828, -4.6858, -4.6932,\n",
      "         -4.6740, -4.6262, -4.6047, -4.5845, -4.5619, -4.5421, -4.5331, -4.5046,\n",
      "         -4.4922, -4.4633, -4.4483, -4.4090, -4.3713, -4.3430]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4767, -1.2777], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7164, -0.7791,  0.9900, -0.7588, -0.7336,  0.9900, -0.6275],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7313e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.4668e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1449, -0.1464, -0.1479,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1576, -0.1592, -0.1608,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1484, -0.1499, -0.1514,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1081, -0.1092, -0.1103,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6987, -0.7599,  0.9656, -0.7401, -0.7155,  0.9656, -0.6120],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9877, -3.7951, -0.7071,  ..., -5.1681, -5.1237, -5.1022],\n",
      "        [-2.2701, -3.8367, -0.6069,  ..., -4.1302, -4.1158, -4.1171],\n",
      "        [-2.2598, -3.8405, -0.6055,  ..., -4.0518, -4.0439, -4.0421],\n",
      "        ...,\n",
      "        [-2.2748, -3.8454, -0.6014,  ..., -4.0350, -4.0242, -4.0254],\n",
      "        [-2.8349, -3.8376, -0.7058,  ..., -5.7378, -5.7281, -5.7225],\n",
      "        [-2.3473, -3.8336, -0.5985,  ..., -4.7156, -4.6897, -4.6678]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8605, -0.6720, -0.7089,  0.8968, -0.7142, -0.6358,  0.8971, -0.6494],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7038,  0.9900, -0.7510, -1.6000, -1.1934,  0.9900, -0.7730, -0.7595],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2953e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1424, -0.1438, -0.1453,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1519, -0.1535, -0.1550,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1564, -0.1580, -0.1596,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1537, -0.1552, -0.1568,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6865,  0.9656, -0.7325, -1.5683, -1.1411,  0.9656, -0.7540, -0.7408],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2709, -3.8447, -0.6162,  ..., -4.1039, -4.0933, -4.0939],\n",
      "        [-2.7668, -3.8594, -0.7073,  ..., -5.6517, -5.6605, -5.6635],\n",
      "        [-2.2543, -3.8490, -0.6142,  ..., -4.0794, -4.0673, -4.0668],\n",
      "        ...,\n",
      "        [-2.9820, -3.8082, -0.7209,  ..., -5.2156, -5.1670, -5.1383],\n",
      "        [-2.2589, -3.8442, -0.6145,  ..., -4.0605, -4.0487, -4.0489],\n",
      "        [-2.2664, -3.8461, -0.6082,  ..., -4.0555, -4.0481, -4.0485]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6991,  0.9285, -0.7423, -1.4909, -1.2142,  0.8076, -0.7285, -0.7163],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6995,  0.9900,  0.9900,  0.9900, -0.6086,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5967e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1415, -0.1429, -0.1444,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1048, -0.1059, -0.1070,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6823,  0.9656,  0.9656,  0.9656, -0.5936,  0.9608,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8364, -3.8479, -0.7215,  ..., -5.7187, -5.7172, -5.6979],\n",
      "        [-2.2683, -3.8471, -0.6234,  ..., -4.1118, -4.0994, -4.1000],\n",
      "        [-2.8268, -3.8539, -0.7199,  ..., -5.7623, -5.7628, -5.7511],\n",
      "        ...,\n",
      "        [-2.3486, -3.8418, -0.6074,  ..., -4.6556, -4.6236, -4.5930],\n",
      "        [-2.9744, -3.8054, -0.7247,  ..., -5.2528, -5.1782, -5.1273],\n",
      "        [-2.8253, -3.8495, -0.7232,  ..., -5.7014, -5.6960, -5.6917]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9446, -0.7150,  0.9382,  0.7810,  0.9309, -0.6453,  0.8752,  0.9426],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -1.5963,  0.9900, -0.7682,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.5311e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1554, -0.1570, -0.1586,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -1.5647,  0.9656, -0.7492,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7525, -3.8562, -0.7271,  ..., -5.5214, -5.5351, -5.5345],\n",
      "        [-2.9815, -3.8146, -0.7334,  ..., -5.2208, -5.1745, -5.1459],\n",
      "        [-2.8408, -3.8492, -0.7276,  ..., -5.6211, -5.6136, -5.6087],\n",
      "        ...,\n",
      "        [-2.2388, -3.8505, -0.6190,  ..., -4.0524, -4.0420, -4.0406],\n",
      "        [-2.8261, -3.8467, -0.7247,  ..., -5.7077, -5.7055, -5.6955],\n",
      "        [-2.7525, -3.8562, -0.7271,  ..., -5.5214, -5.5351, -5.5345]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8645,  0.7887,  0.8711, -1.4948,  0.9255, -0.7693,  0.9015,  0.8645],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.1000, -0.7430, -0.6059, -0.7431, -0.7430,  0.9900, -1.4019],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.0965e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1503, -0.1518, -0.1534,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1503, -0.1518, -0.1534,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2724, -0.2752, -0.2780,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.0518, -0.7247, -0.5909, -0.7248, -0.7247,  0.9656, -1.3405],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7551, -3.8442, -0.7070,  ..., -5.7167, -5.7406, -5.7444],\n",
      "        [-2.4439, -3.8010, -0.6465,  ..., -1.0682, -1.0994, -1.0813],\n",
      "        [-2.2504, -3.8501, -0.6178,  ..., -4.1143, -4.1022, -4.1019],\n",
      "        ...,\n",
      "        [-2.2504, -3.8501, -0.6178,  ..., -4.1143, -4.1022, -4.1019],\n",
      "        [-2.9791, -3.8092, -0.7305,  ..., -5.1818, -5.1400, -5.1116],\n",
      "        [-2.1464, -3.8585, -0.5696,  ..., -4.4285, -4.3887, -4.3588]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9006, -1.0817, -0.7616, -0.6429, -0.7724, -0.7616,  0.7703, -1.3925],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7515,  0.9900, -0.7539,  0.9900,  0.9900, -0.7028,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9613e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1520, -0.1536, -0.1551,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1422, -0.1436, -0.1451,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7330,  0.9656, -0.7353,  0.9656,  0.9656, -0.6855,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7515, -3.8650, -0.7201,  ..., -5.6738, -5.6836, -5.6867],\n",
      "        [-2.2523, -3.8495, -0.6175,  ..., -4.0660, -4.0593, -4.0587],\n",
      "        [-2.8172, -3.8552, -0.7242,  ..., -5.7727, -5.7694, -5.7612],\n",
      "        ...,\n",
      "        [-2.8227, -3.8461, -0.7281,  ..., -5.7544, -5.7477, -5.7357],\n",
      "        [-2.2570, -3.8445, -0.6286,  ..., -4.1476, -4.1363, -4.1344],\n",
      "        [-2.8166, -3.8464, -0.7284,  ..., -5.6849, -5.6795, -5.6718]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9270, -0.7514,  0.9461, -0.7793,  0.9070,  0.8870, -0.7275,  0.9370],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7172], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4328e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1451, -0.1466, -0.1480, -0.1495, -0.1510, -0.1526, -0.1541, -0.1557,\n",
      "         -0.1572, -0.1588, -0.1604, -0.1620, -0.1637, -0.1653, -0.1670, -0.1687,\n",
      "         -0.1704, -0.1721, -0.1739, -0.1756, -0.1774, -0.1792, -0.1810, -0.1828,\n",
      "         -0.1847, -0.1865, -0.1884, -0.1903, -0.1922, -0.1942, -0.1961, -0.1981,\n",
      "         -0.2001, -0.2021, -0.2042, -0.2063, -0.2083, -0.2104, -0.2126, -0.2147,\n",
      "         -0.2169, -0.2191, -0.2213, -0.2235, -0.2258, -0.2281, -0.2304, -0.2327,\n",
      "         -0.2350, -0.2374, -0.2398, -0.2422, -0.2447, -0.2472, -0.2497, -0.2522,\n",
      "         -0.2547, -0.2573, -0.2599, -0.2625, -0.2652, -0.2678, -0.2706, -0.2733,\n",
      "         -0.2760, -0.2788, -0.2817, -0.2845, -0.2874, -0.2903, -0.2932, -0.2962,\n",
      "         -0.2992, -0.3022, -0.3052, -0.3083, -0.3114, -0.3146, -0.3178, -0.3210,\n",
      "         -0.3242, -0.3275, -0.3308, -0.3341, -0.3375, -0.3409, -0.3444, -0.3478,\n",
      "         -0.3513, -0.3549, -0.3585, -0.3621, -0.3658, -0.3695, -0.3732, -0.3770,\n",
      "         -0.3808, -0.3846, -0.3885, -0.3924, -0.3964, -0.4004, -0.4044, -0.4085,\n",
      "         -0.4126, -0.4168, -0.4210, -0.4253, -0.4296, -0.4339, -0.4383, -0.4427,\n",
      "         -0.4472, -0.4517, -0.4563, -0.4609, -0.4655, -0.4702, -0.4750, -0.4798,\n",
      "         -0.4846, -0.4895, -0.4945, -0.4995, -0.5045, -0.5096, -0.5148, -0.5200,\n",
      "         -0.5252, -0.5305, -0.5359, -0.5413, -0.5468, -0.5523, -0.5579, -0.5635,\n",
      "         -0.5692, -0.5749, -0.5807, -0.5866, -0.5925, -0.5985, -0.6046, -0.6107,\n",
      "         -0.6168, -0.6231, -0.6294, -0.6357, -0.6421, -0.6486, -0.6552, -0.6618,\n",
      "         -0.6685, -0.6752, -0.6820, -0.6889, -0.6959, -0.7029, -0.7100, -0.7172,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6995], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8238e+00, -3.8522e+00, -7.3133e-01, -2.4654e+00,  1.9965e-01,\n",
      "         -2.4183e+00, -4.6425e+00, -5.1216e-01, -4.2398e+00, -2.7018e+00,\n",
      "         -4.0702e+00,  4.3330e-01, -4.3002e-01, -3.6570e+00, -2.9951e+00,\n",
      "         -2.3905e+00, -3.2764e+00, -1.8558e+00, -1.9231e+00, -1.1026e+00,\n",
      "         -1.3324e+00, -1.5990e+00, -9.7919e-01,  5.6455e-01, -2.1301e+00,\n",
      "         -3.6034e+00, -2.9929e+00, -2.5664e+00, -3.7029e+00,  1.0661e+00,\n",
      "         -1.7882e+00, -3.5366e-01, -9.6655e-01, -3.5731e+00, -1.9890e+00,\n",
      "         -1.8908e+00, -8.0902e-01, -3.1551e+00, -1.6953e+00, -3.5764e-02,\n",
      "         -6.4933e-01, -4.0747e-01, -2.6905e-01, -4.3805e+00, -1.7636e+00,\n",
      "         -3.5650e+00, -3.6870e+00, -3.3073e+00,  6.5004e-01,  5.3144e-01,\n",
      "         -3.9738e+00, -1.6182e+00, -7.3664e+00, -2.9401e+00, -2.9414e+00,\n",
      "         -9.3487e-01, -2.5732e+00, -2.9121e+00, -2.7944e+00, -1.7652e+00,\n",
      "         -3.7980e+00, -1.2852e+00, -5.9446e+00, -2.4071e+00, -3.4415e+00,\n",
      "          1.2507e+00, -1.3461e+00, -2.1257e-01, -3.4607e-01, -3.1729e+00,\n",
      "         -4.1400e+00, -5.6783e+00, -2.8428e+00, -2.0790e-01, -3.9370e+00,\n",
      "         -4.2460e+00, -2.4704e+00, -3.8189e+00, -2.5725e+00, -3.6584e+00,\n",
      "         -4.9361e+00, -2.4937e+00, -3.0492e-01, -1.4455e+00, -1.8780e+00,\n",
      "         -3.1517e+00, -1.8737e+00, -8.0291e-01, -3.7930e+00, -8.4758e-01,\n",
      "         -3.2842e+00, -4.0934e+00, -2.4456e+00, -6.2306e-01, -1.0481e+00,\n",
      "         -3.1171e+00, -2.9098e+00, -3.8613e+00, -3.3876e+00, -3.3947e+00,\n",
      "         -1.1281e-01, -1.1574e+00, -3.7975e+00, -2.2989e+00, -3.8494e-01,\n",
      "         -2.0230e+00, -3.6807e+00, -3.9773e+00,  5.7979e-01, -1.5511e+00,\n",
      "         -1.4605e+00, -5.9991e-01, -4.7758e+00, -4.6910e+00,  5.4933e-01,\n",
      "         -1.0693e+00, -2.2124e+00, -1.4308e+00, -2.3880e-01, -9.7449e-01,\n",
      "         -2.8506e+00, -4.6825e+00, -2.6306e+00, -7.6904e-01, -1.2196e+00,\n",
      "         -3.0352e+00, -2.2109e+00, -8.6512e-01, -7.3309e-01, -4.2793e+00,\n",
      "          2.0265e-01,  5.8079e-01, -3.3255e+00, -4.1437e+00, -1.2777e+00,\n",
      "         -2.0526e+00, -3.0639e+00, -2.6022e+00, -1.7108e+00, -2.8044e+00,\n",
      "         -2.9738e+00, -5.8934e+00, -1.6977e+00, -2.5903e+00, -1.6443e+00,\n",
      "         -8.7567e-01, -5.8370e-01, -4.4124e+00, -3.7847e+00, -3.6677e-01,\n",
      "         -2.1011e+00, -1.1344e+00, -1.1754e+00, -4.3352e-01,  3.2153e-02,\n",
      "         -9.1279e-01, -4.2316e+00, -3.2473e+00, -4.9338e+00, -1.4037e+00,\n",
      "          9.8314e-01,  9.5284e-01,  9.3218e-01,  9.4013e-01,  8.5380e-01,\n",
      "          1.0117e+00, -4.8038e+00, -4.9704e+00, -5.3464e+00, -5.2142e+00,\n",
      "         -5.4308e+00, -5.4649e+00, -5.4524e+00, -5.4647e+00, -5.4683e+00,\n",
      "         -5.4401e+00, -5.3756e+00, -5.2718e+00, -5.2112e+00, -5.1883e+00,\n",
      "         -5.2143e+00, -5.2383e+00, -5.2557e+00, -5.2685e+00, -5.2757e+00,\n",
      "         -5.2749e+00, -5.2863e+00, -5.3112e+00, -5.3588e+00, -5.4118e+00,\n",
      "         -5.4411e+00, -5.4417e+00, -5.4264e+00, -5.4241e+00, -5.4528e+00,\n",
      "         -5.5067e+00, -5.5461e+00, -5.5498e+00, -5.5303e+00, -5.4948e+00,\n",
      "         -5.4727e+00, -5.4617e+00, -5.4652e+00, -5.4903e+00, -5.5181e+00,\n",
      "         -5.5613e+00, -5.5944e+00, -5.6191e+00, -5.6325e+00, -5.6235e+00,\n",
      "         -5.6324e+00, -5.6492e+00, -5.6764e+00, -5.7258e+00, -5.7571e+00,\n",
      "         -5.7679e+00, -5.7685e+00, -5.7745e+00, -5.7730e+00, -5.7743e+00,\n",
      "         -5.7698e+00, -5.7639e+00],\n",
      "        [-2.2464e+00, -3.8533e+00, -6.1875e-01, -2.1056e+00,  3.5175e-01,\n",
      "         -2.3089e+00, -4.3909e+00, -2.5270e-03, -3.6145e+00, -2.3958e+00,\n",
      "         -3.8948e+00,  4.5376e-01, -2.8583e-01, -3.4651e+00, -3.1353e+00,\n",
      "         -2.0768e+00, -3.1493e+00, -1.7004e+00, -1.9959e+00, -8.1336e-01,\n",
      "         -1.1796e+00, -1.2819e+00, -7.4103e-01,  4.5467e-01, -1.8082e+00,\n",
      "         -3.3957e+00, -3.0997e+00, -2.3204e+00, -3.7139e+00,  1.0264e+00,\n",
      "         -1.5916e+00, -2.5337e-02, -9.8809e-01, -3.4365e+00, -1.9249e+00,\n",
      "         -2.0704e+00, -9.0965e-01, -3.1023e+00, -1.4638e+00,  2.3372e-01,\n",
      "         -5.8711e-01, -4.7257e-01, -2.2273e-01, -4.4633e+00, -1.5052e+00,\n",
      "         -3.0429e+00, -3.3679e+00, -3.1727e+00, -1.4000e-01, -1.4709e-01,\n",
      "         -3.8536e+00, -1.4977e+00, -7.3567e+00, -2.7768e+00, -2.9338e+00,\n",
      "         -7.6193e-01, -2.5938e+00, -2.8757e+00, -2.6412e+00, -1.5095e+00,\n",
      "         -3.6243e+00, -1.1781e+00, -5.9219e+00, -2.2959e+00, -3.4549e+00,\n",
      "          1.1761e+00, -1.1716e+00,  1.2489e-01, -4.1973e-01, -3.0297e+00,\n",
      "         -3.9390e+00, -5.7065e+00, -2.6877e+00, -3.2618e-01, -3.7868e+00,\n",
      "         -4.3287e+00, -2.3191e+00, -3.8129e+00, -2.3920e+00, -3.5129e+00,\n",
      "         -4.9665e+00, -2.3576e+00, -2.4124e-01, -1.1508e+00, -1.7798e+00,\n",
      "         -2.9065e+00, -1.6808e+00, -4.9854e-01, -3.6274e+00, -7.5268e-01,\n",
      "         -3.1110e+00, -4.1560e+00, -2.3124e+00, -5.7355e-01, -7.4110e-01,\n",
      "         -3.2037e+00, -2.6769e+00, -3.4194e+00, -3.3039e+00, -3.2487e+00,\n",
      "         -1.6266e-02, -8.7367e-01, -3.8202e+00, -2.2908e+00, -4.6649e-01,\n",
      "         -1.7374e+00, -3.2811e+00, -3.7855e+00,  4.3742e-01, -1.3541e+00,\n",
      "         -1.5091e+00, -9.3694e-01, -4.5205e+00, -4.8014e+00,  4.7438e-01,\n",
      "         -8.0964e-01, -2.0523e+00, -1.1886e+00, -6.1845e-02, -9.2234e-01,\n",
      "         -2.5215e+00, -4.7182e+00, -2.5852e+00, -8.0365e-01, -1.0529e+00,\n",
      "         -2.9235e+00, -2.0087e+00, -7.0730e-01, -3.4826e-01, -4.1299e+00,\n",
      "         -5.6021e-01, -1.0304e-01, -3.3101e+00, -4.0368e+00, -1.1137e+00,\n",
      "         -2.0718e+00, -3.0589e+00, -2.4768e+00, -1.5611e+00, -2.8222e+00,\n",
      "         -2.5434e+00, -3.1981e+00, -5.7253e+00, -1.1583e+00, -7.0113e-01,\n",
      "         -7.1967e-01, -6.6915e-01, -7.0521e-01, -7.3915e-01, -6.5512e-01,\n",
      "         -4.9239e+00, -4.9467e+00, -5.3034e+00, -4.9388e+00, -4.8531e+00,\n",
      "         -4.6527e+00, -4.5342e+00, -4.6103e+00, -4.6236e+00, -4.6835e+00,\n",
      "         -4.8446e+00, -4.9357e+00, -4.9636e+00, -4.9133e+00, -4.8061e+00,\n",
      "         -4.7253e+00, -4.7192e+00, -4.7353e+00, -4.7349e+00, -4.7038e+00,\n",
      "         -4.6619e+00, -4.6528e+00, -4.6774e+00, -4.7155e+00, -4.7524e+00,\n",
      "         -4.7637e+00, -4.7273e+00, -4.6831e+00, -4.6757e+00, -4.6707e+00,\n",
      "         -4.7105e+00, -4.7554e+00, -4.7728e+00, -4.7585e+00, -4.7541e+00,\n",
      "         -4.7416e+00, -4.7619e+00, -4.7830e+00, -4.7932e+00, -4.7979e+00,\n",
      "         -4.8113e+00, -4.8497e+00, -4.8958e+00, -4.9144e+00, -4.9109e+00,\n",
      "         -4.8964e+00, -4.8848e+00, -4.8506e+00, -4.8421e+00, -4.8347e+00,\n",
      "         -4.8301e+00, -4.8369e+00, -4.8328e+00, -4.8151e+00, -4.8205e+00,\n",
      "         -4.8244e+00, -4.7980e+00, -4.7786e+00, -4.7370e+00, -4.6813e+00,\n",
      "         -4.6705e+00, -4.6141e+00, -4.5651e+00, -4.5035e+00, -4.4666e+00,\n",
      "         -4.3512e+00, -4.2608e+00, -4.2008e+00, -4.1609e+00, -4.1302e+00,\n",
      "         -4.1216e+00, -4.1218e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9456, -0.6982], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7123,  0.9900,  0.9900,  0.9900,  0.9900, -0.7616,  0.9900, -0.7531],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0295e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1441, -0.1456, -0.1470,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1541, -0.1556, -0.1572,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1523, -0.1539, -0.1554,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6947,  0.9656,  0.9656,  0.9656,  0.9656, -0.7428,  0.9656, -0.7345],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2460, -3.8432, -0.6195,  ..., -4.1507, -4.1374, -4.1371],\n",
      "        [-2.8108, -3.8479, -0.7219,  ..., -5.7166, -5.7118, -5.7074],\n",
      "        [-2.7463, -3.8627, -0.7155,  ..., -5.6795, -5.6901, -5.6951],\n",
      "        ...,\n",
      "        [-2.2411, -3.8484, -0.6101,  ..., -4.0664, -4.0583, -4.0590],\n",
      "        [-2.8130, -3.8450, -0.7229,  ..., -5.6907, -5.6843, -5.6751],\n",
      "        [-2.2348, -3.8466, -0.6089,  ..., -4.1142, -4.1036, -4.1022]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7231,  0.9620,  0.9421,  0.9521,  0.9019, -0.7463,  0.9523, -0.7580],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -1.6092, -0.7100, -0.7347,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1526e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1436, -0.1451, -0.1465,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1486, -0.1501, -0.1516,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9608,  0.9656, -1.5773, -0.6925, -0.7166,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9695, -3.8064, -0.7211,  ..., -5.1747, -5.1302, -5.1039],\n",
      "        [-2.8119, -3.8446, -0.7213,  ..., -5.6918, -5.6865, -5.6771],\n",
      "        [-2.9603, -3.8042, -0.7188,  ..., -5.2531, -5.1759, -5.1265],\n",
      "        ...,\n",
      "        [-2.2437, -3.8445, -0.6146,  ..., -4.1184, -4.1093, -4.1062],\n",
      "        [-2.2455, -3.8518, -0.6084,  ..., -4.0512, -4.0416, -4.0439],\n",
      "        [-2.9718, -3.8031, -0.7203,  ..., -5.1697, -5.1277, -5.1064]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.7841,  0.9576,  0.9025,  0.9674, -1.4674, -0.7143, -0.6744,  0.8530],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7658,  0.9900,  0.9900, -0.7561,  0.9900, -0.6293, -1.6073, -0.7561],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0036e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1549, -0.1565, -0.1581,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1084, -0.1095, -0.1106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3284, -0.3318, -0.3351,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1530, -0.1545, -0.1561,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7469,  0.9656,  0.9656, -0.7374,  0.9656, -0.6138, -1.5755, -0.7374],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2399, -3.8469, -0.5999,  ..., -4.0659, -4.0571, -4.0584],\n",
      "        [-2.7406, -3.8513, -0.7122,  ..., -5.5343, -5.5499, -5.5518],\n",
      "        [-2.8121, -3.8509, -0.7111,  ..., -5.7828, -5.7836, -5.7750],\n",
      "        ...,\n",
      "        [-2.3266, -3.8353, -0.6087,  ..., -4.6936, -4.6691, -4.6468],\n",
      "        [-2.0458, -3.8679, -0.5362,  ..., -4.2085, -4.2125, -4.2159],\n",
      "        [-2.2298, -3.8481, -0.6066,  ..., -4.0915, -4.0813, -4.0784]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7361,  0.9098,  0.9684, -0.7463,  0.9156, -0.6100, -1.4612, -0.7463],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7083,  0.9900, -0.7696, -0.7204,  0.9900,  0.9900, -0.7017],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2496e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.1448e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1433, -0.1447, -0.1462,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1420, -0.1434, -0.1448,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6908,  0.9656, -0.7506, -0.7027,  0.9656,  0.9656, -0.6844],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8160, -3.8506, -0.7058,  ..., -5.7840, -5.7864, -5.7772],\n",
      "        [-2.2509, -3.8396, -0.6079,  ..., -4.1511, -4.1380, -4.1362],\n",
      "        [-2.8328, -3.8428, -0.7081,  ..., -5.6391, -5.6283, -5.6224],\n",
      "        ...,\n",
      "        [-2.9742, -3.8100, -0.7127,  ..., -5.2142, -5.1696, -5.1456],\n",
      "        [-2.9790, -3.8040, -0.7091,  ..., -5.1784, -5.1334, -5.1091],\n",
      "        [-2.2492, -3.8433, -0.6044,  ..., -4.1174, -4.1053, -4.1063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9714, -0.7085,  0.9066, -0.7476, -0.6831,  0.8250,  0.7949, -0.7017],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6019, -1.4008,  0.9900, -0.7506, -0.7592,  0.9900, -0.6019,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8290e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1037, -0.1047, -0.1058,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2722, -0.2750, -0.2778,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5870, -1.3395,  0.9656, -0.7321, -0.7405,  0.9656, -0.5870,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3361, -3.8349, -0.5859,  ..., -4.6270, -4.5944, -4.5664],\n",
      "        [-2.1466, -3.8498, -0.5430,  ..., -4.4331, -4.3902, -4.3603],\n",
      "        [-2.7524, -3.8371, -0.6814,  ..., -5.7287, -5.7545, -5.7589],\n",
      "        ...,\n",
      "        [-2.8374, -3.8412, -0.7028,  ..., -5.6404, -5.6319, -5.6216],\n",
      "        [-2.3361, -3.8349, -0.5859,  ..., -4.6270, -4.5944, -4.5664],\n",
      "        [-2.7524, -3.8371, -0.6814,  ..., -5.7287, -5.7545, -5.7589]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5826, -1.4249,  0.9448, -0.7453, -0.7397,  0.9146, -0.5826,  0.9448],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -1.1831,  0.9900,  0.9900, -0.7293, -1.1000, -0.7309],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4147e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1377, -0.1391, -0.1405,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1475, -0.1490, -0.1505,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1479, -0.1494, -0.1509,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -1.1313,  0.9656,  0.9656, -0.7113, -1.0518, -0.7129],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8279, -3.8442, -0.7017,  ..., -5.7844, -5.7793, -5.7736],\n",
      "        [-2.8218, -3.8352, -0.6931,  ..., -5.7321, -5.7297, -5.7147],\n",
      "        [-2.4166, -3.7980, -0.5906,  ..., -4.1145, -4.1100, -4.0999],\n",
      "        ...,\n",
      "        [-2.2486, -3.8394, -0.5797,  ..., -4.1075, -4.0954, -4.0952],\n",
      "        [-2.4413, -3.7896, -0.6128,  ..., -0.9828, -1.0127, -1.1008],\n",
      "        [-2.2562, -3.8435, -0.5881,  ..., -4.0626, -4.0508, -4.0485]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9841,  0.9542, -1.2737,  0.8438,  0.9723, -0.7276, -1.0086, -0.7309],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7506,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4328e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(5.7928e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1518, -0.1534, -0.1549, -0.1565, -0.1581, -0.1597, -0.1613, -0.1629,\n",
      "         -0.1646, -0.1662, -0.1679, -0.1696, -0.1713, -0.1730, -0.1748, -0.1766,\n",
      "         -0.1783, -0.1801, -0.1820, -0.1838, -0.1857, -0.1875, -0.1894, -0.1913,\n",
      "         -0.1933, -0.1952, -0.1972, -0.1992, -0.2012, -0.2032, -0.2053, -0.2074,\n",
      "         -0.2094, -0.2116, -0.2137, -0.2159, -0.2180, -0.2202, -0.2225, -0.2247,\n",
      "         -0.2270, -0.2293, -0.2316, -0.2339, -0.2363, -0.2387, -0.2411, -0.2435,\n",
      "         -0.2460, -0.2485, -0.2510, -0.2535, -0.2561, -0.2587, -0.2613, -0.2639,\n",
      "         -0.2666, -0.2693, -0.2720, -0.2747, -0.2775, -0.2803, -0.2832, -0.2860,\n",
      "         -0.2889, -0.2918, -0.2948, -0.2977, -0.3008, -0.3038, -0.3069, -0.3100,\n",
      "         -0.3131, -0.3162, -0.3194, -0.3227, -0.3259, -0.3292, -0.3325, -0.3359,\n",
      "         -0.3393, -0.3427, -0.3462, -0.3497, -0.3532, -0.3568, -0.3604, -0.3640,\n",
      "         -0.3677, -0.3714, -0.3752, -0.3790, -0.3828, -0.3867, -0.3906, -0.3945,\n",
      "         -0.3985, -0.4025, -0.4066, -0.4107, -0.4148, -0.4190, -0.4233, -0.4275,\n",
      "         -0.4319, -0.4362, -0.4406, -0.4451, -0.4496, -0.4541, -0.4587, -0.4633,\n",
      "         -0.4680, -0.4727, -0.4775, -0.4823, -0.4872, -0.4921, -0.4971, -0.5021,\n",
      "         -0.5072, -0.5123, -0.5175, -0.5227, -0.5280, -0.5333, -0.5387, -0.5442,\n",
      "         -0.5497, -0.5552, -0.5608, -0.5665, -0.5722, -0.5780, -0.5838, -0.5897,\n",
      "         -0.5957, -0.6017, -0.6078, -0.6139, -0.6201, -0.6264, -0.6327, -0.6391,\n",
      "         -0.6456, -0.6521, -0.6587, -0.6653, -0.6720, -0.6788, -0.6857, -0.6926,\n",
      "         -0.6996, -0.7067, -0.7138, -0.7210, -0.7283, -0.7357, -0.7431, -0.7506,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7321,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2304e+00, -3.8335e+00, -5.7659e-01, -2.0831e+00,  3.9711e-01,\n",
      "         -2.2859e+00, -4.3968e+00,  3.7492e-02, -3.6140e+00, -2.3852e+00,\n",
      "         -3.8729e+00,  4.9873e-01, -2.2831e-01, -3.4194e+00, -3.1434e+00,\n",
      "         -2.0271e+00, -3.1366e+00, -1.6723e+00, -1.9543e+00, -7.9533e-01,\n",
      "         -1.1647e+00, -1.2599e+00, -7.1995e-01,  4.6599e-01, -1.7879e+00,\n",
      "         -3.3516e+00, -3.1056e+00, -2.2659e+00, -3.6877e+00,  1.0653e+00,\n",
      "         -1.5703e+00, -2.4737e-03, -9.7394e-01, -3.3985e+00, -1.9234e+00,\n",
      "         -2.0570e+00, -8.8981e-01, -3.0904e+00, -1.4218e+00,  2.6123e-01,\n",
      "         -5.7182e-01, -4.4331e-01, -1.8199e-01, -4.4592e+00, -1.4764e+00,\n",
      "         -3.0441e+00, -3.3710e+00, -3.1369e+00, -1.8580e-01, -1.2348e-01,\n",
      "         -3.8170e+00, -1.4824e+00, -7.3758e+00, -2.7305e+00, -2.9117e+00,\n",
      "         -7.1657e-01, -2.5594e+00, -2.8386e+00, -2.6265e+00, -1.4769e+00,\n",
      "         -3.5796e+00, -1.1605e+00, -5.9170e+00, -2.2469e+00, -3.4213e+00,\n",
      "          1.2273e+00, -1.1376e+00,  1.6224e-01, -3.9919e-01, -2.9819e+00,\n",
      "         -3.9203e+00, -5.7552e+00, -2.6330e+00, -2.6395e-01, -3.7334e+00,\n",
      "         -4.3379e+00, -2.2586e+00, -3.7977e+00, -2.3539e+00, -3.4571e+00,\n",
      "         -5.0051e+00, -2.2952e+00, -2.0776e-01, -1.1270e+00, -1.7711e+00,\n",
      "         -2.8711e+00, -1.6573e+00, -4.6724e-01, -3.6052e+00, -7.2576e-01,\n",
      "         -3.0513e+00, -4.1587e+00, -2.2477e+00, -5.3974e-01, -7.1051e-01,\n",
      "         -3.1980e+00, -2.6423e+00, -3.4175e+00, -3.3038e+00, -3.2179e+00,\n",
      "          1.9418e-02, -8.4367e-01, -3.7986e+00, -2.2601e+00, -4.5725e-01,\n",
      "         -1.6997e+00, -3.2802e+00, -3.7680e+00,  5.0863e-01, -1.3393e+00,\n",
      "         -1.4288e+00, -1.2231e+00, -4.2089e+00, -4.6628e+00,  5.7163e-01,\n",
      "         -8.4589e-01, -2.0425e+00, -1.2158e+00, -7.8926e-02, -7.9688e-01,\n",
      "         -2.4984e+00, -4.6809e+00, -2.5585e+00, -7.6394e-01, -1.0461e+00,\n",
      "         -2.8510e+00, -2.0115e+00, -6.8039e-01, -4.6938e-01, -4.1093e+00,\n",
      "         -6.7681e-01, -1.0290e-01, -3.2868e+00, -4.0396e+00, -1.1051e+00,\n",
      "         -2.0329e+00, -3.0282e+00, -2.4378e+00, -1.5215e+00, -2.8077e+00,\n",
      "         -2.5540e+00, -3.2275e+00, -5.8035e+00, -1.1361e+00, -6.6516e-01,\n",
      "         -7.2101e-01, -7.1675e-01, -7.1716e-01, -7.2876e-01, -8.1286e-01,\n",
      "         -4.8509e+00, -4.9390e+00, -5.3508e+00, -4.9407e+00, -4.8672e+00,\n",
      "         -4.6194e+00, -4.5091e+00, -4.6018e+00, -4.5709e+00, -4.6644e+00,\n",
      "         -4.7846e+00, -4.8282e+00, -4.8614e+00, -4.8328e+00, -4.7365e+00,\n",
      "         -4.6394e+00, -4.6018e+00, -4.5894e+00, -4.5672e+00, -4.5294e+00,\n",
      "         -4.5002e+00, -4.5018e+00, -4.5223e+00, -4.5574e+00, -4.5904e+00,\n",
      "         -4.5867e+00, -4.5495e+00, -4.5162e+00, -4.5241e+00, -4.5300e+00,\n",
      "         -4.5610e+00, -4.5951e+00, -4.5986e+00, -4.5956e+00, -4.5991e+00,\n",
      "         -4.6061e+00, -4.6328e+00, -4.6502e+00, -4.6588e+00, -4.6606e+00,\n",
      "         -4.6640e+00, -4.6863e+00, -4.7204e+00, -4.7407e+00, -4.7451e+00,\n",
      "         -4.7471e+00, -4.7586e+00, -4.7602e+00, -4.7644e+00, -4.7677e+00,\n",
      "         -4.7752e+00, -4.8009e+00, -4.8061e+00, -4.7921e+00, -4.7884e+00,\n",
      "         -4.7855e+00, -4.7680e+00, -4.7535e+00, -4.7109e+00, -4.6413e+00,\n",
      "         -4.6326e+00, -4.5807e+00, -4.5191e+00, -4.4287e+00, -4.3837e+00,\n",
      "         -4.2703e+00, -4.1730e+00, -4.1067e+00, -4.0678e+00, -4.0421e+00,\n",
      "         -4.0294e+00, -4.0285e+00],\n",
      "        [-2.7458e+00, -3.8386e+00, -6.8778e-01, -2.3787e+00,  2.7349e-01,\n",
      "         -2.3892e+00, -4.6178e+00, -3.7234e-01, -4.1656e+00, -2.6356e+00,\n",
      "         -4.0112e+00,  4.7065e-01, -3.5657e-01, -3.5644e+00, -3.0218e+00,\n",
      "         -2.3066e+00, -3.2616e+00, -1.7979e+00, -1.9028e+00, -1.0391e+00,\n",
      "         -1.2783e+00, -1.5286e+00, -9.2766e-01,  5.5515e-01, -2.0574e+00,\n",
      "         -3.5064e+00, -3.0064e+00, -2.4897e+00, -3.6757e+00,  1.0953e+00,\n",
      "         -1.7335e+00, -2.7740e-01, -9.4634e-01, -3.5102e+00, -1.9754e+00,\n",
      "         -1.8904e+00, -7.9231e-01, -3.1572e+00, -1.6147e+00,  1.6872e-02,\n",
      "         -6.2109e-01, -3.8632e-01, -2.2822e-01, -4.3737e+00, -1.6975e+00,\n",
      "         -3.5049e+00, -3.6328e+00, -3.2445e+00,  4.7036e-01,  4.7587e-01,\n",
      "         -3.9042e+00, -1.5864e+00, -7.3777e+00, -2.8817e+00, -2.9369e+00,\n",
      "         -8.6180e-01, -2.5311e+00, -2.8802e+00, -2.7625e+00, -1.6888e+00,\n",
      "         -3.7042e+00, -1.2473e+00, -5.9332e+00, -2.3503e+00, -3.4064e+00,\n",
      "          1.2833e+00, -1.2842e+00, -1.2421e-01, -3.2639e-01, -3.1002e+00,\n",
      "         -4.0859e+00, -5.7341e+00, -2.7790e+00, -1.6246e-01, -3.8451e+00,\n",
      "         -4.2666e+00, -2.4002e+00, -3.8239e+00, -2.5065e+00, -3.5617e+00,\n",
      "         -4.9790e+00, -2.4222e+00, -2.5623e-01, -1.3699e+00, -1.8611e+00,\n",
      "         -3.0993e+00, -1.8142e+00, -7.1730e-01, -3.7654e+00, -8.1492e-01,\n",
      "         -3.1746e+00, -4.0983e+00, -2.3708e+00, -5.7360e-01, -9.6695e-01,\n",
      "         -3.1192e+00, -2.8431e+00, -3.8098e+00, -3.3770e+00, -3.3534e+00,\n",
      "         -6.1484e-02, -1.0831e+00, -3.8001e+00, -2.2623e+00, -3.9167e-01,\n",
      "         -1.9464e+00, -3.6361e+00, -3.9378e+00,  6.2975e-01, -1.7357e+00,\n",
      "         -1.4873e+00, -9.5547e-01, -4.5804e+00, -4.7865e+00,  5.8188e-01,\n",
      "         -1.0062e+00, -2.1172e+00, -1.3384e+00, -1.2246e-01, -9.0615e-01,\n",
      "         -2.7287e+00, -4.7339e+00, -2.5826e+00, -7.2025e-01, -1.0927e+00,\n",
      "         -2.9617e+00, -2.1769e+00, -8.1652e-01, -5.7318e-01, -4.1813e+00,\n",
      "         -4.9998e-02,  4.8107e-01, -3.2934e+00, -4.1634e+00, -1.2237e+00,\n",
      "         -2.0066e+00, -3.0623e+00, -2.5613e+00, -1.6741e+00, -2.8521e+00,\n",
      "         -2.9508e+00, -5.8864e+00, -1.6576e+00, -2.5694e+00, -1.6002e+00,\n",
      "         -9.1380e-01, -6.0151e-01, -4.3450e+00, -3.7823e+00, -4.1200e-01,\n",
      "         -2.1012e+00, -1.0004e+00, -1.2386e+00, -4.7803e-01, -1.4396e-01,\n",
      "         -1.0266e+00, -4.2482e+00, -5.2428e+00, -1.4527e+00, -2.4178e+00,\n",
      "         -1.2562e-01,  3.9753e-01,  2.2408e-01, -4.2311e+00, -3.8016e+00,\n",
      "         -3.7369e-01, -1.1680e+00, -4.4812e+00, -6.3919e+00, -4.8940e+00,\n",
      "         -3.0692e+00, -4.9061e+00, -1.2575e+00,  9.3684e-01,  9.5452e-01,\n",
      "          9.9563e-01,  1.0224e+00,  7.8381e-01,  9.8064e-01, -4.6142e+00,\n",
      "         -4.9014e+00, -5.3066e+00, -5.1080e+00, -5.2535e+00, -5.1895e+00,\n",
      "         -5.0388e+00, -5.1009e+00, -5.1496e+00, -5.1172e+00, -5.0396e+00,\n",
      "         -4.9453e+00, -4.9059e+00, -4.9340e+00, -4.9933e+00, -5.0445e+00,\n",
      "         -5.0933e+00, -5.1333e+00, -5.1651e+00, -5.1836e+00, -5.2092e+00,\n",
      "         -5.2553e+00, -5.3218e+00, -5.3894e+00, -5.4494e+00, -5.4693e+00,\n",
      "         -5.4719e+00, -5.4842e+00, -5.5322e+00, -5.5641e+00, -5.5621e+00,\n",
      "         -5.5527e+00, -5.5300e+00, -5.4987e+00, -5.4771e+00, -5.4639e+00,\n",
      "         -5.4617e+00, -5.4814e+00, -5.5046e+00, -5.5229e+00, -5.5320e+00,\n",
      "         -5.5482e+00, -5.5497e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7270,  0.9456], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.7313,  0.9900, -0.7218, -0.6812,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.0656e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1460, -0.1475, -0.1490,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1378, -0.1392, -0.1406,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.7132,  0.9656, -0.7040, -0.6644,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8230, -3.8254, -0.6792,  ..., -5.7617, -5.7531, -5.7427],\n",
      "        [-2.7519, -3.8444, -0.6737,  ..., -5.6798, -5.6921, -5.6984],\n",
      "        [-2.8185, -3.8335, -0.6778,  ..., -5.7789, -5.7799, -5.7666],\n",
      "        ...,\n",
      "        [-2.2389, -3.8322, -0.5772,  ..., -4.0676, -4.0565, -4.0536],\n",
      "        [-2.2564, -3.8235, -0.5814,  ..., -4.1332, -4.1205, -4.1190],\n",
      "        [-2.9801, -3.7893, -0.6817,  ..., -5.1736, -5.1269, -5.1024]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9556,  0.9861,  0.9959, -0.6949,  0.9970, -0.6853, -0.6688,  0.8319],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.5758,  0.9900, -0.7256,  0.9900, -0.5895,  0.9900, -0.7526],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7366e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3220, -0.3253, -0.3285,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1015, -0.1026, -0.1036,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1522, -0.1538, -0.1553,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608, -1.5446,  0.9656, -0.7077,  0.9656, -0.5749,  0.9656, -0.7340],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9661, -3.7819, -0.6721,  ..., -5.2389, -5.1617, -5.1101],\n",
      "        [-2.0349, -3.8460, -0.4973,  ..., -4.1767, -4.1818, -4.1876],\n",
      "        [-2.7439, -3.8181, -0.6514,  ..., -5.7134, -5.7411, -5.7432],\n",
      "        ...,\n",
      "        [-2.3279, -3.8152, -0.5590,  ..., -4.5988, -4.5647, -4.5353],\n",
      "        [-2.9799, -3.7819, -0.6739,  ..., -5.1577, -5.1152, -5.0966],\n",
      "        [-2.2200, -3.8225, -0.5601,  ..., -4.0318, -4.0192, -4.0180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9485, -1.4254,  0.9772, -0.6971,  0.9487, -0.5571,  0.9063, -0.7066],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.3953, -0.7345, -0.6075,  0.9900,  0.9900,  0.9900, -0.7315,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.6004e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(6.8515e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2711, -0.2739, -0.2766,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1486, -0.1501, -0.1516,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1046, -0.1057, -0.1068,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1480, -0.1495, -0.1510,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3341, -0.7164, -0.5925,  0.9656,  0.9656,  0.9656, -0.7135,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1305, -3.8238, -0.5092,  ..., -4.4118, -4.3697, -4.3378],\n",
      "        [-2.2339, -3.8209, -0.5601,  ..., -4.0450, -4.0281, -4.0278],\n",
      "        [-2.3184, -3.8086, -0.5622,  ..., -4.6575, -4.6325, -4.6092],\n",
      "        ...,\n",
      "        [-2.8030, -3.8181, -0.6668,  ..., -5.7184, -5.7136, -5.7052],\n",
      "        [-2.2231, -3.8219, -0.5617,  ..., -4.0605, -4.0507, -4.0491],\n",
      "        [-2.8109, -3.8162, -0.6561,  ..., -5.7389, -5.7383, -5.7324]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3544, -0.6948, -0.5449,  1.0064,  0.9833,  1.0236, -0.6583,  1.0012],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7047, -0.5986, -0.6878,  0.9900,  0.9900,  0.9900, -1.1846],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5291e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.5680e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1426, -0.1440, -0.1455,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1031, -0.1042, -0.1052,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1379, -0.1393, -0.1407,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6874, -0.5838, -0.6708,  0.9656,  0.9656,  0.9656, -1.1327],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7326, -3.8287, -0.6546,  ..., -5.6732, -5.6845, -5.6927],\n",
      "        [-2.2278, -3.8182, -0.5536,  ..., -4.1013, -4.0905, -4.0905],\n",
      "        [-2.3144, -3.8051, -0.5467,  ..., -4.5855, -4.5505, -4.5201],\n",
      "        ...,\n",
      "        [-2.8104, -3.8136, -0.6617,  ..., -5.7313, -5.7274, -5.7073],\n",
      "        [-2.9663, -3.7792, -0.6648,  ..., -5.1961, -5.1495, -5.1257],\n",
      "        [-2.3941, -3.7730, -0.5594,  ..., -4.0803, -4.0742, -4.0610]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9950, -0.6250, -0.5515, -0.6283,  1.0066,  1.0124,  0.8889, -1.1951],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7520, -0.6989, -1.5877, -0.7616,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5620e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(2.5145e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1521, -0.1537, -0.1552,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1541, -0.1556, -0.1572,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7335, -0.6816, -1.5563, -0.7428,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7984, -3.8097, -0.6636,  ..., -5.7576, -5.7492, -5.7370],\n",
      "        [-2.9572, -3.7716, -0.6625,  ..., -5.1485, -5.1049, -5.0809],\n",
      "        [-2.2080, -3.8101, -0.5456,  ..., -4.1152, -4.1018, -4.1000],\n",
      "        ...,\n",
      "        [-2.2132, -3.8119, -0.5575,  ..., -4.0300, -4.0188, -4.0158],\n",
      "        [-2.9570, -3.7773, -0.6645,  ..., -5.1891, -5.1432, -5.1184],\n",
      "        [-2.7933, -3.8086, -0.6619,  ..., -5.7166, -5.7148, -5.7007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9683,  0.8609, -0.6809, -0.6470, -1.4192, -0.6887,  0.8893,  0.9716],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7616, -0.6908, -0.7397,  0.9900,  0.9900, -0.7176,  0.9900, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8725e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1541, -0.1556, -0.1572,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1398, -0.1412, -0.1426,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1497, -0.1512, -0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1452, -0.1466, -0.1481,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7428, -0.6738, -0.7215,  0.9656,  0.9656, -0.6999,  0.9656, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1977, -3.8166, -0.5595,  ..., -4.0260, -4.0124, -4.0121],\n",
      "        [-2.2111, -3.8154, -0.5609,  ..., -4.0809, -4.0680, -4.0657],\n",
      "        [-2.1952, -3.8172, -0.5475,  ..., -4.0726, -4.0602, -4.0596],\n",
      "        ...,\n",
      "        [-2.2122, -3.8220, -0.5542,  ..., -4.0073, -3.9979, -3.9982],\n",
      "        [-2.7785, -3.8242, -0.6624,  ..., -5.7745, -5.7772, -5.7641],\n",
      "        [-2.4011, -3.7666, -0.5823,  ..., -0.9591, -0.9894, -1.0416]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6969, -0.6403, -0.7089,  0.9647,  0.9647, -0.6218,  0.9820, -0.9820],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7432,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.3904e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1503, -0.1519, -0.1534, -0.1549, -0.1565, -0.1581, -0.1597, -0.1613,\n",
      "         -0.1629, -0.1646, -0.1662, -0.1679, -0.1696, -0.1713, -0.1731, -0.1748,\n",
      "         -0.1766, -0.1784, -0.1802, -0.1820, -0.1838, -0.1857, -0.1876, -0.1894,\n",
      "         -0.1914, -0.1933, -0.1952, -0.1972, -0.1992, -0.2012, -0.2033, -0.2053,\n",
      "         -0.2074, -0.2095, -0.2116, -0.2137, -0.2159, -0.2181, -0.2203, -0.2225,\n",
      "         -0.2247, -0.2270, -0.2293, -0.2316, -0.2340, -0.2363, -0.2387, -0.2411,\n",
      "         -0.2436, -0.2460, -0.2485, -0.2510, -0.2536, -0.2561, -0.2587, -0.2613,\n",
      "         -0.2640, -0.2666, -0.2693, -0.2720, -0.2748, -0.2776, -0.2804, -0.2832,\n",
      "         -0.2861, -0.2889, -0.2919, -0.2948, -0.2978, -0.3008, -0.3038, -0.3069,\n",
      "         -0.3100, -0.3131, -0.3163, -0.3195, -0.3227, -0.3260, -0.3293, -0.3326,\n",
      "         -0.3360, -0.3393, -0.3428, -0.3462, -0.3497, -0.3533, -0.3568, -0.3604,\n",
      "         -0.3641, -0.3678, -0.3715, -0.3752, -0.3790, -0.3828, -0.3867, -0.3906,\n",
      "         -0.3946, -0.3985, -0.4026, -0.4066, -0.4107, -0.4149, -0.4191, -0.4233,\n",
      "         -0.4276, -0.4319, -0.4363, -0.4407, -0.4451, -0.4496, -0.4542, -0.4588,\n",
      "         -0.4634, -0.4681, -0.4728, -0.4776, -0.4824, -0.4873, -0.4922, -0.4972,\n",
      "         -0.5022, -0.5073, -0.5124, -0.5176, -0.5228, -0.5281, -0.5334, -0.5388,\n",
      "         -0.5442, -0.5497, -0.5553, -0.5609, -0.5666, -0.5723, -0.5781, -0.5839,\n",
      "         -0.5898, -0.5958, -0.6018, -0.6079, -0.6140, -0.6202, -0.6265, -0.6328,\n",
      "         -0.6392, -0.6456, -0.6522, -0.6588, -0.6654, -0.6721, -0.6789, -0.6858,\n",
      "         -0.6927, -0.6997, -0.7068, -0.7139, -0.7211, -0.7284, -0.7358, -0.7432,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7249,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1975, -3.8222, -0.5556, -2.0657,  0.4276, -2.2728, -4.4104,  0.0698,\n",
      "         -3.5810, -2.3334, -3.8712,  0.5253, -0.1788, -3.4227, -3.1263, -2.0220,\n",
      "         -3.1091, -1.6449, -1.9458, -0.7625, -1.1660, -1.2351, -0.6945,  0.4740,\n",
      "         -1.7302, -3.3485, -3.0958, -2.2613, -3.6808,  1.0958, -1.5395,  0.0298,\n",
      "         -0.9786, -3.3961, -1.9126, -2.0770, -0.8865, -3.0592, -1.4126,  0.2749,\n",
      "         -0.5363, -0.4328, -0.1589, -4.4486, -1.4223, -2.9985, -3.3152, -3.1324,\n",
      "         -0.1572, -0.1270, -3.8120, -1.4582, -7.3482, -2.7276, -2.8821, -0.6908,\n",
      "         -2.5716, -2.8088, -2.5995, -1.4170, -3.5738, -1.1379, -5.8931, -2.2461,\n",
      "         -3.4132,  1.2573, -1.1056,  0.1950, -0.4003, -2.9835, -3.9121, -5.7545,\n",
      "         -2.6356, -0.3416, -3.7336, -4.3587, -2.2591, -3.7702, -2.3439, -3.4561,\n",
      "         -5.0040, -2.2976, -0.1810, -1.0962, -1.7711, -2.8514, -1.6233, -0.4355,\n",
      "         -3.6074, -0.6911, -3.0502, -4.1733, -2.2500, -0.5159, -0.6819, -3.1843,\n",
      "         -2.5956, -3.3734, -3.3005, -3.2050,  0.0490, -0.8174, -3.7979, -2.2197,\n",
      "         -0.4493, -1.6726, -3.2371, -3.7782,  0.4291, -1.3787, -1.7865, -1.1286,\n",
      "         -3.8127, -4.6102,  0.6340, -0.8320, -2.0246, -1.1943, -0.0540, -0.8387,\n",
      "         -2.4278, -4.6636, -2.5481, -0.7561, -1.0005, -2.8388, -1.9505, -0.6478,\n",
      "         -0.3068, -4.1235, -0.6653, -0.1244, -3.2200, -4.0879, -1.0749, -2.0420,\n",
      "         -2.9946, -2.4160, -1.4666, -2.8009, -2.5005, -3.1801, -5.7070, -1.0662,\n",
      "         -0.7562, -0.6109, -0.6792, -0.7253, -0.6873, -0.6994, -4.8347, -4.9089,\n",
      "         -5.3282, -4.8923, -4.7687, -4.4440, -4.3269, -4.4559, -4.4851, -4.5960,\n",
      "         -4.7495, -4.8251, -4.8652, -4.8490, -4.7658, -4.6715, -4.6435, -4.6556,\n",
      "         -4.6555, -4.6190, -4.5817, -4.5808, -4.6141, -4.6559, -4.6805, -4.6739,\n",
      "         -4.6283, -4.5891, -4.5880, -4.5871, -4.6310, -4.6623, -4.6623, -4.6429,\n",
      "         -4.6353, -4.6259, -4.6494, -4.6729, -4.6825, -4.6938, -4.7023, -4.7337,\n",
      "         -4.7693, -4.7865, -4.7817, -4.7768, -4.7760, -4.7544, -4.7507, -4.7428,\n",
      "         -4.7344, -4.7436, -4.7438, -4.7382, -4.7450, -4.7436, -4.7118, -4.6939,\n",
      "         -4.6445, -4.5942, -4.5886, -4.5367, -4.5029, -4.4127, -4.3587, -4.2505,\n",
      "         -4.1562, -4.0939, -4.0541, -4.0257, -4.0171, -4.0175],\n",
      "        [-2.7848, -3.8228, -0.6711, -2.4296,  0.2786, -2.3833, -4.6586, -0.4579,\n",
      "         -4.2172, -2.6573, -4.0408,  0.4912, -0.3290, -3.6238, -2.9983, -2.3614,\n",
      "         -3.2347, -1.7971, -1.8729, -1.0475, -1.3073, -1.5456, -0.9282,  0.5870,\n",
      "         -2.0395, -3.5655, -2.9942, -2.5276, -3.6645,  1.1414, -1.7384, -0.2998,\n",
      "         -0.9494, -3.5349, -1.9652, -1.8901, -0.7827, -3.1078, -1.6492,  0.0092,\n",
      "         -0.6044, -0.3677, -0.2067, -4.3508, -1.6746, -3.5278, -3.6386, -3.2645,\n",
      "          0.6434,  0.5932, -3.9369, -1.5788, -7.3549, -2.9031, -2.8849, -0.8658,\n",
      "         -2.5620, -2.8438, -2.7615, -1.6598, -3.7514, -1.2440, -5.9147, -2.3689,\n",
      "         -3.3924,  1.3363, -1.2845, -0.1456, -0.3176, -3.1262, -4.1215, -5.7209,\n",
      "         -2.8061, -0.2250, -3.8878, -4.2755, -2.4229, -3.7666, -2.5296, -3.6057,\n",
      "         -4.9647, -2.4458, -0.2420, -1.3929, -1.8639, -3.1058, -1.8178, -0.7387,\n",
      "         -3.7752, -0.7885, -3.2270, -4.1097, -2.3937, -0.5601, -0.9894, -3.0942,\n",
      "         -2.8212, -3.8240, -3.3855, -3.3528, -0.0442, -1.1041, -3.7776, -2.2181,\n",
      "         -0.3607, -1.9640, -3.6408, -3.9721,  0.5740, -1.4311, -1.5886, -1.1307,\n",
      "         -4.2155, -4.6666,  0.6059, -1.1023, -2.1499, -1.4276, -0.2514, -0.9567,\n",
      "         -2.8006, -4.6349, -2.5903, -0.7321, -1.1909, -3.0181, -2.1406, -0.8014,\n",
      "         -0.6131, -4.2441,  0.1640,  0.6296, -3.2496, -4.1915, -1.2127, -2.0367,\n",
      "         -3.0007, -2.5487, -1.6148, -2.7924, -2.9184, -5.8371, -1.6366, -2.4937,\n",
      "         -1.3827, -0.9290, -0.5681, -3.9679, -3.8533, -0.3210, -2.0242, -0.3209,\n",
      "         -1.0565, -0.5245,  0.1179, -0.6727, -4.1557, -3.9739, -4.9687, -1.2981,\n",
      "          0.9062,  1.0001,  0.9711,  0.9148,  0.7821,  1.0007, -4.6644, -4.8687,\n",
      "         -5.2540, -5.0626, -5.2693, -5.3454, -5.3722, -5.3528, -5.3477, -5.3123,\n",
      "         -5.2130, -5.0864, -5.0195, -4.9973, -5.0120, -5.0302, -5.0523, -5.0852,\n",
      "         -5.1032, -5.1079, -5.1271, -5.1686, -5.2302, -5.2998, -5.3412, -5.3553,\n",
      "         -5.3500, -5.3550, -5.3838, -5.4357, -5.4612, -5.4599, -5.4463, -5.4127,\n",
      "         -5.3913, -5.3805, -5.3847, -5.4062, -5.4374, -5.4753, -5.5130, -5.5311,\n",
      "         -5.5318, -5.5141, -5.5121, -5.5247, -5.5538, -5.5959, -5.6215, -5.6258,\n",
      "         -5.6216, -5.6216, -5.6167, -5.6163, -5.6071, -5.5955]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6931,  0.9292], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6899, -0.7541,  0.9900, -1.1000, -1.3940,  0.9900, -0.7099,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8663e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.0951e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1396, -0.1410, -0.1424,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1526, -0.1541, -0.1556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1436, -0.1451, -0.1465,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6729, -0.7355,  0.9656, -1.0518, -1.3329,  0.9608, -0.6924,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1953, -3.8222, -0.5689,  ..., -4.1102, -4.0944, -4.0919],\n",
      "        [-2.1836, -3.8265, -0.5640,  ..., -4.0220, -4.0093, -4.0061],\n",
      "        [-2.7657, -3.8243, -0.6716,  ..., -5.7480, -5.7396, -5.7267],\n",
      "        ...,\n",
      "        [-2.9182, -3.7843, -0.6732,  ..., -5.1975, -5.1221, -5.0718],\n",
      "        [-2.1992, -3.8315, -0.5604,  ..., -4.0032, -3.9914, -3.9925],\n",
      "        [-2.9274, -3.7929, -0.6766,  ..., -5.1708, -5.1254, -5.1027]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6802, -0.7198,  0.9373, -0.9924, -1.2801,  0.9455, -0.6458,  0.8613],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.5761,  0.9900,  0.9900, -0.7309,  0.9900,  0.9900,  0.9900, -0.7388],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1983e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.1864e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3221, -0.3253, -0.3286,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1495, -0.1510, -0.1525,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5449,  0.9656,  0.9656, -0.7129,  0.9656,  0.9656,  0.9656, -0.7206],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-1.9869, -3.8547, -0.4941,  ..., -4.1543, -4.1615, -4.1651],\n",
      "        [-2.7551, -3.8411, -0.6711,  ..., -5.7676, -5.7685, -5.7541],\n",
      "        [-2.7583, -3.8343, -0.6781,  ..., -5.6760, -5.6777, -5.6626],\n",
      "        ...,\n",
      "        [-2.7551, -3.8411, -0.6711,  ..., -5.7676, -5.7685, -5.7541],\n",
      "        [-2.7755, -3.8351, -0.6760,  ..., -5.6117, -5.5994, -5.5929],\n",
      "        [-2.1871, -3.8344, -0.5604,  ..., -4.0245, -4.0156, -4.0143]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4648,  0.9430,  0.9247, -0.7386,  0.9708,  0.9430,  0.9131, -0.7172],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1789, -0.5858,  0.9900, -0.6921, -0.6760, -0.7226,  0.9900, -0.5993],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2487e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1372, -0.1386, -0.1400,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1009, -0.1019, -0.1029,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1462, -0.1477, -0.1492,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1032, -0.1043, -0.1053,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1273, -0.5713,  0.9656, -0.6750, -0.6594, -0.7048,  0.9656, -0.5845],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3553, -3.7947, -0.5715,  ..., -4.0655, -4.0596, -4.0450],\n",
      "        [-2.2753, -3.8299, -0.5584,  ..., -4.5462, -4.5095, -4.4799],\n",
      "        [-2.7652, -3.8415, -0.6772,  ..., -5.7070, -5.7037, -5.6832],\n",
      "        ...,\n",
      "        [-2.1772, -3.8422, -0.5684,  ..., -4.0537, -4.0398, -4.0348],\n",
      "        [-2.6854, -3.8341, -0.6532,  ..., -5.6810, -5.7100, -5.7170],\n",
      "        [-2.2747, -3.8281, -0.5684,  ..., -4.6161, -4.5911, -4.5624]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1255, -0.6005,  0.9451, -0.7112, -0.6871, -0.6915,  0.9413, -0.5720],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7171, -0.7189, -0.7445,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9284e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1451, -0.1465, -0.1480,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6994, -0.7011, -0.7262,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9297, -3.8026, -0.6835,  ..., -5.1302, -5.0838, -5.0593],\n",
      "        [-2.7600, -3.8441, -0.6694,  ..., -5.7279, -5.7239, -5.7204],\n",
      "        [-2.1768, -3.8475, -0.5702,  ..., -4.0520, -4.0388, -4.0350],\n",
      "        ...,\n",
      "        [-2.6891, -3.8584, -0.6702,  ..., -5.6611, -5.6729, -5.6783],\n",
      "        [-2.7605, -3.8435, -0.6814,  ..., -5.7441, -5.7367, -5.7228],\n",
      "        [-2.6817, -3.8491, -0.6741,  ..., -5.4900, -5.5071, -5.5074]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8301,  0.9447, -0.6975, -0.7867, -0.7520,  0.9292,  0.9196,  0.9296],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7256,  0.9900,  0.9900, -0.5768,  0.9900, -0.7258, -0.7137, -0.7388],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7017e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1468, -0.1483, -0.1498,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1468, -0.1483, -0.1498,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1444, -0.1458, -0.1473,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1495, -0.1510, -0.1525,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7077,  0.9656,  0.9656, -0.5626,  0.9656, -0.7079, -0.6961, -0.7206],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1928, -3.8481, -0.5656,  ..., -4.0271, -4.0159, -4.0147],\n",
      "        [-2.9311, -3.8068, -0.6854,  ..., -5.1288, -5.0816, -5.0591],\n",
      "        [-2.6851, -3.8426, -0.6558,  ..., -5.6764, -5.7062, -5.7134],\n",
      "        ...,\n",
      "        [-2.1828, -3.8446, -0.5618,  ..., -4.1108, -4.0947, -4.0911],\n",
      "        [-2.1830, -3.8452, -0.5617,  ..., -4.0712, -4.0572, -4.0546],\n",
      "        [-2.1866, -3.8462, -0.5735,  ..., -4.0181, -4.0047, -4.0002]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7407,  0.8333,  0.9393, -0.6093,  0.8470, -0.7346, -0.7947, -0.7554],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900, -1.5646, -0.6745],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.6994e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3197, -0.3229, -0.3262,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1364, -0.1378, -0.1392,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656,  0.9656,  0.9656, -1.5336, -0.6578],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7592, -3.8488, -0.6793,  ..., -5.7060, -5.7050, -5.6878],\n",
      "        [-2.7661, -3.8578, -0.6861,  ..., -5.7597, -5.7565, -5.7467],\n",
      "        [-2.6840, -3.8552, -0.6761,  ..., -5.4889, -5.5070, -5.5063],\n",
      "        ...,\n",
      "        [-2.7567, -3.8509, -0.6816,  ..., -5.6735, -5.6729, -5.6578],\n",
      "        [-1.9871, -3.8702, -0.4979,  ..., -4.1502, -4.1569, -4.1625],\n",
      "        [-2.2016, -3.8439, -0.5768,  ..., -4.1081, -4.0921, -4.0881]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8986,  0.9468,  0.9383,  0.8729,  0.9150,  0.9017, -1.4805, -0.7169],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6693,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4259e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.5841e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1354, -0.1368, -0.1382, -0.1395, -0.1410, -0.1424, -0.1438, -0.1453,\n",
      "         -0.1467, -0.1482, -0.1497, -0.1512, -0.1528, -0.1543, -0.1559, -0.1574,\n",
      "         -0.1590, -0.1606, -0.1623, -0.1639, -0.1655, -0.1672, -0.1689, -0.1706,\n",
      "         -0.1723, -0.1741, -0.1758, -0.1776, -0.1794, -0.1812, -0.1831, -0.1849,\n",
      "         -0.1868, -0.1887, -0.1906, -0.1925, -0.1944, -0.1964, -0.1984, -0.2004,\n",
      "         -0.2024, -0.2044, -0.2065, -0.2086, -0.2107, -0.2128, -0.2150, -0.2172,\n",
      "         -0.2193, -0.2216, -0.2238, -0.2261, -0.2283, -0.2307, -0.2330, -0.2353,\n",
      "         -0.2377, -0.2401, -0.2425, -0.2450, -0.2475, -0.2500, -0.2525, -0.2550,\n",
      "         -0.2576, -0.2602, -0.2628, -0.2655, -0.2682, -0.2709, -0.2736, -0.2764,\n",
      "         -0.2792, -0.2820, -0.2849, -0.2877, -0.2906, -0.2936, -0.2965, -0.2995,\n",
      "         -0.3026, -0.3056, -0.3087, -0.3118, -0.3150, -0.3182, -0.3214, -0.3246,\n",
      "         -0.3279, -0.3312, -0.3345, -0.3379, -0.3413, -0.3448, -0.3483, -0.3518,\n",
      "         -0.3553, -0.3589, -0.3626, -0.3662, -0.3699, -0.3737, -0.3774, -0.3812,\n",
      "         -0.3851, -0.3890, -0.3929, -0.3969, -0.4009, -0.4049, -0.4090, -0.4132,\n",
      "         -0.4173, -0.4215, -0.4258, -0.4301, -0.4345, -0.4388, -0.4433, -0.4478,\n",
      "         -0.4523, -0.4568, -0.4615, -0.4661, -0.4708, -0.4756, -0.4804, -0.4852,\n",
      "         -0.4901, -0.4951, -0.5001, -0.5051, -0.5102, -0.5154, -0.5206, -0.5259,\n",
      "         -0.5312, -0.5365, -0.5420, -0.5474, -0.5530, -0.5586, -0.5642, -0.5699,\n",
      "         -0.5756, -0.5815, -0.5873, -0.5933, -0.5993, -0.6053, -0.6114, -0.6176,\n",
      "         -0.6238, -0.6301, -0.6365, -0.6429, -0.6494, -0.6560, -0.6626, -0.6693,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6528,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2024, -3.8484, -0.5733, -2.0891,  0.4311, -2.2836, -4.4248,  0.0761,\n",
      "         -3.5845, -2.3278, -3.8920,  0.5044, -0.2351, -3.4192, -3.1474, -2.0402,\n",
      "         -3.1175, -1.6293, -1.9709, -0.7719, -1.1631, -1.2372, -0.6860,  0.4735,\n",
      "         -1.7195, -3.3529, -3.1207, -2.2797, -3.6944,  1.0919, -1.5362,  0.0201,\n",
      "         -0.9845, -3.4218, -1.9053, -2.0234, -0.8846, -3.0636, -1.4173,  0.2850,\n",
      "         -0.4963, -0.4382, -0.1631, -4.4629, -1.4074, -3.0036, -3.3020, -3.1562,\n",
      "         -0.1506, -0.1299, -3.8041, -1.4689, -7.3447, -2.7439, -2.8821, -0.6850,\n",
      "         -2.5975, -2.8348, -2.6197, -1.4045, -3.5791, -1.1457, -5.8948, -2.2614,\n",
      "         -3.4254,  1.2493, -1.1064,  0.1799, -0.4054, -3.0115, -3.9219, -5.7666,\n",
      "         -2.6580, -0.3041, -3.7388, -4.4022, -2.2807, -3.7755, -2.3435, -3.4643,\n",
      "         -5.0194, -2.3177, -0.1650, -1.1010, -1.7784, -2.8492, -1.6197, -0.4460,\n",
      "         -3.6279, -0.6535, -3.0598, -4.2171, -2.2722, -0.4979, -0.6896, -3.1982,\n",
      "         -2.5804, -3.3848, -3.3258, -3.2221,  0.0627, -0.8278, -3.8045, -2.2168,\n",
      "         -0.4460, -1.6595, -3.2438, -3.7805,  0.4696, -0.9991, -1.5674, -0.4770,\n",
      "         -4.2609, -4.7006,  0.6069, -0.7328, -2.0376, -1.1743, -0.0449, -0.8003,\n",
      "         -2.4089, -4.6955, -2.5298, -0.7625, -0.9948, -2.8945, -1.9420, -0.6519,\n",
      "         -0.3273, -4.1698, -0.5509, -0.1173, -3.2355, -4.1041, -1.0319, -2.0707,\n",
      "         -3.0216, -2.4557, -1.4705, -2.7964, -2.5147, -3.1231, -5.8283, -1.1492,\n",
      "         -0.6037, -0.7187, -0.6668, -0.7365, -0.6736, -0.7757, -4.8469, -4.9394,\n",
      "         -5.3917, -4.9606, -4.8447, -4.5521, -4.4197, -4.5391, -4.4995, -4.5877,\n",
      "         -4.8104, -4.9325, -4.9822, -4.9553, -4.8477, -4.7285, -4.6773, -4.6574,\n",
      "         -4.6327, -4.5941, -4.5589, -4.5446, -4.5580, -4.5786, -4.5982, -4.5999,\n",
      "         -4.5657, -4.5350, -4.5371, -4.5250, -4.5603, -4.6082, -4.6267, -4.6153,\n",
      "         -4.6140, -4.6048, -4.6256, -4.6515, -4.6631, -4.6714, -4.6859, -4.7242,\n",
      "         -4.7701, -4.8033, -4.8175, -4.8202, -4.8334, -4.8354, -4.8382, -4.8427,\n",
      "         -4.8464, -4.8618, -4.8593, -4.8408, -4.8326, -4.8308, -4.8079, -4.7946,\n",
      "         -4.7578, -4.7070, -4.6927, -4.6293, -4.5809, -4.4930, -4.4464, -4.3171,\n",
      "         -4.2106, -4.1511, -4.1088, -4.0758, -4.0621, -4.0587],\n",
      "        [-2.6925, -3.8664, -0.6685, -2.3801,  0.3134, -2.3914, -4.6301, -0.3335,\n",
      "         -4.1036, -2.5639, -4.0316,  0.4720, -0.3660, -3.5455, -3.0307, -2.2947,\n",
      "         -3.2491, -1.7497, -1.9216, -1.0097, -1.2800, -1.5005, -0.8952,  0.5493,\n",
      "         -1.9935, -3.4926, -3.0273, -2.4856, -3.6868,  1.1234, -1.7012, -0.2493,\n",
      "         -0.9580, -3.5366, -1.9632, -1.8623, -0.7977, -3.1405, -1.6001,  0.0426,\n",
      "         -0.5464, -0.3892, -0.2073, -4.3868, -1.6323, -3.4431, -3.5588, -3.2630,\n",
      "          0.4719,  0.4461, -3.8763, -1.5771, -7.3574, -2.8790, -2.9167, -0.8211,\n",
      "         -2.5622, -2.8731, -2.7538, -1.6245, -3.6910, -1.2354, -5.9209, -2.3515,\n",
      "         -3.4133,  1.3059, -1.2533, -0.1003, -0.3358, -3.1313, -4.0639, -5.7432,\n",
      "         -2.7833, -0.2052, -3.8343, -4.3299, -2.4029, -3.8092, -2.4885, -3.5517,\n",
      "         -4.9935, -2.4266, -0.2148, -1.3395, -1.8610, -3.0630, -1.7753, -0.6918,\n",
      "         -3.7749, -0.7410, -3.1668, -4.1563, -2.3775, -0.5310, -0.9391, -3.1200,\n",
      "         -2.7837, -3.7538, -3.3962, -3.3566, -0.0135, -1.0612, -3.7909, -2.2214,\n",
      "         -0.3919, -1.9017, -3.5748, -3.9290,  0.5904, -1.2998, -1.5326, -1.0228,\n",
      "         -4.6886, -4.6950,  0.5396, -1.0266, -2.1654, -1.3760, -0.1956, -0.9603,\n",
      "         -2.7110, -4.7502, -2.5441, -0.7342, -1.1622, -3.0534, -2.1226, -0.7970,\n",
      "         -0.5845, -4.2623, -0.0615,  0.4616, -3.3288, -4.1897, -1.1589, -2.0346,\n",
      "         -3.0376, -2.5585, -1.5955, -2.7508, -2.8346, -5.8906, -1.5804, -2.5410,\n",
      "         -1.1817, -0.9418, -0.8122, -4.2534, -3.9295, -0.3892, -2.0475, -0.4267,\n",
      "         -1.2127, -0.9038,  0.2006, -1.0304, -4.3385, -5.0707, -1.5069, -2.3214,\n",
      "         -0.1620, -0.0295,  0.0578, -4.1020, -3.8693, -0.5175, -1.2438, -4.4786,\n",
      "         -6.3093, -4.6774, -3.0707, -4.9936, -1.3193,  0.9112,  0.8727,  0.9672,\n",
      "          0.9930,  0.9402,  0.9737, -4.7198, -4.9133, -5.2964, -5.1466, -5.3205,\n",
      "         -5.2265, -5.0466, -5.0756, -5.1188, -5.0955, -5.0180, -4.9532, -4.9527,\n",
      "         -4.9924, -5.0471, -5.0874, -5.1230, -5.1531, -5.1741, -5.2010, -5.2309,\n",
      "         -5.2889, -5.3709, -5.4527, -5.5246, -5.5502, -5.5595, -5.5733, -5.6209,\n",
      "         -5.6522, -5.6601, -5.6494, -5.6295, -5.6022, -5.5864, -5.5779, -5.5804,\n",
      "         -5.6078, -5.6303, -5.6485, -5.6609, -5.6746, -5.6804]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6958,  0.9430], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.5698,  0.9900,  0.9900, -1.1806, -0.7321, -0.7185, -0.6714, -0.5823],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.4226e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3208, -0.3240, -0.3273,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1454, -0.1468, -0.1483,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1358, -0.1372, -0.1386,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1003, -0.1013, -0.1023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5387,  0.9656,  0.9656, -1.1288, -0.7141, -0.7008, -0.6548, -0.5679],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-1.9886, -3.8696, -0.4947,  ..., -4.1483, -4.1564, -4.1607],\n",
      "        [-2.7637, -3.8504, -0.6748,  ..., -5.7413, -5.7342, -5.7188],\n",
      "        [-2.7637, -3.8504, -0.6748,  ..., -5.7413, -5.7342, -5.7188],\n",
      "        ...,\n",
      "        [-2.1855, -3.8531, -0.5690,  ..., -4.0472, -4.0342, -4.0285],\n",
      "        [-2.2030, -3.8480, -0.5714,  ..., -4.0723, -4.0579, -4.0558],\n",
      "        [-2.2819, -3.8405, -0.5565,  ..., -4.5480, -4.5081, -4.4788]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4659,  0.9495,  0.9495, -1.1092, -0.7275, -0.6836, -0.6874, -0.5976],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6758,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(4.2732e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1367, -0.1381, -0.1395,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6592,  0.9656,  0.9656,  0.9608,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7525, -3.8511, -0.6755,  ..., -5.6703, -5.6685, -5.6541],\n",
      "        [-2.9237, -3.8136, -0.6768,  ..., -5.1514, -5.1064, -5.0835],\n",
      "        [-2.1974, -3.8469, -0.5683,  ..., -4.0724, -4.0593, -4.0569],\n",
      "        ...,\n",
      "        [-2.9163, -3.8065, -0.6752,  ..., -5.1796, -5.1025, -5.0528],\n",
      "        [-2.6801, -3.8556, -0.6698,  ..., -5.4869, -5.5026, -5.5011],\n",
      "        [-2.9265, -3.8075, -0.6745,  ..., -5.1206, -5.0737, -5.0488]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9247,  0.8923, -0.6779,  0.9624,  0.8827,  0.9654,  0.9731,  0.8827],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.7499,  0.9900,  0.9900, -0.6834, -0.7368],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1631e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1383, -0.1396, -0.1411,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1490, -0.1506, -0.1521,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.7314,  0.9656,  0.9656, -0.6665, -0.7186],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7631, -3.8497, -0.6669,  ..., -5.7007, -5.6959, -5.6756],\n",
      "        [-2.7593, -3.8568, -0.6757,  ..., -5.7556, -5.7582, -5.7436],\n",
      "        [-2.6813, -3.8422, -0.6442,  ..., -5.6648, -5.6932, -5.7017],\n",
      "        ...,\n",
      "        [-2.6879, -3.8645, -0.6590,  ..., -5.6546, -5.6685, -5.6731],\n",
      "        [-2.1961, -3.8413, -0.5650,  ..., -4.1020, -4.0864, -4.0804],\n",
      "        [-2.1813, -3.8437, -0.5503,  ..., -4.1086, -4.0914, -4.0847]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9845,  0.9862,  0.9864, -0.7249,  0.9850,  0.9760, -0.6868, -0.6979],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.6911,  0.9900, -0.5854, -0.7223,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1021e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.0585e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1008, -0.1018, -0.1029,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1461, -0.1476, -0.1491,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.6741,  0.9656, -0.5709, -0.7045,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6781, -3.8403, -0.6409,  ..., -5.6628, -5.6915, -5.6991],\n",
      "        [-2.9239, -3.8103, -0.6694,  ..., -5.1500, -5.1006, -5.0784],\n",
      "        [-2.6868, -3.8616, -0.6562,  ..., -5.6517, -5.6634, -5.6684],\n",
      "        ...,\n",
      "        [-2.2743, -3.8344, -0.5481,  ..., -4.5418, -4.5060, -4.4740],\n",
      "        [-2.1758, -3.8463, -0.5581,  ..., -4.0428, -4.0301, -4.0249],\n",
      "        [-2.9256, -3.8014, -0.6668,  ..., -5.1095, -5.0645, -5.0459]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9937,  0.9160,  0.9834, -0.7054,  0.9797, -0.5775, -0.6536,  0.9211],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7483, -0.5972,  0.9900, -1.3846,  0.9900,  0.9900,  0.9900, -0.7218],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5448e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(6.8525e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1514, -0.1529, -0.1545,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1029, -0.1039, -0.1050,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1460, -0.1475, -0.1490,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7298, -0.5824,  0.9656, -1.3239,  0.9656,  0.9656,  0.9656, -0.7040],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1595, -3.8396, -0.5472,  ..., -4.0045, -3.9890, -3.9830],\n",
      "        [-2.2714, -3.8323, -0.5554,  ..., -4.6097, -4.5862, -4.5581],\n",
      "        [-2.7660, -3.8469, -0.6633,  ..., -5.6011, -5.5896, -5.5767],\n",
      "        ...,\n",
      "        [-2.7660, -3.8469, -0.6633,  ..., -5.6011, -5.5896, -5.5767],\n",
      "        [-2.7547, -3.8455, -0.6512,  ..., -5.7141, -5.7100, -5.7039],\n",
      "        [-2.1752, -3.8411, -0.5455,  ..., -4.0620, -4.0472, -4.0443]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7080, -0.5325,  0.9836, -1.2540,  0.9494,  0.9836,  1.0052, -0.7601],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7468,  0.9900, -1.5662, -0.7324, -0.7204,  0.9900, -0.6981, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.4614e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.8707e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1511, -0.1526, -0.1541,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3200, -0.3233, -0.3265,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1412, -0.1427, -0.1441,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7284,  0.9656, -1.5352, -0.7143, -0.7026,  0.9656, -0.6809, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1790, -3.8408, -0.5548,  ..., -4.0148, -3.9979, -3.9945],\n",
      "        [-2.7465, -3.8492, -0.6609,  ..., -5.6837, -5.6793, -5.6688],\n",
      "        [-1.9779, -3.8624, -0.4799,  ..., -4.1515, -4.1590, -4.1636],\n",
      "        ...,\n",
      "        [-2.7530, -3.8460, -0.6636,  ..., -5.6611, -5.6603, -5.6460],\n",
      "        [-2.1934, -3.8471, -0.5490,  ..., -3.9968, -3.9843, -3.9823],\n",
      "        [-2.3828, -3.7894, -0.5779,  ..., -1.0098, -1.0394, -1.0704]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7049,  1.0301, -1.4363, -0.6895, -0.7549,  0.9479, -0.6427, -1.0279],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7229, -0.6774], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(6.7764e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1463, -0.1477, -0.1492, -0.1507, -0.1523, -0.1538, -0.1553, -0.1569,\n",
      "         -0.1585, -0.1601, -0.1617, -0.1633, -0.1650, -0.1667, -0.1683, -0.1700,\n",
      "         -0.1718, -0.1735, -0.1753, -0.1770, -0.1788, -0.1806, -0.1824, -0.1843,\n",
      "         -0.1861, -0.1880, -0.1899, -0.1918, -0.1938, -0.1957, -0.1977, -0.1997,\n",
      "         -0.2017, -0.2038, -0.2058, -0.2079, -0.2100, -0.2121, -0.2143, -0.2164,\n",
      "         -0.2186, -0.2208, -0.2231, -0.2253, -0.2276, -0.2299, -0.2322, -0.2346,\n",
      "         -0.2369, -0.2393, -0.2417, -0.2442, -0.2466, -0.2491, -0.2517, -0.2542,\n",
      "         -0.2568, -0.2594, -0.2620, -0.2646, -0.2673, -0.2700, -0.2727, -0.2755,\n",
      "         -0.2783, -0.2811, -0.2839, -0.2868, -0.2897, -0.2926, -0.2956, -0.2985,\n",
      "         -0.3016, -0.3046, -0.3077, -0.3108, -0.3139, -0.3171, -0.3203, -0.3235,\n",
      "         -0.3268, -0.3301, -0.3334, -0.3368, -0.3402, -0.3436, -0.3471, -0.3506,\n",
      "         -0.3542, -0.3577, -0.3614, -0.3650, -0.3687, -0.3724, -0.3762, -0.3800,\n",
      "         -0.3838, -0.3877, -0.3916, -0.3956, -0.3996, -0.4036, -0.4077, -0.4118,\n",
      "         -0.4160, -0.4202, -0.4244, -0.4287, -0.4330, -0.4374, -0.4418, -0.4463,\n",
      "         -0.4508, -0.4553, -0.4599, -0.4646, -0.4693, -0.4740, -0.4788, -0.4836,\n",
      "         -0.4885, -0.4934, -0.4984, -0.5035, -0.5086, -0.5137, -0.5189, -0.5241,\n",
      "         -0.5294, -0.5348, -0.5402, -0.5456, -0.5511, -0.5567, -0.5623, -0.5680,\n",
      "         -0.5737, -0.5795, -0.5854, -0.5913, -0.5973, -0.6033, -0.6094, -0.6156,\n",
      "         -0.6218, -0.6281, -0.6344, -0.6408, -0.6473, -0.6538, -0.6604, -0.6671,\n",
      "         -0.6738, -0.6806, -0.6875, -0.6945, -0.7015, -0.7086, -0.7157, -0.7229,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1370, -0.1384, -0.1398, -0.1412, -0.1427, -0.1441, -0.1456, -0.1470,\n",
      "         -0.1485, -0.1500, -0.1515, -0.1531, -0.1546, -0.1562, -0.1577, -0.1593,\n",
      "         -0.1609, -0.1626, -0.1642, -0.1659, -0.1675, -0.1692, -0.1709, -0.1727,\n",
      "         -0.1744, -0.1762, -0.1780, -0.1798, -0.1816, -0.1834, -0.1853, -0.1871,\n",
      "         -0.1890, -0.1909, -0.1929, -0.1948, -0.1968, -0.1988, -0.2008, -0.2028,\n",
      "         -0.2048, -0.2069, -0.2090, -0.2111, -0.2132, -0.2154, -0.2176, -0.2198,\n",
      "         -0.2220, -0.2242, -0.2265, -0.2288, -0.2311, -0.2334, -0.2358, -0.2382,\n",
      "         -0.2406, -0.2430, -0.2455, -0.2479, -0.2504, -0.2530, -0.2555, -0.2581,\n",
      "         -0.2607, -0.2634, -0.2660, -0.2687, -0.2714, -0.2742, -0.2769, -0.2797,\n",
      "         -0.2826, -0.2854, -0.2883, -0.2912, -0.2941, -0.2971, -0.3001, -0.3031,\n",
      "         -0.3062, -0.3093, -0.3124, -0.3156, -0.3188, -0.3220, -0.3252, -0.3285,\n",
      "         -0.3318, -0.3352, -0.3386, -0.3420, -0.3455, -0.3489, -0.3525, -0.3560,\n",
      "         -0.3596, -0.3633, -0.3669, -0.3706, -0.3744, -0.3782, -0.3820, -0.3858,\n",
      "         -0.3897, -0.3937, -0.3976, -0.4017, -0.4057, -0.4098, -0.4140, -0.4181,\n",
      "         -0.4224, -0.4266, -0.4309, -0.4353, -0.4397, -0.4441, -0.4486, -0.4532,\n",
      "         -0.4577, -0.4624, -0.4670, -0.4717, -0.4765, -0.4813, -0.4862, -0.4911,\n",
      "         -0.4961, -0.5011, -0.5061, -0.5112, -0.5164, -0.5216, -0.5269, -0.5322,\n",
      "         -0.5376, -0.5430, -0.5485, -0.5540, -0.5596, -0.5653, -0.5710, -0.5768,\n",
      "         -0.5826, -0.5885, -0.5944, -0.6004, -0.6065, -0.6126, -0.6188, -0.6251,\n",
      "         -0.6314, -0.6377, -0.6442, -0.6507, -0.6573, -0.6639, -0.6706, -0.6774,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7051, -0.6607], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1829, -3.8446, -0.5487, -2.0627,  0.4500, -2.2820, -4.4119,  0.0979,\n",
      "         -3.5693, -2.3310, -3.9037,  0.5116, -0.2007, -3.3978, -3.1307, -1.9984,\n",
      "         -3.1107, -1.6235, -1.9411, -0.7735, -1.1636, -1.2329, -0.6760,  0.4794,\n",
      "         -1.6998, -3.3323, -3.1042, -2.2400, -3.6855,  1.1199, -1.5312,  0.0227,\n",
      "         -0.9765, -3.4250, -1.9076, -1.9725, -0.8766, -3.0511, -1.3978,  0.2965,\n",
      "         -0.4920, -0.4211, -0.1368, -4.4452, -1.3866, -2.9917, -3.3156, -3.1605,\n",
      "         -0.1581, -0.0840, -3.7841, -1.4556, -7.3429, -2.7068, -2.8689, -0.6711,\n",
      "         -2.5712, -2.8252, -2.6049, -1.3767, -3.5560, -1.1319, -5.8761, -2.2217,\n",
      "         -3.4128,  1.2822, -1.0916,  0.1923, -0.3902, -3.0086, -3.9156, -5.7830,\n",
      "         -2.6156, -0.2906, -3.7142, -4.4013, -2.2364, -3.7627, -2.3246, -3.4393,\n",
      "         -5.0279, -2.2731, -0.1436, -1.0974, -1.7659, -2.8284, -1.6103, -0.4375,\n",
      "         -3.6267, -0.6400, -3.0283, -4.2097, -2.2280, -0.4778, -0.6792, -3.1900,\n",
      "         -2.5587, -3.3752, -3.3371, -3.2040,  0.0838, -0.8200, -3.7869, -2.1841,\n",
      "         -0.4401, -1.6489, -3.2310, -3.7600,  0.4910, -1.5154, -1.5755, -0.6257,\n",
      "         -4.1978, -4.6757,  0.6342, -0.7535, -2.0416, -1.1364,  0.0124, -0.7241,\n",
      "         -2.3165, -4.8706, -2.4532, -0.6905, -0.9013, -2.8024, -1.9297, -0.6344,\n",
      "         -0.3719, -4.1411, -0.6504, -0.0566, -3.2448, -4.0972, -1.0432, -2.0502,\n",
      "         -3.0086, -2.3969, -1.4107, -2.6995, -2.4619, -3.1605, -5.8470, -1.0572,\n",
      "         -0.7470, -0.6982, -0.7103, -0.6426, -0.7238, -0.7493, -4.8303, -4.9052,\n",
      "         -5.3263, -4.9387, -4.9000, -4.5961, -4.4651, -4.5578, -4.4820, -4.5733,\n",
      "         -4.7282, -4.8079, -4.8577, -4.8378, -4.7388, -4.6351, -4.6041, -4.6092,\n",
      "         -4.5970, -4.5604, -4.5240, -4.5185, -4.5331, -4.5510, -4.5682, -4.5659,\n",
      "         -4.5247, -4.4895, -4.4948, -4.4967, -4.5329, -4.5653, -4.5734, -4.5616,\n",
      "         -4.5647, -4.5606, -4.5936, -4.6214, -4.6355, -4.6526, -4.6700, -4.7072,\n",
      "         -4.7492, -4.7740, -4.7765, -4.7779, -4.7846, -4.7819, -4.7843, -4.7846,\n",
      "         -4.7794, -4.7983, -4.8073, -4.7973, -4.7957, -4.7905, -4.7607, -4.7383,\n",
      "         -4.6930, -4.6325, -4.6236, -4.5686, -4.5258, -4.4374, -4.4003, -4.2756,\n",
      "         -4.1612, -4.0996, -4.0560, -4.0201, -4.0042, -4.0007],\n",
      "        [-2.1867, -3.8360, -0.5537, -2.0621,  0.4438, -2.2910, -4.4108,  0.0939,\n",
      "         -3.5782, -2.3350, -3.9095,  0.5152, -0.2054, -3.3991, -3.1252, -2.0008,\n",
      "         -3.1134, -1.6293, -1.9411, -0.7815, -1.1613, -1.2408, -0.6842,  0.4822,\n",
      "         -1.7077, -3.3326, -3.1005, -2.2406, -3.6867,  1.1154, -1.5352,  0.0159,\n",
      "         -0.9826, -3.4295, -1.9121, -1.9692, -0.8751, -3.0573, -1.3996,  0.2854,\n",
      "         -0.4975, -0.4228, -0.1423, -4.4462, -1.3930, -2.9969, -3.3212, -3.1660,\n",
      "         -0.1540, -0.0767, -3.7890, -1.4613, -7.3433, -2.7083, -2.8695, -0.6756,\n",
      "         -2.5800, -2.8238, -2.6080, -1.3842, -3.5599, -1.1363, -5.8790, -2.2213,\n",
      "         -3.4146,  1.2792, -1.0966,  0.1832, -0.3970, -3.0171, -3.9168, -5.7836,\n",
      "         -2.6178, -0.2862, -3.7196, -4.4037, -2.2358, -3.7654, -2.3283, -3.4436,\n",
      "         -5.0317, -2.2736, -0.1484, -1.1056, -1.7668, -2.8333, -1.6175, -0.4449,\n",
      "         -3.6312, -0.6467, -3.0342, -4.2121, -2.2267, -0.4812, -0.6903, -3.1912,\n",
      "         -2.5675, -3.3777, -3.3304, -3.2025,  0.0804, -0.8299, -3.7885, -2.1880,\n",
      "         -0.4387, -1.6534, -3.2360, -3.7630,  0.4948, -1.4535, -1.5720, -0.4870,\n",
      "         -4.3304, -4.6761,  0.6868, -0.7557, -2.0572, -1.1472, -0.0125, -0.7443,\n",
      "         -2.4372, -4.7222, -2.4932, -0.7320, -0.9499, -2.8288, -1.9294, -0.6423,\n",
      "         -0.3613, -4.1451, -0.6229, -0.0493, -3.2419, -4.0984, -1.0557, -2.0437,\n",
      "         -2.9829, -2.4319, -1.4320, -2.7495, -2.4996, -3.1637, -5.7836, -1.0695,\n",
      "         -0.7229, -0.7092, -0.6206, -0.6001, -0.6050, -0.7159, -4.8267, -4.9042,\n",
      "         -5.3430, -4.9464, -4.8688, -4.5827, -4.4862, -4.6526, -4.6085, -4.6747,\n",
      "         -4.8157, -4.8795, -4.9164, -4.8907, -4.7973, -4.7035, -4.6841, -4.6941,\n",
      "         -4.6840, -4.6423, -4.6009, -4.5867, -4.6043, -4.6255, -4.6377, -4.6278,\n",
      "         -4.5809, -4.5390, -4.5378, -4.5304, -4.5689, -4.6146, -4.6364, -4.6275,\n",
      "         -4.6247, -4.6070, -4.6274, -4.6506, -4.6613, -4.6732, -4.6922, -4.7419,\n",
      "         -4.8004, -4.8382, -4.8502, -4.8483, -4.8480, -4.8524, -4.8606, -4.8721,\n",
      "         -4.8710, -4.8801, -4.8702, -4.8456, -4.8402, -4.8366, -4.8165, -4.7939,\n",
      "         -4.7524, -4.6904, -4.6762, -4.6142, -4.5748, -4.5064, -4.4695, -4.3607,\n",
      "         -4.2419, -4.1773, -4.1312, -4.1023, -4.0850, -4.0803]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7118, -0.6623], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -0.6643,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(4.4803e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.1705, 0.1723, 0.1740,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1496, 0.1512, 0.1527,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -0.6479,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6736, -3.8464, -0.6531,  ..., -5.4791, -5.4972, -5.5006],\n",
      "        [-2.7570, -3.8440, -0.6546,  ..., -5.7007, -5.6983, -5.6787],\n",
      "        [-2.7558, -3.8508, -0.6628,  ..., -5.7557, -5.7545, -5.7446],\n",
      "        ...,\n",
      "        [-2.7608, -3.8437, -0.6574,  ..., -5.6102, -5.5997, -5.5880],\n",
      "        [-2.6758, -3.8351, -0.6320,  ..., -5.6594, -5.6915, -5.6977],\n",
      "        [-2.6758, -3.8351, -0.6320,  ..., -5.6594, -5.6915, -5.6977]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 1.0028,  0.9972,  1.0000,  0.9832, -0.6346,  0.9877,  0.9976,  0.9976],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.5583,  0.9900, -0.7386, -0.6900, -0.6620,  0.9900, -1.3786,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0961e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(8.5337e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3184, -0.3216, -0.3249,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1494, -0.1509, -0.1525,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2679, -0.2706, -0.2733,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5274,  0.9656, -0.7204, -0.6730, -0.6457,  0.9656, -1.3182,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-1.9680, -3.8584, -0.4751,  ..., -4.1540, -4.1627, -4.1677],\n",
      "        [-2.7473, -3.8397, -0.6545,  ..., -5.7503, -5.7407, -5.7266],\n",
      "        [-2.1724, -3.8368, -0.5495,  ..., -4.0183, -4.0017, -3.9969],\n",
      "        ...,\n",
      "        [-2.6771, -3.8548, -0.6449,  ..., -5.6563, -5.6689, -5.6782],\n",
      "        [-2.0696, -3.8416, -0.4938,  ..., -4.4165, -4.3722, -4.3383],\n",
      "        [-2.7473, -3.8397, -0.6545,  ..., -5.7503, -5.7407, -5.7266]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4433,  0.9832, -0.6895, -0.6321, -0.6301,  0.9839, -1.2794,  0.9832],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000, -1.5639, -0.6757,  0.9900, -0.7128, -0.7278,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9488e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.3196, -0.3228, -0.3261,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1367, -0.1381, -0.1395,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1472, -0.1487, -0.1502,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518, -1.5329, -0.6590,  0.9656, -0.6952, -0.7098,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3696, -3.7825, -0.5814,  ..., -1.0588, -1.0884, -1.0521],\n",
      "        [-1.9621, -3.8589, -0.4829,  ..., -4.1650, -4.1718, -4.1788],\n",
      "        [-2.1789, -3.8326, -0.5609,  ..., -4.1198, -4.1025, -4.0976],\n",
      "        ...,\n",
      "        [-2.1725, -3.8372, -0.5495,  ..., -4.0418, -4.0290, -4.0253],\n",
      "        [-2.9195, -3.7946, -0.6658,  ..., -5.1421, -5.0991, -5.0800],\n",
      "        [-2.9158, -3.8033, -0.6688,  ..., -5.1825, -5.1374, -5.1096]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0650, -1.4568, -0.6640,  0.9214, -0.6331, -0.6878,  0.8996,  0.9170],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.5858, -0.6873, -0.7253, -0.6005, -0.6792,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8718e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1009, -0.1019, -0.1030,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1034, -0.1045, -0.1055,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.5713, -0.6704, -0.7074, -0.5857, -0.6625,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7319, -3.8429, -0.6744,  ..., -5.7230, -5.7188, -5.7078],\n",
      "        [-2.7356, -3.8373, -0.6721,  ..., -5.7368, -5.7383, -5.7204],\n",
      "        [-2.2608, -3.8259, -0.5524,  ..., -4.5640, -4.5279, -4.4969],\n",
      "        ...,\n",
      "        [-2.2588, -3.8264, -0.5622,  ..., -4.6422, -4.6151, -4.5890],\n",
      "        [-2.1775, -3.8313, -0.5693,  ..., -4.1290, -4.1123, -4.1098],\n",
      "        [-2.7536, -3.8402, -0.6732,  ..., -5.6425, -5.6325, -5.6223]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9849,  0.9103, -0.5718, -0.7012, -0.7238, -0.5331, -0.6743,  0.9502],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.5892,  0.9900, -0.7496,  0.9900,  0.9900, -0.7354,  0.9900, -0.7515],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5093e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(5.0835e-10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1015, -0.1025, -0.1035,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1516, -0.1532, -0.1547,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1488, -0.1503, -0.1518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1520, -0.1536, -0.1551,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5746,  0.9656, -0.7311,  0.9656,  0.9656, -0.7173,  0.9608, -0.7330],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2588, -3.8265, -0.5589,  ..., -4.5728, -4.5361, -4.5039],\n",
      "        [-2.7407, -3.8385, -0.6705,  ..., -5.7632, -5.7620, -5.7552],\n",
      "        [-2.1479, -3.8363, -0.5626,  ..., -4.0473, -4.0292, -4.0260],\n",
      "        ...,\n",
      "        [-2.1625, -3.8343, -0.5586,  ..., -4.1463, -4.1291, -4.1233],\n",
      "        [-2.9064, -3.7950, -0.6821,  ..., -5.2211, -5.1456, -5.0927],\n",
      "        [-2.1668, -3.8353, -0.5710,  ..., -4.0552, -4.0378, -4.0335]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5768,  0.9444, -0.7142,  0.8860,  0.9331, -0.6863,  0.9426, -0.7258],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7270,  0.9900, -0.7270, -1.1939,  0.9900,  0.9900, -0.7229, -0.7392],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.7161e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1471, -0.1485, -0.1500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1471, -0.1485, -0.1500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1462, -0.1477, -0.1492,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1495, -0.1510, -0.1526,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7090,  0.9656, -0.7090, -1.1416,  0.9656,  0.9656, -0.7051, -0.7209],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1648, -3.8392, -0.5695,  ..., -4.1105, -4.0966, -4.0941],\n",
      "        [-2.6790, -3.8570, -0.6783,  ..., -5.7018, -5.7136, -5.7185],\n",
      "        [-2.1648, -3.8392, -0.5695,  ..., -4.1105, -4.0966, -4.0941],\n",
      "        ...,\n",
      "        [-2.9203, -3.8053, -0.6944,  ..., -5.2057, -5.1596, -5.1355],\n",
      "        [-2.1635, -3.8443, -0.5778,  ..., -4.0928, -4.0775, -4.0723],\n",
      "        [-2.1720, -3.8404, -0.5711,  ..., -4.0682, -4.0562, -4.0519]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7883,  0.9308, -0.7883, -1.1711,  0.8696,  0.8678, -0.6703, -0.7288],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([0.9900, 0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(4.6353e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[0.1496, 0.1512, 0.1527, 0.1542, 0.1558, 0.1574, 0.1589, 0.1605, 0.1622,\n",
      "         0.1638, 0.1655, 0.1671, 0.1688, 0.1705, 0.1723, 0.1740, 0.1757, 0.1775,\n",
      "         0.1793, 0.1811, 0.1830, 0.1848, 0.1867, 0.1886, 0.1905, 0.1924, 0.1943,\n",
      "         0.1963, 0.1983, 0.2003, 0.2023, 0.2043, 0.2064, 0.2085, 0.2106, 0.2127,\n",
      "         0.2149, 0.2170, 0.2192, 0.2215, 0.2237, 0.2259, 0.2282, 0.2305, 0.2329,\n",
      "         0.2352, 0.2376, 0.2400, 0.2424, 0.2449, 0.2473, 0.2498, 0.2524, 0.2549,\n",
      "         0.2575, 0.2601, 0.2627, 0.2654, 0.2680, 0.2708, 0.2735, 0.2763, 0.2790,\n",
      "         0.2819, 0.2847, 0.2876, 0.2905, 0.2934, 0.2964, 0.2994, 0.3024, 0.3055,\n",
      "         0.3085, 0.3117, 0.3148, 0.3180, 0.3212, 0.3244, 0.3277, 0.3310, 0.3344,\n",
      "         0.3378, 0.3412, 0.3446, 0.3481, 0.3516, 0.3552, 0.3587, 0.3624, 0.3660,\n",
      "         0.3697, 0.3735, 0.3772, 0.3810, 0.3849, 0.3888, 0.3927, 0.3967, 0.4007,\n",
      "         0.4047, 0.4088, 0.4130, 0.4171, 0.4213, 0.4256, 0.4299, 0.4342, 0.4386,\n",
      "         0.4430, 0.4475, 0.4520, 0.4566, 0.4612, 0.4659, 0.4706, 0.4753, 0.4801,\n",
      "         0.4850, 0.4899, 0.4948, 0.4998, 0.5049, 0.5100, 0.5151, 0.5203, 0.5256,\n",
      "         0.5309, 0.5363, 0.5417, 0.5472, 0.5527, 0.5583, 0.5639, 0.5696, 0.5754,\n",
      "         0.5812, 0.5870, 0.5930, 0.5990, 0.6050, 0.6111, 0.6173, 0.6235, 0.6298,\n",
      "         0.6362, 0.6426, 0.6491, 0.6557, 0.6623, 0.6690, 0.6757, 0.6826, 0.6894,\n",
      "         0.6964, 0.7034, 0.7106, 0.7177, 0.7250, 0.7323, 0.7397, 0.7472, 0.7547,\n",
      "         0.7623, 0.7700, 0.7778, 0.7857, 0.7936, 0.8016, 0.8097, 0.8179, 0.8262,\n",
      "         0.8345, 0.8429, 0.8515, 0.8601, 0.8687, 0.8775, 0.8864, 0.8953, 0.9044,\n",
      "         0.9135, 0.9227, 0.9321, 0.9415, 0.9510, 0.9606, 0.9703, 0.9801, 0.9900,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1705, 0.1723, 0.1740, 0.1757, 0.1775, 0.1793, 0.1811, 0.1830, 0.1848,\n",
      "         0.1867, 0.1886, 0.1905, 0.1924, 0.1943, 0.1963, 0.1983, 0.2003, 0.2023,\n",
      "         0.2043, 0.2064, 0.2085, 0.2106, 0.2127, 0.2149, 0.2170, 0.2192, 0.2215,\n",
      "         0.2237, 0.2259, 0.2282, 0.2305, 0.2329, 0.2352, 0.2376, 0.2400, 0.2424,\n",
      "         0.2449, 0.2473, 0.2498, 0.2524, 0.2549, 0.2575, 0.2601, 0.2627, 0.2654,\n",
      "         0.2680, 0.2708, 0.2735, 0.2763, 0.2790, 0.2819, 0.2847, 0.2876, 0.2905,\n",
      "         0.2934, 0.2964, 0.2994, 0.3024, 0.3055, 0.3085, 0.3117, 0.3148, 0.3180,\n",
      "         0.3212, 0.3244, 0.3277, 0.3310, 0.3344, 0.3378, 0.3412, 0.3446, 0.3481,\n",
      "         0.3516, 0.3552, 0.3587, 0.3624, 0.3660, 0.3697, 0.3735, 0.3772, 0.3810,\n",
      "         0.3849, 0.3888, 0.3927, 0.3967, 0.4007, 0.4047, 0.4088, 0.4130, 0.4171,\n",
      "         0.4213, 0.4256, 0.4299, 0.4342, 0.4386, 0.4430, 0.4475, 0.4520, 0.4566,\n",
      "         0.4612, 0.4659, 0.4706, 0.4753, 0.4801, 0.4850, 0.4899, 0.4948, 0.4998,\n",
      "         0.5049, 0.5100, 0.5151, 0.5203, 0.5256, 0.5309, 0.5363, 0.5417, 0.5472,\n",
      "         0.5527, 0.5583, 0.5639, 0.5696, 0.5754, 0.5812, 0.5870, 0.5930, 0.5990,\n",
      "         0.6050, 0.6111, 0.6173, 0.6235, 0.6298, 0.6362, 0.6426, 0.6491, 0.6557,\n",
      "         0.6623, 0.6690, 0.6757, 0.6826, 0.6894, 0.6964, 0.7034, 0.7106, 0.7177,\n",
      "         0.7250, 0.7323, 0.7397, 0.7472, 0.7547, 0.7623, 0.7700, 0.7778, 0.7857,\n",
      "         0.7936, 0.8016, 0.8097, 0.8179, 0.8262, 0.8345, 0.8429, 0.8515, 0.8601,\n",
      "         0.8687, 0.8775, 0.8864, 0.8953, 0.9044, 0.9135, 0.9227, 0.9321, 0.9415,\n",
      "         0.9510, 0.9606, 0.9703, 0.9801, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([0.9656, 0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6752, -3.8502, -0.6926, -2.3634,  0.3134, -2.4105, -4.6379, -0.3584,\n",
      "         -4.0842, -2.5760, -4.0598,  0.5043, -0.3076, -3.5858, -3.0444, -2.3190,\n",
      "         -3.2681, -1.7484, -1.9497, -1.0093, -1.3244, -1.5064, -0.8990,  0.5338,\n",
      "         -2.0008, -3.5410, -3.0370, -2.5132, -3.7067,  1.0979, -1.7008, -0.2481,\n",
      "         -0.9537, -3.5695, -1.9576, -1.8875, -0.8141, -3.1595, -1.6168,  0.0487,\n",
      "         -0.5563, -0.4045, -0.2022, -4.4064, -1.6393, -3.4203, -3.5592, -3.2957,\n",
      "          0.4432,  0.4214, -3.9102, -1.5769, -7.3475, -2.9028, -2.9358, -0.8296,\n",
      "         -2.5655, -2.8733, -2.7756, -1.6409, -3.7363, -1.2363, -5.9162, -2.3856,\n",
      "         -3.4344,  1.2758, -1.2624, -0.1061, -0.3272, -3.1688, -4.0890, -5.7523,\n",
      "         -2.8171, -0.2245, -3.8811, -4.3408, -2.4389, -3.8300, -2.4997, -3.6069,\n",
      "         -5.0079, -2.4642, -0.2189, -1.3383, -1.8832, -3.0743, -1.7789, -0.6913,\n",
      "         -3.7795, -0.7535, -3.2222, -4.1710, -2.4188, -0.5296, -0.9339, -3.1650,\n",
      "         -2.7922, -3.7415, -3.3760, -3.3801, -0.0194, -1.0617, -3.7724, -2.2006,\n",
      "         -0.3990, -1.9234, -3.5701, -3.9190,  0.5631, -1.8319, -1.5027, -1.1037,\n",
      "         -4.5081, -4.7981,  0.6298, -0.9782, -2.1725, -1.3286, -0.1628, -0.9178,\n",
      "         -2.7202, -4.7362, -2.5658, -0.7209, -1.0637, -2.9686, -2.1202, -0.7950,\n",
      "         -0.5355, -4.2480, -0.0453,  0.4282, -3.3009, -4.2138, -1.1918, -2.0477,\n",
      "         -3.0454, -2.5714, -1.6239, -2.8207, -2.9086, -5.8599, -1.6571, -2.6046,\n",
      "         -1.7190, -0.9396, -0.8185, -4.2884, -3.8002, -0.3712, -2.0822, -1.1174,\n",
      "         -1.2556, -0.6560, -0.2506, -0.9326, -4.1981, -5.1840, -1.4364, -2.4655,\n",
      "         -0.1558,  0.3029,  0.3152, -4.2090, -3.8156, -0.3050, -1.1703, -4.5530,\n",
      "         -6.4239, -4.8303, -3.0424, -4.8778, -1.2648,  0.9652,  0.8657,  0.9356,\n",
      "          0.8881,  0.9182,  0.9792, -4.6257, -4.9130, -5.3199, -5.1255, -5.2791,\n",
      "         -5.2277, -5.1028, -5.1559, -5.1814, -5.1246, -5.0260, -4.9323, -4.9032,\n",
      "         -4.9291, -4.9902, -5.0407, -5.0922, -5.1375, -5.1728, -5.1941, -5.2245,\n",
      "         -5.2805, -5.3620, -5.4459, -5.5113, -5.5358, -5.5353, -5.5424, -5.5837,\n",
      "         -5.6069, -5.5980, -5.5814, -5.5507, -5.5114, -5.4849, -5.4693, -5.4665,\n",
      "         -5.4851, -5.5060, -5.5276, -5.5360, -5.5534, -5.5557],\n",
      "        [-2.7465, -3.8457, -0.7004, -2.4235,  0.2827, -2.4133, -4.6695, -0.4638,\n",
      "         -4.1741, -2.6421, -4.0845,  0.5112, -0.3210, -3.6347, -3.0141, -2.3638,\n",
      "         -3.2570, -1.7797, -1.9237, -1.0473, -1.3576, -1.5488, -0.9243,  0.5733,\n",
      "         -2.0329, -3.5952, -3.0133, -2.5423, -3.6996,  1.1098, -1.7309, -0.2999,\n",
      "         -0.9427, -3.5842, -1.9632, -1.8594, -0.8065, -3.1304, -1.6531,  0.0119,\n",
      "         -0.5656, -0.3931, -0.2032, -4.3900, -1.6656, -3.4866, -3.6095, -3.3155,\n",
      "          0.5940,  0.5357, -3.9430, -1.5896, -7.3444, -2.9201, -2.9087, -0.8623,\n",
      "         -2.5823, -2.8756, -2.7893, -1.6643, -3.7804, -1.2531, -5.9161, -2.3987,\n",
      "         -3.4271,  1.2947, -1.2898, -0.1609, -0.3071, -3.1860, -4.1283, -5.7399,\n",
      "         -2.8336, -0.2020, -3.9187, -4.3284, -2.4568, -3.8003, -2.5275, -3.6461,\n",
      "         -4.9923, -2.4814, -0.2277, -1.3885, -1.8834, -3.0973, -1.8132, -0.7442,\n",
      "         -3.7933, -0.7625, -3.2694, -4.1578, -2.4354, -0.5408, -0.9877, -3.1474,\n",
      "         -2.8188, -3.7978, -3.3907, -3.3913, -0.0283, -1.1097, -3.7438, -2.1995,\n",
      "         -0.3646, -1.9687, -3.6228, -3.9414,  0.5881, -0.7798, -1.3190, -0.9475,\n",
      "         -4.5367, -4.7140,  0.6054, -0.9903, -2.2113, -1.4184, -0.2886, -0.9503,\n",
      "         -2.8364, -4.6650, -2.5524, -0.7328, -1.2268, -3.0798, -2.1353, -0.8006,\n",
      "         -0.6278, -4.2848,  0.2046,  0.5599, -3.3395, -4.2056, -1.2047, -2.0589,\n",
      "         -3.0115, -2.6270, -1.6237, -2.8531, -2.9481, -5.8788, -1.6927, -2.5899,\n",
      "         -1.5121, -0.8768, -1.0917, -4.1279, -3.7388, -0.2740, -1.9916, -0.1167,\n",
      "         -1.0009, -0.7803, -0.0384, -0.8966, -4.1249, -3.5637, -4.8748, -1.3487,\n",
      "          0.9762,  0.9252,  0.8990,  0.9495,  0.5244,  0.9846, -4.6755, -4.8655,\n",
      "         -5.2654, -5.1353, -5.3543, -5.3976, -5.4018, -5.4230, -5.4023, -5.3506,\n",
      "         -5.2400, -5.1301, -5.0844, -5.0725, -5.0885, -5.0965, -5.1143, -5.1485,\n",
      "         -5.1826, -5.2067, -5.2415, -5.2944, -5.3653, -5.4438, -5.4871, -5.5022,\n",
      "         -5.4990, -5.5093, -5.5467, -5.6028, -5.6304, -5.6268, -5.6081, -5.5698,\n",
      "         -5.5467, -5.5290, -5.5215, -5.5336, -5.5517, -5.5881, -5.6206, -5.6475,\n",
      "         -5.6625, -5.6471, -5.6415, -5.6450, -5.6645, -5.6967, -5.7241, -5.7234,\n",
      "         -5.7167, -5.7132, -5.7096, -5.7153, -5.7147, -5.7006]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([0.9253, 0.8765], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7381, -0.7529, -1.5734, -0.7305, -1.5734,  0.9900, -0.6922,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.5413e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(6.3296e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1493, -0.1508, -0.1523,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1523, -0.1538, -0.1554,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3215, -0.3248, -0.3280,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1400, -0.1415, -0.1429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7199, -0.7343, -1.5422, -0.7125, -1.5422,  0.9656, -0.6752,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1824, -3.8430, -0.5846,  ..., -4.0752, -4.0626, -4.0622],\n",
      "        [-2.1798, -3.8427, -0.5922,  ..., -4.0686, -4.0547, -4.0480],\n",
      "        [-1.9798, -3.8639, -0.5187,  ..., -4.1959, -4.2022, -4.2076],\n",
      "        ...,\n",
      "        [-2.6845, -3.8392, -0.6785,  ..., -5.7137, -5.7447, -5.7511],\n",
      "        [-2.1866, -3.8474, -0.5896,  ..., -4.1333, -4.1188, -4.1168],\n",
      "        [-2.7610, -3.8457, -0.6917,  ..., -5.7752, -5.7712, -5.7642]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7521, -0.7653, -1.5007, -0.7712, -1.5007,  0.9195, -0.7449,  0.9201],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7326,  0.9900,  0.9900, -0.6735,  0.9900,  0.9900, -0.7182],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8671e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1482, -0.1497, -0.1512,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1453, -0.1468, -0.1482,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7145,  0.9656,  0.9656, -0.6568,  0.9656,  0.9656, -0.7004],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6987, -3.8585, -0.6944,  ..., -5.6998, -5.7106, -5.7169],\n",
      "        [-2.1769, -3.8401, -0.5812,  ..., -4.1676, -4.1507, -4.1451],\n",
      "        [-2.7779, -3.8458, -0.7039,  ..., -5.6597, -5.6511, -5.6367],\n",
      "        ...,\n",
      "        [-2.9447, -3.8007, -0.7071,  ..., -5.1716, -5.1285, -5.1079],\n",
      "        [-2.7779, -3.8458, -0.7039,  ..., -5.6597, -5.6511, -5.6367],\n",
      "        [-2.1772, -3.8452, -0.5917,  ..., -4.1071, -4.0911, -4.0860]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9235, -0.7352,  0.9143,  0.9221, -0.7077,  0.7838,  0.9143, -0.7013],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6721, -0.7173,  0.9900, -1.1000,  0.9900, -0.5830],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4885e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1360, -0.1373, -0.1387,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1004, -0.1014, -0.1025,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656, -0.6555, -0.6996,  0.9656, -1.0518,  0.9656, -0.5686],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9365, -3.7993, -0.7074,  ..., -5.2378, -5.1612, -5.1082],\n",
      "        [-2.7802, -3.8447, -0.7031,  ..., -5.7479, -5.7440, -5.7223],\n",
      "        [-2.1939, -3.8395, -0.5949,  ..., -4.1345, -4.1174, -4.1152],\n",
      "        ...,\n",
      "        [-2.3884, -3.7850, -0.6168,  ..., -1.0975, -1.1266, -1.0772],\n",
      "        [-2.7665, -3.8487, -0.7005,  ..., -5.8060, -5.8059, -5.7906],\n",
      "        [-2.2829, -3.8297, -0.5821,  ..., -4.5861, -4.5510, -4.5217]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9150,  0.9217, -0.7101, -0.7042,  0.9365, -1.0967,  0.9198, -0.6019],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7449, -1.1883,  0.9900,  0.9900,  0.9900,  0.9900, -0.7190,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.8923e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1507, -0.1522, -0.1537,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1383, -0.1397, -0.1411,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1454, -0.1469, -0.1484,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7265, -1.1363,  0.9656,  0.9656,  0.9656,  0.9656, -0.7012,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1734, -3.8370, -0.5820,  ..., -4.0635, -4.0495, -4.0445],\n",
      "        [-2.3642, -3.7921, -0.5924,  ..., -4.1057, -4.1018, -4.0836],\n",
      "        [-2.7694, -3.8422, -0.7034,  ..., -5.7448, -5.7389, -5.7262],\n",
      "        ...,\n",
      "        [-2.9489, -3.7989, -0.7056,  ..., -5.1800, -5.1330, -5.1089],\n",
      "        [-2.1866, -3.8370, -0.5796,  ..., -4.1227, -4.1084, -4.1055],\n",
      "        [-2.7700, -3.8465, -0.6992,  ..., -5.8088, -5.8080, -5.7925]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7584, -1.1933,  0.9555,  0.8316,  0.9284,  0.8537, -0.8144,  0.9278],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7346,  0.9900,  0.9900, -0.6803, -0.6010, -1.3877,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3353e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1486, -0.1501, -0.1516,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1035, -0.1046, -0.1056,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2697, -0.2724, -0.2751,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7165,  0.9656,  0.9656, -0.6635, -0.5862, -1.3269,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7746, -3.8338, -0.6981,  ..., -5.7614, -5.7603, -5.7436],\n",
      "        [-2.2014, -3.8347, -0.5806,  ..., -4.0795, -4.0681, -4.0649],\n",
      "        [-2.7012, -3.8380, -0.6943,  ..., -5.5414, -5.5597, -5.5612],\n",
      "        ...,\n",
      "        [-2.2879, -3.8212, -0.5894,  ..., -4.6700, -4.6420, -4.6176],\n",
      "        [-2.0903, -3.8372, -0.5341,  ..., -4.4504, -4.4037, -4.3729],\n",
      "        [-2.7814, -3.8331, -0.6986,  ..., -5.7982, -5.7905, -5.7749]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9109, -0.7565,  0.9452,  0.8388, -0.7285, -0.5709, -1.3468,  0.9075],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6856,  0.9900, -0.7067, -0.7557,  0.9900, -0.5921],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3914e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(5.2497e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1387, -0.1401, -0.1415,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1529, -0.1544, -0.1560,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1020, -0.1030, -0.1041,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.6687,  0.9656, -0.6892, -0.7371,  0.9656, -0.5775],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9571, -3.7890, -0.6949,  ..., -5.1797, -5.1329, -5.1084],\n",
      "        [-2.7852, -3.8279, -0.6918,  ..., -5.8040, -5.7945, -5.7788],\n",
      "        [-2.2082, -3.8231, -0.5866,  ..., -4.1567, -4.1405, -4.1376],\n",
      "        ...,\n",
      "        [-2.1984, -3.8291, -0.5846,  ..., -4.0678, -4.0529, -4.0485],\n",
      "        [-2.7803, -3.8289, -0.6951,  ..., -5.7289, -5.7274, -5.7135],\n",
      "        [-2.2911, -3.8169, -0.5721,  ..., -4.5968, -4.5606, -4.5306]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8762,  0.9204, -0.7212,  0.9626, -0.6884, -0.7622,  0.9357, -0.5915],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7341,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4427e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1485, -0.1500, -0.1515, -0.1530, -0.1546, -0.1562, -0.1577, -0.1593,\n",
      "         -0.1609, -0.1626, -0.1642, -0.1659, -0.1675, -0.1692, -0.1709, -0.1727,\n",
      "         -0.1744, -0.1762, -0.1779, -0.1797, -0.1816, -0.1834, -0.1852, -0.1871,\n",
      "         -0.1890, -0.1909, -0.1928, -0.1948, -0.1968, -0.1988, -0.2008, -0.2028,\n",
      "         -0.2048, -0.2069, -0.2090, -0.2111, -0.2132, -0.2154, -0.2176, -0.2198,\n",
      "         -0.2220, -0.2242, -0.2265, -0.2288, -0.2311, -0.2334, -0.2358, -0.2382,\n",
      "         -0.2406, -0.2430, -0.2455, -0.2479, -0.2504, -0.2530, -0.2555, -0.2581,\n",
      "         -0.2607, -0.2633, -0.2660, -0.2687, -0.2714, -0.2741, -0.2769, -0.2797,\n",
      "         -0.2825, -0.2854, -0.2883, -0.2912, -0.2941, -0.2971, -0.3001, -0.3031,\n",
      "         -0.3062, -0.3093, -0.3124, -0.3156, -0.3188, -0.3220, -0.3252, -0.3285,\n",
      "         -0.3318, -0.3352, -0.3386, -0.3420, -0.3454, -0.3489, -0.3525, -0.3560,\n",
      "         -0.3596, -0.3632, -0.3669, -0.3706, -0.3744, -0.3781, -0.3820, -0.3858,\n",
      "         -0.3897, -0.3937, -0.3976, -0.4016, -0.4057, -0.4098, -0.4139, -0.4181,\n",
      "         -0.4223, -0.4266, -0.4309, -0.4353, -0.4397, -0.4441, -0.4486, -0.4531,\n",
      "         -0.4577, -0.4623, -0.4670, -0.4717, -0.4765, -0.4813, -0.4862, -0.4911,\n",
      "         -0.4960, -0.5010, -0.5061, -0.5112, -0.5164, -0.5216, -0.5269, -0.5322,\n",
      "         -0.5376, -0.5430, -0.5485, -0.5540, -0.5596, -0.5653, -0.5710, -0.5767,\n",
      "         -0.5826, -0.5884, -0.5944, -0.6004, -0.6065, -0.6126, -0.6188, -0.6250,\n",
      "         -0.6313, -0.6377, -0.6442, -0.6507, -0.6572, -0.6639, -0.6706, -0.6774,\n",
      "         -0.6842, -0.6911, -0.6981, -0.7051, -0.7123, -0.7195, -0.7267, -0.7341,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7160,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1938e+00, -3.8228e+00, -5.6701e-01, -2.0371e+00,  4.4621e-01,\n",
      "         -2.3011e+00, -4.4040e+00,  7.6620e-02, -3.5615e+00, -2.3456e+00,\n",
      "         -3.9153e+00,  4.8989e-01, -2.4172e-01, -3.4176e+00, -3.1506e+00,\n",
      "         -2.0071e+00, -3.1439e+00, -1.6233e+00, -1.9849e+00, -7.8241e-01,\n",
      "         -1.2186e+00, -1.2560e+00, -7.0167e-01,  4.6042e-01, -1.7554e+00,\n",
      "         -3.3672e+00, -3.1118e+00, -2.2519e+00, -3.7000e+00,  1.0712e+00,\n",
      "         -1.5403e+00,  1.4135e-02, -9.8725e-01, -3.4389e+00, -1.9192e+00,\n",
      "         -1.9865e+00, -8.9420e-01, -3.0921e+00, -1.4160e+00,  2.7999e-01,\n",
      "         -5.1657e-01, -4.4062e-01, -1.3380e-01, -4.4722e+00, -1.4407e+00,\n",
      "         -2.9916e+00, -3.3224e+00, -3.1761e+00, -2.1073e-01, -1.0837e-01,\n",
      "         -3.8076e+00, -1.4692e+00, -7.3462e+00, -2.7175e+00, -2.9019e+00,\n",
      "         -6.7635e-01, -2.5651e+00, -2.8037e+00, -2.6564e+00, -1.4469e+00,\n",
      "         -3.5919e+00, -1.1433e+00, -5.8945e+00, -2.2419e+00, -3.4312e+00,\n",
      "          1.2260e+00, -1.1150e+00,  1.6352e-01, -3.9668e-01, -3.0309e+00,\n",
      "         -3.9095e+00, -5.8048e+00, -2.6357e+00, -2.3797e-01, -3.7528e+00,\n",
      "         -4.3972e+00, -2.2604e+00, -3.7997e+00, -2.3411e+00, -3.4833e+00,\n",
      "         -5.0575e+00, -2.3022e+00, -1.6527e-01, -1.1160e+00, -1.8014e+00,\n",
      "         -2.8569e+00, -1.6211e+00, -4.5746e-01, -3.6409e+00, -6.8333e-01,\n",
      "         -3.0767e+00, -4.2152e+00, -2.2597e+00, -4.9148e-01, -6.9905e-01,\n",
      "         -3.2490e+00, -2.6189e+00, -3.3731e+00, -3.2996e+00, -3.2376e+00,\n",
      "          5.9236e-02, -8.4190e-01, -3.7568e+00, -2.2229e+00, -4.5682e-01,\n",
      "         -1.6695e+00, -3.2352e+00, -3.7632e+00,  5.3296e-01, -1.4197e+00,\n",
      "         -1.6662e+00, -1.1379e+00, -4.3084e+00, -4.6320e+00,  5.7218e-01,\n",
      "         -8.1350e-01, -2.1162e+00, -1.2002e+00, -5.0168e-02, -8.7117e-01,\n",
      "         -2.4187e+00, -4.7172e+00, -2.4899e+00, -7.4396e-01, -1.0222e+00,\n",
      "         -2.9335e+00, -1.9837e+00, -6.6519e-01, -3.4417e-01, -4.1832e+00,\n",
      "         -7.5950e-01, -9.8480e-02, -3.3072e+00, -4.1019e+00, -1.0515e+00,\n",
      "         -2.0376e+00, -2.9842e+00, -2.4756e+00, -1.4875e+00, -2.7525e+00,\n",
      "         -2.5065e+00, -3.1313e+00, -5.6323e+00, -1.0324e+00, -8.5501e-01,\n",
      "         -7.3427e-01, -7.5526e-01, -7.3524e-01, -9.6357e-01, -6.8395e-01,\n",
      "         -4.8987e+00, -4.9538e+00, -5.4025e+00, -4.9878e+00, -4.8864e+00,\n",
      "         -4.6114e+00, -4.4690e+00, -4.5630e+00, -4.5293e+00, -4.6439e+00,\n",
      "         -4.8019e+00, -4.8920e+00, -4.9709e+00, -4.9892e+00, -4.9340e+00,\n",
      "         -4.8438e+00, -4.8000e+00, -4.7846e+00, -4.7692e+00, -4.7326e+00,\n",
      "         -4.7020e+00, -4.6991e+00, -4.7212e+00, -4.7507e+00, -4.7716e+00,\n",
      "         -4.7699e+00, -4.7281e+00, -4.6807e+00, -4.6680e+00, -4.6479e+00,\n",
      "         -4.6864e+00, -4.7272e+00, -4.7342e+00, -4.7227e+00, -4.7193e+00,\n",
      "         -4.7057e+00, -4.7245e+00, -4.7434e+00, -4.7507e+00, -4.7552e+00,\n",
      "         -4.7656e+00, -4.7950e+00, -4.8358e+00, -4.8561e+00, -4.8540e+00,\n",
      "         -4.8448e+00, -4.8476e+00, -4.8449e+00, -4.8476e+00, -4.8532e+00,\n",
      "         -4.8505e+00, -4.8611e+00, -4.8501e+00, -4.8378e+00, -4.8409e+00,\n",
      "         -4.8477e+00, -4.8378e+00, -4.8206e+00, -4.7751e+00, -4.7147e+00,\n",
      "         -4.7013e+00, -4.6419e+00, -4.6202e+00, -4.5482e+00, -4.5097e+00,\n",
      "         -4.3851e+00, -4.2695e+00, -4.1957e+00, -4.1507e+00, -4.1199e+00,\n",
      "         -4.1044e+00, -4.1034e+00],\n",
      "        [-2.7822e+00, -3.8233e+00, -6.8740e-01, -2.4109e+00,  2.9705e-01,\n",
      "         -2.4118e+00, -4.6732e+00, -4.5291e-01, -4.2024e+00, -2.6615e+00,\n",
      "         -4.0802e+00,  4.6067e-01, -3.7940e-01, -3.6188e+00, -3.0116e+00,\n",
      "         -2.3387e+00, -3.2638e+00, -1.7753e+00, -1.9134e+00, -1.0655e+00,\n",
      "         -1.3779e+00, -1.5637e+00, -9.3828e-01,  5.8650e-01, -2.0564e+00,\n",
      "         -3.5808e+00, -3.0038e+00, -2.5120e+00, -3.6836e+00,  1.1137e+00,\n",
      "         -1.7304e+00, -3.1311e-01, -9.3816e-01, -3.5694e+00, -1.9738e+00,\n",
      "         -1.7875e+00, -7.9268e-01, -3.1325e+00, -1.6380e+00,  4.9558e-03,\n",
      "         -5.6957e-01, -3.7675e-01, -1.7567e-01, -4.3800e+00, -1.6800e+00,\n",
      "         -3.5152e+00, -3.6345e+00, -3.3001e+00,  5.8645e-01,  6.3326e-01,\n",
      "         -3.9274e+00, -1.5937e+00, -7.3549e+00, -2.8882e+00, -2.9028e+00,\n",
      "         -8.4884e-01, -2.5523e+00, -2.8503e+00, -2.8118e+00, -1.6838e+00,\n",
      "         -3.7662e+00, -1.2561e+00, -5.9203e+00, -2.3632e+00, -3.4069e+00,\n",
      "          1.3001e+00, -1.2862e+00, -1.7509e-01, -2.9282e-01, -3.1652e+00,\n",
      "         -4.1191e+00, -5.7677e+00, -2.8007e+00, -1.1274e-01, -3.8990e+00,\n",
      "         -4.3184e+00, -2.4228e+00, -3.7988e+00, -2.5153e+00, -3.6261e+00,\n",
      "         -5.0142e+00, -2.4470e+00, -2.1383e-01, -1.4041e+00, -1.8829e+00,\n",
      "         -3.1064e+00, -1.8084e+00, -7.6055e-01, -3.8055e+00, -7.7192e-01,\n",
      "         -3.2478e+00, -4.1484e+00, -2.3989e+00, -5.2504e-01, -1.0036e+00,\n",
      "         -3.1573e+00, -2.8397e+00, -3.8193e+00, -3.3777e+00, -3.3889e+00,\n",
      "         -2.0949e-02, -1.1247e+00, -3.7287e+00, -2.2122e+00, -3.5544e-01,\n",
      "         -1.9572e+00, -3.6445e+00, -3.9563e+00,  6.8033e-01, -7.6844e-01,\n",
      "         -1.2497e+00, -9.2095e-01, -4.5608e+00, -4.6987e+00,  6.2136e-01,\n",
      "         -1.0010e+00, -2.2302e+00, -1.4274e+00, -2.7734e-01, -9.3710e-01,\n",
      "         -2.8516e+00, -4.6388e+00, -2.5304e+00, -7.1603e-01, -1.2383e+00,\n",
      "         -3.1038e+00, -2.1466e+00, -7.9644e-01, -6.3908e-01, -4.2697e+00,\n",
      "          1.9647e-01,  6.6099e-01, -3.3439e+00, -4.1979e+00, -1.1999e+00,\n",
      "         -2.0281e+00, -2.9816e+00, -2.6480e+00, -1.6307e+00, -2.8475e+00,\n",
      "         -2.9602e+00, -5.8735e+00, -1.6766e+00, -2.5841e+00, -1.5003e+00,\n",
      "         -7.9786e-01, -1.0303e+00, -4.1495e+00, -3.7396e+00, -2.8299e-01,\n",
      "         -1.9863e+00, -9.5862e-02, -9.2600e-01, -7.3705e-01,  4.2631e-02,\n",
      "         -7.4358e-01, -4.1459e+00, -3.5644e+00, -4.8605e+00, -1.3331e+00,\n",
      "          9.5430e-01,  1.0403e+00,  9.2429e-01,  1.0568e+00,  6.9862e-01,\n",
      "          1.0226e+00, -4.6756e+00, -4.8660e+00, -5.2643e+00, -5.1322e+00,\n",
      "         -5.3625e+00, -5.4044e+00, -5.4098e+00, -5.4350e+00, -5.4111e+00,\n",
      "         -5.3550e+00, -5.2484e+00, -5.1356e+00, -5.0875e+00, -5.0802e+00,\n",
      "         -5.0942e+00, -5.1021e+00, -5.1152e+00, -5.1433e+00, -5.1768e+00,\n",
      "         -5.2007e+00, -5.2364e+00, -5.2854e+00, -5.3565e+00, -5.4297e+00,\n",
      "         -5.4753e+00, -5.4934e+00, -5.4934e+00, -5.5065e+00, -5.5434e+00,\n",
      "         -5.5994e+00, -5.6263e+00, -5.6230e+00, -5.6057e+00, -5.5690e+00,\n",
      "         -5.5467e+00, -5.5319e+00, -5.5273e+00, -5.5412e+00, -5.5611e+00,\n",
      "         -5.5979e+00, -5.6301e+00, -5.6578e+00, -5.6710e+00, -5.6547e+00,\n",
      "         -5.6504e+00, -5.6562e+00, -5.6758e+00, -5.7130e+00, -5.7403e+00,\n",
      "         -5.7391e+00, -5.7350e+00, -5.7289e+00, -5.7242e+00, -5.7319e+00,\n",
      "         -5.7281e+00, -5.7140e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7879,  0.9495], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.5831,  0.9900,  0.9900, -0.7351, -0.7351, -0.7330, -0.7470,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9909e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3235, -0.3268, -0.3301,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1483, -0.1498, -0.1513,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1511, -0.1526, -0.1542,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5517,  0.9656,  0.9656, -0.7170, -0.7170, -0.7149, -0.7286,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-1.9946, -3.8386, -0.4948,  ..., -4.1961, -4.2038, -4.2105],\n",
      "        [-2.7885, -3.8167, -0.6802,  ..., -5.7366, -5.7342, -5.7192],\n",
      "        [-2.9644, -3.7778, -0.6784,  ..., -5.1820, -5.1327, -5.1105],\n",
      "        ...,\n",
      "        [-2.1989, -3.8207, -0.5690,  ..., -4.0971, -4.0824, -4.0798],\n",
      "        [-2.1991, -3.8145, -0.5573,  ..., -4.1601, -4.1470, -4.1391],\n",
      "        [-2.7931, -3.8153, -0.6769,  ..., -5.8111, -5.8018, -5.7863]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4662,  0.9635,  0.8988, -0.7712, -0.7712, -0.6677, -0.7064,  0.9411],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.5979, -0.6851,  0.9900, -0.7459,  0.9900,  0.9900, -0.7001, -0.7616],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9234e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(9.7417e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1030, -0.1040, -0.1051,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1386, -0.1400, -0.1414,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1416, -0.1431, -0.1445,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1541, -0.1556, -0.1572,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5831, -0.6682,  0.9656, -0.7275,  0.9656,  0.9656, -0.6828, -0.7428],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3025, -3.8002, -0.5493,  ..., -4.6025, -4.5647, -4.5350],\n",
      "        [-2.2199, -3.8106, -0.5633,  ..., -4.1250, -4.1121, -4.1108],\n",
      "        [-2.7174, -3.8168, -0.6654,  ..., -5.5554, -5.5724, -5.5724],\n",
      "        ...,\n",
      "        [-2.8126, -3.8123, -0.6697,  ..., -5.6764, -5.6668, -5.6519],\n",
      "        [-2.2143, -3.8160, -0.5579,  ..., -4.1305, -4.1175, -4.1185],\n",
      "        [-2.2078, -3.8095, -0.5631,  ..., -4.0688, -4.0533, -4.0507]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5693, -0.6592,  0.9867, -0.7128,  0.9913,  0.9886, -0.6969, -0.7257],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7380,  0.9900, -1.1000,  0.9900,  0.9900,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.8896e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1493, -0.1508, -0.1523,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7198,  0.9656, -1.0518,  0.9656,  0.9656,  0.9656,  0.9656,  0.9608],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2150, -3.8092, -0.5511,  ..., -4.0737, -4.0572, -4.0544],\n",
      "        [-2.8009, -3.8056, -0.6651,  ..., -5.7437, -5.7412, -5.7278],\n",
      "        [-2.4089, -3.7509, -0.5762,  ..., -1.0285, -1.0596, -1.0627],\n",
      "        ...,\n",
      "        [-2.8102, -3.8155, -0.6695,  ..., -5.8307, -5.8284, -5.8183],\n",
      "        [-2.8048, -3.8058, -0.6624,  ..., -5.8214, -5.8120, -5.7962],\n",
      "        [-2.9664, -3.7644, -0.6630,  ..., -5.2474, -5.1689, -5.1179]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7157,  0.9794, -1.0391,  1.0161,  0.9839,  1.0018,  0.9489,  0.9579],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6872,  0.9900,  0.9900,  0.9900, -0.7570, -0.6872, -0.7266,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3956e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(4.8043e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1390, -0.1404, -0.1418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1390, -0.1404, -0.1418,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1470, -0.1485, -0.1500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6703,  0.9656,  0.9656,  0.9656, -0.7384, -0.6703, -0.7087,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2235, -3.7962, -0.5495,  ..., -4.1622, -4.1429, -4.1408],\n",
      "        [-2.8101, -3.8025, -0.6439,  ..., -5.8045, -5.8024, -5.7913],\n",
      "        [-2.8060, -3.7998, -0.6548,  ..., -5.7836, -5.7814, -5.7620],\n",
      "        ...,\n",
      "        [-2.2235, -3.7962, -0.5495,  ..., -4.1622, -4.1429, -4.1408],\n",
      "        [-2.2083, -3.8061, -0.5458,  ..., -4.1027, -4.0872, -4.0812],\n",
      "        [-2.9832, -3.7687, -0.6605,  ..., -5.2214, -5.1708, -5.1469]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6617,  0.9792,  0.9573,  0.9112, -0.7035, -0.6617, -0.6333,  0.8763],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7508, -0.7004,  0.9900,  0.9900, -1.5677,  0.9900, -1.3905],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.7223e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1519, -0.1534, -0.1550,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1417, -0.1431, -0.1446,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3203, -0.3236, -0.3269,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2702, -0.2729, -0.2757,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7323, -0.6831,  0.9656,  0.9656, -1.5366,  0.9656, -1.3295],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8262, -3.8019, -0.6551,  ..., -5.6851, -5.6736, -5.6603],\n",
      "        [-2.1970, -3.7991, -0.5360,  ..., -4.0627, -4.0478, -4.0437],\n",
      "        [-2.2292, -3.8036, -0.5422,  ..., -4.0503, -4.0378, -4.0399],\n",
      "        ...,\n",
      "        [-2.0070, -3.8234, -0.4707,  ..., -4.1987, -4.2052, -4.2127],\n",
      "        [-2.7417, -3.8148, -0.6439,  ..., -5.7168, -5.7295, -5.7367],\n",
      "        [-2.1085, -3.8053, -0.4882,  ..., -4.4547, -4.4057, -4.3724]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9863, -0.6876, -0.6330,  0.9779,  0.9819, -1.4686,  0.9872, -1.3235],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.5814,  0.9900, -0.7294,  0.9900,  0.9900,  0.9900, -1.1899, -0.5960],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.3384e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(1.1756e-11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1001, -0.1012, -0.1022,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1476, -0.1490, -0.1506,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1385, -0.1399, -0.1413,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1027, -0.1037, -0.1047,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.5670,  0.9656, -0.7114,  0.9656,  0.9656,  0.9656, -1.1378, -0.5813],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3129, -3.7881, -0.5358,  ..., -4.6103, -4.5747, -4.5423],\n",
      "        [-2.9946, -3.7558, -0.6568,  ..., -5.1874, -5.1412, -5.1233],\n",
      "        [-2.2213, -3.7997, -0.5373,  ..., -4.0828, -4.0715, -4.0678],\n",
      "        ...,\n",
      "        [-2.7344, -3.7918, -0.6297,  ..., -5.7414, -5.7693, -5.7751],\n",
      "        [-2.3896, -3.7526, -0.5461,  ..., -4.0987, -4.0910, -4.0788],\n",
      "        [-2.3143, -3.7872, -0.5449,  ..., -4.6929, -4.6619, -4.6378]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.5624,  0.7909, -0.6920,  0.9669,  0.8573,  0.9669, -1.1629, -0.5389],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.6645], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4858e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(5.7870e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1344, -0.1358, -0.1372, -0.1385, -0.1399, -0.1413, -0.1428, -0.1442,\n",
      "         -0.1457, -0.1471, -0.1486, -0.1501, -0.1517, -0.1532, -0.1547, -0.1563,\n",
      "         -0.1579, -0.1595, -0.1611, -0.1627, -0.1643, -0.1660, -0.1677, -0.1694,\n",
      "         -0.1711, -0.1728, -0.1746, -0.1763, -0.1781, -0.1799, -0.1817, -0.1836,\n",
      "         -0.1854, -0.1873, -0.1892, -0.1911, -0.1930, -0.1950, -0.1969, -0.1989,\n",
      "         -0.2009, -0.2030, -0.2050, -0.2071, -0.2092, -0.2113, -0.2134, -0.2156,\n",
      "         -0.2178, -0.2200, -0.2222, -0.2244, -0.2267, -0.2290, -0.2313, -0.2336,\n",
      "         -0.2360, -0.2384, -0.2408, -0.2432, -0.2457, -0.2482, -0.2507, -0.2532,\n",
      "         -0.2558, -0.2583, -0.2609, -0.2636, -0.2662, -0.2689, -0.2716, -0.2744,\n",
      "         -0.2772, -0.2800, -0.2828, -0.2856, -0.2885, -0.2914, -0.2944, -0.2974,\n",
      "         -0.3004, -0.3034, -0.3065, -0.3096, -0.3127, -0.3158, -0.3190, -0.3223,\n",
      "         -0.3255, -0.3288, -0.3321, -0.3355, -0.3389, -0.3423, -0.3457, -0.3492,\n",
      "         -0.3528, -0.3563, -0.3599, -0.3636, -0.3672, -0.3709, -0.3747, -0.3785,\n",
      "         -0.3823, -0.3862, -0.3901, -0.3940, -0.3980, -0.4020, -0.4061, -0.4102,\n",
      "         -0.4143, -0.4185, -0.4227, -0.4270, -0.4313, -0.4357, -0.4401, -0.4445,\n",
      "         -0.4490, -0.4535, -0.4581, -0.4627, -0.4674, -0.4721, -0.4769, -0.4817,\n",
      "         -0.4866, -0.4915, -0.4965, -0.5015, -0.5066, -0.5117, -0.5168, -0.5221,\n",
      "         -0.5273, -0.5327, -0.5380, -0.5435, -0.5490, -0.5545, -0.5601, -0.5658,\n",
      "         -0.5715, -0.5773, -0.5831, -0.5890, -0.5949, -0.6009, -0.6070, -0.6131,\n",
      "         -0.6193, -0.6256, -0.6319, -0.6383, -0.6447, -0.6512, -0.6578, -0.6645,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6481], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8100e+00, -3.7927e+00, -6.5515e-01, -2.3700e+00,  2.9807e-01,\n",
      "         -2.3963e+00, -4.6427e+00, -4.4112e-01, -4.2261e+00, -2.6640e+00,\n",
      "         -4.0724e+00,  5.2074e-01, -3.4818e-01, -3.6206e+00, -2.9813e+00,\n",
      "         -2.3104e+00, -3.2568e+00, -1.7924e+00, -1.8576e+00, -1.0721e+00,\n",
      "         -1.3873e+00, -1.5590e+00, -9.4281e-01,  5.9170e-01, -2.0636e+00,\n",
      "         -3.5814e+00, -2.9742e+00, -2.4829e+00, -3.6829e+00,  1.1259e+00,\n",
      "         -1.7397e+00, -3.1396e-01, -9.4759e-01, -3.5537e+00, -1.9818e+00,\n",
      "         -1.8335e+00, -7.9098e-01, -3.1327e+00, -1.6302e+00,  4.3693e-03,\n",
      "         -5.9691e-01, -3.6546e-01, -1.5456e-01, -4.3504e+00, -1.6897e+00,\n",
      "         -3.5359e+00, -3.6568e+00, -3.2851e+00,  6.1076e-01,  5.9374e-01,\n",
      "         -3.9363e+00, -1.5765e+00, -7.3509e+00, -2.8618e+00, -2.8924e+00,\n",
      "         -8.4806e-01, -2.5313e+00, -2.8140e+00, -2.7963e+00, -1.6859e+00,\n",
      "         -3.7621e+00, -1.2373e+00, -5.9019e+00, -2.3347e+00, -3.4024e+00,\n",
      "          1.3197e+00, -1.2831e+00, -1.6415e-01, -2.9672e-01, -3.1429e+00,\n",
      "         -4.1077e+00, -5.7793e+00, -2.7716e+00, -1.7393e-01, -3.8960e+00,\n",
      "         -4.2767e+00, -2.3884e+00, -3.7865e+00, -2.5146e+00, -3.6212e+00,\n",
      "         -5.0169e+00, -2.4111e+00, -2.2393e-01, -1.4115e+00, -1.8801e+00,\n",
      "         -3.1035e+00, -1.8156e+00, -7.5816e-01, -3.7851e+00, -7.9361e-01,\n",
      "         -3.2407e+00, -4.1091e+00, -2.3634e+00, -5.3654e-01, -1.0039e+00,\n",
      "         -3.1495e+00, -2.8524e+00, -3.8375e+00, -3.3681e+00, -3.3639e+00,\n",
      "         -3.2373e-02, -1.1269e+00, -3.6994e+00, -2.2080e+00, -3.5534e-01,\n",
      "         -1.9592e+00, -3.6561e+00, -3.9536e+00,  6.2432e-01, -1.4102e+00,\n",
      "         -1.1895e+00, -1.2464e+00, -4.6309e+00, -4.6831e+00,  5.9673e-01,\n",
      "         -1.1006e+00, -2.2518e+00, -1.4433e+00, -2.9151e-01, -9.0217e-01,\n",
      "         -2.8832e+00, -4.6152e+00, -2.5423e+00, -7.1071e-01, -1.2223e+00,\n",
      "         -3.0307e+00, -2.1655e+00, -7.9207e-01, -7.5227e-01, -4.2582e+00,\n",
      "          1.4353e-01,  6.4036e-01, -3.3184e+00, -4.1893e+00, -1.2129e+00,\n",
      "         -2.0070e+00, -2.9609e+00, -2.5691e+00, -1.6307e+00, -2.7741e+00,\n",
      "         -2.9627e+00, -5.8582e+00, -1.6672e+00, -2.5303e+00, -1.3875e+00,\n",
      "         -8.5686e-01, -7.0470e-01, -4.3610e+00, -3.7946e+00, -2.7619e-01,\n",
      "         -2.0178e+00, -4.7476e-01, -8.3255e-01, -6.1610e-01,  8.0568e-02,\n",
      "         -6.6988e-01, -4.1111e+00, -3.4368e+00, -4.9243e+00, -1.3457e+00,\n",
      "          8.6444e-01,  9.3625e-01,  1.0184e+00,  9.6954e-01,  1.0146e+00,\n",
      "          9.9426e-01, -4.7347e+00, -4.9092e+00, -5.2822e+00, -5.1402e+00,\n",
      "         -5.3772e+00, -5.4126e+00, -5.4538e+00, -5.4640e+00, -5.4822e+00,\n",
      "         -5.4050e+00, -5.2553e+00, -5.1315e+00, -5.0769e+00, -5.0542e+00,\n",
      "         -5.0718e+00, -5.0913e+00, -5.1241e+00, -5.1633e+00, -5.1952e+00,\n",
      "         -5.2119e+00, -5.2405e+00, -5.2917e+00, -5.3628e+00, -5.4385e+00,\n",
      "         -5.4795e+00, -5.4890e+00, -5.4807e+00, -5.4840e+00, -5.5173e+00,\n",
      "         -5.5736e+00, -5.5999e+00, -5.5964e+00, -5.5788e+00, -5.5402e+00,\n",
      "         -5.5094e+00, -5.4904e+00, -5.4904e+00, -5.5124e+00, -5.5410e+00,\n",
      "         -5.5844e+00, -5.6211e+00, -5.6451e+00, -5.6519e+00, -5.6350e+00,\n",
      "         -5.6404e+00, -5.6559e+00, -5.6804e+00, -5.7272e+00, -5.7563e+00,\n",
      "         -5.7643e+00, -5.7691e+00, -5.7726e+00, -5.7748e+00, -5.7765e+00,\n",
      "         -5.7725e+00, -5.7481e+00],\n",
      "        [-2.2191e+00, -3.7918e+00, -5.4716e-01, -2.0121e+00,  4.4806e-01,\n",
      "         -2.2790e+00, -4.4085e+00,  8.5664e-02, -3.5934e+00, -2.3565e+00,\n",
      "         -3.9038e+00,  5.4739e-01, -2.1335e-01, -3.4181e+00, -3.1214e+00,\n",
      "         -1.9840e+00, -3.1327e+00, -1.6365e+00, -1.9359e+00, -7.9305e-01,\n",
      "         -1.2382e+00, -1.2590e+00, -7.0606e-01,  4.7972e-01, -1.7630e+00,\n",
      "         -3.3679e+00, -3.0856e+00, -2.2264e+00, -3.6962e+00,  1.0825e+00,\n",
      "         -1.5431e+00,  1.1447e-02, -9.7833e-01, -3.4215e+00, -1.9295e+00,\n",
      "         -2.0167e+00, -8.8576e-01, -3.0826e+00, -1.4021e+00,  2.7400e-01,\n",
      "         -5.3136e-01, -4.2484e-01, -1.1134e-01, -4.4513e+00, -1.4449e+00,\n",
      "         -3.0192e+00, -3.3453e+00, -3.1586e+00, -1.7071e-01, -1.3060e-01,\n",
      "         -3.8177e+00, -1.4686e+00, -7.3545e+00, -2.6970e+00, -2.8871e+00,\n",
      "         -6.7732e-01, -2.5458e+00, -2.7786e+00, -2.6510e+00, -1.4514e+00,\n",
      "         -3.5902e+00, -1.1426e+00, -5.8891e+00, -2.2192e+00, -3.4268e+00,\n",
      "          1.2412e+00, -1.1100e+00,  1.6788e-01, -3.8298e-01, -3.0081e+00,\n",
      "         -3.9133e+00, -5.8290e+00, -2.6112e+00, -2.8511e-01, -3.7474e+00,\n",
      "         -4.3698e+00, -2.2336e+00, -3.7891e+00, -2.3353e+00, -3.4805e+00,\n",
      "         -5.0719e+00, -2.2718e+00, -1.6920e-01, -1.1255e+00, -1.8017e+00,\n",
      "         -2.8548e+00, -1.6296e+00, -4.6090e-01, -3.6268e+00, -6.9626e-01,\n",
      "         -3.0726e+00, -4.1843e+00, -2.2291e+00, -4.9676e-01, -7.0377e-01,\n",
      "         -3.2413e+00, -2.6323e+00, -3.4002e+00, -3.2869e+00, -3.2147e+00,\n",
      "          5.2476e-02, -8.5091e-01, -3.7297e+00, -2.2173e+00, -4.4504e-01,\n",
      "         -1.6709e+00, -3.2634e+00, -3.7746e+00,  4.8597e-01, -9.5788e-01,\n",
      "         -1.5729e+00, -3.9531e-01, -4.2814e+00, -4.6561e+00,  6.1827e-01,\n",
      "         -7.4350e-01, -2.1132e+00, -1.2000e+00, -2.0912e-02, -8.0536e-01,\n",
      "         -2.4311e+00, -4.6377e+00, -2.4951e+00, -7.4432e-01, -1.0115e+00,\n",
      "         -2.9241e+00, -1.9840e+00, -6.3358e-01, -3.3801e-01, -4.1774e+00,\n",
      "         -5.8504e-01, -1.1668e-01, -3.2638e+00, -4.1136e+00, -1.0488e+00,\n",
      "         -2.0207e+00, -2.9569e+00, -2.4793e+00, -1.5146e+00, -2.8181e+00,\n",
      "         -2.5294e+00, -3.1405e+00, -5.7892e+00, -1.1593e+00, -5.7133e-01,\n",
      "         -6.9924e-01, -5.8349e-01, -7.2067e-01, -5.2311e-01, -7.2635e-01,\n",
      "         -4.8930e+00, -4.9783e+00, -5.4294e+00, -5.0047e+00, -4.9010e+00,\n",
      "         -4.6159e+00, -4.4864e+00, -4.6198e+00, -4.5743e+00, -4.6669e+00,\n",
      "         -4.8796e+00, -5.0005e+00, -5.0547e+00, -5.0282e+00, -4.9267e+00,\n",
      "         -4.8083e+00, -4.7536e+00, -4.7283e+00, -4.6980e+00, -4.6546e+00,\n",
      "         -4.6184e+00, -4.6028e+00, -4.6113e+00, -4.6276e+00, -4.6469e+00,\n",
      "         -4.6499e+00, -4.6141e+00, -4.5819e+00, -4.5742e+00, -4.5585e+00,\n",
      "         -4.5911e+00, -4.6427e+00, -4.6642e+00, -4.6580e+00, -4.6537e+00,\n",
      "         -4.6418e+00, -4.6655e+00, -4.6861e+00, -4.6956e+00, -4.7040e+00,\n",
      "         -4.7121e+00, -4.7517e+00, -4.8031e+00, -4.8393e+00, -4.8551e+00,\n",
      "         -4.8582e+00, -4.8754e+00, -4.8821e+00, -4.8888e+00, -4.8940e+00,\n",
      "         -4.8991e+00, -4.9169e+00, -4.9118e+00, -4.8939e+00, -4.8918e+00,\n",
      "         -4.8919e+00, -4.8722e+00, -4.8586e+00, -4.8196e+00, -4.7681e+00,\n",
      "         -4.7573e+00, -4.6906e+00, -4.6510e+00, -4.5679e+00, -4.5269e+00,\n",
      "         -4.3935e+00, -4.2778e+00, -4.2079e+00, -4.1620e+00, -4.1267e+00,\n",
      "         -4.1106e+00, -4.1107e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9663, -0.6374], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900, -0.6622, -0.5804,  0.9900, -0.7421,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1277e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1501, -0.1516, -0.1532,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656, -0.6459, -0.5660,  0.9656, -0.7238,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7203, -3.7999, -0.6472,  ..., -5.7196, -5.7341, -5.7405],\n",
      "        [-2.7909, -3.7842, -0.6568,  ..., -5.7453, -5.7450, -5.7286],\n",
      "        [-2.8060, -3.7853, -0.6564,  ..., -5.6842, -5.6730, -5.6598],\n",
      "        ...,\n",
      "        [-2.7909, -3.7842, -0.6568,  ..., -5.7453, -5.7450, -5.7286],\n",
      "        [-2.1800, -3.7845, -0.5366,  ..., -4.0511, -4.0357, -4.0321],\n",
      "        [-2.7939, -3.7849, -0.6446,  ..., -5.8045, -5.8016, -5.7901]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9684,  0.9734,  0.9666, -0.6401, -0.5624,  0.9734, -0.6907,  0.9558],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6814, -1.5645,  0.9900,  0.9900,  0.9900, -0.7103, -0.6716, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8624e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1378, -0.1392, -0.1406,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3197, -0.3229, -0.3262,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1437, -0.1452, -0.1466,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1359, -0.1372, -0.1386,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6646, -1.5335,  0.9656,  0.9656,  0.9656, -0.6928, -0.6551, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1998, -3.7851, -0.5458,  ..., -4.1198, -4.1065, -4.1052],\n",
      "        [-1.9742, -3.8045, -0.4727,  ..., -4.1795, -4.1869, -4.1953],\n",
      "        [-2.9610, -3.7407, -0.6625,  ..., -5.1818, -5.1380, -5.1147],\n",
      "        ...,\n",
      "        [-2.1861, -3.7838, -0.5484,  ..., -4.0846, -4.0697, -4.0682],\n",
      "        [-2.2022, -3.7761, -0.5514,  ..., -4.1453, -4.1289, -4.1253],\n",
      "        [-2.3844, -3.7254, -0.5710,  ..., -1.0450, -1.0741, -1.0578]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6645, -1.4808,  0.8623,  0.7754,  0.9651, -0.6433, -0.6638, -1.0507],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.5666,  0.9900,  0.9900, -0.7122,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.8917e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(3.4364e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3201, -0.3234, -0.3266,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.5356,  0.9656,  0.9656, -0.6947,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6943, -3.7775, -0.6535,  ..., -5.5655, -5.5805, -5.5836],\n",
      "        [-1.9653, -3.7983, -0.4724,  ..., -4.1702, -4.1799, -4.1873],\n",
      "        [-2.6954, -3.7665, -0.6337,  ..., -5.7340, -5.7651, -5.7707],\n",
      "        ...,\n",
      "        [-2.6943, -3.7775, -0.6535,  ..., -5.5655, -5.5805, -5.5836],\n",
      "        [-2.7700, -3.7711, -0.6574,  ..., -5.7803, -5.7786, -5.7611],\n",
      "        [-2.7681, -3.7759, -0.6601,  ..., -5.7692, -5.7644, -5.7502]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9528, -1.4800,  0.9555,  0.9726, -0.7158,  0.9528,  0.9401,  0.9860],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6634, -1.3922,  0.9900, -0.7258, -0.6716,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.2849e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1342, -0.1356, -0.1369,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2705, -0.2733, -0.2760,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6470, -1.3312,  0.9656, -0.7079, -0.6550,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1928, -3.7694, -0.5520,  ..., -4.1052, -4.0900, -4.0894],\n",
      "        [-2.0655, -3.7754, -0.4923,  ..., -4.4307, -4.3804, -4.3447],\n",
      "        [-2.9419, -3.7364, -0.6665,  ..., -5.2134, -5.1656, -5.1397],\n",
      "        ...,\n",
      "        [-2.7686, -3.7676, -0.6588,  ..., -5.8147, -5.8059, -5.7883],\n",
      "        [-2.7740, -3.7710, -0.6586,  ..., -5.7686, -5.7647, -5.7429],\n",
      "        [-2.9419, -3.7364, -0.6665,  ..., -5.2134, -5.1656, -5.1397]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6457, -1.3178,  0.8364, -0.6810, -0.6635,  0.9225,  0.9572,  0.8364],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7145,  0.9900,  0.9900, -0.5868, -0.7298, -0.6038,  0.9900, -0.7298],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8759e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1446, -0.1460, -0.1475,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1040, -0.1050, -0.1061,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1476, -0.1491, -0.1506,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6969,  0.9656,  0.9656, -0.5724, -0.7118, -0.5889,  0.9656, -0.7118],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1701, -3.7670, -0.5375,  ..., -4.0905, -4.0737, -4.0739],\n",
      "        [-2.7558, -3.7701, -0.6532,  ..., -5.8204, -5.8233, -5.8067],\n",
      "        [-2.7758, -3.7640, -0.6585,  ..., -5.6756, -5.6668, -5.6513],\n",
      "        ...,\n",
      "        [-2.2618, -3.7551, -0.5461,  ..., -4.6642, -4.6357, -4.6086],\n",
      "        [-2.7641, -3.7628, -0.6579,  ..., -5.8082, -5.8009, -5.7832],\n",
      "        [-2.1767, -3.7673, -0.5411,  ..., -4.0463, -4.0326, -4.0347]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7089,  0.9650,  0.9614, -0.5617, -0.6881, -0.5451,  0.9260, -0.6881],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7511,  0.9900, -0.7008,  0.9900, -0.7190, -1.2027, -0.7275],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.8831e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1519, -0.1535, -0.1550,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1455, -0.1469, -0.1484,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1400, -0.1414, -0.1428,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1472, -0.1487, -0.1502,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7326,  0.9656, -0.6835,  0.9608, -0.7013, -1.1500, -0.7096],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.9351, -3.7228, -0.6616,  ..., -5.1681, -5.1231, -5.0992],\n",
      "        [-2.1687, -3.7623, -0.5495,  ..., -4.0284, -4.0160, -4.0119],\n",
      "        [-2.7513, -3.7671, -0.6542,  ..., -5.8141, -5.8184, -5.8011],\n",
      "        ...,\n",
      "        [-2.1658, -3.7667, -0.5464,  ..., -4.0618, -4.0456, -4.0425],\n",
      "        [-2.3374, -3.7166, -0.5472,  ..., -4.0722, -4.0712, -4.0569],\n",
      "        [-2.1733, -3.7659, -0.5452,  ..., -4.0368, -4.0215, -4.0196]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.8668, -0.6994,  0.9686, -0.6321,  0.9365, -0.6438, -1.1530, -0.6979],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7563], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.4806e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  0.1542,  0.1558,  0.1574,  0.1589,  0.1605,\n",
      "          0.1622,  0.1638,  0.1655,  0.1671,  0.1688,  0.1705,  0.1723,  0.1740,\n",
      "          0.1757,  0.1775,  0.1793,  0.1811,  0.1830,  0.1848,  0.1867,  0.1886,\n",
      "          0.1905,  0.1924,  0.1943,  0.1963,  0.1983,  0.2003,  0.2023,  0.2043,\n",
      "          0.2064,  0.2085,  0.2106,  0.2127,  0.2149,  0.2170,  0.2192,  0.2215,\n",
      "          0.2237,  0.2259,  0.2282,  0.2305,  0.2329,  0.2352,  0.2376,  0.2400,\n",
      "          0.2424,  0.2449,  0.2473,  0.2498,  0.2524,  0.2549,  0.2575,  0.2601,\n",
      "          0.2627,  0.2654,  0.2680,  0.2708,  0.2735,  0.2763,  0.2790,  0.2819,\n",
      "          0.2847,  0.2876,  0.2905,  0.2934,  0.2964,  0.2994,  0.3024,  0.3055,\n",
      "          0.3085,  0.3117,  0.3148,  0.3180,  0.3212,  0.3244,  0.3277,  0.3310,\n",
      "          0.3344,  0.3378,  0.3412,  0.3446,  0.3481,  0.3516,  0.3552,  0.3587,\n",
      "          0.3624,  0.3660,  0.3697,  0.3735,  0.3772,  0.3810,  0.3849,  0.3888,\n",
      "          0.3927,  0.3967,  0.4007,  0.4047,  0.4088,  0.4130,  0.4171,  0.4213,\n",
      "          0.4256,  0.4299,  0.4342,  0.4386,  0.4430,  0.4475,  0.4520,  0.4566,\n",
      "          0.4612,  0.4659,  0.4706,  0.4753,  0.4801,  0.4850,  0.4899,  0.4948,\n",
      "          0.4998,  0.5049,  0.5100,  0.5151,  0.5203,  0.5256,  0.5309,  0.5363,\n",
      "          0.5417,  0.5472,  0.5527,  0.5583,  0.5639,  0.5696,  0.5754,  0.5812,\n",
      "          0.5870,  0.5930,  0.5990,  0.6050,  0.6111,  0.6173,  0.6235,  0.6298,\n",
      "          0.6362,  0.6426,  0.6491,  0.6557,  0.6623,  0.6690,  0.6757,  0.6826,\n",
      "          0.6894,  0.6964,  0.7034,  0.7106,  0.7177,  0.7250,  0.7323,  0.7397,\n",
      "          0.7472,  0.7547,  0.7623,  0.7700,  0.7778,  0.7857,  0.7936,  0.8016,\n",
      "          0.8097,  0.8179,  0.8262,  0.8345,  0.8429,  0.8515,  0.8601,  0.8687,\n",
      "          0.8775,  0.8864,  0.8953,  0.9044,  0.9135,  0.9227,  0.9321,  0.9415,\n",
      "          0.9510,  0.9606,  0.9703,  0.9801,  0.9900,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1530, -0.1546, -0.1561, -0.1577, -0.1593, -0.1609, -0.1625, -0.1642,\n",
      "         -0.1658, -0.1675, -0.1692, -0.1709, -0.1726, -0.1744, -0.1761, -0.1779,\n",
      "         -0.1797, -0.1815, -0.1833, -0.1852, -0.1871, -0.1890, -0.1909, -0.1928,\n",
      "         -0.1947, -0.1967, -0.1987, -0.2007, -0.2027, -0.2048, -0.2068, -0.2089,\n",
      "         -0.2110, -0.2132, -0.2153, -0.2175, -0.2197, -0.2219, -0.2242, -0.2264,\n",
      "         -0.2287, -0.2310, -0.2334, -0.2357, -0.2381, -0.2405, -0.2429, -0.2454,\n",
      "         -0.2479, -0.2504, -0.2529, -0.2555, -0.2580, -0.2606, -0.2633, -0.2659,\n",
      "         -0.2686, -0.2713, -0.2741, -0.2768, -0.2796, -0.2825, -0.2853, -0.2882,\n",
      "         -0.2911, -0.2941, -0.2970, -0.3000, -0.3031, -0.3061, -0.3092, -0.3123,\n",
      "         -0.3155, -0.3187, -0.3219, -0.3251, -0.3284, -0.3317, -0.3351, -0.3385,\n",
      "         -0.3419, -0.3453, -0.3488, -0.3524, -0.3559, -0.3595, -0.3631, -0.3668,\n",
      "         -0.3705, -0.3743, -0.3780, -0.3819, -0.3857, -0.3896, -0.3936, -0.3975,\n",
      "         -0.4015, -0.4056, -0.4097, -0.4138, -0.4180, -0.4222, -0.4265, -0.4308,\n",
      "         -0.4352, -0.4396, -0.4440, -0.4485, -0.4530, -0.4576, -0.4622, -0.4669,\n",
      "         -0.4716, -0.4764, -0.4812, -0.4860, -0.4909, -0.4959, -0.5009, -0.5060,\n",
      "         -0.5111, -0.5162, -0.5215, -0.5267, -0.5320, -0.5374, -0.5428, -0.5483,\n",
      "         -0.5539, -0.5595, -0.5651, -0.5708, -0.5766, -0.5824, -0.5883, -0.5942,\n",
      "         -0.6002, -0.6063, -0.6124, -0.6186, -0.6249, -0.6312, -0.6375, -0.6440,\n",
      "         -0.6505, -0.6571, -0.6637, -0.6704, -0.6772, -0.6840, -0.6909, -0.6979,\n",
      "         -0.7050, -0.7121, -0.7193, -0.7265, -0.7339, -0.7413, -0.7488, -0.7563,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.7377], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6780, -3.7522, -0.6334, -2.3297,  0.3000, -2.3934, -4.5757, -0.3191,\n",
      "         -4.0883, -2.5915, -4.0403,  0.4820, -0.2919, -3.5480, -2.9542, -2.2184,\n",
      "         -3.2454, -1.7841, -1.8722, -1.0296, -1.3254, -1.5159, -0.9104,  0.5881,\n",
      "         -2.0223, -3.5140, -2.9352, -2.4194, -3.6745,  1.1097, -1.7247, -0.2612,\n",
      "         -0.9613, -3.5469, -1.9691, -1.8786, -0.8481, -3.1419, -1.5833,  0.0374,\n",
      "         -0.6324, -0.4192, -0.1684, -4.3401, -1.6595, -3.4141, -3.5935, -3.2799,\n",
      "          0.4940,  0.4678, -3.8911, -1.5570, -7.3320, -2.8119, -2.9112, -0.8472,\n",
      "         -2.5501, -2.7910, -2.7551, -1.6560, -3.7038, -1.2166, -5.8924, -2.2989,\n",
      "         -3.4046,  1.2875, -1.2866, -0.1225, -0.3304, -3.1415, -4.0516, -5.7772,\n",
      "         -2.7263, -0.1721, -3.8463, -4.2326, -2.3469, -3.8008, -2.4789, -3.5731,\n",
      "         -5.0164, -2.3748, -0.2217, -1.3729, -1.8830, -3.0670, -1.8133, -0.7110,\n",
      "         -3.7454, -0.8227, -3.1898, -4.0698, -2.3280, -0.5404, -0.9664, -3.1309,\n",
      "         -2.8227, -3.7281, -3.3259, -3.3459, -0.0267, -1.0948, -3.6945, -2.2158,\n",
      "         -0.3572, -1.9148, -3.5623, -3.9258,  0.6247, -1.2997, -1.5587, -2.6347,\n",
      "         -4.5294, -4.6530,  0.5617, -1.0147, -2.2268, -1.3575, -0.1778, -0.9155,\n",
      "         -2.5744, -4.7981, -2.4528, -0.6646, -1.0937, -2.9906, -2.1370, -0.7482,\n",
      "         -0.5455, -4.2653, -0.0531,  0.4891, -3.3298, -4.2067, -1.1935, -2.0238,\n",
      "         -2.9558, -2.5319, -1.6267, -2.7113, -2.8406, -5.8428, -1.5903, -4.0270,\n",
      "         -0.9838, -1.3356, -4.3869, -4.3431, -0.3766, -1.5714, -4.5351, -3.0164,\n",
      "         -2.8110, -4.2479,  0.2263, -2.3709, -1.1402, -0.5195, -3.4811, -0.8856,\n",
      "          0.1308, -4.5971, -5.8610, -3.7818,  0.0475, -4.5257, -4.5191, -3.0995,\n",
      "         -3.6180, -3.7470, -2.8690, -5.0933, -1.2638,  0.9678,  0.9261,  0.9193,\n",
      "          0.9826,  0.9764,  1.0131, -4.6533, -4.8828, -5.3623, -5.2468, -5.3792,\n",
      "         -5.4351, -5.2994, -5.2145, -5.3775, -5.4125, -5.3217, -5.2271, -5.1531,\n",
      "         -5.1398, -5.1861, -5.2358, -5.2703, -5.2842, -5.2919, -5.3006, -5.3222,\n",
      "         -5.3919, -5.4753, -5.5519, -5.5963, -5.5958, -5.5896, -5.6040, -5.6562,\n",
      "         -5.7034, -5.7044, -5.6961, -5.6588, -5.6149, -5.5996, -5.5943, -5.6076,\n",
      "         -5.6315, -5.6494, -5.6819, -5.7207, -5.7501, -5.7559],\n",
      "        [-2.1650, -3.7610, -0.5483, -2.0274,  0.4343, -2.2963, -4.3707,  0.1004,\n",
      "         -3.5400, -2.3299, -3.8871,  0.5252, -0.1596, -3.4071, -3.1029, -1.9473,\n",
      "         -3.1131, -1.6532, -1.9513, -0.7782, -1.2299, -1.2470, -0.7074,  0.4702,\n",
      "         -1.7516, -3.3558, -3.0602, -2.1988, -3.6990,  1.0611, -1.5610,  0.0216,\n",
      "         -1.0062, -3.4209, -1.9262, -2.0445, -0.9383, -3.0722, -1.4120,  0.2677,\n",
      "         -0.5757, -0.4785, -0.1376, -4.4508, -1.4435, -2.9684, -3.3216, -3.1635,\n",
      "         -0.1731, -0.1607, -3.8096, -1.4541, -7.3368, -2.6704, -2.8853, -0.7008,\n",
      "         -2.5755, -2.7648, -2.6365, -1.4492, -3.5784, -1.1331, -5.8813, -2.1980,\n",
      "         -3.4361,  1.2121, -1.1419,  0.1689, -0.4170, -3.0125, -3.8899, -5.8027,\n",
      "         -2.5851, -0.2709, -3.7394, -4.3218, -2.2102, -3.7780, -2.3419, -3.4725,\n",
      "         -5.0496, -2.2503, -0.1858, -1.1257, -1.8117, -2.8561, -1.6586, -0.4580,\n",
      "         -3.6064, -0.7377, -3.0649, -4.1426, -2.2093, -0.5132, -0.7066, -3.2455,\n",
      "         -2.6271, -3.3567, -3.2517, -3.2207,  0.0385, -0.8521, -3.7300, -2.2081,\n",
      "         -0.4492, -1.6678, -3.2228, -3.7641,  0.4998, -1.6894, -1.6728, -1.0478,\n",
      "         -4.1474, -4.6899,  0.6411, -0.7787, -2.0859, -1.1424, -0.0291, -0.7974,\n",
      "         -2.3941, -4.6732, -2.4819, -0.7102, -0.9322, -2.8175, -1.9912, -0.6414,\n",
      "         -0.2867, -4.1209, -0.7028, -0.1580, -3.2418, -4.1191, -1.0924, -2.0478,\n",
      "         -2.9646, -2.4463, -1.5241, -2.7965, -2.5166, -3.1827, -5.7673, -1.1447,\n",
      "         -0.6860, -0.6369, -0.7216, -0.7421, -0.6929, -0.7259, -4.8913, -4.9518,\n",
      "         -5.3537, -4.9615, -4.8730, -4.6034, -4.5008, -4.6542, -4.5851, -4.6508,\n",
      "         -4.7896, -4.8707, -4.9304, -4.9191, -4.8356, -4.7422, -4.7117, -4.6982,\n",
      "         -4.6672, -4.6169, -4.5699, -4.5430, -4.5479, -4.5630, -4.5734, -4.5585,\n",
      "         -4.5082, -4.4554, -4.4363, -4.4234, -4.4623, -4.4981, -4.5027, -4.4834,\n",
      "         -4.4797, -4.4676, -4.4944, -4.5247, -4.5423, -4.5587, -4.5752, -4.6225,\n",
      "         -4.6766, -4.7168, -4.7308, -4.7277, -4.7379, -4.7522, -4.7733, -4.7902,\n",
      "         -4.7922, -4.8014, -4.7983, -4.7854, -4.7880, -4.7843, -4.7581, -4.7321,\n",
      "         -4.6857, -4.6154, -4.6053, -4.5509, -4.5187, -4.4459, -4.3894, -4.2779,\n",
      "         -4.1706, -4.0991, -4.0553, -4.0235, -4.0095, -4.0075]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9642, -0.7009], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6893, -0.6150,  0.9900, -0.7453,  0.9900,  0.9900, -1.4043, -1.5820],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.9560e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1395, -0.1409, -0.1423,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1059, -0.1070, -0.1081,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2729, -0.2757, -0.2784,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3233, -0.3265, -0.3298,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6723, -0.5998,  0.9656, -0.7269,  0.9656,  0.9656, -1.3428, -1.5507],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1725, -3.7528, -0.5482,  ..., -4.1131, -4.0961, -4.0936],\n",
      "        [-2.2536, -3.7460, -0.5456,  ..., -4.6461, -4.6226, -4.5910],\n",
      "        [-2.7634, -3.7563, -0.6563,  ..., -5.7449, -5.7408, -5.7165],\n",
      "        ...,\n",
      "        [-2.9314, -3.7169, -0.6589,  ..., -5.1530, -5.1055, -5.0778],\n",
      "        [-2.0530, -3.7636, -0.4910,  ..., -4.4226, -4.3745, -4.3396],\n",
      "        [-1.9487, -3.7820, -0.4709,  ..., -4.1566, -4.1656, -4.1712]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6669, -0.5518,  0.9705, -0.6921,  0.8755,  0.8755, -1.3167, -1.4597],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.5807, -0.5963, -0.6990, -0.6805,  0.9900, -1.1000,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.0717e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3230, -0.3263, -0.3296,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1027, -0.1038, -0.1048,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1414, -0.1428, -0.1443,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5494, -0.5816, -0.6817, -0.6637,  0.9656, -1.0518,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-1.9481, -3.7796, -0.4710,  ..., -4.1553, -4.1642, -4.1703],\n",
      "        [-2.2542, -3.7442, -0.5353,  ..., -4.5634, -4.5288, -4.4990],\n",
      "        [-2.1638, -3.7607, -0.5420,  ..., -4.0802, -4.0685, -4.0644],\n",
      "        ...,\n",
      "        [-2.3619, -3.6992, -0.5689,  ..., -1.0214, -1.0515, -1.0633],\n",
      "        [-2.7683, -3.7531, -0.6556,  ..., -5.6433, -5.6374, -5.6226],\n",
      "        [-2.7683, -3.7531, -0.6556,  ..., -5.6433, -5.6374, -5.6226]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4607, -0.5709, -0.6616, -0.6586,  0.9630, -1.0329,  0.9726,  0.9726],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7248,  0.9900,  0.9900,  0.9900,  0.9900, -0.7381, -0.7356,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.9371e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1466, -0.1481, -0.1496,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1493, -0.1508, -0.1523,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1488, -0.1503, -0.1518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7069,  0.9656,  0.9656,  0.9656,  0.9656, -0.7199, -0.7174,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1503, -3.7526, -0.5312,  ..., -4.0722, -4.0562, -4.0557],\n",
      "        [-2.9248, -3.7193, -0.6581,  ..., -5.1805, -5.1292, -5.1032],\n",
      "        [-2.9285, -3.7083, -0.6553,  ..., -5.1373, -5.0912, -5.0713],\n",
      "        ...,\n",
      "        [-2.1555, -3.7543, -0.5356,  ..., -4.0230, -4.0114, -4.0106],\n",
      "        [-2.1499, -3.7522, -0.5313,  ..., -4.1114, -4.0964, -4.0904],\n",
      "        [-2.6733, -3.7452, -0.6292,  ..., -5.6990, -5.7283, -5.7377]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7200,  0.8629,  0.8029,  0.9724,  0.9642, -0.7017, -0.6975,  0.9642],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7464,  0.9900, -0.5839,  0.9900, -0.7163,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.2112e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1510, -0.1525, -0.1541,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1449, -0.1464, -0.1479,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7280,  0.9656, -0.5695,  0.9608, -0.6987,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7555, -3.7465, -0.6517,  ..., -5.7729, -5.7661, -5.7478],\n",
      "        [-2.7499, -3.7454, -0.6545,  ..., -5.7052, -5.7023, -5.6873],\n",
      "        [-2.1564, -3.7517, -0.5405,  ..., -4.0100, -3.9950, -3.9912],\n",
      "        ...,\n",
      "        [-2.9156, -3.7069, -0.6520,  ..., -5.2058, -5.1265, -5.0735],\n",
      "        [-2.1512, -3.7496, -0.5297,  ..., -4.0658, -4.0526, -4.0491],\n",
      "        [-2.7487, -3.7462, -0.6508,  ..., -5.7384, -5.7396, -5.7200]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9400,  1.0016, -0.7183,  0.9637, -0.5776,  0.9443, -0.7214,  0.9599],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6889, -0.7057,  0.9900,  0.9900,  0.9900,  0.9900, -0.7405, -0.7057],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4551e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1394, -0.1408, -0.1422,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1428, -0.1442, -0.1457,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1498, -0.1513, -0.1528,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1428, -0.1442, -0.1457,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6719, -0.6883,  0.9656,  0.9656,  0.9656,  0.9656, -0.7222, -0.6883],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1723, -3.7522, -0.5373,  ..., -3.9908, -3.9779, -3.9780],\n",
      "        [-2.1544, -3.7528, -0.5406,  ..., -4.0418, -4.0262, -4.0206],\n",
      "        [-2.7457, -3.7443, -0.6518,  ..., -5.7056, -5.7022, -5.6841],\n",
      "        ...,\n",
      "        [-2.7431, -3.7482, -0.6486,  ..., -5.7247, -5.7215, -5.7089],\n",
      "        [-2.1377, -3.7472, -0.5308,  ..., -4.0002, -3.9827, -3.9810],\n",
      "        [-2.1544, -3.7528, -0.5406,  ..., -4.0418, -4.0262, -4.0206]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6546, -0.6662,  1.0029,  0.9418,  0.8725,  1.0000, -0.7087, -0.6662],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900,  0.9900,  0.9900, -1.1901, -0.7331, -0.6509, -0.7084],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5064e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1483, -0.1498, -0.1513,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1317, -0.1330, -0.1343,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1433, -0.1447, -0.1462,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656,  0.9656,  0.9656, -1.1380, -0.7150, -0.6348, -0.6909],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.6671, -3.7499, -0.6436,  ..., -5.5327, -5.5509, -5.5564],\n",
      "        [-2.6760, -3.7590, -0.6377,  ..., -5.6870, -5.7031, -5.7136],\n",
      "        [-2.7390, -3.7503, -0.6446,  ..., -5.7851, -5.7866, -5.7702],\n",
      "        ...,\n",
      "        [-2.1611, -3.7471, -0.5410,  ..., -3.9990, -3.9829, -3.9804],\n",
      "        [-2.1732, -3.7448, -0.5429,  ..., -4.0737, -4.0584, -4.0558],\n",
      "        [-2.1665, -3.7518, -0.5373,  ..., -4.0094, -3.9892, -3.9894]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9631,  0.9729,  0.9805,  0.9805, -1.1607, -0.7196, -0.6691, -0.7151],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.6596,  0.9900], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(2.5181e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(5.9288e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1334, -0.1348, -0.1361, -0.1375, -0.1389, -0.1403, -0.1417, -0.1432,\n",
      "         -0.1446, -0.1461, -0.1475, -0.1490, -0.1505, -0.1521, -0.1536, -0.1551,\n",
      "         -0.1567, -0.1583, -0.1599, -0.1615, -0.1631, -0.1648, -0.1665, -0.1681,\n",
      "         -0.1698, -0.1715, -0.1733, -0.1750, -0.1768, -0.1786, -0.1804, -0.1822,\n",
      "         -0.1840, -0.1859, -0.1878, -0.1897, -0.1916, -0.1935, -0.1955, -0.1975,\n",
      "         -0.1995, -0.2015, -0.2035, -0.2056, -0.2076, -0.2097, -0.2119, -0.2140,\n",
      "         -0.2162, -0.2183, -0.2205, -0.2228, -0.2250, -0.2273, -0.2296, -0.2319,\n",
      "         -0.2343, -0.2366, -0.2390, -0.2414, -0.2439, -0.2463, -0.2488, -0.2513,\n",
      "         -0.2539, -0.2564, -0.2590, -0.2616, -0.2643, -0.2670, -0.2696, -0.2724,\n",
      "         -0.2751, -0.2779, -0.2807, -0.2835, -0.2864, -0.2893, -0.2922, -0.2952,\n",
      "         -0.2982, -0.3012, -0.3042, -0.3073, -0.3104, -0.3135, -0.3167, -0.3199,\n",
      "         -0.3231, -0.3264, -0.3297, -0.3330, -0.3364, -0.3398, -0.3432, -0.3467,\n",
      "         -0.3502, -0.3537, -0.3573, -0.3609, -0.3645, -0.3682, -0.3719, -0.3757,\n",
      "         -0.3795, -0.3833, -0.3872, -0.3911, -0.3951, -0.3990, -0.4031, -0.4071,\n",
      "         -0.4113, -0.4154, -0.4196, -0.4239, -0.4281, -0.4325, -0.4368, -0.4412,\n",
      "         -0.4457, -0.4502, -0.4547, -0.4593, -0.4640, -0.4687, -0.4734, -0.4782,\n",
      "         -0.4830, -0.4879, -0.4928, -0.4978, -0.5028, -0.5079, -0.5130, -0.5182,\n",
      "         -0.5234, -0.5287, -0.5341, -0.5395, -0.5449, -0.5504, -0.5560, -0.5616,\n",
      "         -0.5673, -0.5730, -0.5788, -0.5846, -0.5905, -0.5965, -0.6025, -0.6086,\n",
      "         -0.6148, -0.6210, -0.6272, -0.6336, -0.6400, -0.6464, -0.6530, -0.6596,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  0.1757,  0.1775,  0.1793,  0.1811,  0.1830,\n",
      "          0.1848,  0.1867,  0.1886,  0.1905,  0.1924,  0.1943,  0.1963,  0.1983,\n",
      "          0.2003,  0.2023,  0.2043,  0.2064,  0.2085,  0.2106,  0.2127,  0.2149,\n",
      "          0.2170,  0.2192,  0.2215,  0.2237,  0.2259,  0.2282,  0.2305,  0.2329,\n",
      "          0.2352,  0.2376,  0.2400,  0.2424,  0.2449,  0.2473,  0.2498,  0.2524,\n",
      "          0.2549,  0.2575,  0.2601,  0.2627,  0.2654,  0.2680,  0.2708,  0.2735,\n",
      "          0.2763,  0.2790,  0.2819,  0.2847,  0.2876,  0.2905,  0.2934,  0.2964,\n",
      "          0.2994,  0.3024,  0.3055,  0.3085,  0.3117,  0.3148,  0.3180,  0.3212,\n",
      "          0.3244,  0.3277,  0.3310,  0.3344,  0.3378,  0.3412,  0.3446,  0.3481,\n",
      "          0.3516,  0.3552,  0.3587,  0.3624,  0.3660,  0.3697,  0.3735,  0.3772,\n",
      "          0.3810,  0.3849,  0.3888,  0.3927,  0.3967,  0.4007,  0.4047,  0.4088,\n",
      "          0.4130,  0.4171,  0.4213,  0.4256,  0.4299,  0.4342,  0.4386,  0.4430,\n",
      "          0.4475,  0.4520,  0.4566,  0.4612,  0.4659,  0.4706,  0.4753,  0.4801,\n",
      "          0.4850,  0.4899,  0.4948,  0.4998,  0.5049,  0.5100,  0.5151,  0.5203,\n",
      "          0.5256,  0.5309,  0.5363,  0.5417,  0.5472,  0.5527,  0.5583,  0.5639,\n",
      "          0.5696,  0.5754,  0.5812,  0.5870,  0.5930,  0.5990,  0.6050,  0.6111,\n",
      "          0.6173,  0.6235,  0.6298,  0.6362,  0.6426,  0.6491,  0.6557,  0.6623,\n",
      "          0.6690,  0.6757,  0.6826,  0.6894,  0.6964,  0.7034,  0.7106,  0.7177,\n",
      "          0.7250,  0.7323,  0.7397,  0.7472,  0.7547,  0.7623,  0.7700,  0.7778,\n",
      "          0.7857,  0.7936,  0.8016,  0.8097,  0.8179,  0.8262,  0.8345,  0.8429,\n",
      "          0.8515,  0.8601,  0.8687,  0.8775,  0.8864,  0.8953,  0.9044,  0.9135,\n",
      "          0.9227,  0.9321,  0.9415,  0.9510,  0.9606,  0.9703,  0.9801,  0.9900,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6433,  0.9656], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1712, -3.7406, -0.5439, -2.0312,  0.4296, -2.3067, -4.3581,  0.1063,\n",
      "         -3.5587, -2.3465, -3.8791,  0.4950, -0.2166, -3.3962, -3.0863, -1.9374,\n",
      "         -3.1162, -1.6661, -1.9334, -0.7853, -1.2089, -1.2577, -0.7134,  0.4805,\n",
      "         -1.7537, -3.3521, -3.0431, -2.1854, -3.6925,  1.0697, -1.5742,  0.0106,\n",
      "         -1.0224, -3.4074, -1.9449, -2.0160, -0.9412, -3.0724, -1.4130,  0.2518,\n",
      "         -0.5925, -0.4831, -0.1360, -4.4370, -1.4456, -2.9831, -3.3464, -3.1577,\n",
      "         -0.1626, -0.1122, -3.8120, -1.4623, -7.3483, -2.6596, -2.8816, -0.7074,\n",
      "         -2.5847, -2.7563, -2.6312, -1.4463, -3.5735, -1.1376, -5.8848, -2.1807,\n",
      "         -3.4308,  1.2255, -1.1506,  0.1610, -0.4293, -3.0024, -3.9014, -5.8179,\n",
      "         -2.5692, -0.2625, -3.7335, -4.3012, -2.1883, -3.7748, -2.3465, -3.4654,\n",
      "         -5.0584, -2.2313, -0.1931, -1.1388, -1.8147, -2.8640, -1.6735, -0.4654,\n",
      "         -3.6124, -0.7494, -3.0599, -4.1223, -2.1886, -0.5202, -0.7167, -3.2291,\n",
      "         -2.6288, -3.3659, -3.2465, -3.2139,  0.0338, -0.8632, -3.7268, -2.2175,\n",
      "         -0.4445, -1.6717, -3.2319, -3.7705,  0.5104, -1.4664, -1.6271, -0.5328,\n",
      "         -4.3207, -4.5774,  0.6541, -0.7824, -2.1118, -1.1752, -0.0511, -0.7830,\n",
      "         -2.4473, -4.6169, -2.4658, -0.7324, -1.0060, -2.8537, -1.9822, -0.6302,\n",
      "         -0.3840, -4.1492, -0.6491, -0.0796, -3.2724, -4.1042, -1.1003, -2.0484,\n",
      "         -2.9222, -2.4536, -1.5044, -2.7607, -2.5011, -3.1530, -5.7374, -1.0763,\n",
      "         -0.7221, -0.7457, -0.5988, -0.6593, -0.6229, -0.7297, -4.8276, -4.9076,\n",
      "         -5.3465, -4.9488, -4.8792, -4.6109, -4.5140, -4.6857, -4.6393, -4.7188,\n",
      "         -4.8733, -4.9374, -4.9688, -4.9375, -4.8406, -4.7484, -4.7237, -4.7272,\n",
      "         -4.7083, -4.6613, -4.6172, -4.5985, -4.6042, -4.6165, -4.6232, -4.6062,\n",
      "         -4.5542, -4.4959, -4.4731, -4.4549, -4.4935, -4.5442, -4.5686, -4.5578,\n",
      "         -4.5459, -4.5194, -4.5259, -4.5457, -4.5525, -4.5590, -4.5695, -4.6261,\n",
      "         -4.6985, -4.7538, -4.7825, -4.7850, -4.7981, -4.8204, -4.8396, -4.8682,\n",
      "         -4.8743, -4.8902, -4.8736, -4.8433, -4.8367, -4.8358, -4.8336, -4.8162,\n",
      "         -4.7751, -4.7113, -4.7025, -4.6383, -4.6082, -4.5425, -4.5058, -4.3887,\n",
      "         -4.2604, -4.1805, -4.1260, -4.0916, -4.0748, -4.0744],\n",
      "        [-2.7442, -3.7477, -0.6530, -2.3927,  0.2864, -2.4156, -4.5959, -0.4104,\n",
      "         -4.1835, -2.6476, -4.0371,  0.4656, -0.3514, -3.5908, -2.9269, -2.2605,\n",
      "         -3.2291, -1.8141, -1.8557, -1.0557, -1.3431, -1.5526, -0.9408,  0.6052,\n",
      "         -2.0484, -3.5609, -2.9156, -2.4405, -3.6734,  1.1146, -1.7683, -0.3070,\n",
      "         -0.9873, -3.5337, -1.9937, -1.8296, -0.8550, -3.1069, -1.6392, -0.0119,\n",
      "         -0.6543, -0.4296, -0.1770, -4.3343, -1.6851, -3.4908, -3.6526, -3.2775,\n",
      "          0.6083,  0.6059, -3.9209, -1.5692, -7.3436, -2.8229, -2.8798, -0.8707,\n",
      "         -2.5647, -2.7903, -2.7606, -1.6726, -3.7400, -1.2336, -5.8973, -2.2944,\n",
      "         -3.4005,  1.3040, -1.3236, -0.1652, -0.3406, -3.1277, -4.0857, -5.7587,\n",
      "         -2.7240, -0.1494, -3.8751, -4.1960, -2.3432, -3.7647, -2.5214, -3.5996,\n",
      "         -4.9945, -2.3678, -0.2450, -1.4175, -1.8898, -3.1050, -1.8559, -0.7551,\n",
      "         -3.7686, -0.8425, -3.2192, -4.0333, -2.3207, -0.5594, -1.0098, -3.1330,\n",
      "         -2.8408, -3.7928, -3.3275, -3.3574, -0.0486, -1.1330, -3.7017, -2.2052,\n",
      "         -0.3438, -1.9489, -3.6192, -3.9494,  0.6518, -1.4549, -1.4208, -0.5424,\n",
      "         -4.7329, -4.6150,  0.6302, -1.0369, -2.2281, -1.3971, -0.2356, -0.8917,\n",
      "         -2.8026, -4.5974, -2.5076, -0.7066, -1.1868, -3.0023, -2.1478, -0.7713,\n",
      "         -0.6753, -4.2556,  0.1410,  0.6612, -3.2933, -4.1973, -1.2299, -2.0326,\n",
      "         -2.9346, -2.5562, -1.6303, -2.7542, -2.9278, -5.8456, -1.6354, -2.5759,\n",
      "         -1.5496, -0.8256, -0.5107, -4.4193, -3.8465, -0.3170, -2.0515, -1.0065,\n",
      "         -1.1181, -0.3475,  0.0889, -0.7989, -4.1673, -3.2127, -4.8638, -1.3489,\n",
      "          0.9325,  1.0060,  0.9506,  0.9815,  1.0277,  0.9746, -4.7581, -4.9239,\n",
      "         -5.3041, -5.1627, -5.4039, -5.4375, -5.4569, -5.4293, -5.4382, -5.4053,\n",
      "         -5.3174, -5.1925, -5.1177, -5.0919, -5.1242, -5.1606, -5.1837, -5.2040,\n",
      "         -5.2147, -5.2216, -5.2345, -5.2678, -5.3280, -5.3949, -5.4296, -5.4342,\n",
      "         -5.4163, -5.4078, -5.4384, -5.5031, -5.5444, -5.5500, -5.5248, -5.4811,\n",
      "         -5.4491, -5.4338, -5.4332, -5.4598, -5.4897, -5.5325, -5.5700, -5.6026,\n",
      "         -5.6205, -5.6121, -5.6211, -5.6381, -5.6626, -5.7118, -5.7468, -5.7552,\n",
      "         -5.7604, -5.7717, -5.7749, -5.7796, -5.7774, -5.7670]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6798,  0.9788], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.5763,  0.9900,  0.9900, -1.5570, -0.6982,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.3032e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0993, -0.1003, -0.1013,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1412, -0.1427, -0.1441,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.5621,  0.9656,  0.9656, -1.5261, -0.6809,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7385, -3.7377, -0.6426,  ..., -5.7372, -5.7310, -5.7079],\n",
      "        [-2.2373, -3.7294, -0.5239,  ..., -4.5651, -4.5257, -4.4925],\n",
      "        [-2.9072, -3.7046, -0.6458,  ..., -5.1814, -5.1385, -5.1141],\n",
      "        ...,\n",
      "        [-2.1493, -3.7404, -0.5280,  ..., -4.0535, -4.0387, -4.0381],\n",
      "        [-2.7278, -3.7362, -0.6461,  ..., -5.7065, -5.7066, -5.6870],\n",
      "        [-2.7461, -3.7380, -0.6442,  ..., -5.6476, -5.6389, -5.6239]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9738, -0.5800,  0.8885,  0.9762, -1.4679, -0.7157,  1.0037,  0.9762],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.7340, -0.6619,  0.9900,  0.9900,  0.9900, -0.6670],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.1202e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1485, -0.1500, -0.1515,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1349, -0.1363, -0.1377,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656,  0.9656, -0.7158, -0.6456,  0.9656,  0.9656,  0.9656, -0.6505],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7136, -3.7378, -0.6423,  ..., -5.7280, -5.7207, -5.7107],\n",
      "        [-2.6498, -3.7509, -0.6316,  ..., -5.6858, -5.7035, -5.7156],\n",
      "        [-2.1448, -3.7388, -0.5373,  ..., -3.9855, -3.9708, -3.9683],\n",
      "        ...,\n",
      "        [-2.6433, -3.7293, -0.6162,  ..., -5.6873, -5.7218, -5.7318],\n",
      "        [-2.7208, -3.7332, -0.6314,  ..., -5.7558, -5.7525, -5.7441],\n",
      "        [-2.1536, -3.7440, -0.5342,  ..., -4.0541, -4.0389, -4.0378]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9999,  0.9717, -0.7082, -0.6669,  0.9460,  0.9681,  0.9656, -0.6544],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900,  0.9900, -0.6530, -1.5592,  0.9900,  0.9900,  0.9900, -0.5970],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(3.2892e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(4.5502e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1983,  0.2003,  0.2023,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1321, -0.1334, -0.1348,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1028, -0.1039, -0.1049,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9608,  0.9656, -0.6369, -1.5284,  0.9656,  0.9656,  0.9656, -0.5823],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8822, -3.6912, -0.6375,  ..., -5.2011, -5.1248, -5.0700],\n",
      "        [-2.6353, -3.7257, -0.6142,  ..., -5.6809, -5.7133, -5.7263],\n",
      "        [-2.1465, -3.7347, -0.5363,  ..., -4.0625, -4.0475, -4.0444],\n",
      "        ...,\n",
      "        [-2.6369, -3.7371, -0.6355,  ..., -5.5281, -5.5468, -5.5530],\n",
      "        [-2.8948, -3.6914, -0.6389,  ..., -5.1309, -5.0906, -5.0715],\n",
      "        [-2.2204, -3.7251, -0.5308,  ..., -4.6289, -4.6022, -4.5698]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9571,  0.9675, -0.6530, -1.4660,  0.9047,  0.9630,  0.8349, -0.5562],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7039, -0.7022,  0.9900, -1.1965, -0.6856,  0.9900,  0.9900,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.5282e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1424, -0.1438, -0.1453,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1420, -0.1435, -0.1449,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.6865, -0.6848,  0.9656, -1.1441, -0.6687,  0.9656,  0.9656,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1231, -3.7334, -0.5205,  ..., -4.0482, -4.0309, -4.0307],\n",
      "        [-2.1261, -3.7374, -0.5329,  ..., -4.0286, -4.0102, -4.0056],\n",
      "        [-2.7035, -3.7376, -0.6340,  ..., -5.7735, -5.7753, -5.7588],\n",
      "        ...,\n",
      "        [-2.7054, -3.7307, -0.6395,  ..., -5.6942, -5.6962, -5.6763],\n",
      "        [-2.7142, -3.7385, -0.6449,  ..., -5.7687, -5.7677, -5.7543],\n",
      "        [-2.7035, -3.7376, -0.6340,  ..., -5.7735, -5.7753, -5.7588]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6950, -0.6439,  0.9776, -1.1501, -0.6347,  1.0012,  0.9747,  0.9776],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000,  0.9900, -0.6664, -0.7211,  0.9900, -0.7134, -0.7381, -1.3885],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(1.2314e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1079, -0.1090, -0.1101,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1348, -0.1362, -0.1375,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1443, -0.1458, -0.1473,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1493, -0.1508, -0.1523,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2698, -0.2726, -0.2753,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0518,  0.9656, -0.6499, -0.7033,  0.9656, -0.6958, -0.7199, -1.3277],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3161, -3.6762, -0.5504,  ..., -1.0121, -1.0417, -1.0521],\n",
      "        [-2.6368, -3.7439, -0.6275,  ..., -5.6717, -5.6859, -5.7003],\n",
      "        [-2.1328, -3.7288, -0.5318,  ..., -4.0817, -4.0647, -4.0613],\n",
      "        ...,\n",
      "        [-2.1282, -3.7383, -0.5260,  ..., -3.9939, -3.9787, -3.9740],\n",
      "        [-2.1040, -3.7322, -0.5191,  ..., -3.9796, -3.9639, -3.9607],\n",
      "        [-2.0117, -3.7385, -0.4720,  ..., -4.4011, -4.3495, -4.3136]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0205,  0.9677, -0.6498, -0.6734,  0.9470, -0.6852, -0.6810, -1.3168],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -0.7032, -0.7218, -0.5782,  0.9900,  0.9900, -0.6568,  0.9900],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 232])\n",
      "prune_penalty\n",
      "tensor(2.4202e-07, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(8.2489e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1423, -0.1437, -0.1452,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1460, -0.1475, -0.1490,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1496,  0.1512,  0.1527,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1329, -0.1342, -0.1356,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2003,  0.2023,  0.2043,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -0.6859, -0.7040, -0.5639,  0.9656,  0.9656, -0.6406,  0.9656],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8834, -3.6916, -0.6358,  ..., -5.1247, -5.0777, -5.0541],\n",
      "        [-2.1153, -3.7363, -0.5291,  ..., -4.0264, -4.0092, -4.0035],\n",
      "        [-2.1212, -3.7343, -0.5205,  ..., -3.9993, -3.9868, -3.9850],\n",
      "        ...,\n",
      "        [-2.6250, -3.7340, -0.6330,  ..., -5.5084, -5.5283, -5.5357],\n",
      "        [-2.1299, -3.7317, -0.5306,  ..., -4.0549, -4.0421, -4.0383],\n",
      "        [-2.8806, -3.6985, -0.6361,  ..., -5.1588, -5.1123, -5.0861]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([ 0.9109, -0.6384, -0.6716, -0.5685,  0.9557,  0.9615, -0.6397,  0.9084],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "turn_level_rewards\n",
      "tensor([-0.7195, -0.7395], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 232])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(3.3146e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1456, -0.1470, -0.1485, -0.1500, -0.1515, -0.1531, -0.1546, -0.1562,\n",
      "         -0.1578, -0.1593, -0.1610, -0.1626, -0.1642, -0.1659, -0.1676, -0.1692,\n",
      "         -0.1710, -0.1727, -0.1744, -0.1762, -0.1780, -0.1798, -0.1816, -0.1834,\n",
      "         -0.1853, -0.1871, -0.1890, -0.1909, -0.1929, -0.1948, -0.1968, -0.1988,\n",
      "         -0.2008, -0.2028, -0.2049, -0.2069, -0.2090, -0.2111, -0.2133, -0.2154,\n",
      "         -0.2176, -0.2198, -0.2220, -0.2243, -0.2265, -0.2288, -0.2311, -0.2335,\n",
      "         -0.2358, -0.2382, -0.2406, -0.2430, -0.2455, -0.2480, -0.2505, -0.2530,\n",
      "         -0.2556, -0.2581, -0.2607, -0.2634, -0.2660, -0.2687, -0.2714, -0.2742,\n",
      "         -0.2770, -0.2797, -0.2826, -0.2854, -0.2883, -0.2912, -0.2942, -0.2971,\n",
      "         -0.3001, -0.3032, -0.3062, -0.3093, -0.3124, -0.3156, -0.3188, -0.3220,\n",
      "         -0.3253, -0.3286, -0.3319, -0.3352, -0.3386, -0.3420, -0.3455, -0.3490,\n",
      "         -0.3525, -0.3561, -0.3597, -0.3633, -0.3670, -0.3707, -0.3744, -0.3782,\n",
      "         -0.3820, -0.3859, -0.3898, -0.3937, -0.3977, -0.4017, -0.4058, -0.4099,\n",
      "         -0.4140, -0.4182, -0.4224, -0.4267, -0.4310, -0.4353, -0.4397, -0.4442,\n",
      "         -0.4487, -0.4532, -0.4578, -0.4624, -0.4671, -0.4718, -0.4765, -0.4814,\n",
      "         -0.4862, -0.4911, -0.4961, -0.5011, -0.5062, -0.5113, -0.5164, -0.5217,\n",
      "         -0.5269, -0.5322, -0.5376, -0.5431, -0.5485, -0.5541, -0.5597, -0.5653,\n",
      "         -0.5710, -0.5768, -0.5826, -0.5885, -0.5945, -0.6005, -0.6065, -0.6127,\n",
      "         -0.6189, -0.6251, -0.6314, -0.6378, -0.6442, -0.6507, -0.6573, -0.6640,\n",
      "         -0.6707, -0.6774, -0.6843, -0.6912, -0.6982, -0.7052, -0.7124, -0.7195,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1496, -0.1511, -0.1526, -0.1542, -0.1557, -0.1573, -0.1589, -0.1605,\n",
      "         -0.1621, -0.1638, -0.1654, -0.1671, -0.1688, -0.1705, -0.1722, -0.1739,\n",
      "         -0.1757, -0.1775, -0.1793, -0.1811, -0.1829, -0.1848, -0.1866, -0.1885,\n",
      "         -0.1904, -0.1923, -0.1943, -0.1962, -0.1982, -0.2002, -0.2022, -0.2043,\n",
      "         -0.2064, -0.2084, -0.2105, -0.2127, -0.2148, -0.2170, -0.2192, -0.2214,\n",
      "         -0.2236, -0.2259, -0.2282, -0.2305, -0.2328, -0.2352, -0.2375, -0.2399,\n",
      "         -0.2424, -0.2448, -0.2473, -0.2498, -0.2523, -0.2548, -0.2574, -0.2600,\n",
      "         -0.2626, -0.2653, -0.2680, -0.2707, -0.2734, -0.2762, -0.2790, -0.2818,\n",
      "         -0.2846, -0.2875, -0.2904, -0.2933, -0.2963, -0.2993, -0.3023, -0.3054,\n",
      "         -0.3085, -0.3116, -0.3147, -0.3179, -0.3211, -0.3244, -0.3276, -0.3309,\n",
      "         -0.3343, -0.3377, -0.3411, -0.3445, -0.3480, -0.3515, -0.3551, -0.3587,\n",
      "         -0.3623, -0.3659, -0.3696, -0.3734, -0.3771, -0.3809, -0.3848, -0.3887,\n",
      "         -0.3926, -0.3966, -0.4006, -0.4046, -0.4087, -0.4128, -0.4170, -0.4212,\n",
      "         -0.4255, -0.4298, -0.4341, -0.4385, -0.4429, -0.4474, -0.4519, -0.4565,\n",
      "         -0.4611, -0.4658, -0.4705, -0.4752, -0.4800, -0.4849, -0.4898, -0.4947,\n",
      "         -0.4997, -0.5048, -0.5099, -0.5150, -0.5202, -0.5255, -0.5308, -0.5361,\n",
      "         -0.5415, -0.5470, -0.5525, -0.5581, -0.5638, -0.5695, -0.5752, -0.5810,\n",
      "         -0.5869, -0.5928, -0.5988, -0.6049, -0.6110, -0.6171, -0.6234, -0.6297,\n",
      "         -0.6360, -0.6424, -0.6489, -0.6555, -0.6621, -0.6688, -0.6756, -0.6824,\n",
      "         -0.6893, -0.6962, -0.7033, -0.7104, -0.7175, -0.7248, -0.7321, -0.7395,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 232])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-0.7018, -0.7213], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.1148e+00, -3.7303e+00, -5.1741e-01, -2.0201e+00,  4.4272e-01,\n",
      "         -2.3039e+00, -4.3391e+00,  1.2901e-01, -3.5050e+00, -2.3163e+00,\n",
      "         -3.8812e+00,  5.6132e-01, -1.5363e-01, -3.3823e+00, -3.0693e+00,\n",
      "         -1.9164e+00, -3.0866e+00, -1.6597e+00, -1.9040e+00, -7.6225e-01,\n",
      "         -1.1948e+00, -1.2294e+00, -7.0192e-01,  4.8127e-01, -1.7186e+00,\n",
      "         -3.3397e+00, -3.0306e+00, -2.1706e+00, -3.6876e+00,  1.0847e+00,\n",
      "         -1.5591e+00,  3.4849e-02, -1.0193e+00, -3.4160e+00, -1.9419e+00,\n",
      "         -2.0116e+00, -9.4771e-01, -3.0467e+00, -1.4007e+00,  2.5953e-01,\n",
      "         -5.8447e-01, -4.8627e-01, -1.3296e-01, -4.4290e+00, -1.4194e+00,\n",
      "         -2.9298e+00, -3.3227e+00, -3.1642e+00, -1.6213e-01, -1.6427e-01,\n",
      "         -3.8029e+00, -1.4423e+00, -7.3308e+00, -2.6523e+00, -2.8632e+00,\n",
      "         -6.9911e-01, -2.5869e+00, -2.7469e+00, -2.6106e+00, -1.4143e+00,\n",
      "         -3.5623e+00, -1.1186e+00, -5.8667e+00, -2.1699e+00, -3.4255e+00,\n",
      "          1.2425e+00, -1.1331e+00,  1.8820e-01, -4.2831e-01, -3.0057e+00,\n",
      "         -3.8942e+00, -5.8182e+00, -2.5568e+00, -2.9836e-01, -3.7261e+00,\n",
      "         -4.2882e+00, -2.1777e+00, -3.7526e+00, -2.3401e+00, -3.4561e+00,\n",
      "         -5.0564e+00, -2.2205e+00, -1.9243e-01, -1.1214e+00, -1.8152e+00,\n",
      "         -2.8471e+00, -1.6597e+00, -4.4097e-01, -3.5965e+00, -7.3313e-01,\n",
      "         -3.0481e+00, -4.1109e+00, -2.1778e+00, -5.2495e-01, -6.9652e-01,\n",
      "         -3.2174e+00, -2.6021e+00, -3.3180e+00, -3.2357e+00, -3.1989e+00,\n",
      "          3.9966e-02, -8.4443e-01, -3.7200e+00, -2.1864e+00, -4.4312e-01,\n",
      "         -1.6595e+00, -3.1882e+00, -3.7425e+00,  4.7776e-01, -1.4353e+00,\n",
      "         -1.6283e+00, -8.3868e-01, -4.0604e+00, -4.6004e+00,  6.2741e-01,\n",
      "         -8.0076e-01, -2.0680e+00, -1.2044e+00, -8.0796e-02, -8.4470e-01,\n",
      "         -2.4513e+00, -4.6243e+00, -2.4622e+00, -7.1062e-01, -1.0816e+00,\n",
      "         -2.9174e+00, -1.9884e+00, -6.0256e-01, -3.7051e-01, -4.2053e+00,\n",
      "         -6.6572e-01, -1.5316e-01, -3.2476e+00, -4.1244e+00, -1.0832e+00,\n",
      "         -2.0457e+00, -2.9238e+00, -2.4501e+00, -1.4806e+00, -2.8016e+00,\n",
      "         -2.5136e+00, -3.1207e+00, -5.6758e+00, -1.0605e+00, -5.7405e-01,\n",
      "         -6.6169e-01, -6.7642e-01, -6.9736e-01, -6.9053e-01, -7.2278e-01,\n",
      "         -4.8430e+00, -4.9285e+00, -5.3839e+00, -4.9170e+00, -4.8228e+00,\n",
      "         -4.4884e+00, -4.3651e+00, -4.5488e+00, -4.5468e+00, -4.6592e+00,\n",
      "         -4.8170e+00, -4.8727e+00, -4.8988e+00, -4.8741e+00, -4.7932e+00,\n",
      "         -4.7089e+00, -4.6655e+00, -4.6373e+00, -4.6032e+00, -4.5590e+00,\n",
      "         -4.5213e+00, -4.4986e+00, -4.4972e+00, -4.5101e+00, -4.5273e+00,\n",
      "         -4.5316e+00, -4.4888e+00, -4.4255e+00, -4.3887e+00, -4.3616e+00,\n",
      "         -4.3822e+00, -4.4250e+00, -4.4479e+00, -4.4486e+00, -4.4457e+00,\n",
      "         -4.4276e+00, -4.4431e+00, -4.4535e+00, -4.4562e+00, -4.4511e+00,\n",
      "         -4.4525e+00, -4.4880e+00, -4.5561e+00, -4.6244e+00, -4.6617e+00,\n",
      "         -4.6725e+00, -4.6754e+00, -4.6842e+00, -4.7057e+00, -4.7491e+00,\n",
      "         -4.7689e+00, -4.7920e+00, -4.7853e+00, -4.7662e+00, -4.7657e+00,\n",
      "         -4.7791e+00, -4.7808e+00, -4.7730e+00, -4.7391e+00, -4.6951e+00,\n",
      "         -4.6880e+00, -4.6320e+00, -4.5714e+00, -4.5094e+00, -4.4770e+00,\n",
      "         -4.3485e+00, -4.2473e+00, -4.1710e+00, -4.1180e+00, -4.0838e+00,\n",
      "         -4.0666e+00, -4.0612e+00],\n",
      "        [-2.1197e+00, -3.7327e+00, -5.2789e-01, -2.0219e+00,  4.4282e-01,\n",
      "         -2.2997e+00, -4.3373e+00,  1.2685e-01, -3.5075e+00, -2.3131e+00,\n",
      "         -3.8718e+00,  5.5341e-01, -1.4906e-01, -3.3843e+00, -3.0722e+00,\n",
      "         -1.9148e+00, -3.0864e+00, -1.6581e+00, -1.9135e+00, -7.5758e-01,\n",
      "         -1.1998e+00, -1.2293e+00, -6.9645e-01,  4.8280e-01, -1.7183e+00,\n",
      "         -3.3399e+00, -3.0334e+00, -2.1681e+00, -3.6876e+00,  1.0810e+00,\n",
      "         -1.5599e+00,  3.9099e-02, -1.0178e+00, -3.4060e+00, -1.9353e+00,\n",
      "         -2.0180e+00, -9.5109e-01, -3.0464e+00, -1.4021e+00,  2.6663e-01,\n",
      "         -5.8602e-01, -4.9015e-01, -1.3042e-01, -4.4327e+00, -1.4170e+00,\n",
      "         -2.9318e+00, -3.3180e+00, -3.1548e+00, -1.6919e-01, -1.6130e-01,\n",
      "         -3.8017e+00, -1.4388e+00, -7.3404e+00, -2.6479e+00, -2.8641e+00,\n",
      "         -6.9672e-01, -2.5913e+00, -2.7502e+00, -2.6110e+00, -1.4143e+00,\n",
      "         -3.5616e+00, -1.1171e+00, -5.8757e+00, -2.1692e+00, -3.4246e+00,\n",
      "          1.2367e+00, -1.1364e+00,  1.9236e-01, -4.2684e-01, -2.9963e+00,\n",
      "         -3.8937e+00, -5.8083e+00, -2.5550e+00, -2.9625e-01, -3.7220e+00,\n",
      "         -4.2919e+00, -2.1758e+00, -3.7540e+00, -2.3385e+00, -3.4543e+00,\n",
      "         -5.0482e+00, -2.2177e+00, -1.8687e-01, -1.1139e+00, -1.8087e+00,\n",
      "         -2.8438e+00, -1.6605e+00, -4.3825e-01, -3.5998e+00, -7.3672e-01,\n",
      "         -3.0459e+00, -4.1139e+00, -2.1763e+00, -5.1741e-01, -6.8987e-01,\n",
      "         -3.2248e+00, -2.6015e+00, -3.3196e+00, -3.2370e+00, -3.2009e+00,\n",
      "          4.5657e-02, -8.3850e-01, -3.7240e+00, -2.1868e+00, -4.3901e-01,\n",
      "         -1.6611e+00, -3.1897e+00, -3.7439e+00,  4.7703e-01, -1.6258e+00,\n",
      "         -1.6435e+00, -1.0552e+00, -4.1267e+00, -4.6672e+00,  6.4518e-01,\n",
      "         -7.6424e-01, -2.0591e+00, -1.1294e+00, -2.8392e-03, -7.7748e-01,\n",
      "         -2.3483e+00, -4.6616e+00, -2.4620e+00, -7.0306e-01, -9.2826e-01,\n",
      "         -2.7977e+00, -1.9784e+00, -6.1989e-01, -2.6731e-01, -4.1080e+00,\n",
      "         -7.1264e-01, -1.6109e-01, -3.2140e+00, -4.1292e+00, -1.0904e+00,\n",
      "         -2.0584e+00, -2.9555e+00, -2.4251e+00, -1.5021e+00, -2.7850e+00,\n",
      "         -2.5041e+00, -3.1513e+00, -5.7772e+00, -1.1349e+00, -6.3367e-01,\n",
      "         -6.3985e-01, -6.8294e-01, -7.0543e-01, -7.3250e-01, -7.2844e-01,\n",
      "         -4.8582e+00, -4.9256e+00, -5.3318e+00, -4.9303e+00, -4.8378e+00,\n",
      "         -4.5609e+00, -4.4569e+00, -4.6035e+00, -4.5334e+00, -4.6027e+00,\n",
      "         -4.7439e+00, -4.8256e+00, -4.8808e+00, -4.8655e+00, -4.7786e+00,\n",
      "         -4.6831e+00, -4.6453e+00, -4.6329e+00, -4.6040e+00, -4.5565e+00,\n",
      "         -4.5110e+00, -4.4856e+00, -4.4841e+00, -4.4916e+00, -4.4941e+00,\n",
      "         -4.4824e+00, -4.4300e+00, -4.3790e+00, -4.3540e+00, -4.3400e+00,\n",
      "         -4.3689e+00, -4.4019e+00, -4.4078e+00, -4.3847e+00, -4.3776e+00,\n",
      "         -4.3573e+00, -4.3799e+00, -4.4107e+00, -4.4258e+00, -4.4373e+00,\n",
      "         -4.4510e+00, -4.5038e+00, -4.5622e+00, -4.6090e+00, -4.6324e+00,\n",
      "         -4.6311e+00, -4.6479e+00, -4.6741e+00, -4.6991e+00, -4.7250e+00,\n",
      "         -4.7323e+00, -4.7447e+00, -4.7385e+00, -4.7268e+00, -4.7269e+00,\n",
      "         -4.7272e+00, -4.7089e+00, -4.6841e+00, -4.6394e+00, -4.5690e+00,\n",
      "         -4.5592e+00, -4.5039e+00, -4.4771e+00, -4.4068e+00, -4.3503e+00,\n",
      "         -4.2406e+00, -4.1307e+00, -4.0635e+00, -4.0136e+00, -3.9821e+00,\n",
      "         -3.9678e+00, -3.9618e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6705, -0.6871], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "  #  remov\n",
    "  if i < 3:\n",
    "    continue\n",
    "  # Train on the existing teacher data\n",
    "  dqn_trainer.update(1.0, i == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([15643], device='cuda:0')\n",
      "logit at token tensor([0.1533], device='cuda:0')\n",
      "max logit tensor(0.1533, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:fin\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([680], device='cuda:0')\n",
      "logit at token tensor([-0.3629], device='cuda:0')\n",
      "max logit tensor(-0.3629, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643,   680]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:finish\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([2124], device='cuda:0')\n",
      "logit at token tensor([1.2185], device='cuda:0')\n",
      "max logit tensor(1.2185, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643,   680,  2124]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:finish x\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([796], device='cuda:0')\n",
      "logit at token tensor([0.7879], device='cuda:0')\n",
      "max logit tensor(0.7879, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643,   680,  2124,   796]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:finish x =\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([8699], device='cuda:0')\n",
      "logit at token tensor([0.4058], device='cuda:0')\n",
      "max logit tensor(0.4058, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643,   680,  2124,   796,  8699]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:finish x = 78\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([50256], device='cuda:0')\n",
      "logit at token tensor([0.4804], device='cuda:0')\n",
      "max logit tensor(0.4804, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           838,  1343,   838,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 15643,   680,  2124,   796,  8699,\n",
      "         50256]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 10 + 10\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:finish x = 78<|endoftext|>\n",
      "completion response ids\n",
      "tensor([[15643,   680,  2124,   796,  8699, 50256]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finish x = 78',\n",
       " {'z': tensor([[ 0.3084,  0.2326, -0.0653,  ..., -0.0443, -0.4103,  0.1985]],\n",
       "         device='cuda:0'),\n",
       "  'response_ids': tensor([[15643,   680,  2124,   796,  8699, 50256]])})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are to solve the an algebraic expression.\n",
    "You have access to two commands:\n",
    "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
    "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
    "The following is the algebraic expression:\n",
    "x = 10 + 10\n",
    "What is the value of x?\n",
    "When you have an answer, use the 'finish' command to submit your answer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "player:\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved at 300\n",
      "## DEBUG sim run len\n",
      "4530\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([32604], device='cuda:0')\n",
      "logit at token tensor([-0.6913], device='cuda:0')\n",
      "max logit tensor(-0.6913, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([220], device='cuda:0')\n",
      "logit at token tensor([-0.7808], device='cuda:0')\n",
      "max logit tensor(-0.7808, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604,   220]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean \n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.8180], device='cuda:0')\n",
      "max logit tensor(-0.8180, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604,   220,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean  79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([860], device='cuda:0')\n",
      "logit at token tensor([-0.7564], device='cuda:0')\n",
      "max logit tensor(-0.7564, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604,   220,  9225,   860]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean  79 9\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([860], device='cuda:0')\n",
      "logit at token tensor([-0.8167], device='cuda:0')\n",
      "max logit tensor(-0.8167, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604,   220,  9225,   860,   860]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean  79 9 9\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([50256], device='cuda:0')\n",
      "logit at token tensor([-0.6935], device='cuda:0')\n",
      "max logit tensor(-0.6935, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25, 32604,   220,  9225,   860,   860,\n",
      "         50256]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:mean  79 9 9<|endoftext|>\n",
      "completion response ids\n",
      "tensor([[32604,   220,  9225,   860,   860, 50256]], device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.8945], device='cuda:0')\n",
      "max logit tensor(-0.8945, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0035], device='cuda:0')\n",
      "max logit tensor(-1.0035, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9927], device='cuda:0')\n",
      "max logit tensor(-0.9927, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9981], device='cuda:0')\n",
      "max logit tensor(-0.9981, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9952], device='cuda:0')\n",
      "max logit tensor(-0.9952, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9456], device='cuda:0')\n",
      "max logit tensor(-0.9456, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.8944], device='cuda:0')\n",
      "max logit tensor(-0.8944, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9059], device='cuda:0')\n",
      "max logit tensor(-0.9059, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9286], device='cuda:0')\n",
      "max logit tensor(-0.9286, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: mean  79 9 9\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.6885, -0.7799, -0.8172, -0.7574, -0.8186, -0.6797,\n",
      "         -0.9899]], device='cuda:0')\n",
      "## DEBUG priority: 0.4626547396183014\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9496], device='cuda:0')\n",
      "max logit tensor(-0.9496, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0032], device='cuda:0')\n",
      "max logit tensor(-1.0032, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9935], device='cuda:0')\n",
      "max logit tensor(-0.9935, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0105], device='cuda:0')\n",
      "max logit tensor(-1.0105, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0116], device='cuda:0')\n",
      "max logit tensor(-1.0116, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0243], device='cuda:0')\n",
      "max logit tensor(-1.0243, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0182], device='cuda:0')\n",
      "max logit tensor(-1.0182, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0349], device='cuda:0')\n",
      "max logit tensor(-1.0349, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0848], device='cuda:0')\n",
      "max logit tensor(-1.0848, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.8998, -1.0105, -0.9999, -1.0055, -1.0022, -0.9526, -0.9026, -0.9138,\n",
      "         -0.9358, -1.1731]], device='cuda:0')\n",
      "## DEBUG priority: 0.7561634182929993\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9693], device='cuda:0')\n",
      "max logit tensor(-0.9693, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0544], device='cuda:0')\n",
      "max logit tensor(-1.0544, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0104], device='cuda:0')\n",
      "max logit tensor(-1.0104, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0357], device='cuda:0')\n",
      "max logit tensor(-1.0357, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0475], device='cuda:0')\n",
      "max logit tensor(-1.0475, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0528], device='cuda:0')\n",
      "max logit tensor(-1.0528, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0490], device='cuda:0')\n",
      "max logit tensor(-1.0490, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0721], device='cuda:0')\n",
      "max logit tensor(-1.0721, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1358], device='cuda:0')\n",
      "max logit tensor(-1.1358, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.9545, -1.0099, -0.9991, -1.0168, -1.0182, -1.0302,\n",
      "         -1.0246, -1.0410, -1.0903, -1.3468]], device='cuda:0')\n",
      "## DEBUG priority: 0.8895232081413269\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9636], device='cuda:0')\n",
      "max logit tensor(-0.9636, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0607], device='cuda:0')\n",
      "max logit tensor(-1.0607, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0023], device='cuda:0')\n",
      "max logit tensor(-1.0023, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0337], device='cuda:0')\n",
      "max logit tensor(-1.0337, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0455], device='cuda:0')\n",
      "max logit tensor(-1.0455, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0549], device='cuda:0')\n",
      "max logit tensor(-1.0549, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0565], device='cuda:0')\n",
      "max logit tensor(-1.0565, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0821], device='cuda:0')\n",
      "max logit tensor(-1.0821, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1618], device='cuda:0')\n",
      "max logit tensor(-1.1618, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.9726, -1.0588, -1.0153, -1.0415,\n",
      "         -1.0523, -1.0573, -1.0543, -1.0762, -1.1402, -1.3722]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.9488103985786438\n",
      "done\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "          7618,  1343,  8275,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25,  1612,   220,  9225,   860,   860,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,   198,\n",
      "           198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 58 + 67\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: mean  79 9 9\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.9667, -1.0652,\n",
      "         -1.0062, -1.0375, -1.0489, -1.0587, -1.0600, -1.0866, -1.1657, -1.3452]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.0002535274834372103\n",
      "## DEBUG sim run len\n",
      "4545\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9948], device='cuda:0')\n",
      "logit at token tensor([-0.7153], device='cuda:0')\n",
      "max logit tensor(-0.7153, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:cal\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([66], device='cuda:0')\n",
      "logit at token tensor([-0.6583], device='cuda:0')\n",
      "max logit tensor(-0.6583, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.5922], device='cuda:0')\n",
      "max logit tensor(-0.5922, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([1343], device='cuda:0')\n",
      "logit at token tensor([-0.7589], device='cuda:0')\n",
      "max logit tensor(-0.7589, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 +\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.7113], device='cuda:0')\n",
      "max logit tensor(-0.7113, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.7709], device='cuda:0')\n",
      "max logit tensor(-0.7709, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.6062], device='cuda:0')\n",
      "max logit tensor(-0.6062, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.4842], device='cuda:0')\n",
      "max logit tensor(-0.4842, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([50256], device='cuda:0')\n",
      "logit at token tensor([-0.4326], device='cuda:0')\n",
      "max logit tensor(-0.4326, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225,  9225, 50256]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79 79<|endoftext|>\n",
      "completion response ids\n",
      "tensor([[ 9948,    66,  9225,  1343,  9225,  9225,  9225,  9225, 50256]],\n",
      "       device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9446], device='cuda:0')\n",
      "max logit tensor(-0.9446, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0917], device='cuda:0')\n",
      "max logit tensor(-1.0917, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9872], device='cuda:0')\n",
      "max logit tensor(-0.9872, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9551], device='cuda:0')\n",
      "max logit tensor(-0.9551, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9964], device='cuda:0')\n",
      "max logit tensor(-0.9964, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0362], device='cuda:0')\n",
      "max logit tensor(-1.0362, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0188], device='cuda:0')\n",
      "max logit tensor(-1.0188, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0208], device='cuda:0')\n",
      "max logit tensor(-1.0208, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0145], device='cuda:0')\n",
      "max logit tensor(-1.0145, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 79 + 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.7185, -0.6630, -0.5901, -0.7425, -0.7098, -0.7704,\n",
      "         -0.6058, -0.4854, -0.4204, -0.9876]], device='cuda:0')\n",
      "## DEBUG priority: 0.3128964304924011\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9553], device='cuda:0')\n",
      "max logit tensor(-0.9553, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0554], device='cuda:0')\n",
      "max logit tensor(-1.0554, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0132], device='cuda:0')\n",
      "max logit tensor(-1.0132, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0137], device='cuda:0')\n",
      "max logit tensor(-1.0137, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0358], device='cuda:0')\n",
      "max logit tensor(-1.0358, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0724], device='cuda:0')\n",
      "max logit tensor(-1.0724, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0735], device='cuda:0')\n",
      "max logit tensor(-1.0735, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0706], device='cuda:0')\n",
      "max logit tensor(-1.0706, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1116], device='cuda:0')\n",
      "max logit tensor(-1.1116, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.9511, -1.0996, -0.9941, -0.9624, -1.0052, -1.0443,\n",
      "         -1.0264, -1.0288, -1.0223, -1.1961]], device='cuda:0')\n",
      "## DEBUG priority: 0.8519458770751953\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9675], device='cuda:0')\n",
      "max logit tensor(-0.9675, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0975], device='cuda:0')\n",
      "max logit tensor(-1.0975, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0207], device='cuda:0')\n",
      "max logit tensor(-1.0207, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0281], device='cuda:0')\n",
      "max logit tensor(-1.0281, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0513], device='cuda:0')\n",
      "max logit tensor(-1.0513, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0787], device='cuda:0')\n",
      "max logit tensor(-1.0787, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0832], device='cuda:0')\n",
      "max logit tensor(-1.0832, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0967], device='cuda:0')\n",
      "max logit tensor(-1.0967, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1542], device='cuda:0')\n",
      "max logit tensor(-1.1542, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.9596, -1.0601, -1.0194, -1.0194,\n",
      "         -1.0426, -1.0792, -1.0785, -1.0768, -1.1169, -1.3291]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.937341034412384\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9629], device='cuda:0')\n",
      "max logit tensor(-0.9629, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0988], device='cuda:0')\n",
      "max logit tensor(-1.0988, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0176], device='cuda:0')\n",
      "max logit tensor(-1.0176, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0229], device='cuda:0')\n",
      "max logit tensor(-1.0229, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0458], device='cuda:0')\n",
      "max logit tensor(-1.0458, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0663], device='cuda:0')\n",
      "max logit tensor(-1.0663, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0684], device='cuda:0')\n",
      "max logit tensor(-1.0684, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0833], device='cuda:0')\n",
      "max logit tensor(-1.0833, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1540], device='cuda:0')\n",
      "max logit tensor(-1.1540, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.9718, -1.1016,\n",
      "         -1.0253, -1.0333, -1.0556, -1.0840, -1.0880, -1.1015, -1.1587, -1.3361]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.9714096784591675\n",
      "done\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         25307,  1343,  4317,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 164 + 70\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.9661, -1.1030, -1.0214, -1.0273, -1.0500, -1.0701, -1.0715, -1.0874,\n",
      "         -1.1589, -1.3310]], device='cuda:0')\n",
      "## DEBUG priority: 0.00012879895803052932\n",
      "## DEBUG sim run len\n",
      "4560\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9948], device='cuda:0')\n",
      "logit at token tensor([-0.6573], device='cuda:0')\n",
      "max logit tensor(-0.6573, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:cal\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([66], device='cuda:0')\n",
      "logit at token tensor([-0.6975], device='cuda:0')\n",
      "max logit tensor(-0.6975, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.6117], device='cuda:0')\n",
      "max logit tensor(-0.6117, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([1343], device='cuda:0')\n",
      "logit at token tensor([-0.7381], device='cuda:0')\n",
      "max logit tensor(-0.7381, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 +\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.7218], device='cuda:0')\n",
      "max logit tensor(-0.7218, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.7063], device='cuda:0')\n",
      "max logit tensor(-0.7063, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.5915], device='cuda:0')\n",
      "max logit tensor(-0.5915, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.4993], device='cuda:0')\n",
      "max logit tensor(-0.4993, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([50256], device='cuda:0')\n",
      "logit at token tensor([-0.5476], device='cuda:0')\n",
      "max logit tensor(-0.5476, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,  1343,  9225,\n",
      "          9225,  9225,  9225, 50256]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 + 79 79 79 79<|endoftext|>\n",
      "completion response ids\n",
      "tensor([[ 9948,    66,  9225,  1343,  9225,  9225,  9225,  9225, 50256]],\n",
      "       device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9307], device='cuda:0')\n",
      "max logit tensor(-0.9307, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0827], device='cuda:0')\n",
      "max logit tensor(-1.0827, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9798], device='cuda:0')\n",
      "max logit tensor(-0.9798, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9500], device='cuda:0')\n",
      "max logit tensor(-0.9500, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9980], device='cuda:0')\n",
      "max logit tensor(-0.9980, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0356], device='cuda:0')\n",
      "max logit tensor(-1.0356, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0139], device='cuda:0')\n",
      "max logit tensor(-1.0139, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0133], device='cuda:0')\n",
      "max logit tensor(-1.0133, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0077], device='cuda:0')\n",
      "max logit tensor(-1.0077, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 79 + 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.6585, -0.7012, -0.6092, -0.7221, -0.7208, -0.7038,\n",
      "         -0.5902, -0.4998, -0.5358, -0.9963]], device='cuda:0')\n",
      "## DEBUG priority: 0.3178405165672302\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9489], device='cuda:0')\n",
      "max logit tensor(-0.9489, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0557], device='cuda:0')\n",
      "max logit tensor(-1.0557, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0118], device='cuda:0')\n",
      "max logit tensor(-1.0118, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0134], device='cuda:0')\n",
      "max logit tensor(-1.0134, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0390], device='cuda:0')\n",
      "max logit tensor(-1.0390, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0757], device='cuda:0')\n",
      "max logit tensor(-1.0757, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0719], device='cuda:0')\n",
      "max logit tensor(-1.0719, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0668], device='cuda:0')\n",
      "max logit tensor(-1.0668, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1068], device='cuda:0')\n",
      "max logit tensor(-1.1068, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,\n",
      "            25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.9378, -1.0917, -0.9884, -0.9577, -1.0064, -1.0436,\n",
      "         -1.0219, -1.0223, -1.0157, -1.1995]], device='cuda:0')\n",
      "## DEBUG priority: 0.8436747789382935\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9609], device='cuda:0')\n",
      "max logit tensor(-0.9609, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0935], device='cuda:0')\n",
      "max logit tensor(-1.0935, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0161], device='cuda:0')\n",
      "max logit tensor(-1.0161, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0237], device='cuda:0')\n",
      "max logit tensor(-1.0237, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0492], device='cuda:0')\n",
      "max logit tensor(-1.0492, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0772], device='cuda:0')\n",
      "max logit tensor(-1.0772, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0800], device='cuda:0')\n",
      "max logit tensor(-1.0800, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0907], device='cuda:0')\n",
      "max logit tensor(-1.0907, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1473], device='cuda:0')\n",
      "max logit tensor(-1.1473, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.9538, -1.0618, -1.0174, -1.0180,\n",
      "         -1.0442, -1.0805, -1.0775, -1.0723, -1.1124, -1.3254]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.9337640404701233\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9566], device='cuda:0')\n",
      "max logit tensor(-0.9566, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0926], device='cuda:0')\n",
      "max logit tensor(-1.0926, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0099], device='cuda:0')\n",
      "max logit tensor(-1.0099, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0179], device='cuda:0')\n",
      "max logit tensor(-1.0179, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0411], device='cuda:0')\n",
      "max logit tensor(-1.0411, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0635], device='cuda:0')\n",
      "max logit tensor(-1.0635, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0637], device='cuda:0')\n",
      "max logit tensor(-1.0637, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0777], device='cuda:0')\n",
      "max logit tensor(-1.0777, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1486], device='cuda:0')\n",
      "max logit tensor(-1.1486, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.9649, -1.0980,\n",
      "         -1.0199, -1.0296, -1.0535, -1.0826, -1.0849, -1.0959, -1.1517, -1.3337]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.9633376002311707\n",
      "done\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "         39768,  1343,  4019,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,  1343,  9225,  9225,  9225,\n",
      "          9225,   198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198,\n",
      "         10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,\n",
      "            25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003,\n",
      "         42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 274 + 80\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 + 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.9600, -1.0966, -1.0142, -1.0221, -1.0456, -1.0668, -1.0677, -1.0820,\n",
      "         -1.1523, -1.3232]], device='cuda:0')\n",
      "## DEBUG priority: 0.0002874807105399668\n",
      "## DEBUG sim run len\n",
      "4575\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9948], device='cuda:0')\n",
      "logit at token tensor([-0.7606], device='cuda:0')\n",
      "max logit tensor(-0.7606, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:cal\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([66], device='cuda:0')\n",
      "logit at token tensor([-0.6555], device='cuda:0')\n",
      "max logit tensor(-0.6555, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0282], device='cuda:0')\n",
      "max logit tensor(-1.0282, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([542], device='cuda:0')\n",
      "logit at token tensor([-0.8811], device='cuda:0')\n",
      "max logit tensor(-0.8811, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,   542]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 cont\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.5466], device='cuda:0')\n",
      "max logit tensor(-0.5466, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,   542,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 cont 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.7223], device='cuda:0')\n",
      "max logit tensor(-0.7223, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,   542,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 cont 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.6475], device='cuda:0')\n",
      "max logit tensor(-0.6475, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,   542,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 cont 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([50256], device='cuda:0')\n",
      "logit at token tensor([-0.4592], device='cuda:0')\n",
      "max logit tensor(-0.4592, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25,  9948,    66,  9225,   542,  9225,\n",
      "          9225,  9225, 50256]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:calc 79 cont 79 79 79<|endoftext|>\n",
      "completion response ids\n",
      "tensor([[ 9948,    66,  9225,   542,  9225,  9225,  9225, 50256]],\n",
      "       device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9557], device='cuda:0')\n",
      "max logit tensor(-0.9557, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1273], device='cuda:0')\n",
      "max logit tensor(-1.1273, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0198], device='cuda:0')\n",
      "max logit tensor(-1.0198, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9762], device='cuda:0')\n",
      "max logit tensor(-0.9762, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0008], device='cuda:0')\n",
      "max logit tensor(-1.0008, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9757], device='cuda:0')\n",
      "max logit tensor(-0.9757, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9599], device='cuda:0')\n",
      "max logit tensor(-0.9599, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0016], device='cuda:0')\n",
      "max logit tensor(-1.0016, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0262], device='cuda:0')\n",
      "max logit tensor(-1.0262, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   628,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "player:\n",
      "response: calc 79 cont 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.7672, -0.6618, -1.0270, -0.8752, -0.5446, -0.7200,\n",
      "         -0.6453, -0.4476, -1.0789]], device='cuda:0')\n",
      "## DEBUG priority: 0.41210171580314636\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9815], device='cuda:0')\n",
      "max logit tensor(-0.9815, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0968], device='cuda:0')\n",
      "max logit tensor(-1.0968, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0442], device='cuda:0')\n",
      "max logit tensor(-1.0442, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0431], device='cuda:0')\n",
      "max logit tensor(-1.0431, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0588], device='cuda:0')\n",
      "max logit tensor(-1.0588, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0635], device='cuda:0')\n",
      "max logit tensor(-1.0635, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0329], device='cuda:0')\n",
      "max logit tensor(-1.0329, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0370], device='cuda:0')\n",
      "max logit tensor(-1.0370, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0914], device='cuda:0')\n",
      "max logit tensor(-1.0914, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.9631, -1.1368, -1.0275, -0.9843, -1.0098, -0.9853, -0.9683,\n",
      "         -1.0092, -1.0345, -1.2049]], device='cuda:0')\n",
      "## DEBUG priority: 0.8507416844367981\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9931], device='cuda:0')\n",
      "max logit tensor(-0.9931, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1318], device='cuda:0')\n",
      "max logit tensor(-1.1318, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0490], device='cuda:0')\n",
      "max logit tensor(-1.0490, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0631], device='cuda:0')\n",
      "max logit tensor(-1.0631, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0775], device='cuda:0')\n",
      "max logit tensor(-1.0775, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0787], device='cuda:0')\n",
      "max logit tensor(-1.0787, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0514], device='cuda:0')\n",
      "max logit tensor(-1.0514, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0607], device='cuda:0')\n",
      "max logit tensor(-1.0607, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1323], device='cuda:0')\n",
      "max logit tensor(-1.1323, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.9871, -1.1039, -1.0510, -1.0501, -1.0662,\n",
      "         -1.0699, -1.0397, -1.0438, -1.0967, -1.3118]], device='cuda:0')\n",
      "## DEBUG priority: 0.9448043704032898\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-0.9959], device='cuda:0')\n",
      "max logit tensor(-0.9959, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1492], device='cuda:0')\n",
      "max logit tensor(-1.1492, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0535], device='cuda:0')\n",
      "max logit tensor(-1.0535, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0587], device='cuda:0')\n",
      "max logit tensor(-1.0587, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0751], device='cuda:0')\n",
      "max logit tensor(-1.0751, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0743], device='cuda:0')\n",
      "max logit tensor(-1.0743, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225]],\n",
      "       device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0475], device='cuda:0')\n",
      "max logit tensor(-1.0475, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.0574], device='cuda:0')\n",
      "max logit tensor(-1.0574, device='cuda:0')\n",
      "DEBUG GENERATE\n",
      "seq_cat tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79\n",
      "DEBUG SAMPLE TOKENS\n",
      "token tensor([9225], device='cuda:0')\n",
      "logit at token tensor([-1.1411], device='cuda:0')\n",
      "max logit tensor(-1.1411, device='cuda:0')\n",
      "--------- DEBUG GENERATE FINAL -------- \n",
      "final_seq tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225]], device='cuda:0')\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "completion response ids\n",
      "tensor([[9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225, 9225]],\n",
      "       device='cuda:0')\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.9971, -1.1358, -1.0534,\n",
      "         -1.0684, -1.0826, -1.0837, -1.0556, -1.0664, -1.1370, -1.3241]],\n",
      "       device='cuda:0')\n",
      "## DEBUG priority: 0.9809104800224304\n",
      "done\n",
      "## DEBUG prompt\n",
      "\n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "tensor([[  198,  1639,   389,   284,  8494,   262,   281, 37139,   291,  5408,\n",
      "            13,   198,  1639,   423,  1895,   284,   734,  9729,    25,   198,\n",
      "            12,   705,  9948,    66,  1279, 38011,    29,     6,   284, 15284,\n",
      "           262,  1988,   286,   281,  5408,    13,  1279, 38011,    29,  1276,\n",
      "           307,   287,   262,  1296,   705,    64,  1343,   275,     6,   810,\n",
      "           257,   290,   275,   389, 37014,    13,   198,    12,   705, 15643,\n",
      "           680,  1279,    82,  2122,    29,     6,   284,  9199,   534,  3280,\n",
      "            13,  1279,    82,  2122,    29,  1276,   307,   287,   262,  1296,\n",
      "           705,    27,  9291,    29,   796,  1279, 17618,    29,     6,   810,\n",
      "          1279,  9291,    29,   318,   262,  3850,   973,   287,   262,  1917,\n",
      "           290,  1279, 17618,    29,   318,   262,  4610,    13,   198,   464,\n",
      "          1708,   318,   262, 37139,   291,  5408,    25,   198,    87,   796,\n",
      "           362,  1343, 31773,   198,  2061,   318,   262,  1988,   286,  2124,\n",
      "            30,   198,  2215,   345,   423,   281,  3280,    11,   779,   262,\n",
      "           705, 15643,   680,     6,  3141,   284,  9199,   534,  3280,    13,\n",
      "           628,   198,  7829,    25, 42302,  9225,   542,  9225,  9225,  9225,\n",
      "           198, 10456,   469,    25, 26003, 42790,   198,  7829,    25,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,\n",
      "           469,    25, 26003, 42790,   198,  7829,    25,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,  9225,  9225,   198, 10456,   469,    25,\n",
      "         26003, 42790,   198,  7829,    25,  9225,  9225,  9225,  9225,  9225,\n",
      "          9225,  9225,  9225,  9225,   198, 10456,   469,    25, 26003, 42790,\n",
      "           198,   198,  7829,    25]])\n",
      "prompt: \n",
      "You are to solve the an algebraic expression.\n",
      "You have access to two commands:\n",
      "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
      "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
      "The following is the algebraic expression:\n",
      "x = 2 + 232\n",
      "What is the value of x?\n",
      "When you have an answer, use the 'finish' command to submit your answer.\n",
      "\n",
      "\n",
      "player: calc 79 cont 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "player: 79 79 79 79 79 79 79 79 79\n",
      "judge: Cannot decipher\n",
      "\n",
      "player:\n",
      "response:  79 79 79 79 79 79 79 79 79\n",
      "## DEBUG tmp: actor_log_probs * response_mask[:, 1:]\n",
      "tensor([[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.9995,\n",
      "         -1.1528, -1.0574, -1.0627, -1.0796, -1.0788, -1.0511, -1.0611, -1.1447,\n",
      "         -1.3199]], device='cuda:0')\n",
      "## DEBUG priority: 5.733782018069178e-07\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5273, -1.1000, -1.5635, -1.2160, -1.0526, -1.0915, -1.5471, -1.5090],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2088, -0.2109, -0.2130,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1766, -0.1784, -0.1802,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2121, -0.2143, -0.2164,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1747, -0.1765, -0.1783,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2063, -0.2084, -0.2105,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4604, -1.0782, -1.4950, -1.1627, -1.0064, -1.0437, -1.4793, -1.4429],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3328, -4.2349, -1.1120,  ..., -4.8384, -4.8214, -4.8218],\n",
      "        [-2.3907, -4.2401, -1.1299,  ..., -4.9639, -4.9484, -4.9367],\n",
      "        [-2.3910, -4.2249, -1.1215,  ..., -5.2536, -5.2189, -5.1802],\n",
      "        ...,\n",
      "        [-2.4540, -4.2542, -1.1372,  ..., -5.1685, -5.1379, -5.1224],\n",
      "        [-2.3941, -4.2247, -1.1240,  ..., -5.2210, -5.1902, -5.1454],\n",
      "        [-2.3535, -4.2321, -1.1096,  ..., -5.1193, -5.1112, -5.1186]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3051, -1.0822, -1.0254, -1.1178, -0.8592, -0.7653, -1.0978, -1.0712],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5595, -1.5266, -1.6021, -1.0910, -1.6373, -1.1000, -1.5483, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2197, -0.2219, -0.2242,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1759, -0.1777, -0.1795,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2212, -0.2235, -0.2257,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1058, -0.1068, -0.1079,  ..., -1.1000,  0.0000,  0.0000],\n",
      "        [-0.2203, -0.2226, -0.2248,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4912, -1.4597, -1.5319, -1.0588, -1.5656, -1.0518, -1.4804, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3291, -4.2289, -1.1105,  ..., -5.1411, -5.1552, -5.1605],\n",
      "        [-2.3705, -4.2194, -1.1154,  ..., -5.1150, -5.1013, -5.0794],\n",
      "        [-2.3671, -4.2407, -1.1202,  ..., -4.9931, -4.9997, -5.0086],\n",
      "        ...,\n",
      "        [-2.3775, -4.2251, -1.1308,  ..., -1.4052, -4.8750, -4.7375],\n",
      "        [-2.3561, -4.2366, -1.1147,  ..., -4.6639, -4.6520, -4.6419],\n",
      "        [-2.3686, -4.2412, -1.1359,  ..., -1.1735, -1.2041, -1.5342]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0596, -1.1026, -1.2441, -0.8057, -1.1831, -1.2802, -1.2562, -1.1854],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5302, -1.1000, -1.1186, -1.3672, -1.5604, -1.4677, -1.5805, -1.1547],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1728, -0.1746, -0.1763,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2196, -0.2218, -0.2240,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2380, -0.2404, -0.2429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2227, -0.2249, -0.2272,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2244, -0.2267, -0.2290,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4631, -1.0518, -1.0749, -1.3073, -1.4920, -1.4531, -1.5570, -1.1041],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.4024, -4.2184, -1.1178,  ..., -5.1393, -5.1877, -5.2138],\n",
      "        [-2.3863, -4.2264, -1.1381,  ..., -1.1211, -1.2224, -1.3156],\n",
      "        [-2.4587, -4.2576, -1.1371,  ..., -5.1789, -5.1637, -5.1540],\n",
      "        ...,\n",
      "        [-2.3503, -4.2585, -1.1273,  ..., -4.8454, -4.8405, -4.8389],\n",
      "        [-2.3537, -4.2430, -1.1277,  ..., -4.9565, -4.9522, -4.9421],\n",
      "        [-2.4439, -4.2430, -1.1184,  ..., -5.1405, -5.1220, -5.1076]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0881, -1.1483, -0.7820, -1.1741, -1.5743, -1.2693, -1.2018, -0.9433],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.3897, -1.3050, -1.1631, -1.1969, -2.4415, -1.4895, -1.5894, -1.1795],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2277, -0.2300, -0.2323,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2388, -0.2412, -0.2436,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2260, -0.2283, -0.2306,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2440, -0.2465, -0.2490,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1831, -0.1850, -0.1869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2292, -0.2315, -0.2339,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3288, -1.2856, -1.1121, -1.1444, -2.3345, -1.4243, -1.5197, -1.1278],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3697, -4.2310, -1.1189,  ..., -4.9929, -5.0206, -5.0914],\n",
      "        [-2.3837, -4.2781, -1.1323,  ..., -5.2143, -5.2236, -5.1881],\n",
      "        [-2.4522, -4.2561, -1.1354,  ..., -5.1514, -5.1430, -5.1294],\n",
      "        ...,\n",
      "        [-2.4096, -4.2472, -1.1121,  ..., -5.3756, -5.4342, -5.4444],\n",
      "        [-2.3608, -4.2164, -1.1152,  ..., -4.8921, -4.8703, -4.8406],\n",
      "        [-2.4463, -4.2410, -1.1323,  ..., -5.1519, -5.1390, -5.1267]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1201, -1.2253, -0.8129, -0.8480, -1.4968, -1.4479, -1.3037, -0.8427],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.6270, -2.6557, -1.6224, -1.4688, -1.3528, -1.6395, -1.5856],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(1.9500e-09, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.2064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.3618, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1837, -0.1856, -0.1875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3856, -0.3895, -0.3934,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2238, -0.2261, -0.2284,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2219, -0.2241, -0.2264,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1791, -0.1809, -0.1827,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.5557, -2.5773, -1.5513, -1.4044, -1.2935, -1.5677, -1.5162],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.7349, -4.2761, -1.2163,  ..., -5.6524, -5.6493, -5.6209],\n",
      "        [-2.3608, -4.2118, -1.1068,  ..., -5.2720, -5.2288, -5.1699],\n",
      "        [-2.2744, -4.2304, -1.1196,  ..., -4.7269, -4.7203, -4.7365],\n",
      "        ...,\n",
      "        [-2.3553, -4.2517, -1.1207,  ..., -5.1538, -5.2163, -5.2671],\n",
      "        [-2.3080, -4.2314, -1.1081,  ..., -5.1521, -5.1403, -5.1144],\n",
      "        [-2.3441, -4.2132, -1.1279,  ..., -5.1854, -5.1824, -5.1683]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6723, -1.2436, -2.0368, -1.3000, -1.2230, -1.1198, -1.1445, -1.1750],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.3590, -1.1668, -1.6558, -1.6176, -1.2671, -1.4200, -1.5614, -1.2320],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2511, -0.2537, -0.2562,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2337, -0.2360, -0.2384,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1870, -0.1889, -0.1908,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2421, -0.2446, -0.2471,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2178, -0.2200, -0.2222,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2518, -0.2543, -0.2569,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3321, -1.1324, -1.5833, -1.5467, -1.2116, -1.3577, -1.4929, -1.2076],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3689, -4.2704, -1.1430,  ..., -5.1812, -5.1622, -5.1196],\n",
      "        [-2.4290, -4.2608, -1.1480,  ..., -4.9856, -4.9842, -4.9832],\n",
      "        [-2.3457, -4.2242, -1.1370,  ..., -5.0116, -5.0043, -4.9743],\n",
      "        ...,\n",
      "        [-2.3128, -4.2407, -1.1006,  ..., -4.8409, -4.8437, -4.8579],\n",
      "        [-2.3112, -4.2183, -1.1032,  ..., -5.0615, -5.0691, -5.0735],\n",
      "        [-2.3427, -4.2558, -1.1354,  ..., -5.0517, -5.0471, -5.0434]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1493, -0.8295, -1.5162, -1.0715, -0.8616, -1.1367, -1.2030, -1.0659],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4589, -1.5399], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2414, -0.2438, -0.2463, -0.2488, -0.2513, -0.2538, -0.2564, -0.2590,\n",
      "         -0.2616, -0.2643, -0.2669, -0.2696, -0.2723, -0.2751, -0.2779, -0.2807,\n",
      "         -0.2835, -0.2864, -0.2893, -0.2922, -0.2951, -0.2981, -0.3011, -0.3042,\n",
      "         -0.3072, -0.3104, -0.3135, -0.3167, -0.3199, -0.3231, -0.3263, -0.3296,\n",
      "         -0.3330, -0.3363, -0.3397, -0.3432, -0.3466, -0.3501, -0.3537, -0.3572,\n",
      "         -0.3608, -0.3645, -0.3682, -0.3719, -0.3757, -0.3794, -0.3833, -0.3871,\n",
      "         -0.3911, -0.3950, -0.3990, -0.4030, -0.4071, -0.4112, -0.4154, -0.4196,\n",
      "         -0.4238, -0.4281, -0.4324, -0.4368, -0.4412, -0.4456, -0.4501, -0.4547,\n",
      "         -0.4593, -0.4639, -0.4686, -0.4733, -0.4781, -0.4830, -0.4878, -0.4928,\n",
      "         -0.4977, -0.5028, -0.5078, -0.5130, -0.5182, -0.5234, -0.5287, -0.5340,\n",
      "         -0.5394, -0.5449, -0.5504, -0.5559, -0.5615, -0.5672, -0.5729, -0.5787,\n",
      "         -0.5846, -0.5905, -0.5964, -0.6025, -0.6085, -0.6147, -0.6209, -0.6272,\n",
      "         -0.6335, -0.6399, -0.6464, -0.6529, -0.6595, -0.6662, -0.6729, -0.6797,\n",
      "         -0.6866, -0.6935, -0.7005, -0.7076, -0.7147, -0.7219, -0.7292, -0.7366,\n",
      "         -0.7440, -0.7515, -0.7591, -0.7668, -0.7746, -0.7824, -0.7903, -0.7983,\n",
      "         -0.8063, -0.8145, -0.8227, -0.8310, -0.8394, -0.8479, -0.8564, -0.8651,\n",
      "         -0.8738, -0.8827, -0.8916, -0.9006, -0.9097, -0.9189, -0.9281, -0.9375,\n",
      "         -0.9470, -0.9566, -0.9662, -0.9760, -0.9858, -0.9958, -1.0059, -1.0160,\n",
      "         -1.0263, -1.0366, -1.0471, -1.0577, -1.0684, -1.0792, -1.0901, -1.1011,\n",
      "         -1.1122, -1.1234, -1.1348, -1.1462, -1.1578, -1.1695, -1.1813, -1.1933,\n",
      "         -1.2053, -1.2175, -1.2298, -1.2422, -1.2548, -1.2674, -1.2802, -1.2932,\n",
      "         -1.3062, -1.3194, -1.3328, -1.3462, -1.3598, -1.3735, -1.3874, -1.4014,\n",
      "         -1.4156, -1.4299, -1.4443, -1.4589,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2447, -0.2472, -0.2497, -0.2522, -0.2548, -0.2574, -0.2600, -0.2626,\n",
      "         -0.2652, -0.2679, -0.2706, -0.2734, -0.2761, -0.2789, -0.2817, -0.2846,\n",
      "         -0.2874, -0.2904, -0.2933, -0.2962, -0.2992, -0.3023, -0.3053, -0.3084,\n",
      "         -0.3115, -0.3147, -0.3178, -0.3210, -0.3243, -0.3276, -0.3309, -0.3342,\n",
      "         -0.3376, -0.3410, -0.3444, -0.3479, -0.3514, -0.3550, -0.3586, -0.3622,\n",
      "         -0.3659, -0.3696, -0.3733, -0.3771, -0.3809, -0.3847, -0.3886, -0.3925,\n",
      "         -0.3965, -0.4005, -0.4045, -0.4086, -0.4128, -0.4169, -0.4211, -0.4254,\n",
      "         -0.4297, -0.4340, -0.4384, -0.4428, -0.4473, -0.4518, -0.4564, -0.4610,\n",
      "         -0.4657, -0.4704, -0.4751, -0.4799, -0.4848, -0.4897, -0.4946, -0.4996,\n",
      "         -0.5046, -0.5097, -0.5149, -0.5201, -0.5253, -0.5307, -0.5360, -0.5414,\n",
      "         -0.5469, -0.5524, -0.5580, -0.5636, -0.5693, -0.5751, -0.5809, -0.5868,\n",
      "         -0.5927, -0.5987, -0.6047, -0.6108, -0.6170, -0.6232, -0.6295, -0.6359,\n",
      "         -0.6423, -0.6488, -0.6553, -0.6620, -0.6687, -0.6754, -0.6822, -0.6891,\n",
      "         -0.6961, -0.7031, -0.7102, -0.7174, -0.7246, -0.7320, -0.7393, -0.7468,\n",
      "         -0.7544, -0.7620, -0.7697, -0.7775, -0.7853, -0.7932, -0.8012, -0.8093,\n",
      "         -0.8175, -0.8258, -0.8341, -0.8425, -0.8511, -0.8596, -0.8683, -0.8771,\n",
      "         -0.8860, -0.8949, -0.9040, -0.9131, -0.9223, -0.9316, -0.9410, -0.9505,\n",
      "         -0.9601, -0.9698, -0.9796, -0.9895, -0.9995, -1.0096, -1.0198, -1.0301,\n",
      "         -1.0405, -1.0510, -1.0617, -1.0724, -1.0832, -1.0941, -1.1052, -1.1164,\n",
      "         -1.1276, -1.1390, -1.1505, -1.1622, -1.1739, -1.1858, -1.1977, -1.2098,\n",
      "         -1.2220, -1.2344, -1.2469, -1.2595, -1.2722, -1.2850, -1.2980, -1.3111,\n",
      "         -1.3244, -1.3377, -1.3513, -1.3649, -1.3787, -1.3926, -1.4067, -1.4209,\n",
      "         -1.4352, -1.4497, -1.4644, -1.4792, -1.4941, -1.5092, -1.5245, -1.5399,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3950, -1.5245], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3252e+00, -4.2313e+00, -1.1036e+00, -2.2497e+00, -3.1125e-01,\n",
      "         -2.5884e+00, -4.3289e+00, -4.9639e-01, -3.4522e+00, -2.5254e+00,\n",
      "         -3.5314e+00, -1.9452e-01, -4.4609e-01, -3.6186e+00, -3.3293e+00,\n",
      "         -2.6803e+00, -3.3277e+00, -2.0349e+00, -2.2975e+00, -1.5104e+00,\n",
      "         -1.7319e+00, -1.6088e+00, -1.2770e+00, -8.9502e-02, -2.1623e+00,\n",
      "         -3.6830e+00, -3.5336e+00, -2.7829e+00, -3.7516e+00,  3.3055e-01,\n",
      "         -1.9931e+00, -7.1466e-01, -1.4846e+00, -3.0853e+00, -2.2035e+00,\n",
      "         -1.4093e+00, -1.2669e+00, -3.3332e+00, -1.9383e+00, -5.0461e-01,\n",
      "         -1.0615e+00, -1.0567e+00, -7.0145e-01, -5.0274e+00, -1.8695e+00,\n",
      "         -2.8961e+00, -3.5524e+00, -2.9716e+00, -1.2146e+00, -1.2100e+00,\n",
      "         -3.8758e+00, -1.8022e+00, -7.5227e+00, -3.2331e+00, -3.2259e+00,\n",
      "         -1.2668e+00, -2.7557e+00, -3.2069e+00, -2.9726e+00, -1.9602e+00,\n",
      "         -3.7436e+00, -1.5163e+00, -6.5825e+00, -2.8454e+00, -3.6256e+00,\n",
      "          2.8721e-01, -1.8151e+00, -7.9250e-01, -1.0331e+00, -2.8325e+00,\n",
      "         -4.2051e+00, -5.7279e+00, -3.2488e+00, -9.4424e-01, -3.8934e+00,\n",
      "         -4.7624e+00, -2.9257e+00, -3.9970e+00, -2.5688e+00, -3.6649e+00,\n",
      "         -5.2546e+00, -3.0543e+00, -1.0092e+00, -1.7431e+00, -2.1237e+00,\n",
      "         -3.4114e+00, -2.0972e+00, -1.2135e+00, -3.8539e+00, -1.5046e+00,\n",
      "         -3.5211e+00, -4.7939e+00, -2.9944e+00, -1.1818e+00, -1.4662e+00,\n",
      "         -3.4667e+00, -2.9790e+00, -3.3284e+00, -3.5295e+00, -3.4469e+00,\n",
      "         -7.4486e-01, -1.4772e+00, -3.9880e+00, -2.7751e+00, -9.8381e-01,\n",
      "         -2.1159e+00, -3.3678e+00, -4.4361e+00, -4.2315e-01, -4.6578e+00,\n",
      "         -1.4278e+00, -1.6082e+00, -3.8424e+00, -4.9613e+00, -3.2895e-02,\n",
      "         -1.3041e+00, -2.5856e+00, -1.5780e+00, -1.4823e+00, -1.6423e+00,\n",
      "         -2.6454e+00, -4.7481e+00, -2.7629e+00, -1.3840e+00, -1.4407e+00,\n",
      "         -3.0628e+00, -2.7174e+00, -1.0134e+00, -9.5523e-01, -3.8145e+00,\n",
      "         -1.7098e+00, -1.4664e+00, -3.6774e+00, -3.9165e+00, -1.8255e+00,\n",
      "         -2.4562e+00, -3.1400e+00, -2.7956e+00, -2.2088e+00, -3.2404e+00,\n",
      "         -2.4453e+00, -6.0126e+00, -1.9376e+00, -2.3260e+00, -9.4376e-01,\n",
      "         -8.9130e-01, -9.4944e-01, -9.5600e-01, -8.8249e-01, -8.8531e-01,\n",
      "         -3.8646e+00, -4.8572e+00, -9.1161e-01, -2.5293e+00, -5.0973e+00,\n",
      "         -6.1489e+00, -4.5965e+00, -3.1622e+00, -5.3053e+00, -2.1916e+00,\n",
      "         -1.3297e+00, -1.2050e+00, -1.2283e+00, -1.2209e+00, -1.2198e+00,\n",
      "         -1.2302e+00, -1.2090e+00, -1.1848e+00, -1.1775e+00, -1.2819e+00,\n",
      "         -4.9909e+00, -4.9071e+00, -5.0799e+00, -5.0135e+00, -4.9350e+00,\n",
      "         -4.9967e+00, -5.2248e+00, -5.4384e+00, -5.4552e+00, -5.4136e+00,\n",
      "         -5.3281e+00, -5.2082e+00, -5.1345e+00, -5.1151e+00, -5.1177e+00,\n",
      "         -5.1213e+00, -5.1211e+00, -5.1120e+00, -5.1004e+00, -5.0935e+00,\n",
      "         -5.0981e+00, -5.1070e+00, -5.1088e+00, -5.1069e+00, -5.1100e+00,\n",
      "         -5.1273e+00, -5.1521e+00, -5.1784e+00, -5.2015e+00, -5.2221e+00,\n",
      "         -5.2468e+00, -5.2628e+00, -5.2674e+00, -5.2645e+00, -5.2543e+00,\n",
      "         -5.2515e+00, -5.2647e+00, -5.2805e+00, -5.3002e+00, -5.3221e+00,\n",
      "         -5.3356e+00, -5.3282e+00, -5.3105e+00, -5.2824e+00, -5.2580e+00,\n",
      "         -5.2469e+00, -5.2388e+00, -5.2199e+00, -5.2033e+00, -5.2039e+00,\n",
      "         -5.2040e+00, -5.2105e+00, -5.2339e+00, -5.2440e+00, -5.2996e+00,\n",
      "         -5.3554e+00],\n",
      "        [-2.3427e+00, -4.2378e+00, -1.1256e+00, -2.2302e+00, -3.3066e-01,\n",
      "         -2.5910e+00, -4.2866e+00, -4.7455e-01, -3.4583e+00, -2.5121e+00,\n",
      "         -3.5231e+00, -1.8125e-01, -4.6206e-01, -3.5881e+00, -3.3032e+00,\n",
      "         -2.6639e+00, -3.3280e+00, -2.0604e+00, -2.3128e+00, -1.5146e+00,\n",
      "         -1.7346e+00, -1.6125e+00, -1.2733e+00, -9.9185e-02, -2.1698e+00,\n",
      "         -3.6540e+00, -3.5124e+00, -2.7756e+00, -3.7603e+00,  3.1192e-01,\n",
      "         -2.0071e+00, -7.1466e-01, -1.4924e+00, -3.0753e+00, -2.2081e+00,\n",
      "         -1.4117e+00, -1.2831e+00, -3.3356e+00, -1.9344e+00, -5.1777e-01,\n",
      "         -1.0680e+00, -1.0746e+00, -7.2391e-01, -5.0316e+00, -1.8770e+00,\n",
      "         -2.9083e+00, -3.5523e+00, -2.9633e+00, -1.2214e+00, -1.2137e+00,\n",
      "         -3.8557e+00, -1.8186e+00, -7.5157e+00, -3.2279e+00, -3.2284e+00,\n",
      "         -1.2853e+00, -2.7796e+00, -3.2355e+00, -2.9840e+00, -1.9687e+00,\n",
      "         -3.7191e+00, -1.5310e+00, -6.5811e+00, -2.8363e+00, -3.6352e+00,\n",
      "          2.6838e-01, -1.8260e+00, -7.9291e-01, -1.0415e+00, -2.8232e+00,\n",
      "         -4.1712e+00, -5.7255e+00, -3.2439e+00, -9.4067e-01, -3.8737e+00,\n",
      "         -4.7371e+00, -2.9206e+00, -4.0017e+00, -2.5690e+00, -3.6422e+00,\n",
      "         -5.2496e+00, -3.0476e+00, -1.0294e+00, -1.7491e+00, -2.1496e+00,\n",
      "         -3.4180e+00, -2.1059e+00, -1.2139e+00, -3.8523e+00, -1.5099e+00,\n",
      "         -3.4987e+00, -4.7696e+00, -2.9885e+00, -1.2015e+00, -1.4709e+00,\n",
      "         -3.4682e+00, -2.9856e+00, -3.3385e+00, -3.5280e+00, -3.4643e+00,\n",
      "         -7.5330e-01, -1.4735e+00, -3.9823e+00, -2.7659e+00, -9.9647e-01,\n",
      "         -2.1059e+00, -3.3683e+00, -4.4277e+00, -4.2058e-01, -2.4383e+00,\n",
      "         -1.3370e+00, -1.4838e+00, -3.9144e+00, -5.0222e+00, -2.0525e-03,\n",
      "         -1.2460e+00, -2.5677e+00, -1.5311e+00, -1.3675e+00, -1.6342e+00,\n",
      "         -2.4958e+00, -4.9040e+00, -2.6885e+00, -1.3664e+00, -1.4538e+00,\n",
      "         -3.0673e+00, -2.7013e+00, -9.8051e-01, -8.6776e-01, -3.7968e+00,\n",
      "         -1.7167e+00, -1.4769e+00, -3.6756e+00, -3.8884e+00, -1.8228e+00,\n",
      "         -2.4805e+00, -3.1578e+00, -2.8124e+00, -2.2145e+00, -3.1623e+00,\n",
      "         -2.4049e+00, -5.9836e+00, -1.9506e+00, -2.9484e+00, -1.4769e+00,\n",
      "         -1.2756e+00, -1.3729e+00, -1.0212e+00, -1.0279e+00, -9.1119e-01,\n",
      "         -1.0835e+00, -4.1048e+00, -5.2083e+00, -8.0450e-01, -2.5363e+00,\n",
      "         -4.7526e+00, -6.1146e+00, -4.7261e+00, -5.4881e+00, -2.1496e+00,\n",
      "         -1.0299e+00, -4.2895e+00, -4.5911e+00, -9.3423e-01, -2.0718e+00,\n",
      "         -4.2963e+00, -5.4231e+00, -4.1906e+00, -3.3186e+00, -5.1434e+00,\n",
      "         -2.2086e+00, -1.0805e+00, -1.3032e+00, -1.3113e+00, -4.8220e+00,\n",
      "         -4.7999e+00, -4.9585e+00, -5.2736e+00, -4.6851e+00, -4.4829e+00,\n",
      "         -4.7020e+00, -5.0034e+00, -5.1958e+00, -5.2419e+00, -5.2377e+00,\n",
      "         -5.1740e+00, -5.1308e+00, -5.0974e+00, -5.0686e+00, -5.0536e+00,\n",
      "         -5.0463e+00, -5.0499e+00, -5.0577e+00, -5.0575e+00, -5.0334e+00,\n",
      "         -5.0058e+00, -4.9847e+00, -4.9841e+00, -4.9899e+00, -4.9970e+00,\n",
      "         -5.0082e+00, -5.0210e+00, -5.0329e+00, -5.0412e+00, -5.0384e+00,\n",
      "         -5.0342e+00, -5.0371e+00, -5.0427e+00, -5.0474e+00, -5.0493e+00,\n",
      "         -5.0445e+00, -5.0355e+00, -5.0242e+00, -5.0123e+00, -4.9966e+00,\n",
      "         -4.9753e+00, -4.9537e+00, -4.9313e+00, -4.9157e+00, -4.9016e+00,\n",
      "         -4.8902e+00, -4.8877e+00, -4.8695e+00, -4.8409e+00, -4.8121e+00,\n",
      "         -4.7904e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.2287, -1.2317], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4516, -1.6616, -1.3454, -1.6316, -1.6415, -1.6182, -1.3127, -1.4876],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2656, -0.2683, -0.2710,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2341, -0.2365, -0.2389,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2615, -0.2641, -0.2668,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1828, -0.1846, -0.1865,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2577, -0.2603, -0.2629,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2537, -0.2562, -0.2588,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4300, -1.6369, -1.2865, -1.5601, -1.5696, -1.5473, -1.2614, -1.4224],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3570, -4.2654, -1.1391,  ..., -5.1129, -5.1334, -5.1092],\n",
      "        [-2.3376, -4.2316, -1.1285,  ..., -4.8973, -4.8883, -4.8786],\n",
      "        [-2.4041, -4.2344, -1.1331,  ..., -5.1843, -5.1680, -5.1476],\n",
      "        ...,\n",
      "        [-2.3697, -4.2086, -1.1206,  ..., -5.2337, -5.2014, -5.1613],\n",
      "        [-2.4135, -4.2458, -1.1312,  ..., -5.2318, -5.2086, -5.1948],\n",
      "        [-2.3016, -4.2363, -1.1034,  ..., -4.8178, -4.8164, -4.8344]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3202, -1.3571, -0.9373, -1.5954, -1.3713, -1.2190, -0.9381, -1.1911],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.3006, -1.6645, -1.3928, -1.3088, -1.6218, -1.1000, -1.6171],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(8.3483e-10, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.1651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2527, -0.2553, -0.2579,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2345, -0.2369, -0.2393,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2262, -0.2285, -0.2308,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.1826, -0.1845, -0.1863,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.2436, -1.5915, -1.3317, -1.2702, -1.5507, -1.0518, -1.5463],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8103, -4.2899, -1.2374,  ..., -5.5051, -5.5151, -5.4945],\n",
      "        [-2.4456, -4.2576, -1.1296,  ..., -4.8925, -4.8742, -4.8655],\n",
      "        [-2.2913, -4.2117, -1.1144,  ..., -5.0757, -5.0840, -5.0826],\n",
      "        ...,\n",
      "        [-2.3009, -4.2106, -1.1115,  ..., -5.0180, -5.0279, -5.0292],\n",
      "        [-2.3610, -4.2248, -1.1399,  ..., -1.1613, -1.1908, -1.7099],\n",
      "        [-2.3679, -4.2039, -1.1229,  ..., -5.2034, -5.1841, -5.1481]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.6288, -1.1413, -1.3533, -0.9870, -0.9132, -1.3690, -1.1926, -1.3084],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-2.5416, -1.7333, -1.7207, -1.7485, -1.1000, -1.5727, -1.4144, -1.6432],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.1771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.4205, -0.4248, -0.4291,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2208, -0.2231, -0.2253,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2329, -0.2352, -0.2376,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2576, -0.2602, -0.2629,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2749, -0.2776, -0.2804,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2246, -0.2269, -0.2292,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-2.4302, -1.6574, -1.6453, -1.6718, -1.0518, -1.5038, -1.3524, -1.5712],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.5761, -4.2608, -1.1494,  ..., -5.4185, -5.4549, -5.4818],\n",
      "        [-2.3255, -4.2073, -1.1138,  ..., -4.9798, -4.9868, -4.9954],\n",
      "        [-2.3064, -4.2201, -1.1231,  ..., -5.1504, -5.1439, -5.1265],\n",
      "        ...,\n",
      "        [-2.3110, -4.2152, -1.1119,  ..., -5.1628, -5.1580, -5.1997],\n",
      "        [-2.3861, -4.2420, -1.1352,  ..., -5.1898, -5.1692, -5.1486],\n",
      "        [-2.2914, -4.2131, -1.1193,  ..., -4.7920, -4.7759, -4.7845]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5343, -1.4483, -1.3256, -1.5337, -1.5200, -1.2012, -1.0373, -1.4490],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.6373, -1.1000, -1.7139, -1.5867, -1.4333, -1.4945, -1.7082, -1.5480],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0506, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1849, -0.1868, -0.1887,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2439, -0.2464, -0.2489,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2904, -0.2934, -0.2963,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2312, -0.2335, -0.2359,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3008, -0.3039, -0.3069,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5655, -1.0782, -1.6388, -1.5172, -1.3910, -1.4290, -1.6334, -1.4802],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3822, -4.1977, -1.1294,  ..., -5.0709, -5.1293, -5.1694],\n",
      "        [-2.3607, -4.2079, -1.1403,  ..., -4.8657, -4.8274, -4.7975],\n",
      "        [-2.3150, -4.2120, -1.1315,  ..., -4.2976, -4.3010, -4.3096],\n",
      "        ...,\n",
      "        [-2.3115, -4.2121, -1.1314,  ..., -5.0343, -5.0218, -5.0054],\n",
      "        [-2.2973, -4.1957, -1.1275,  ..., -5.1698, -5.1819, -5.2001],\n",
      "        [-2.3423, -4.2106, -1.1173,  ..., -5.0647, -5.0865, -5.0946]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1177, -1.2799, -1.3743, -1.4297, -1.0378, -1.0111, -1.2703, -1.2834],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4519, -1.4848, -1.6453, -2.7776, -1.6800, -1.6544, -1.5128, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0475, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2822, -0.2850, -0.2879,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3034, -0.3065, -0.3096,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1896, -0.1915, -0.1934,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2262, -0.2284, -0.2307,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2940, -0.2969, -0.2999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3883, -1.4554, -1.5732, -2.6957, -1.6063, -1.5819, -1.4465, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3117, -4.2134, -1.1222,  ..., -5.1535, -5.1166, -5.0961],\n",
      "        [-2.2708, -4.2440, -1.1417,  ..., -4.8706, -4.8661, -4.8603],\n",
      "        [-2.3513, -4.1933, -1.1370,  ..., -4.9898, -4.9638, -4.9395],\n",
      "        ...,\n",
      "        [-2.3052, -4.2014, -1.1291,  ..., -4.9329, -4.9328, -4.9593],\n",
      "        [-2.3716, -4.2413, -1.1378,  ..., -5.1736, -5.1621, -5.1484],\n",
      "        [-2.3818, -4.1994, -1.1596,  ..., -1.3822, -1.4826, -1.6082]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.0662, -1.2870, -1.5572, -2.1959, -1.5503, -1.5409, -1.1012, -1.4123],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.6878, -1.7311, -1.6005, -1.6978, -1.7476, -1.6021, -1.7344, -1.7058],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2737, -0.2765, -0.2793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1955, -0.1975, -0.1995,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2958, -0.2988, -0.3018,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2651, -0.2678, -0.2705,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2019, -0.2039, -0.2060,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2766, -0.2794, -0.2822,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6138, -1.6552, -1.5688, -1.6809, -1.6710, -1.5319, -1.6584, -1.6888],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3137, -4.2170, -1.1412,  ..., -4.6438, -4.6406, -4.6648],\n",
      "        [-2.3540, -4.1907, -1.1389,  ..., -5.2262, -5.1987, -5.1646],\n",
      "        [-2.3058, -4.2429, -1.1488,  ..., -5.0244, -5.0028, -4.9403],\n",
      "        ...,\n",
      "        [-2.3218, -4.2252, -1.1391,  ..., -4.9626, -5.0420, -5.1355],\n",
      "        [-2.3331, -4.2004, -1.1402,  ..., -4.9055, -4.8658, -4.8393],\n",
      "        [-2.2819, -4.2194, -1.1496,  ..., -4.6164, -4.5914, -4.5708]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5962, -1.3482, -1.3343, -1.4993, -1.4972, -1.5172, -1.6233, -1.5267],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.6803, -1.7626], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2752, -0.2780, -0.2808, -0.2837, -0.2865, -0.2894, -0.2924, -0.2953,\n",
      "         -0.2983, -0.3013, -0.3043, -0.3074, -0.3105, -0.3137, -0.3168, -0.3200,\n",
      "         -0.3233, -0.3265, -0.3298, -0.3332, -0.3365, -0.3399, -0.3434, -0.3468,\n",
      "         -0.3503, -0.3539, -0.3574, -0.3610, -0.3647, -0.3684, -0.3721, -0.3759,\n",
      "         -0.3797, -0.3835, -0.3874, -0.3913, -0.3952, -0.3992, -0.4033, -0.4073,\n",
      "         -0.4114, -0.4156, -0.4198, -0.4240, -0.4283, -0.4326, -0.4370, -0.4414,\n",
      "         -0.4459, -0.4504, -0.4549, -0.4595, -0.4642, -0.4689, -0.4736, -0.4784,\n",
      "         -0.4832, -0.4881, -0.4930, -0.4980, -0.5030, -0.5081, -0.5133, -0.5184,\n",
      "         -0.5237, -0.5290, -0.5343, -0.5397, -0.5452, -0.5507, -0.5562, -0.5618,\n",
      "         -0.5675, -0.5733, -0.5790, -0.5849, -0.5908, -0.5968, -0.6028, -0.6089,\n",
      "         -0.6150, -0.6212, -0.6275, -0.6339, -0.6403, -0.6467, -0.6533, -0.6599,\n",
      "         -0.6665, -0.6733, -0.6801, -0.6869, -0.6939, -0.7009, -0.7080, -0.7151,\n",
      "         -0.7223, -0.7296, -0.7370, -0.7444, -0.7520, -0.7596, -0.7672, -0.7750,\n",
      "         -0.7828, -0.7907, -0.7987, -0.8068, -0.8149, -0.8232, -0.8315, -0.8399,\n",
      "         -0.8484, -0.8569, -0.8656, -0.8743, -0.8831, -0.8921, -0.9011, -0.9102,\n",
      "         -0.9194, -0.9287, -0.9380, -0.9475, -0.9571, -0.9668, -0.9765, -0.9864,\n",
      "         -0.9964, -1.0064, -1.0166, -1.0268, -1.0372, -1.0477, -1.0583, -1.0690,\n",
      "         -1.0798, -1.0907, -1.1017, -1.1128, -1.1241, -1.1354, -1.1469, -1.1585,\n",
      "         -1.1702, -1.1820, -1.1939, -1.2060, -1.2182, -1.2305, -1.2429, -1.2555,\n",
      "         -1.2681, -1.2809, -1.2939, -1.3070, -1.3202, -1.3335, -1.3470, -1.3606,\n",
      "         -1.3743, -1.3882, -1.4022, -1.4164, -1.4307, -1.4451, -1.4597, -1.4745,\n",
      "         -1.4894, -1.5044, -1.5196, -1.5350, -1.5505, -1.5661, -1.5820, -1.5979,\n",
      "         -1.6141, -1.6304, -1.6468, -1.6635, -1.6803,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1991, -0.2011, -0.2031, -0.2052, -0.2072, -0.2093, -0.2114, -0.2136,\n",
      "         -0.2157, -0.2179, -0.2201, -0.2223, -0.2246, -0.2268, -0.2291, -0.2315,\n",
      "         -0.2338, -0.2362, -0.2385, -0.2409, -0.2434, -0.2458, -0.2483, -0.2508,\n",
      "         -0.2534, -0.2559, -0.2585, -0.2611, -0.2638, -0.2664, -0.2691, -0.2718,\n",
      "         -0.2746, -0.2773, -0.2801, -0.2830, -0.2858, -0.2887, -0.2916, -0.2946,\n",
      "         -0.2976, -0.3006, -0.3036, -0.3067, -0.3098, -0.3129, -0.3161, -0.3193,\n",
      "         -0.3225, -0.3257, -0.3290, -0.3323, -0.3357, -0.3391, -0.3425, -0.3460,\n",
      "         -0.3495, -0.3530, -0.3566, -0.3602, -0.3638, -0.3675, -0.3712, -0.3749,\n",
      "         -0.3787, -0.3826, -0.3864, -0.3903, -0.3943, -0.3983, -0.4023, -0.4063,\n",
      "         -0.4104, -0.4146, -0.4188, -0.4230, -0.4273, -0.4316, -0.4360, -0.4404,\n",
      "         -0.4448, -0.4493, -0.4538, -0.4584, -0.4631, -0.4677, -0.4725, -0.4772,\n",
      "         -0.4820, -0.4869, -0.4918, -0.4968, -0.5018, -0.5069, -0.5120, -0.5172,\n",
      "         -0.5224, -0.5277, -0.5330, -0.5384, -0.5438, -0.5493, -0.5549, -0.5605,\n",
      "         -0.5661, -0.5719, -0.5776, -0.5835, -0.5894, -0.5953, -0.6013, -0.6074,\n",
      "         -0.6135, -0.6197, -0.6260, -0.6323, -0.6387, -0.6452, -0.6517, -0.6583,\n",
      "         -0.6649, -0.6716, -0.6784, -0.6853, -0.6922, -0.6992, -0.7062, -0.7134,\n",
      "         -0.7206, -0.7279, -0.7352, -0.7426, -0.7501, -0.7577, -0.7654, -0.7731,\n",
      "         -0.7809, -0.7888, -0.7968, -0.8048, -0.8129, -0.8212, -0.8294, -0.8378,\n",
      "         -0.8463, -0.8548, -0.8635, -0.8722, -0.8810, -0.8899, -0.8989, -0.9080,\n",
      "         -0.9171, -0.9264, -0.9358, -0.9452, -0.9548, -0.9644, -0.9741, -0.9840,\n",
      "         -0.9939, -1.0040, -1.0141, -1.0244, -1.0347, -1.0452, -1.0557, -1.0664,\n",
      "         -1.0771, -1.0880, -1.0990, -1.1101, -1.1213, -1.1327, -1.1441, -1.1557,\n",
      "         -1.1673, -1.1791, -1.1910, -1.2031, -1.2152, -1.2275, -1.2399, -1.2524,\n",
      "         -1.2651, -1.2778, -1.2907, -1.3038, -1.3169, -1.3303, -1.3437, -1.3573,\n",
      "         -1.3710, -1.3848, -1.3988, -1.4129, -1.4272, -1.4416, -1.4562, -1.4709,\n",
      "         -1.4858, -1.5008, -1.5159, -1.5312, -1.5467, -1.5623, -1.5781, -1.5940,\n",
      "         -1.6101, -1.6264, -1.6428, -1.6594, -1.6762, -1.6931, -1.7102, -1.7275,\n",
      "         -1.7450, -1.7626,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6066, -1.6853], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2977, -4.2043, -1.1554, -2.1847, -0.3422, -2.5583, -4.1480, -0.4125,\n",
      "         -3.4136, -2.3728, -3.4361, -0.2574, -0.4591, -3.6406, -3.3581, -2.7590,\n",
      "         -3.2717, -2.0781, -2.4186, -1.4130, -1.6431, -1.5314, -1.2191, -0.1781,\n",
      "         -2.1555, -3.7060, -3.5738, -2.8972, -3.8115,  0.1813, -2.0036, -0.6170,\n",
      "         -1.5240, -3.0163, -2.1936, -1.6029, -1.4089, -3.3077, -1.9876, -0.4668,\n",
      "         -1.0915, -1.1970, -0.8039, -5.1039, -1.8838, -2.8756, -3.4135, -2.8986,\n",
      "         -1.4329, -1.4113, -3.9349, -1.7906, -7.4763, -3.3787, -3.2266, -1.3001,\n",
      "         -2.8701, -3.2824, -2.9613, -1.9538, -3.7800, -1.4939, -6.5572, -3.0043,\n",
      "         -3.6942,  0.1316, -1.8236, -0.6919, -1.0947, -2.7567, -4.1744, -5.6436,\n",
      "         -3.4044, -1.1597, -3.9381, -4.7725, -3.0757, -3.9838, -2.6120, -3.7096,\n",
      "         -5.1737, -3.2098, -1.1174, -1.6503, -2.1402, -3.3919, -2.0980, -1.1207,\n",
      "         -3.7967, -1.4985, -3.5750, -4.8122, -3.1558, -1.2969, -1.3726, -3.5245,\n",
      "         -2.9642, -3.3087, -3.4977, -3.5303, -0.8305, -1.3725, -4.0874, -2.7994,\n",
      "         -1.0537, -2.0760, -3.3558, -4.3913, -0.6508, -5.5949, -1.5394, -5.4040,\n",
      "         -3.7574, -4.9909, -0.1053, -1.1327, -2.5460, -1.5247, -1.5055, -1.6940,\n",
      "         -2.6162, -4.8191, -2.7631, -1.4446, -1.3440, -3.0958, -2.5716, -1.0140,\n",
      "         -0.7865, -3.7372, -1.8294, -1.6274, -3.6619, -3.9078, -1.8165, -2.5708,\n",
      "         -3.2142, -2.8195, -2.2052, -3.3785, -2.4805, -6.0444, -1.9474, -3.0573,\n",
      "         -2.0638, -1.4279, -1.0748, -1.6079, -1.3890, -1.3648, -1.0605, -4.2008,\n",
      "         -5.2821, -0.9025, -2.4062, -4.8779, -6.1132, -4.6236, -3.0583, -5.2013,\n",
      "         -2.1051, -1.2900, -1.6689, -1.5119, -1.4102, -0.6696, -1.5314, -0.8533,\n",
      "         -1.2111, -1.4602, -1.7798, -4.9130, -4.8544, -5.1032, -5.3042, -4.6505,\n",
      "         -4.7557, -4.8235, -4.9045, -4.8523, -4.8040, -4.8709, -4.9837, -4.9792,\n",
      "         -4.9144, -4.8775, -4.8662, -4.8661, -4.8749, -4.8848, -4.8967, -4.9148,\n",
      "         -4.9396, -4.9626, -4.9747, -4.9795, -4.9790, -4.9811, -4.9936, -5.0219,\n",
      "         -5.0596, -5.0743, -5.0572, -5.0214, -4.9693, -4.9157, -4.8550, -4.8105,\n",
      "         -4.7718, -4.7469, -4.7253, -4.7079, -4.7016, -4.6908, -4.6768, -4.6537,\n",
      "         -4.6397, -4.6456, -4.6501, -4.6461, -4.6299, -4.6218, -4.6103, -4.6098,\n",
      "         -4.6277, -4.7019],\n",
      "        [-2.3394, -4.1969, -1.1740, -2.2134, -0.3917, -2.5706, -4.1549, -0.4557,\n",
      "         -3.4570, -2.4018, -3.5008, -0.2596, -0.4781, -3.6447, -3.3455, -2.7725,\n",
      "         -3.3031, -2.1065, -2.4440, -1.4500, -1.6664, -1.5671, -1.2450, -0.1786,\n",
      "         -2.1664, -3.7096, -3.5648, -2.9007, -3.8155,  0.1746, -2.0222, -0.6583,\n",
      "         -1.5248, -3.0825, -2.2164, -1.5980, -1.3956, -3.3333, -2.0174, -0.5000,\n",
      "         -1.0950, -1.1881, -0.8179, -5.0773, -1.8902, -2.9072, -3.4244, -2.9580,\n",
      "         -1.3400, -1.3577, -3.9365, -1.8147, -7.4706, -3.3777, -3.2414, -1.3303,\n",
      "         -2.8769, -3.2838, -2.9583, -1.9664, -3.7874, -1.5147, -6.5571, -3.0000,\n",
      "         -3.6977,  0.1289, -1.8417, -0.7335, -1.0930, -2.8165, -4.1756, -5.6474,\n",
      "         -3.4024, -1.1324, -3.9460, -4.7656, -3.0753, -4.0020, -2.6368, -3.7167,\n",
      "         -5.1793, -3.2069, -1.1126, -1.6915, -2.1453, -3.4093, -2.1169, -1.1532,\n",
      "         -3.7998, -1.5109, -3.5902, -4.8048, -3.1580, -1.2902, -1.4144, -3.4866,\n",
      "         -2.9792, -3.3337, -3.5094, -3.5326, -0.8247, -1.4129, -4.0560, -2.7938,\n",
      "         -1.0626, -2.0938, -3.3796, -4.3794, -0.6198, -2.2419, -1.4933, -2.7443,\n",
      "         -3.3233, -5.0288, -0.1125, -1.3005, -2.5165, -1.5540, -1.5490, -1.7168,\n",
      "         -2.5541, -4.8460, -2.7811, -1.4787, -1.4744, -3.0880, -2.6114, -1.0081,\n",
      "         -0.8833, -3.8128, -1.8629, -1.6200, -3.6612, -3.9008, -1.8457, -2.5712,\n",
      "         -3.2304, -2.8060, -2.2251, -3.3611, -2.4819, -5.9847, -1.9606, -2.5721,\n",
      "         -1.3042, -1.2314, -1.4690, -1.3499, -1.2492, -1.2340, -1.2653, -1.3143,\n",
      "         -3.9863, -4.9524, -0.9683, -2.6373, -5.0662, -6.2311, -4.6965, -5.3872,\n",
      "         -2.1811, -1.8486, -0.6253, -0.9320, -1.4395, -1.2541, -1.5677, -0.7770,\n",
      "         -1.3773, -1.4960, -4.4504, -4.6383, -0.5817, -1.9841, -4.4435, -5.5849,\n",
      "         -4.1089, -5.2727, -1.9372, -1.4620, -1.1046, -1.3739, -1.6213, -1.6649,\n",
      "         -1.6827, -1.6443, -1.5946, -1.5293, -4.3481, -4.5563, -0.6773, -2.0396,\n",
      "         -4.4051, -5.7244, -4.1939, -3.3827, -5.2465, -2.1244, -1.4538, -1.4022,\n",
      "         -1.4865, -1.6442, -1.5656, -1.4209, -1.6148, -1.4402, -1.4411, -2.0840,\n",
      "         -4.9629, -4.8280, -5.0934, -5.4011, -4.8825, -4.8419, -4.9111, -4.8017,\n",
      "         -4.7798, -4.8810, -4.9319, -4.9218, -4.9211, -4.9342, -4.9567, -4.9766,\n",
      "         -4.9837, -4.9664]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3386, -1.5553], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.6621, -1.5601, -1.7535, -1.7333, -1.1000, -2.8131, -1.5674, -1.7298],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3041, -0.3072, -0.3103,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3188, -0.3220, -0.3253,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2041, -0.2062, -0.2082,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.4085, -0.4126, -0.4167,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3046, -0.3077, -0.3108,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2749, -0.2777, -0.2805,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6374, -1.5292, -1.6766, -1.6574, -1.0518, -2.7301, -1.4987, -1.7126],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3011, -4.2357, -1.1723,  ..., -4.8983, -4.9362, -4.9486],\n",
      "        [-2.2592, -4.2392, -1.1583,  ..., -4.7862, -4.7850, -4.7862],\n",
      "        [-2.3349, -4.1954, -1.1587,  ..., -4.8883, -4.8538, -4.8289],\n",
      "        ...,\n",
      "        [-2.2629, -4.2019, -1.1718,  ..., -4.7101, -4.6809, -4.6479],\n",
      "        [-2.3278, -4.2271, -1.1539,  ..., -5.1179, -5.0883, -5.0654],\n",
      "        [-2.3124, -4.2064, -1.1588,  ..., -4.7669, -4.7466, -4.7238]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5439, -1.3903, -1.6959, -1.6847, -1.4977, -2.2648, -1.2565, -1.5775],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.7547, -1.7142, -2.6201, -1.7485, -1.6995, -1.6889, -1.7663, -1.7030],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1982, -0.2002, -0.2022,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1936, -0.1956, -0.1975,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4335, -0.4379, -0.4423,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1907, -0.1927, -0.1946,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2390, -0.2415, -0.2439,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2790, -0.2818, -0.2846,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6778, -1.6391, -2.5053, -1.7311, -1.6251, -1.6149, -1.6889, -1.6284],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3611, -4.1874, -1.1661,  ..., -5.2238, -5.2006, -5.1688],\n",
      "        [-2.3554, -4.1877, -1.1865,  ..., -5.0920, -5.0728, -5.0427],\n",
      "        [-2.3516, -4.2112, -1.1540,  ..., -4.8443, -4.9162, -4.9825],\n",
      "        ...,\n",
      "        [-2.3813, -4.1844, -1.1681,  ..., -5.1029, -5.1568, -5.1947],\n",
      "        [-2.2989, -4.1829, -1.1660,  ..., -5.1078, -5.1247, -5.1395],\n",
      "        [-2.2771, -4.1999, -1.1509,  ..., -5.1362, -5.1317, -5.1601]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4058, -1.7402, -1.7860, -1.6222, -1.4395, -1.2142, -1.4778, -1.3266],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.8107, -1.6993, -1.7309, -1.1000, -1.7933, -1.7078, -1.8105, -1.6962],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2966, -0.2996, -0.3026,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2812, -0.2840, -0.2869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2414, -0.2439, -0.2463,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2335, -0.2358, -0.2382,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2307, -0.2330, -0.2354,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1916, -0.1935, -0.1955,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.7313, -1.6249, -1.6550, -1.0518, -1.7147, -1.6329, -1.7311, -1.6219],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2520, -4.2061, -1.1585,  ..., -4.9585, -5.0134, -5.0412],\n",
      "        [-2.2693, -4.2013, -1.1567,  ..., -5.0433, -5.0949, -5.1628],\n",
      "        [-2.2911, -4.1913, -1.1734,  ..., -4.8202, -4.8449, -4.8626],\n",
      "        ...,\n",
      "        [-2.3062, -4.1924, -1.1752,  ..., -4.8231, -4.8189, -4.8386],\n",
      "        [-2.3248, -4.1902, -1.1684,  ..., -4.9103, -4.9055, -4.9024],\n",
      "        [-2.3747, -4.1857, -1.1820,  ..., -5.1313, -5.1145, -5.0817]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5355, -1.6737, -1.7771, -1.2355, -1.4983, -1.7753, -1.7340, -1.5552],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000, -1.7882, -1.5211, -1.6121, -1.1000, -1.7924, -1.7471, -1.6940],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0974, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2020, -0.2040, -0.2061,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2956, -0.2986, -0.3016,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2525, -0.2551, -0.2576,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2013, -0.2033, -0.2054,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1952, -0.1972, -0.1992,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0782, -1.7098, -1.4545, -1.5415, -1.0518, -1.7657, -1.6705, -1.6198],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3586, -4.1917, -1.2008,  ..., -4.8984, -4.8649, -4.8378],\n",
      "        [-2.3442, -4.1913, -1.2127,  ..., -4.9484, -4.9606, -4.9508],\n",
      "        [-2.3714, -4.2333, -1.1859,  ..., -4.8817, -4.8532, -4.8359],\n",
      "        ...,\n",
      "        [-2.3265, -4.1999, -1.1951,  ..., -4.8608, -4.8469, -4.8341],\n",
      "        [-2.3611, -4.1840, -1.1916,  ..., -4.6929, -4.6820, -4.6667],\n",
      "        [-2.3571, -4.1827, -1.1896,  ..., -4.9681, -4.9420, -4.9173]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5005, -1.6312, -1.5564, -1.2446, -1.8061, -1.7504, -1.5810, -1.8139],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.7700, -1.6138, -1.7314, -1.5888, -1.6346, -1.5655, -1.6356, -1.6817],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2494, -0.2519, -0.2544,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3136, -0.3168, -0.3200,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2836, -0.2865, -0.2894,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3042, -0.3073, -0.3104,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3178, -0.3210, -0.3243,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2783, -0.2811, -0.2839,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6924, -1.5431, -1.6555, -1.5419, -1.5630, -1.4969, -1.5639, -1.6080],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2712, -4.1872, -1.1948,  ..., -4.8745, -4.8966, -4.9034],\n",
      "        [-2.2938, -4.2007, -1.1690,  ..., -4.9062, -4.8858, -4.8745],\n",
      "        [-2.2811, -4.1960, -1.1964,  ..., -4.5830, -4.6029, -4.6513],\n",
      "        ...,\n",
      "        [-2.2574, -4.1988, -1.1751,  ..., -4.9989, -4.9604, -4.9411],\n",
      "        [-2.3335, -4.2256, -1.1908,  ..., -5.0331, -5.0252, -5.0237],\n",
      "        [-2.2979, -4.2103, -1.1928,  ..., -4.7445, -4.8047, -4.8904]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.8045, -1.2024, -1.4874, -1.2804, -1.2992, -1.2635, -1.3344, -1.7322],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([ 0.9900, -1.7131, -1.7507, -1.5461, -1.6808, -1.8000, -1.6002, -1.8382],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.1115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1935, -0.1954, -0.1974,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2839, -0.2868, -0.2897,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2436, -0.2461, -0.2485,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3141, -0.3173, -0.3205,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2538, -0.2564, -0.2590,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([ 0.9656, -1.6381, -1.6740, -1.5005, -1.6072, -1.7211, -1.5376, -1.7576],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.8033, -4.2805, -1.3298,  ..., -5.6370, -5.6616, -5.6186],\n",
      "        [-2.3615, -4.1758, -1.2071,  ..., -5.2043, -5.1681, -5.1247],\n",
      "        [-2.2969, -4.2029, -1.2068,  ..., -4.5042, -4.4999, -4.5204],\n",
      "        ...,\n",
      "        [-2.2954, -4.1927, -1.2084,  ..., -4.9519, -4.9349, -4.9116],\n",
      "        [-2.3075, -4.2106, -1.1973,  ..., -5.1222, -5.1059, -5.0984],\n",
      "        [-2.3152, -4.1883, -1.2108,  ..., -4.6854, -4.6693, -4.6546]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-0.7962, -1.6047, -1.8189, -1.2132, -1.4540, -1.6204, -1.3935, -1.8952],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.7199, -1.6596], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2351, -0.2375, -0.2399, -0.2423, -0.2448, -0.2472, -0.2497, -0.2522,\n",
      "         -0.2548, -0.2574, -0.2600, -0.2626, -0.2652, -0.2679, -0.2706, -0.2734,\n",
      "         -0.2761, -0.2789, -0.2817, -0.2846, -0.2875, -0.2904, -0.2933, -0.2963,\n",
      "         -0.2992, -0.3023, -0.3053, -0.3084, -0.3115, -0.3147, -0.3178, -0.3211,\n",
      "         -0.3243, -0.3276, -0.3309, -0.3342, -0.3376, -0.3410, -0.3445, -0.3479,\n",
      "         -0.3515, -0.3550, -0.3586, -0.3622, -0.3659, -0.3696, -0.3733, -0.3771,\n",
      "         -0.3809, -0.3847, -0.3886, -0.3925, -0.3965, -0.4005, -0.4046, -0.4086,\n",
      "         -0.4128, -0.4169, -0.4211, -0.4254, -0.4297, -0.4340, -0.4384, -0.4429,\n",
      "         -0.4473, -0.4518, -0.4564, -0.4610, -0.4657, -0.4704, -0.4751, -0.4799,\n",
      "         -0.4848, -0.4897, -0.4946, -0.4996, -0.5047, -0.5098, -0.5149, -0.5201,\n",
      "         -0.5254, -0.5307, -0.5360, -0.5414, -0.5469, -0.5524, -0.5580, -0.5637,\n",
      "         -0.5693, -0.5751, -0.5809, -0.5868, -0.5927, -0.5987, -0.6047, -0.6108,\n",
      "         -0.6170, -0.6232, -0.6295, -0.6359, -0.6423, -0.6488, -0.6554, -0.6620,\n",
      "         -0.6687, -0.6754, -0.6823, -0.6891, -0.6961, -0.7031, -0.7102, -0.7174,\n",
      "         -0.7247, -0.7320, -0.7394, -0.7468, -0.7544, -0.7620, -0.7697, -0.7775,\n",
      "         -0.7853, -0.7933, -0.8013, -0.8094, -0.8175, -0.8258, -0.8341, -0.8426,\n",
      "         -0.8511, -0.8597, -0.8684, -0.8771, -0.8860, -0.8949, -0.9040, -0.9131,\n",
      "         -0.9223, -0.9316, -0.9411, -0.9506, -0.9602, -0.9699, -0.9797, -0.9896,\n",
      "         -0.9996, -1.0096, -1.0198, -1.0301, -1.0406, -1.0511, -1.0617, -1.0724,\n",
      "         -1.0832, -1.0942, -1.1052, -1.1164, -1.1277, -1.1391, -1.1506, -1.1622,\n",
      "         -1.1739, -1.1858, -1.1978, -1.2099, -1.2221, -1.2344, -1.2469, -1.2595,\n",
      "         -1.2722, -1.2851, -1.2980, -1.3112, -1.3244, -1.3378, -1.3513, -1.3649,\n",
      "         -1.3787, -1.3927, -1.4067, -1.4209, -1.4353, -1.4498, -1.4644, -1.4792,\n",
      "         -1.4942, -1.5093, -1.5245, -1.5399, -1.5555, -1.5712, -1.5870, -1.6031,\n",
      "         -1.6193, -1.6356, -1.6521, -1.6688, -1.6857, -1.7027, -1.7199,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3067, -0.3098, -0.3129, -0.3161, -0.3193, -0.3225, -0.3258, -0.3291,\n",
      "         -0.3324, -0.3357, -0.3391, -0.3426, -0.3460, -0.3495, -0.3530, -0.3566,\n",
      "         -0.3602, -0.3638, -0.3675, -0.3712, -0.3750, -0.3788, -0.3826, -0.3865,\n",
      "         -0.3904, -0.3943, -0.3983, -0.4023, -0.4064, -0.4105, -0.4146, -0.4188,\n",
      "         -0.4230, -0.4273, -0.4316, -0.4360, -0.4404, -0.4448, -0.4493, -0.4539,\n",
      "         -0.4585, -0.4631, -0.4678, -0.4725, -0.4773, -0.4821, -0.4870, -0.4919,\n",
      "         -0.4968, -0.5019, -0.5069, -0.5121, -0.5172, -0.5225, -0.5277, -0.5331,\n",
      "         -0.5384, -0.5439, -0.5494, -0.5549, -0.5605, -0.5662, -0.5719, -0.5777,\n",
      "         -0.5835, -0.5894, -0.5954, -0.6014, -0.6075, -0.6136, -0.6198, -0.6261,\n",
      "         -0.6324, -0.6388, -0.6452, -0.6517, -0.6583, -0.6650, -0.6717, -0.6785,\n",
      "         -0.6853, -0.6923, -0.6992, -0.7063, -0.7134, -0.7206, -0.7279, -0.7353,\n",
      "         -0.7427, -0.7502, -0.7578, -0.7654, -0.7732, -0.7810, -0.7889, -0.7968,\n",
      "         -0.8049, -0.8130, -0.8212, -0.8295, -0.8379, -0.8464, -0.8549, -0.8636,\n",
      "         -0.8723, -0.8811, -0.8900, -0.8990, -0.9081, -0.9172, -0.9265, -0.9359,\n",
      "         -0.9453, -0.9549, -0.9645, -0.9742, -0.9841, -0.9940, -1.0041, -1.0142,\n",
      "         -1.0244, -1.0348, -1.0453, -1.0558, -1.0665, -1.0772, -1.0881, -1.0991,\n",
      "         -1.1102, -1.1214, -1.1328, -1.1442, -1.1558, -1.1674, -1.1792, -1.1911,\n",
      "         -1.2032, -1.2153, -1.2276, -1.2400, -1.2525, -1.2652, -1.2780, -1.2909,\n",
      "         -1.3039, -1.3171, -1.3304, -1.3438, -1.3574, -1.3711, -1.3850, -1.3989,\n",
      "         -1.4131, -1.4273, -1.4418, -1.4563, -1.4710, -1.4859, -1.5009, -1.5161,\n",
      "         -1.5314, -1.5468, -1.5625, -1.5783, -1.5942, -1.6103, -1.6266, -1.6430,\n",
      "         -1.6596,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6445, -1.6267], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2740e+00, -4.1886e+00, -1.2073e+00, -2.2317e+00, -4.3157e-01,\n",
      "         -2.6061e+00, -4.1227e+00, -4.3418e-01, -3.3658e+00, -2.3409e+00,\n",
      "         -3.4626e+00, -3.0714e-01, -4.9041e-01, -3.6789e+00, -3.3458e+00,\n",
      "         -2.7845e+00, -3.2704e+00, -2.1463e+00, -2.5347e+00, -1.5010e+00,\n",
      "         -1.6830e+00, -1.5842e+00, -1.2762e+00, -2.6977e-01, -2.1884e+00,\n",
      "         -3.7468e+00, -3.5715e+00, -2.9391e+00, -3.8624e+00,  5.5200e-02,\n",
      "         -2.0683e+00, -7.0747e-01, -1.6184e+00, -3.0609e+00, -2.2450e+00,\n",
      "         -1.7330e+00, -1.5030e+00, -3.3144e+00, -2.0705e+00, -5.4001e-01,\n",
      "         -1.1430e+00, -1.3068e+00, -9.0874e-01, -5.1568e+00, -1.9217e+00,\n",
      "         -2.8594e+00, -3.3693e+00, -2.9350e+00, -1.4318e+00, -1.4852e+00,\n",
      "         -3.9899e+00, -1.8090e+00, -7.4470e+00, -3.4298e+00, -3.2394e+00,\n",
      "         -1.3796e+00, -2.9600e+00, -3.3581e+00, -2.9451e+00, -1.9990e+00,\n",
      "         -3.8253e+00, -1.5105e+00, -6.5452e+00, -3.0676e+00, -3.7500e+00,\n",
      "          1.5055e-03, -1.8938e+00, -7.8464e-01, -1.2028e+00, -2.7956e+00,\n",
      "         -4.1418e+00, -5.5760e+00, -3.4596e+00, -1.2565e+00, -3.9882e+00,\n",
      "         -4.7759e+00, -3.1345e+00, -3.9913e+00, -2.6849e+00, -3.7670e+00,\n",
      "         -5.1121e+00, -3.2715e+00, -1.1969e+00, -1.7491e+00, -2.1711e+00,\n",
      "         -3.4284e+00, -2.1705e+00, -1.2211e+00, -3.8125e+00, -1.5485e+00,\n",
      "         -3.6452e+00, -4.8191e+00, -3.2268e+00, -1.3817e+00, -1.4770e+00,\n",
      "         -3.5577e+00, -3.0076e+00, -3.2978e+00, -3.4954e+00, -3.6214e+00,\n",
      "         -9.1439e-01, -1.4759e+00, -4.1067e+00, -2.8241e+00, -1.1439e+00,\n",
      "         -2.0928e+00, -3.3510e+00, -4.3613e+00, -7.5678e-01, -3.3396e+00,\n",
      "         -1.6104e+00, -4.5451e+00, -3.7296e+00, -4.9664e+00, -2.1817e-01,\n",
      "         -1.1841e+00, -2.5412e+00, -1.5340e+00, -1.5605e+00, -1.7352e+00,\n",
      "         -2.5797e+00, -4.7377e+00, -2.8088e+00, -1.5177e+00, -1.4645e+00,\n",
      "         -3.0646e+00, -2.5858e+00, -1.0583e+00, -8.4265e-01, -3.7498e+00,\n",
      "         -1.8508e+00, -1.6622e+00, -3.6888e+00, -3.9130e+00, -1.8808e+00,\n",
      "         -2.6671e+00, -3.2765e+00, -2.8128e+00, -2.2656e+00, -3.3775e+00,\n",
      "         -2.4826e+00, -5.9771e+00, -1.9538e+00, -3.1364e+00, -1.8903e+00,\n",
      "         -1.5850e+00, -1.3581e+00, -1.4118e+00, -1.6758e+00, -1.3744e+00,\n",
      "         -1.3754e+00, -3.8653e+00, -5.2294e+00, -8.8314e-01, -2.5092e+00,\n",
      "         -4.8884e+00, -6.1392e+00, -4.5447e+00, -5.4069e+00, -2.1458e+00,\n",
      "         -1.7619e+00, -1.6300e+00, -1.6177e+00, -1.6165e+00, -1.7690e+00,\n",
      "         -1.7015e+00, -1.7227e+00, -1.7261e+00, -1.4469e+00, -4.6478e+00,\n",
      "         -4.6104e+00, -6.7892e-01, -2.0991e+00, -4.3944e+00, -5.5089e+00,\n",
      "         -4.2371e+00, -3.3372e+00, -5.1605e+00, -2.2178e+00, -1.7598e+00,\n",
      "         -1.7526e+00, -1.6071e+00, -1.5990e+00, -1.6803e+00, -1.6345e+00,\n",
      "         -1.6106e+00, -1.4774e+00, -1.6455e+00, -2.0282e+00, -4.9359e+00,\n",
      "         -4.7987e+00, -5.0604e+00, -5.1668e+00, -5.0284e+00, -5.1909e+00,\n",
      "         -5.1931e+00, -5.1569e+00, -5.1123e+00, -5.0318e+00, -4.9444e+00,\n",
      "         -4.8946e+00, -4.8525e+00, -4.8093e+00, -4.7842e+00, -4.7711e+00,\n",
      "         -4.7610e+00, -4.7505e+00, -4.7374e+00, -4.7291e+00, -4.7317e+00,\n",
      "         -4.7344e+00, -4.7326e+00, -4.7272e+00, -4.7216e+00, -4.7208e+00,\n",
      "         -4.7278e+00, -4.7331e+00, -4.7461e+00, -4.7511e+00, -4.7610e+00,\n",
      "         -4.7688e+00, -4.7708e+00, -4.7673e+00, -4.7638e+00, -4.7600e+00,\n",
      "         -4.7731e+00],\n",
      "        [-2.2837e+00, -4.2242e+00, -1.2203e+00, -2.2394e+00, -3.8819e-01,\n",
      "         -2.5932e+00, -4.1650e+00, -4.3575e-01, -3.3806e+00, -2.3601e+00,\n",
      "         -3.4499e+00, -3.0925e-01, -4.9320e-01, -3.6755e+00, -3.3659e+00,\n",
      "         -2.7880e+00, -3.2863e+00, -2.1255e+00, -2.5399e+00, -1.4892e+00,\n",
      "         -1.7170e+00, -1.5850e+00, -1.2629e+00, -2.7159e-01, -2.1770e+00,\n",
      "         -3.7557e+00, -3.5995e+00, -2.9353e+00, -3.8388e+00,  8.3195e-02,\n",
      "         -2.0469e+00, -6.8643e-01, -1.6145e+00, -3.0565e+00, -2.2263e+00,\n",
      "         -1.7319e+00, -1.4784e+00, -3.3315e+00, -2.0604e+00, -5.2849e-01,\n",
      "         -1.1430e+00, -1.2784e+00, -8.9353e-01, -5.1544e+00, -1.9100e+00,\n",
      "         -2.8759e+00, -3.3986e+00, -2.9319e+00, -1.3662e+00, -1.4099e+00,\n",
      "         -3.9879e+00, -1.8313e+00, -7.4649e+00, -3.4212e+00, -3.2511e+00,\n",
      "         -1.3505e+00, -2.9663e+00, -3.3691e+00, -2.9814e+00, -1.9999e+00,\n",
      "         -3.8321e+00, -1.5354e+00, -6.5694e+00, -3.0593e+00, -3.7282e+00,\n",
      "          2.9981e-02, -1.8721e+00, -7.6629e-01, -1.1944e+00, -2.7926e+00,\n",
      "         -4.1720e+00, -5.6165e+00, -3.4513e+00, -1.2030e+00, -3.9912e+00,\n",
      "         -4.8046e+00, -3.1246e+00, -4.0012e+00, -2.6671e+00, -3.7681e+00,\n",
      "         -5.1575e+00, -3.2628e+00, -1.1835e+00, -1.7340e+00, -2.2097e+00,\n",
      "         -3.4314e+00, -2.1551e+00, -1.2046e+00, -3.8439e+00, -1.5508e+00,\n",
      "         -3.6460e+00, -4.8407e+00, -3.2156e+00, -1.3575e+00, -1.4600e+00,\n",
      "         -3.5794e+00, -2.9917e+00, -3.3107e+00, -3.5119e+00, -3.6200e+00,\n",
      "         -9.0281e-01, -1.4654e+00, -4.0944e+00, -2.8265e+00, -1.1425e+00,\n",
      "         -2.0969e+00, -3.3667e+00, -4.3645e+00, -7.0491e-01, -4.3482e+00,\n",
      "         -1.7060e+00, -3.4432e+00, -3.5017e+00, -4.9836e+00, -1.4283e-01,\n",
      "         -1.2975e+00, -2.6453e+00, -1.5867e+00, -1.5967e+00, -1.7658e+00,\n",
      "         -2.6792e+00, -4.8529e+00, -2.8690e+00, -1.5569e+00, -1.5316e+00,\n",
      "         -3.1148e+00, -2.6344e+00, -1.0758e+00, -9.2561e-01, -3.7555e+00,\n",
      "         -1.8355e+00, -1.6300e+00, -3.6816e+00, -3.9538e+00, -1.8827e+00,\n",
      "         -2.6719e+00, -3.2973e+00, -2.8326e+00, -2.2755e+00, -3.4028e+00,\n",
      "         -2.5047e+00, -6.0448e+00, -1.9507e+00, -2.6302e+00, -3.5510e+00,\n",
      "         -5.2824e+00, -1.0886e+00, -2.5413e+00, -5.0200e+00, -6.1491e+00,\n",
      "         -4.4114e+00, -3.1416e+00, -5.3079e+00, -2.1018e+00, -1.3599e+00,\n",
      "         -1.1758e+00, -1.4611e+00, -1.5179e+00, -1.8235e+00, -4.7950e+00,\n",
      "         -4.8249e+00, -5.0155e+00, -5.4300e+00, -4.8119e+00, -4.6809e+00,\n",
      "         -4.7902e+00, -4.6589e+00, -4.5496e+00, -4.4781e+00, -4.4494e+00,\n",
      "         -4.4655e+00, -4.5171e+00, -4.5801e+00, -4.6384e+00, -4.6923e+00,\n",
      "         -4.7476e+00, -4.7957e+00, -4.8286e+00, -4.8427e+00, -4.8352e+00,\n",
      "         -4.8249e+00, -4.8057e+00, -4.7964e+00, -4.7997e+00, -4.8026e+00,\n",
      "         -4.8053e+00, -4.8101e+00, -4.8257e+00, -4.8445e+00, -4.8665e+00,\n",
      "         -4.8857e+00, -4.9058e+00, -4.9260e+00, -4.9408e+00, -4.9473e+00,\n",
      "         -4.9571e+00, -4.9655e+00, -4.9713e+00, -4.9715e+00, -4.9682e+00,\n",
      "         -4.9603e+00, -4.9515e+00, -4.9411e+00, -4.9355e+00, -4.9289e+00,\n",
      "         -4.9296e+00, -4.9223e+00, -4.9115e+00, -4.8933e+00, -4.8692e+00,\n",
      "         -4.8439e+00, -4.8170e+00, -4.7914e+00, -4.7585e+00, -4.7252e+00,\n",
      "         -4.6825e+00, -4.6454e+00, -4.6450e+00, -4.6969e+00, -4.7276e+00,\n",
      "         -4.7836e+00, -4.8314e+00, -4.8508e+00, -4.8772e+00, -4.8425e+00,\n",
      "         -4.7600e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.6795, -1.4676], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4748, -1.6909, -1.5376, -1.7260, -1.7752, -1.7350, -1.7390, -1.6739],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2866, -0.2895, -0.2924,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1910, -0.1929, -0.1948,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3079, -0.3111, -0.3142,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1959, -0.1979, -0.1999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2024, -0.2045, -0.2065,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2770, -0.2798, -0.2826,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4101, -1.6168, -1.4922, -1.7088, -1.6974, -1.6590, -1.6628, -1.6006],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3544, -4.2243, -1.2284,  ..., -4.8551, -4.8311, -4.8148],\n",
      "        [-2.3522, -4.1710, -1.2287,  ..., -5.1941, -5.1607, -5.1145],\n",
      "        [-2.2728, -4.2025, -1.2378,  ..., -4.8172, -4.8192, -4.8236],\n",
      "        ...,\n",
      "        [-2.3436, -4.1708, -1.2274,  ..., -5.2150, -5.1907, -5.1569],\n",
      "        [-2.3278, -4.1811, -1.2298,  ..., -4.8634, -4.8376, -4.8248],\n",
      "        [-2.2507, -4.1925, -1.2103,  ..., -5.0573, -5.0988, -5.1535]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5875, -1.6342, -1.3126, -1.7099, -1.6350, -1.5045, -1.8115, -1.7235],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.7091, -1.1000, -1.7276, -1.6284, -1.7498, -1.6713, -1.7693, -1.7363],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2336, -0.2360, -0.2384,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2434, -0.2458, -0.2483,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2738, -0.2765, -0.2793,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2898, -0.2927, -0.2957,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2350, -0.2374, -0.2398,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.6342, -1.0518, -1.6519, -1.5570, -1.6731, -1.5981, -1.6917, -1.6602],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2981, -4.1781, -1.2369,  ..., -4.5658, -4.5544, -4.5571],\n",
      "        [-2.3523, -4.1906, -1.2575,  ..., -1.2115, -1.2432, -1.9653],\n",
      "        [-2.2570, -4.1804, -1.2368,  ..., -4.8884, -4.9108, -4.9239],\n",
      "        ...,\n",
      "        [-2.2596, -4.1880, -1.2244,  ..., -5.2034, -5.1753, -5.2180],\n",
      "        [-2.2299, -4.1951, -1.2192,  ..., -5.0362, -5.0702, -5.0949],\n",
      "        [-2.2788, -4.1688, -1.2376,  ..., -5.0944, -5.1105, -5.1260]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.7773, -1.2670, -1.8298, -1.7344, -1.5914, -1.4807, -1.5667, -1.6610],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5271, -1.6515, -1.5201, -1.6249, -1.7574, -1.6475, -1.6940, -1.7055],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2968, -0.2998, -0.3028,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2816, -0.2845, -0.2873,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2954, -0.2984, -0.3014,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1861, -0.1879, -0.1898,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1952, -0.1972, -0.1992,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2766, -0.2794, -0.2822,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4602, -1.5791, -1.4535, -1.6007, -1.6804, -1.5753, -1.6198, -1.6885],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2957, -4.2117, -1.2449,  ..., -5.0280, -4.9945, -4.9724],\n",
      "        [-2.2407, -4.2029, -1.2372,  ..., -4.9343, -4.9389, -4.9561],\n",
      "        [-2.2748, -4.1957, -1.2251,  ..., -4.8628, -4.8429, -4.8316],\n",
      "        ...,\n",
      "        [-2.3542, -4.1694, -1.2485,  ..., -5.1654, -5.2105, -5.2368],\n",
      "        [-2.3318, -4.1733, -1.2511,  ..., -4.7278, -4.7264, -4.7163],\n",
      "        [-2.2616, -4.2004, -1.2570,  ..., -4.6603, -4.6205, -4.5871]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4183, -1.5547, -1.2861, -1.6332, -1.7938, -1.3855, -1.6226, -1.6968],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.6615, -1.4903, -1.6393, -1.7206, -1.4266,  0.9900, -1.1000, -1.7682],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2317, -0.2341, -0.2364,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2925, -0.2955, -0.2985,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2241, -0.2264, -0.2286,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2441, -0.2466, -0.2491,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5887, -1.4321, -1.5675, -1.6949, -1.3845,  0.9656, -1.0518, -1.6907],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2580, -4.1766, -1.2536,  ..., -4.8338, -4.8592, -4.8835],\n",
      "        [-2.2923, -4.2029, -1.2537,  ..., -5.0904, -5.0717, -5.0612],\n",
      "        [-2.2748, -4.1774, -1.2540,  ..., -4.8270, -4.8301, -4.8515],\n",
      "        ...,\n",
      "        [-2.7936, -4.2846, -1.3854,  ..., -5.5414, -5.5560, -5.5238],\n",
      "        [-2.3562, -4.1740, -1.2816,  ..., -1.5050, -1.6057, -1.7324],\n",
      "        [-2.2968, -4.1804, -1.2618,  ..., -4.7580, -4.7481, -4.7462]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.8357, -1.4036, -1.8349, -1.7397, -1.2643, -0.6524, -1.5336, -1.8375],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-2.5355, -1.6451, -1.6201, -1.5527, -1.4642, -1.6495, -1.7069, -1.6438],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.4195, -0.4238, -0.4281,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2668, -0.2695, -0.2722,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1830, -0.1848, -0.1867,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2255, -0.2278, -0.2301,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1928, -0.1947, -0.1967,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1856, -0.1875, -0.1894,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-2.4244, -1.5730, -1.5491, -1.4846, -1.4001, -1.5772, -1.6321, -1.5717],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2857, -4.1916, -1.2525,  ..., -4.6138, -4.6918, -4.7831],\n",
      "        [-2.2764, -4.1993, -1.2685,  ..., -4.6212, -4.5963, -4.6017],\n",
      "        [-2.3355, -4.1701, -1.2712,  ..., -5.0987, -5.0758, -5.0380],\n",
      "        ...,\n",
      "        [-2.2547, -4.1829, -1.2578,  ..., -4.8011, -4.8051, -4.8222],\n",
      "        [-2.3082, -4.1761, -1.2937,  ..., -4.9533, -4.9643, -4.9535],\n",
      "        [-2.3151, -4.1694, -1.2877,  ..., -5.0585, -5.0380, -5.0017]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.8232, -1.7321, -1.6670, -1.4816, -1.3596, -1.6828, -1.7296, -1.8082],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.1000, -1.5310, -1.4285, -1.4666, -1.3884, -1.5974, -1.6025, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2829, -0.2858, -0.2887,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2919, -0.2949, -0.2978,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1841, -0.1859, -0.1878,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2625, -0.2652, -0.2678,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1058, -0.1068, -0.1079,  ..., -1.1000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0782, -1.5007, -1.4003, -1.4023, -1.3275, -1.5274, -1.5323, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3232, -4.1786, -1.2913,  ..., -4.9175, -4.8994, -4.8829],\n",
      "        [-2.2736, -4.2222, -1.2884,  ..., -4.8991, -4.8793, -4.7990],\n",
      "        [-2.2334, -4.2201, -1.2831,  ..., -4.7399, -4.7425, -4.7411],\n",
      "        ...,\n",
      "        [-2.3146, -4.1711, -1.2815,  ..., -4.9310, -4.9014, -4.8778],\n",
      "        [-2.2499, -4.1896, -1.2788,  ..., -4.6729, -4.6872, -4.7184],\n",
      "        [-2.3347, -4.1708, -1.2908,  ..., -1.7528, -4.8174, -4.7181]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5584, -1.4460, -1.4783, -1.5128, -1.2839, -1.7925, -1.6167, -1.7648],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4342, -2.7004], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2787, -0.2815, -0.2844, -0.2872, -0.2901, -0.2931, -0.2960, -0.2990,\n",
      "         -0.3020, -0.3051, -0.3082, -0.3113, -0.3144, -0.3176, -0.3208, -0.3241,\n",
      "         -0.3273, -0.3306, -0.3340, -0.3374, -0.3408, -0.3442, -0.3477, -0.3512,\n",
      "         -0.3547, -0.3583, -0.3619, -0.3656, -0.3693, -0.3730, -0.3768, -0.3806,\n",
      "         -0.3844, -0.3883, -0.3922, -0.3962, -0.4002, -0.4042, -0.4083, -0.4125,\n",
      "         -0.4166, -0.4208, -0.4251, -0.4294, -0.4337, -0.4381, -0.4425, -0.4470,\n",
      "         -0.4515, -0.4561, -0.4607, -0.4653, -0.4700, -0.4748, -0.4796, -0.4844,\n",
      "         -0.4893, -0.4942, -0.4992, -0.5043, -0.5094, -0.5145, -0.5197, -0.5250,\n",
      "         -0.5303, -0.5356, -0.5410, -0.5465, -0.5520, -0.5576, -0.5632, -0.5689,\n",
      "         -0.5747, -0.5805, -0.5863, -0.5923, -0.5982, -0.6043, -0.6104, -0.6166,\n",
      "         -0.6228, -0.6291, -0.6354, -0.6418, -0.6483, -0.6549, -0.6615, -0.6682,\n",
      "         -0.6749, -0.6817, -0.6886, -0.6956, -0.7026, -0.7097, -0.7169, -0.7241,\n",
      "         -0.7314, -0.7388, -0.7463, -0.7538, -0.7614, -0.7691, -0.7769, -0.7847,\n",
      "         -0.7927, -0.8007, -0.8088, -0.8169, -0.8252, -0.8335, -0.8419, -0.8504,\n",
      "         -0.8590, -0.8677, -0.8765, -0.8853, -0.8943, -0.9033, -0.9124, -0.9216,\n",
      "         -0.9309, -0.9404, -0.9499, -0.9594, -0.9691, -0.9789, -0.9888, -0.9988,\n",
      "         -1.0089, -1.0191, -1.0294, -1.0398, -1.0503, -1.0609, -1.0716, -1.0824,\n",
      "         -1.0934, -1.1044, -1.1156, -1.1268, -1.1382, -1.1497, -1.1613, -1.1731,\n",
      "         -1.1849, -1.1969, -1.2090, -1.2212, -1.2335, -1.2460, -1.2585, -1.2713,\n",
      "         -1.2841, -1.2971, -1.3102, -1.3234, -1.3368, -1.3503, -1.3639, -1.3777,\n",
      "         -1.3916, -1.4057, -1.4199, -1.4342,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3921, -0.3960, -0.4000, -0.4041, -0.4082, -0.4123, -0.4165, -0.4207,\n",
      "         -0.4249, -0.4292, -0.4335, -0.4379, -0.4423, -0.4468, -0.4513, -0.4559,\n",
      "         -0.4605, -0.4651, -0.4698, -0.4746, -0.4794, -0.4842, -0.4891, -0.4941,\n",
      "         -0.4990, -0.5041, -0.5092, -0.5143, -0.5195, -0.5248, -0.5301, -0.5354,\n",
      "         -0.5408, -0.5463, -0.5518, -0.5574, -0.5630, -0.5687, -0.5744, -0.5802,\n",
      "         -0.5861, -0.5920, -0.5980, -0.6040, -0.6101, -0.6163, -0.6225, -0.6288,\n",
      "         -0.6352, -0.6416, -0.6481, -0.6546, -0.6612, -0.6679, -0.6747, -0.6815,\n",
      "         -0.6884, -0.6953, -0.7023, -0.7094, -0.7166, -0.7238, -0.7311, -0.7385,\n",
      "         -0.7460, -0.7535, -0.7611, -0.7688, -0.7766, -0.7844, -0.7924, -0.8004,\n",
      "         -0.8084, -0.8166, -0.8249, -0.8332, -0.8416, -0.8501, -0.8587, -0.8674,\n",
      "         -0.8761, -0.8850, -0.8939, -0.9029, -0.9121, -0.9213, -0.9306, -0.9400,\n",
      "         -0.9495, -0.9591, -0.9688, -0.9785, -0.9884, -0.9984, -1.0085, -1.0187,\n",
      "         -1.0290, -1.0394, -1.0499, -1.0605, -1.0712, -1.0820, -1.0929, -1.1040,\n",
      "         -1.1151, -1.1264, -1.1378, -1.1493, -1.1609, -1.1726, -1.1844, -1.1964,\n",
      "         -1.2085, -1.2207, -1.2330, -1.2455, -1.2581, -1.2708, -1.2836, -1.2966,\n",
      "         -1.3097, -1.3229, -1.3363, -1.3498, -1.3634, -1.3772, -1.3911, -1.4051,\n",
      "         -1.4193, -1.4336, -1.4481, -1.4628, -1.4775, -1.4925, -1.5075, -1.5228,\n",
      "         -1.5381, -1.5537, -1.5694, -1.5852, -1.6012, -1.6174, -1.6337, -1.6502,\n",
      "         -1.6669, -1.6838, -1.7008, -1.7179, -1.7353, -1.7528, -1.7705, -1.7884,\n",
      "         -1.8065, -1.8247, -1.8432, -1.8618, -1.8806, -1.8996, -1.9188, -1.9381,\n",
      "         -1.9577, -1.9775, -1.9975, -2.0176, -2.0380, -2.0586, -2.0794, -2.1004,\n",
      "         -2.1216, -2.1431, -2.1647, -2.1866, -2.2087, -2.2310, -2.2535, -2.2763,\n",
      "         -2.2993, -2.3225, -2.3459, -2.3696, -2.3936, -2.4178, -2.4422, -2.4668,\n",
      "         -2.4918, -2.5169, -2.5424, -2.5680, -2.5940, -2.6202, -2.6466, -2.6734,\n",
      "         -2.7004,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3714, -2.6207], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3030, -4.2142, -1.2912, -2.3400, -0.4558, -2.6904, -4.2899, -0.5193,\n",
      "         -3.4006, -2.4826, -3.5367, -0.3988, -0.5836, -3.6777, -3.3413, -2.7647,\n",
      "         -3.3382, -2.1579, -2.5504, -1.6223, -1.7820, -1.7123, -1.3617, -0.3015,\n",
      "         -2.2935, -3.7694, -3.5656, -2.9061, -3.8657,  0.0377, -2.1374, -0.8344,\n",
      "         -1.7059, -3.1429, -2.3087, -1.6815, -1.4621, -3.3732, -2.1029, -0.6201,\n",
      "         -1.2182, -1.2823, -0.9428, -5.1383, -2.0168, -2.8780, -3.5217, -3.0122,\n",
      "         -1.1193, -1.2198, -3.9745, -1.8903, -7.4608, -3.3673, -3.2792, -1.4012,\n",
      "         -2.9813, -3.3776, -3.0261, -2.1105, -3.8397, -1.6040, -6.5701, -3.0039,\n",
      "         -3.7528, -0.0089, -1.9662, -0.9224, -1.2805, -2.8881, -4.1688, -5.5682,\n",
      "         -3.3931, -1.0981, -3.9940, -4.7817, -3.0653, -4.0346, -2.6989, -3.7710,\n",
      "         -5.1092, -3.1962, -1.1903, -1.8801, -2.2133, -3.4902, -2.2563, -1.3641,\n",
      "         -3.9345, -1.6402, -3.6585, -4.8192, -3.1506, -1.3567, -1.6091, -3.5809,\n",
      "         -3.0921, -3.3058, -3.4999, -3.6858, -0.9317, -1.6113, -4.0848, -2.8828,\n",
      "         -1.1776, -2.1442, -3.3568, -4.3507, -0.5966, -4.4147, -1.6039, -2.2060,\n",
      "         -3.9554, -5.0096, -0.1795, -1.4306, -2.6441, -1.6928, -1.4821, -1.7724,\n",
      "         -2.6607, -4.6868, -2.8960, -1.5345, -1.6133, -3.1506, -2.7221, -1.1319,\n",
      "         -1.0798, -3.8966, -1.5230, -1.4265, -3.7243, -4.0045, -1.8906, -2.6704,\n",
      "         -3.2863, -2.8956, -2.3470, -3.2981, -2.7870, -2.8409, -5.9871, -1.7877,\n",
      "         -1.3556, -1.3444, -1.2481, -1.4249, -1.2242, -1.3112, -1.3578, -1.4861,\n",
      "         -1.1535, -1.4179, -4.7714, -4.7903, -5.0562, -5.2259, -4.5484, -4.6724,\n",
      "         -4.9423, -4.9338, -4.9190, -4.8746, -4.8527, -4.8626, -4.8890, -4.9141,\n",
      "         -4.9357, -4.9447, -4.9407, -4.9294, -4.9115, -4.8886, -4.8695, -4.8495,\n",
      "         -4.8385, -4.8306, -4.8327, -4.8377, -4.8392, -4.8408, -4.8520, -4.8734,\n",
      "         -4.8953, -4.9128, -4.9243, -4.9319, -4.9309, -4.9295, -4.9252, -4.9258,\n",
      "         -4.9309, -4.9446, -4.9561, -4.9643, -4.9757, -4.9912, -5.0109, -5.0334,\n",
      "         -5.0656, -5.0930, -5.1188, -5.1445, -5.1504, -5.1638, -5.1684, -5.1688,\n",
      "         -5.1469, -5.1210, -5.0919, -5.0761, -5.0530, -5.0659, -5.0766, -5.1095,\n",
      "         -5.1295, -5.1378, -5.1353, -5.0801, -4.9919, -4.9920, -4.9798, -4.9705,\n",
      "         -4.9559, -4.9461],\n",
      "        [-2.2275, -4.1841, -1.2965, -2.2958, -0.4776, -2.6411, -4.1922, -0.5041,\n",
      "         -3.3458, -2.4177, -3.4878, -0.4062, -0.5892, -3.6655, -3.3458, -2.7919,\n",
      "         -3.2795, -2.1721, -2.5723, -1.5795, -1.7584, -1.6619, -1.3234, -0.3344,\n",
      "         -2.2618, -3.7336, -3.5610, -2.9447, -3.9407, -0.0293, -2.1488, -0.7903,\n",
      "         -1.6780, -3.0862, -2.2891, -1.7676, -1.5328, -3.3223, -2.1127, -0.5783,\n",
      "         -1.2243, -1.3297, -0.9712, -5.1580, -1.9971, -2.8074, -3.4386, -2.9530,\n",
      "         -1.2754, -1.4415, -3.9580, -1.8581, -7.4014, -3.4156, -3.2366, -1.4087,\n",
      "         -2.9718, -3.4048, -2.9795, -2.0722, -3.8072, -1.5648, -6.5131, -3.0650,\n",
      "         -3.8265, -0.0741, -1.9629, -0.8683, -1.2610, -2.8252, -4.1217, -5.5104,\n",
      "         -3.4461, -1.2230, -3.9611, -4.7582, -3.1157, -3.9970, -2.7195, -3.7443,\n",
      "         -5.0498, -3.2483, -1.2440, -1.8302, -2.1661, -3.4781, -2.2342, -1.3083,\n",
      "         -3.8421, -1.6219, -3.6272, -4.7954, -3.2040, -1.4254, -1.5570, -3.5419,\n",
      "         -3.0692, -3.2368, -3.4809, -3.6442, -0.9579, -1.5588, -4.1360, -2.8758,\n",
      "         -1.2034, -2.1252, -3.2784, -4.3538, -0.7097, -4.4379, -1.6794, -3.9861,\n",
      "         -3.5349, -4.9892, -0.2102, -1.3381, -2.6476, -1.6500, -1.5486, -1.7642,\n",
      "         -2.5475, -4.7480, -2.8399, -1.5762, -1.5224, -3.0806, -2.6747, -1.1174,\n",
      "         -0.9627, -3.7730, -1.6505, -1.6114, -3.6545, -3.9676, -1.9441, -2.6716,\n",
      "         -3.3110, -2.8349, -2.3321, -3.2842, -2.4508, -5.9536, -1.9631, -4.5384,\n",
      "         -1.7871, -1.5050, -1.4332, -3.9398, -5.2943, -0.9028, -2.5093, -3.2625,\n",
      "         -4.0495, -1.5483, -4.1796, -3.1737, -3.6789, -5.4103, -1.9586, -3.9593,\n",
      "         -2.0499, -1.7170, -1.5763, -3.9999, -4.7155, -0.8088, -2.0157, -2.8234,\n",
      "         -3.7458, -1.2763, -3.6470, -3.0592, -4.0014, -3.2164, -5.1122, -1.9564,\n",
      "         -2.6645, -2.7356, -2.0252, -2.7026, -1.8004, -1.9386, -1.6067, -4.6830,\n",
      "         -4.7350, -5.0072, -5.2139, -4.9185, -4.7523, -4.6505, -4.6096, -4.5920,\n",
      "         -4.5645, -4.4882, -4.4106, -4.3685, -4.3486, -4.3435, -4.3407, -4.3375,\n",
      "         -4.3320, -4.3258, -4.3243, -4.3375, -4.3631, -4.3912, -4.4094, -4.4197,\n",
      "         -4.4453, -4.4846, -4.5293, -4.5634, -4.5729, -4.5697, -4.5618, -4.5675,\n",
      "         -4.5823, -4.5978, -4.6045, -4.6071, -4.6107, -4.6166, -4.6198, -4.6104,\n",
      "         -4.5967, -4.5799]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3324, -2.2105], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5942, -1.6020, -1.6350, -1.6837, -1.6489, -1.6751, -1.3929, -1.6464],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2224, -0.2246, -0.2269,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2546, -0.2572, -0.2598,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2303, -0.2327, -0.2350,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2267, -0.2290, -0.2313,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2707, -0.2734, -0.2762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1859, -0.1878, -0.1897,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5244, -1.5860, -1.5633, -1.6099, -1.5766, -1.6017, -1.3318, -1.5743],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2581, -4.1826, -1.2884,  ..., -4.8664, -4.8956, -4.9259],\n",
      "        [-2.2898, -4.1995, -1.2972,  ..., -4.8867, -4.8519, -4.8067],\n",
      "        [-2.2453, -4.1819, -1.2914,  ..., -4.9249, -4.9538, -4.9741],\n",
      "        ...,\n",
      "        [-2.2725, -4.1928, -1.2977,  ..., -4.9256, -4.9263, -4.9160],\n",
      "        [-2.2985, -4.2144, -1.2965,  ..., -4.9857, -4.9558, -4.9368],\n",
      "        [-2.3233, -4.1744, -1.2925,  ..., -5.1704, -5.1399, -5.0999]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.7264, -1.6139, -1.7113, -1.7109, -1.6718, -1.5746, -1.3678, -1.5454],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5671, -1.3156, -1.5395, -1.2885, -1.6097, -1.5616, -1.5906, -1.5685],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1770, -0.1788, -0.1806,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2557, -0.2582, -0.2609,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2522, -0.2547, -0.2573,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2135, -0.2156, -0.2178,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2174, -0.2196, -0.2218,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2544, -0.2569, -0.2595,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4984, -1.2580, -1.4720, -1.2505, -1.5392, -1.4932, -1.5209, -1.4997],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3536, -4.1782, -1.3018,  ..., -5.1867, -5.2280, -5.2432],\n",
      "        [-2.2665, -4.2021, -1.2918,  ..., -4.9235, -4.8901, -4.8709],\n",
      "        [-2.2667, -4.1976, -1.2937,  ..., -5.2671, -5.2196, -5.2603],\n",
      "        ...,\n",
      "        [-2.2831, -4.1872, -1.2975,  ..., -4.8544, -4.8627, -4.8911],\n",
      "        [-2.2661, -4.1936, -1.2915,  ..., -4.8211, -4.8276, -4.8490],\n",
      "        [-2.2896, -4.2121, -1.3029,  ..., -4.6838, -4.6572, -4.6507]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4689, -1.2473, -1.5591, -1.2714, -1.7698, -1.6945, -1.6372, -1.6183],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.3406, -1.6798,  0.9900, -1.3467, -1.1000, -1.1000, -1.4515, -1.5503],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(1.9322e-09, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2739, -0.2767, -0.2795,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2319, -0.2343, -0.2367,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2682, -0.2710, -0.2737,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1786, -0.1804, -0.1823,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3141, -1.6062,  0.9656, -1.2877, -1.0782, -1.0518, -1.4228, -1.4823],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2640, -4.2346, -1.3188,  ..., -4.7796, -4.7813, -4.7783],\n",
      "        [-2.3153, -4.1971, -1.3113,  ..., -4.8154, -4.8172, -4.8251],\n",
      "        [-2.8202, -4.2935, -1.4293,  ..., -5.5176, -5.5327, -5.5123],\n",
      "        ...,\n",
      "        [-2.3683, -4.1895, -1.3296,  ..., -1.3613, -1.4629, -1.6093],\n",
      "        [-2.3045, -4.2394, -1.3245,  ..., -4.8947, -4.8828, -4.8072],\n",
      "        [-2.3358, -4.1855, -1.3124,  ..., -4.9162, -4.8884, -4.8650]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4482, -1.6675, -0.3669, -1.3501, -1.5418, -1.3924, -1.4300, -1.6704],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5748, -1.1000, -1.3279, -1.6117, -1.5188, -1.4937, -1.5745, -1.5732],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2554, -0.2580, -0.2606,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1058, -0.1068, -0.1079,  ..., -1.1000,  0.0000,  0.0000],\n",
      "        [-0.2580, -0.2607, -0.2633,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2547, -0.2573, -0.2599,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1778, -0.1796, -0.1814,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1777, -0.1795, -0.1813,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5591, -1.0518, -1.2697, -1.5411, -1.4523, -1.4283, -1.5055, -1.5043],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2926, -4.2216, -1.3231,  ..., -4.7482, -4.6873, -4.6352],\n",
      "        [-2.3675, -4.1910, -1.3269,  ..., -1.6724, -4.8341, -4.7220],\n",
      "        [-2.2946, -4.2152, -1.3227,  ..., -4.9861, -4.9767, -4.9632],\n",
      "        ...,\n",
      "        [-2.2667, -4.2200, -1.3064,  ..., -4.9310, -4.9366, -4.9533],\n",
      "        [-2.3420, -4.1917, -1.3322,  ..., -5.0297, -5.0081, -4.9758],\n",
      "        [-2.3568, -4.1898, -1.3143,  ..., -5.1444, -5.1033, -5.0544]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.6328, -1.6134, -1.3444, -1.6659, -1.6089, -1.4806, -1.6216, -1.6528],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4884, -2.6446, -1.6223, -1.2135, -1.4344, -1.5407, -1.5880, -1.4312],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2463, -0.2488, -0.2513,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3840, -0.3879, -0.3918,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1832, -0.1851, -0.1869,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1740, -0.1758, -0.1775,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2601, -0.2628, -0.2654,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2618, -0.2645, -0.2672,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4232, -2.5666, -1.5512, -1.1603, -1.3716, -1.4732, -1.5185, -1.4098],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2861, -4.2167, -1.3106,  ..., -5.1403, -5.1859, -5.2245],\n",
      "        [-2.2943, -4.2078, -1.3350,  ..., -4.6226, -4.5987, -4.5768],\n",
      "        [-2.3427, -4.2015, -1.3454,  ..., -4.9589, -4.9629, -4.9467],\n",
      "        ...,\n",
      "        [-2.3700, -4.1947, -1.3228,  ..., -5.0532, -5.0267, -4.9830],\n",
      "        [-2.2663, -4.2197, -1.3078,  ..., -5.1949, -5.1862, -5.2006],\n",
      "        [-2.3102, -4.2472, -1.3373,  ..., -4.7733, -4.8311, -4.8585]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4894, -2.1479, -1.7665, -1.2991, -1.4698, -1.6449, -1.5517, -1.5219],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5981, -1.5871, -1.3086, -1.3188, -1.5691, -1.2686, -1.3871, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2251, -0.2274, -0.2297,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2259, -0.2281, -0.2304,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2543, -0.2569, -0.2595,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2490, -0.2515, -0.2541,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2695, -0.2723, -0.2750,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5742, -1.5175, -1.2512, -1.2610, -1.5003, -1.2190, -1.3263, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3509, -4.2185, -1.3381,  ..., -5.0396, -5.0336, -5.0292],\n",
      "        [-2.3175, -4.2133, -1.3246,  ..., -4.6559, -4.6266, -4.6068],\n",
      "        [-2.3715, -4.2427, -1.3387,  ..., -4.9558, -4.9439, -4.9326],\n",
      "        ...,\n",
      "        [-2.3568, -4.2349, -1.3323,  ..., -5.0469, -5.0258, -5.0169],\n",
      "        [-2.3206, -4.2047, -1.3201,  ..., -4.9194, -4.9451, -4.9648],\n",
      "        [-2.3737, -4.2179, -1.3436,  ..., -1.2133, -1.2435, -1.7414]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5413, -1.6177, -1.2534, -1.5452, -1.5443, -1.2506, -1.4499, -1.2458],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.2281, -2.4115], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2460, -0.2484, -0.2510, -0.2535, -0.2561, -0.2586, -0.2613, -0.2639,\n",
      "         -0.2666, -0.2692, -0.2720, -0.2747, -0.2775, -0.2803, -0.2831, -0.2860,\n",
      "         -0.2889, -0.2918, -0.2947, -0.2977, -0.3007, -0.3038, -0.3068, -0.3099,\n",
      "         -0.3131, -0.3162, -0.3194, -0.3226, -0.3259, -0.3292, -0.3325, -0.3359,\n",
      "         -0.3393, -0.3427, -0.3462, -0.3497, -0.3532, -0.3568, -0.3604, -0.3640,\n",
      "         -0.3677, -0.3714, -0.3751, -0.3789, -0.3828, -0.3866, -0.3905, -0.3945,\n",
      "         -0.3985, -0.4025, -0.4065, -0.4107, -0.4148, -0.4190, -0.4232, -0.4275,\n",
      "         -0.4318, -0.4362, -0.4406, -0.4450, -0.4495, -0.4541, -0.4587, -0.4633,\n",
      "         -0.4680, -0.4727, -0.4775, -0.4823, -0.4872, -0.4921, -0.4971, -0.5021,\n",
      "         -0.5071, -0.5123, -0.5174, -0.5227, -0.5280, -0.5333, -0.5387, -0.5441,\n",
      "         -0.5496, -0.5552, -0.5608, -0.5664, -0.5722, -0.5779, -0.5838, -0.5897,\n",
      "         -0.5956, -0.6016, -0.6077, -0.6139, -0.6201, -0.6263, -0.6326, -0.6390,\n",
      "         -0.6455, -0.6520, -0.6586, -0.6653, -0.6720, -0.6788, -0.6856, -0.6925,\n",
      "         -0.6995, -0.7066, -0.7137, -0.7209, -0.7282, -0.7356, -0.7430, -0.7505,\n",
      "         -0.7581, -0.7658, -0.7735, -0.7813, -0.7892, -0.7972, -0.8052, -0.8134,\n",
      "         -0.8216, -0.8299, -0.8383, -0.8467, -0.8553, -0.8639, -0.8726, -0.8815,\n",
      "         -0.8904, -0.8994, -0.9084, -0.9176, -0.9269, -0.9362, -0.9457, -0.9553,\n",
      "         -0.9649, -0.9746, -0.9845, -0.9944, -1.0045, -1.0146, -1.0249, -1.0352,\n",
      "         -1.0457, -1.0562, -1.0669, -1.0777, -1.0886, -1.0996, -1.1107, -1.1219,\n",
      "         -1.1332, -1.1447, -1.1562, -1.1679, -1.1797, -1.1916, -1.2037, -1.2158,\n",
      "         -1.2281,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3990, -0.4030, -0.4071, -0.4112, -0.4154, -0.4196, -0.4238, -0.4281,\n",
      "         -0.4324, -0.4368, -0.4412, -0.4457, -0.4502, -0.4547, -0.4593, -0.4639,\n",
      "         -0.4686, -0.4734, -0.4781, -0.4830, -0.4878, -0.4928, -0.4977, -0.5028,\n",
      "         -0.5079, -0.5130, -0.5182, -0.5234, -0.5287, -0.5340, -0.5394, -0.5449,\n",
      "         -0.5504, -0.5559, -0.5615, -0.5672, -0.5729, -0.5787, -0.5846, -0.5905,\n",
      "         -0.5965, -0.6025, -0.6086, -0.6147, -0.6209, -0.6272, -0.6335, -0.6399,\n",
      "         -0.6464, -0.6529, -0.6595, -0.6662, -0.6729, -0.6797, -0.6866, -0.6935,\n",
      "         -0.7005, -0.7076, -0.7147, -0.7219, -0.7292, -0.7366, -0.7440, -0.7516,\n",
      "         -0.7592, -0.7668, -0.7746, -0.7824, -0.7903, -0.7983, -0.8063, -0.8145,\n",
      "         -0.8227, -0.8310, -0.8394, -0.8479, -0.8565, -0.8651, -0.8739, -0.8827,\n",
      "         -0.8916, -0.9006, -0.9097, -0.9189, -0.9282, -0.9375, -0.9470, -0.9566,\n",
      "         -0.9662, -0.9760, -0.9859, -0.9958, -1.0059, -1.0160, -1.0263, -1.0367,\n",
      "         -1.0471, -1.0577, -1.0684, -1.0792, -1.0901, -1.1011, -1.1122, -1.1235,\n",
      "         -1.1348, -1.1463, -1.1578, -1.1695, -1.1814, -1.1933, -1.2053, -1.2175,\n",
      "         -1.2298, -1.2422, -1.2548, -1.2675, -1.2803, -1.2932, -1.3063, -1.3195,\n",
      "         -1.3328, -1.3462, -1.3598, -1.3736, -1.3875, -1.4015, -1.4156, -1.4299,\n",
      "         -1.4444, -1.4590, -1.4737, -1.4886, -1.5036, -1.5188, -1.5341, -1.5496,\n",
      "         -1.5653, -1.5811, -1.5971, -1.6132, -1.6295, -1.6460, -1.6626, -1.6794,\n",
      "         -1.6963, -1.7135, -1.7308, -1.7483, -1.7659, -1.7838, -1.8018, -1.8200,\n",
      "         -1.8384, -1.8569, -1.8757, -1.8946, -1.9138, -1.9331, -1.9526, -1.9724,\n",
      "         -1.9923, -2.0124, -2.0327, -2.0533, -2.0740, -2.0950, -2.1161, -2.1375,\n",
      "         -2.1591, -2.1809, -2.2029, -2.2252, -2.2477, -2.2704, -2.2933, -2.3165,\n",
      "         -2.3399, -2.3635, -2.3874, -2.4115,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1919, -2.3058], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3319e+00, -4.2328e+00, -1.3516e+00, -2.4464e+00, -4.7193e-01,\n",
      "         -2.7753e+00, -4.4504e+00, -6.2910e-01, -3.4571e+00, -2.6239e+00,\n",
      "         -3.5724e+00, -4.7382e-01, -6.7805e-01, -3.6444e+00, -3.3039e+00,\n",
      "         -2.7546e+00, -3.3790e+00, -2.1607e+00, -2.5259e+00, -1.7463e+00,\n",
      "         -1.8511e+00, -1.8342e+00, -1.4429e+00, -2.9134e-01, -2.3888e+00,\n",
      "         -3.7376e+00, -3.5121e+00, -2.8807e+00, -3.8759e+00,  4.2541e-02,\n",
      "         -2.2016e+00, -9.7199e-01, -1.7382e+00, -3.1664e+00, -2.3469e+00,\n",
      "         -1.5613e+00, -1.4265e+00, -3.3792e+00, -2.0897e+00, -6.9126e-01,\n",
      "         -1.2620e+00, -1.2469e+00, -9.5475e-01, -5.1125e+00, -2.0871e+00,\n",
      "         -2.9140e+00, -3.6483e+00, -3.0397e+00, -8.8323e-01, -9.6117e-01,\n",
      "         -3.9175e+00, -1.9230e+00, -7.4681e+00, -3.3096e+00, -3.2721e+00,\n",
      "         -1.4104e+00, -2.9360e+00, -3.3821e+00, -3.0653e+00, -2.1909e+00,\n",
      "         -3.8030e+00, -1.6515e+00, -6.5701e+00, -2.9443e+00, -3.7506e+00,\n",
      "          8.0725e-03, -2.0265e+00, -1.0626e+00, -1.2945e+00, -2.9245e+00,\n",
      "         -4.1676e+00, -5.5410e+00, -3.3269e+00, -9.3918e-01, -3.9444e+00,\n",
      "         -4.7665e+00, -2.9989e+00, -4.0394e+00, -2.6796e+00, -3.7204e+00,\n",
      "         -5.0743e+00, -3.1230e+00, -1.1548e+00, -2.0024e+00, -2.1789e+00,\n",
      "         -3.5152e+00, -2.3092e+00, -1.4957e+00, -3.9943e+00, -1.7019e+00,\n",
      "         -3.6060e+00, -4.7991e+00, -3.0715e+00, -1.3208e+00, -1.7350e+00,\n",
      "         -3.5572e+00, -3.1700e+00, -3.3366e+00, -3.5137e+00, -3.6953e+00,\n",
      "         -9.1927e-01, -1.7381e+00, -4.0516e+00, -2.8998e+00, -1.1681e+00,\n",
      "         -2.1899e+00, -3.3804e+00, -4.3602e+00, -4.3960e-01, -2.3635e+00,\n",
      "         -1.4652e+00, -1.3823e+00, -3.8377e+00, -5.0182e+00, -1.4483e-01,\n",
      "         -1.5080e+00, -2.6813e+00, -1.7709e+00, -1.3772e+00, -1.7665e+00,\n",
      "         -2.5964e+00, -4.6853e+00, -2.8324e+00, -1.5629e+00, -1.7214e+00,\n",
      "         -3.1802e+00, -2.8391e+00, -1.1669e+00, -1.1191e+00, -3.9013e+00,\n",
      "         -1.2188e+00, -1.1643e+00, -3.7578e+00, -3.9855e+00, -1.9201e+00,\n",
      "         -2.6325e+00, -3.2659e+00, -2.9390e+00, -2.4103e+00, -3.1491e+00,\n",
      "         -2.6063e+00, -2.8578e+00, -5.8260e+00, -1.7707e+00, -1.3727e+00,\n",
      "         -1.1719e+00, -1.0361e+00, -1.3733e+00, -1.4459e+00, -8.9240e-01,\n",
      "         -1.2685e+00, -4.7661e+00, -4.7624e+00, -4.9530e+00, -5.1774e+00,\n",
      "         -4.6393e+00, -4.6970e+00, -5.0479e+00, -4.9158e+00, -4.9616e+00,\n",
      "         -4.9612e+00, -4.9679e+00, -4.9796e+00, -4.9985e+00, -5.0098e+00,\n",
      "         -5.0121e+00, -5.0066e+00, -4.9978e+00, -4.9835e+00, -4.9643e+00,\n",
      "         -4.9378e+00, -4.9104e+00, -4.8943e+00, -4.8918e+00, -4.9035e+00,\n",
      "         -4.9141e+00, -4.9335e+00, -4.9444e+00, -4.9581e+00, -4.9706e+00,\n",
      "         -4.9952e+00, -5.0236e+00, -5.0482e+00, -5.0677e+00, -5.0793e+00,\n",
      "         -5.0813e+00, -5.0833e+00, -5.0864e+00, -5.0885e+00, -5.0889e+00,\n",
      "         -5.0814e+00, -5.0764e+00, -5.0809e+00, -5.1002e+00, -5.1231e+00,\n",
      "         -5.1437e+00, -5.1721e+00, -5.1976e+00, -5.2136e+00, -5.2206e+00,\n",
      "         -5.2201e+00, -5.2071e+00, -5.1918e+00, -5.1675e+00, -5.1364e+00,\n",
      "         -5.1167e+00, -5.1148e+00, -5.1162e+00, -5.0951e+00, -5.0629e+00,\n",
      "         -5.0374e+00, -5.0011e+00, -4.9812e+00, -5.0001e+00, -5.0209e+00,\n",
      "         -5.0298e+00, -4.9705e+00, -4.8728e+00, -4.8588e+00, -4.8504e+00,\n",
      "         -4.8547e+00, -4.8373e+00, -4.8244e+00, -4.8205e+00, -4.8240e+00,\n",
      "         -4.8309e+00],\n",
      "        [-2.3605e+00, -4.2279e+00, -1.3123e+00, -2.3630e+00, -5.3775e-01,\n",
      "         -2.7259e+00, -4.3097e+00, -5.8912e-01, -3.4898e+00, -2.5901e+00,\n",
      "         -3.4881e+00, -4.4400e-01, -6.7131e-01, -3.6158e+00, -3.2754e+00,\n",
      "         -2.7358e+00, -3.2788e+00, -2.2091e+00, -2.4860e+00, -1.7129e+00,\n",
      "         -1.8282e+00, -1.8059e+00, -1.4125e+00, -2.8419e-01, -2.3271e+00,\n",
      "         -3.7122e+00, -3.4958e+00, -2.8847e+00, -3.9129e+00,  3.5782e-02,\n",
      "         -2.2111e+00, -9.3437e-01, -1.6955e+00, -3.0741e+00, -2.3450e+00,\n",
      "         -1.6122e+00, -1.4307e+00, -3.2994e+00, -2.1200e+00, -6.5977e-01,\n",
      "         -1.2534e+00, -1.2368e+00, -9.5457e-01, -5.0997e+00, -2.0478e+00,\n",
      "         -2.9206e+00, -3.6027e+00, -2.9504e+00, -9.2966e-01, -1.0704e+00,\n",
      "         -3.9077e+00, -1.8879e+00, -7.4142e+00, -3.3250e+00, -3.1956e+00,\n",
      "         -1.4554e+00, -2.9544e+00, -3.3775e+00, -3.0492e+00, -2.1208e+00,\n",
      "         -3.7893e+00, -1.6084e+00, -6.5158e+00, -2.9593e+00, -3.7962e+00,\n",
      "         -3.8018e-03, -2.0311e+00, -1.0253e+00, -1.2621e+00, -2.8252e+00,\n",
      "         -4.1594e+00, -5.5345e+00, -3.3453e+00, -1.0383e+00, -3.9350e+00,\n",
      "         -4.7390e+00, -3.0172e+00, -3.9754e+00, -2.7278e+00, -3.7121e+00,\n",
      "         -5.0672e+00, -3.1414e+00, -1.1949e+00, -1.9679e+00, -2.1495e+00,\n",
      "         -3.5255e+00, -2.3010e+00, -1.4553e+00, -3.9651e+00, -1.6916e+00,\n",
      "         -3.5963e+00, -4.7759e+00, -3.0958e+00, -1.3741e+00, -1.6997e+00,\n",
      "         -3.5215e+00, -3.1387e+00, -3.3383e+00, -3.5261e+00, -3.6410e+00,\n",
      "         -9.3356e-01, -1.6924e+00, -4.0698e+00, -2.8342e+00, -1.1698e+00,\n",
      "         -2.1721e+00, -3.3695e+00, -4.3559e+00, -5.2154e-01, -3.3843e+00,\n",
      "         -1.6104e+00, -3.4414e+00, -4.0868e+00, -5.1052e+00, -2.1076e-01,\n",
      "         -1.5151e+00, -2.6965e+00, -1.7567e+00, -1.4576e+00, -1.8090e+00,\n",
      "         -2.6433e+00, -4.8178e+00, -2.8446e+00, -1.6123e+00, -1.6808e+00,\n",
      "         -3.1676e+00, -2.8171e+00, -1.1554e+00, -1.1543e+00, -3.7876e+00,\n",
      "         -1.3994e+00, -1.2939e+00, -3.6848e+00, -3.9725e+00, -1.9882e+00,\n",
      "         -2.6480e+00, -3.2872e+00, -2.8837e+00, -2.3878e+00, -3.1851e+00,\n",
      "         -2.4327e+00, -5.9375e+00, -1.9903e+00, -2.9598e+00, -1.7949e+00,\n",
      "         -1.4585e+00, -1.6251e+00, -4.1759e+00, -5.0356e+00, -1.0249e+00,\n",
      "         -2.4347e+00, -1.3959e+00, -1.3928e+00, -1.1590e+00, -1.4607e+00,\n",
      "         -2.9249e+00, -3.9484e+00, -3.2102e+00, -5.1068e+00, -1.9838e+00,\n",
      "         -1.2059e+00, -1.3974e+00, -1.2437e+00, -1.6838e+00, -1.4861e+00,\n",
      "         -1.8898e+00, -1.4752e+00, -1.5416e+00, -1.7754e+00, -2.5540e+00,\n",
      "         -4.9201e+00, -4.7727e+00, -5.2080e+00, -5.0044e+00, -5.1371e+00,\n",
      "         -5.1066e+00, -4.9872e+00, -4.9590e+00, -4.8559e+00, -4.6957e+00,\n",
      "         -4.5421e+00, -4.4591e+00, -4.4344e+00, -4.4354e+00, -4.4429e+00,\n",
      "         -4.4431e+00, -4.4279e+00, -4.4062e+00, -4.3904e+00, -4.3872e+00,\n",
      "         -4.3961e+00, -4.4121e+00, -4.4327e+00, -4.4463e+00, -4.4586e+00,\n",
      "         -4.4667e+00, -4.4689e+00, -4.4624e+00, -4.4465e+00, -4.4259e+00,\n",
      "         -4.4262e+00, -4.4414e+00, -4.4533e+00, -4.4639e+00, -4.4617e+00,\n",
      "         -4.4615e+00, -4.4710e+00, -4.4876e+00, -4.5001e+00, -4.5078e+00,\n",
      "         -4.5158e+00, -4.5200e+00, -4.5280e+00, -4.5478e+00, -4.5679e+00,\n",
      "         -4.5871e+00, -4.6097e+00, -4.6319e+00, -4.6525e+00, -4.6732e+00,\n",
      "         -4.6967e+00, -4.7173e+00, -4.7263e+00, -4.7482e+00, -4.8201e+00,\n",
      "         -4.9004e+00]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.2230, -1.6253], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.3639, -1.1000, -1.4904, -1.4481,  0.9900, -2.6154, -1.2566, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(1.0540e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2521, -0.2546, -0.2572,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1058, -0.1068, -0.1079,  ..., -1.1000,  0.0000,  0.0000],\n",
      "        [-0.2037, -0.2058, -0.2079,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3797, -0.3836, -0.3875,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2442, -0.2467, -0.2492,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3369, -1.0518, -1.4251, -1.3847,  0.9656, -2.5382, -1.2015, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3390, -4.2601, -1.3521,  ..., -4.8904, -4.8851, -4.8152],\n",
      "        [-2.3791, -4.2041, -1.3402,  ..., -1.5864, -4.8457, -4.7238],\n",
      "        [-2.3181, -4.2132, -1.3239,  ..., -4.9032, -4.9160, -4.9440],\n",
      "        ...,\n",
      "        [-2.3129, -4.2198, -1.3424,  ..., -4.6450, -4.6142, -4.5902],\n",
      "        [-2.3574, -4.2459, -1.3388,  ..., -4.9587, -4.9350, -4.9183],\n",
      "        [-2.3725, -4.2216, -1.3461,  ..., -1.1971, -1.2282, -1.7047]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3798, -1.4637, -1.4700, -1.5585, -0.1004, -2.1187, -1.2559, -1.2279],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5597, -1.2253, -1.2666, -1.2357, -1.5250, -1.5139, -1.3739, -1.5916],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1815, -0.1834, -0.1852,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2405, -0.2430, -0.2454,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2461, -0.2486, -0.2511,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2455, -0.2480, -0.2505,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2514, -0.2539, -0.2565,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2154, -0.2176, -0.2198,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4914, -1.1775, -1.2111, -1.2112, -1.4582, -1.4988, -1.3535, -1.5219],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3471, -4.2138, -1.3303,  ..., -4.8367, -4.8192, -4.8109],\n",
      "        [-2.3564, -4.2434, -1.3379,  ..., -5.0445, -5.0247, -5.0162],\n",
      "        [-2.3741, -4.2510, -1.3438,  ..., -4.9543, -4.9419, -4.9313],\n",
      "        ...,\n",
      "        [-2.3128, -4.2382, -1.3399,  ..., -4.7681, -4.7043, -4.6492],\n",
      "        [-2.3210, -4.2626, -1.3451,  ..., -4.7850, -4.8392, -4.8654],\n",
      "        [-2.3169, -4.2244, -1.3337,  ..., -4.9146, -4.9278, -4.9293]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.5379, -1.2003, -1.2100, -1.3643, -1.5912, -1.5677, -1.4710, -1.4100],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4310, -1.1000, -1.2282, -1.5647, -1.4435, -1.1818, -1.5038, -1.5519],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2368, -0.2392, -0.2416,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2387, -0.2411, -0.2435,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2297, -0.2320, -0.2343,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2056, -0.2076, -0.2097,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2208, -0.2231, -0.2253,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.3683, -1.0518, -1.1743, -1.5414, -1.3802, -1.1300, -1.4379, -1.4839],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2931, -4.2335, -1.3226,  ..., -5.1225, -5.1754, -5.2158],\n",
      "        [-2.3897, -4.2141, -1.3521,  ..., -1.1544, -1.2561, -1.4672],\n",
      "        [-2.3346, -4.2371, -1.3182,  ..., -4.8598, -4.8481, -4.8414],\n",
      "        ...,\n",
      "        [-2.3239, -4.2335, -1.3321,  ..., -4.9221, -4.8916, -4.8777],\n",
      "        [-2.3023, -4.2229, -1.3185,  ..., -4.8447, -4.8556, -4.8781],\n",
      "        [-2.3155, -4.2209, -1.3295,  ..., -4.7139, -4.6867, -4.6676]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3392, -1.1924, -1.3509, -1.4727, -1.5418, -1.1184, -1.4941, -1.5993],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5212, -1.4020, -1.1865, -1.5340, -1.4931, -1.1369, -1.5534, -1.4809],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2492, -0.2517, -0.2542,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2391, -0.2415, -0.2439,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2376, -0.2400, -0.2424,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2277, -0.2300, -0.2323,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2102, -0.2124, -0.2145,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1672, -0.1689, -0.1706,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4545, -1.3405, -1.1515, -1.4668, -1.4277, -1.1033, -1.4854, -1.4160],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2755, -4.2369, -1.3148,  ..., -5.2037, -5.1886, -5.2035],\n",
      "        [-2.2789, -4.2384, -1.3220,  ..., -4.9001, -4.9031, -4.9199],\n",
      "        [-2.3263, -4.2403, -1.3512,  ..., -4.8424, -4.8485, -4.8559],\n",
      "        ...,\n",
      "        [-2.3681, -4.2582, -1.3559,  ..., -4.7920, -4.7983, -4.8044],\n",
      "        [-2.3078, -4.2089, -1.3268,  ..., -5.1062, -5.1202, -5.1340],\n",
      "        [-2.3836, -4.2052, -1.3281,  ..., -5.1986, -5.2340, -5.2439]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4991, -1.3945, -1.1810, -1.3874, -1.5770, -1.2368, -1.5491, -1.4906],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4646, -1.5532, -1.2189, -1.5892, -1.1238, -2.3798, -1.3542, -1.5657],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1688, -0.1705, -0.1722,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1754, -0.1772, -0.1790,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2369, -0.2393, -0.2417,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.3938, -0.3977, -0.4018,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2241, -0.2263, -0.2286,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1768, -0.1786, -0.1804,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4004, -1.4851, -1.1654, -1.5196, -1.0746, -2.2755, -1.2949, -1.4971],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3574, -4.2117, -1.3319,  ..., -4.9255, -4.9063, -4.8869],\n",
      "        [-2.3614, -4.2102, -1.3250,  ..., -5.1324, -5.0891, -5.0413],\n",
      "        [-2.3159, -4.2389, -1.3380,  ..., -5.0174, -5.0136, -5.0020],\n",
      "        ...,\n",
      "        [-2.3368, -4.2422, -1.3120,  ..., -4.6383, -4.6970, -4.7782],\n",
      "        [-2.3300, -4.2517, -1.3326,  ..., -4.9793, -5.0258, -5.0633],\n",
      "        [-2.3443, -4.2170, -1.3520,  ..., -4.9454, -4.9455, -4.9257]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.3847, -1.5164, -1.2702, -1.4467, -1.1316, -1.6459, -1.3090, -1.7421],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5798, -1.5207, -1.3157, -1.1000, -1.4793, -1.5032, -1.5119, -1.4703],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2013, -0.2033, -0.2054,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1752, -0.1770, -0.1788,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2557, -0.2583, -0.2609,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1698, -0.1715, -0.1732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2067, -0.2088, -0.2109,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2337, -0.2361, -0.2384,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.5106, -1.4541, -1.2581, -1.0782, -1.4145, -1.4373, -1.4457, -1.4557],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3267, -4.2179, -1.3169,  ..., -4.9848, -4.9995, -5.0112],\n",
      "        [-2.3517, -4.2126, -1.3275,  ..., -4.8209, -4.8252, -4.8215],\n",
      "        [-2.3201, -4.2213, -1.3238,  ..., -4.9540, -4.9711, -4.9775],\n",
      "        ...,\n",
      "        [-2.3447, -4.2116, -1.3424,  ..., -5.0295, -5.0121, -4.9874],\n",
      "        [-2.3175, -4.2167, -1.3133,  ..., -4.5228, -4.5479, -4.5796],\n",
      "        [-2.3345, -4.2392, -1.3302,  ..., -4.9114, -4.8694, -4.8192]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4537, -1.5049, -1.4146, -1.4310, -1.3698, -1.3630, -1.6493, -1.4507],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.2327, -1.4319], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2396, -0.2420, -0.2444, -0.2469, -0.2494, -0.2519, -0.2544, -0.2570,\n",
      "         -0.2596, -0.2622, -0.2649, -0.2676, -0.2703, -0.2730, -0.2757, -0.2785,\n",
      "         -0.2813, -0.2842, -0.2871, -0.2900, -0.2929, -0.2958, -0.2988, -0.3018,\n",
      "         -0.3049, -0.3080, -0.3111, -0.3142, -0.3174, -0.3206, -0.3238, -0.3271,\n",
      "         -0.3304, -0.3338, -0.3371, -0.3405, -0.3440, -0.3475, -0.3510, -0.3545,\n",
      "         -0.3581, -0.3617, -0.3654, -0.3690, -0.3728, -0.3765, -0.3803, -0.3842,\n",
      "         -0.3881, -0.3920, -0.3959, -0.3999, -0.4040, -0.4081, -0.4122, -0.4164,\n",
      "         -0.4206, -0.4248, -0.4291, -0.4334, -0.4378, -0.4422, -0.4467, -0.4512,\n",
      "         -0.4558, -0.4604, -0.4650, -0.4697, -0.4745, -0.4793, -0.4841, -0.4890,\n",
      "         -0.4939, -0.4989, -0.5040, -0.5090, -0.5142, -0.5194, -0.5246, -0.5299,\n",
      "         -0.5353, -0.5407, -0.5462, -0.5517, -0.5572, -0.5629, -0.5686, -0.5743,\n",
      "         -0.5801, -0.5860, -0.5919, -0.5979, -0.6039, -0.6100, -0.6162, -0.6224,\n",
      "         -0.6287, -0.6350, -0.6414, -0.6479, -0.6545, -0.6611, -0.6677, -0.6745,\n",
      "         -0.6813, -0.6882, -0.6951, -0.7022, -0.7092, -0.7164, -0.7236, -0.7310,\n",
      "         -0.7383, -0.7458, -0.7533, -0.7609, -0.7686, -0.7764, -0.7842, -0.7922,\n",
      "         -0.8002, -0.8082, -0.8164, -0.8246, -0.8330, -0.8414, -0.8499, -0.8585,\n",
      "         -0.8671, -0.8759, -0.8848, -0.8937, -0.9027, -0.9118, -0.9210, -0.9304,\n",
      "         -0.9397, -0.9492, -0.9588, -0.9685, -0.9783, -0.9882, -0.9982, -1.0082,\n",
      "         -1.0184, -1.0287, -1.0391, -1.0496, -1.0602, -1.0709, -1.0817, -1.0927,\n",
      "         -1.1037, -1.1148, -1.1261, -1.1375, -1.1490, -1.1606, -1.1723, -1.1841,\n",
      "         -1.1961, -1.2082, -1.2204, -1.2327,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2322, -0.2346, -0.2369, -0.2393, -0.2417, -0.2442, -0.2466, -0.2491,\n",
      "         -0.2517, -0.2542, -0.2568, -0.2594, -0.2620, -0.2646, -0.2673, -0.2700,\n",
      "         -0.2727, -0.2755, -0.2783, -0.2811, -0.2839, -0.2868, -0.2897, -0.2926,\n",
      "         -0.2956, -0.2985, -0.3016, -0.3046, -0.3077, -0.3108, -0.3139, -0.3171,\n",
      "         -0.3203, -0.3235, -0.3268, -0.3301, -0.3334, -0.3368, -0.3402, -0.3437,\n",
      "         -0.3471, -0.3506, -0.3542, -0.3577, -0.3614, -0.3650, -0.3687, -0.3724,\n",
      "         -0.3762, -0.3800, -0.3838, -0.3877, -0.3916, -0.3956, -0.3996, -0.4036,\n",
      "         -0.4077, -0.4118, -0.4160, -0.4202, -0.4244, -0.4287, -0.4330, -0.4374,\n",
      "         -0.4418, -0.4463, -0.4508, -0.4553, -0.4599, -0.4646, -0.4693, -0.4740,\n",
      "         -0.4788, -0.4836, -0.4885, -0.4935, -0.4984, -0.5035, -0.5086, -0.5137,\n",
      "         -0.5189, -0.5241, -0.5294, -0.5348, -0.5402, -0.5456, -0.5511, -0.5567,\n",
      "         -0.5623, -0.5680, -0.5737, -0.5795, -0.5854, -0.5913, -0.5973, -0.6033,\n",
      "         -0.6094, -0.6156, -0.6218, -0.6281, -0.6344, -0.6408, -0.6473, -0.6538,\n",
      "         -0.6604, -0.6671, -0.6738, -0.6807, -0.6875, -0.6945, -0.7015, -0.7086,\n",
      "         -0.7157, -0.7230, -0.7303, -0.7376, -0.7451, -0.7526, -0.7602, -0.7679,\n",
      "         -0.7757, -0.7835, -0.7914, -0.7994, -0.8075, -0.8156, -0.8239, -0.8322,\n",
      "         -0.8406, -0.8491, -0.8577, -0.8663, -0.8751, -0.8839, -0.8928, -0.9019,\n",
      "         -0.9110, -0.9202, -0.9295, -0.9389, -0.9483, -0.9579, -0.9676, -0.9774,\n",
      "         -0.9872, -0.9972, -1.0073, -1.0175, -1.0277, -1.0381, -1.0486, -1.0592,\n",
      "         -1.0699, -1.0807, -1.0916, -1.1026, -1.1138, -1.1250, -1.1364, -1.1479,\n",
      "         -1.1595, -1.1712, -1.1830, -1.1950, -1.2070, -1.2192, -1.2315, -1.2440,\n",
      "         -1.2565, -1.2692, -1.2821, -1.2950, -1.3081, -1.3213, -1.3346, -1.3481,\n",
      "         -1.3617, -1.3755, -1.3894, -1.4034, -1.4176, -1.4319,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1787, -1.3692], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2913, -4.2311, -1.3372, -2.4133, -0.4849, -2.7971, -4.4373, -0.6223,\n",
      "         -3.4211, -2.6484, -3.5930, -0.5359, -0.7282, -3.6597, -3.2944, -2.7322,\n",
      "         -3.3832, -2.1587, -2.5185, -1.7589, -1.8664, -1.8477, -1.4473, -0.2720,\n",
      "         -2.3847, -3.7324, -3.4836, -2.8553, -3.9054,  0.0403, -2.2158, -0.9834,\n",
      "         -1.7429, -3.1752, -2.3815, -1.5727, -1.4237, -3.3817, -2.0548, -0.6911,\n",
      "         -1.2610, -1.2470, -0.9361, -5.0934, -2.0829, -2.8787, -3.6676, -3.0477,\n",
      "         -0.9241, -1.0022, -3.9194, -1.9429, -7.4705, -3.2855, -3.2721, -1.4163,\n",
      "         -2.9135, -3.3651, -3.0446, -2.1776, -3.7965, -1.6650, -6.5587, -2.9225,\n",
      "         -3.7752,  0.0077, -2.0330, -1.0693, -1.3004, -2.9311, -4.1598, -5.5629,\n",
      "         -3.3015, -0.9739, -3.9417, -4.7734, -2.9713, -4.0438, -2.6518, -3.7183,\n",
      "         -5.0960, -3.0937, -1.1502, -2.0020, -2.1655, -3.5171, -2.3172, -1.5089,\n",
      "         -3.9977, -1.7015, -3.5975, -4.8016, -3.0412, -1.3206, -1.7366, -3.5520,\n",
      "         -3.1719, -3.3120, -3.5008, -3.6670, -0.9143, -1.7418, -4.0430, -2.9210,\n",
      "         -1.1472, -2.1864, -3.3448, -4.4143, -0.4687, -5.0271, -1.6062, -4.9930,\n",
      "         -3.4024, -4.9795, -0.2097, -1.5852, -2.7641, -1.8383, -1.5131, -1.8009,\n",
      "         -2.6613, -4.6478, -2.8614, -1.5976, -1.6307, -3.1535, -2.8972, -1.1943,\n",
      "         -1.2269, -3.8997, -1.2984, -1.2199, -3.7133, -4.0041, -1.9843, -2.6182,\n",
      "         -3.2941, -2.9130, -2.4302, -3.1793, -2.6791, -2.9283, -5.8771, -1.6779,\n",
      "         -1.2709, -1.5138, -1.4701, -1.5045, -1.5005, -1.5277, -1.5407, -1.5349,\n",
      "         -1.5345, -1.3247, -4.9395, -4.9015, -5.1499, -5.3572, -4.6978, -4.7885,\n",
      "         -4.8954, -4.9434, -4.9686, -4.9731, -5.0025, -5.0598, -5.1124, -5.1229,\n",
      "         -5.1008, -5.0612, -5.0264, -4.9923, -4.9692, -4.9495, -4.9315, -4.9141,\n",
      "         -4.9042, -4.8842, -4.8609, -4.8341, -4.8156, -4.8054, -4.8085, -4.8206,\n",
      "         -4.8371, -4.8466, -4.8516, -4.8519, -4.8559, -4.8657, -4.8821, -4.9050,\n",
      "         -4.9195, -4.9398, -4.9540, -4.9662, -4.9832, -5.0014, -5.0167, -5.0337,\n",
      "         -5.0495, -5.0607, -5.0687, -5.0674, -5.0520, -5.0331, -5.0225, -5.0086,\n",
      "         -4.9996, -4.9925, -4.9902, -4.9904, -4.9706, -4.9441, -4.9318, -4.9387,\n",
      "         -4.9561, -4.9777, -5.0038, -4.9138, -4.9023, -4.8882, -4.8927, -4.9049,\n",
      "         -4.8999, -4.8931],\n",
      "        [-2.3150, -4.2459, -1.3240, -2.4124, -0.5177, -2.7729, -4.4051, -0.5968,\n",
      "         -3.4239, -2.6491, -3.6136, -0.5432, -0.7281, -3.6290, -3.2697, -2.7165,\n",
      "         -3.3873, -2.1955, -2.5091, -1.7612, -1.8678, -1.8410, -1.4339, -0.2727,\n",
      "         -2.3658, -3.7157, -3.4700, -2.8496, -3.8864,  0.0483, -2.2057, -0.9686,\n",
      "         -1.7351, -3.1871, -2.3794, -1.5751, -1.4218, -3.3918, -2.0591, -0.6734,\n",
      "         -1.2631, -1.2389, -0.9282, -5.0640, -2.0639, -2.8857, -3.6557, -3.0574,\n",
      "         -0.8966, -0.9981, -3.9089, -1.9354, -7.4629, -3.2861, -3.2770, -1.4378,\n",
      "         -2.9395, -3.3744, -3.0428, -2.1680, -3.7841, -1.6596, -6.5584, -2.9248,\n",
      "         -3.7609,  0.0136, -2.0198, -1.0563, -1.2950, -2.9406, -4.1597, -5.5495,\n",
      "         -3.3032, -0.9708, -3.9243, -4.7468, -2.9754, -4.0558, -2.6607, -3.7027,\n",
      "         -5.0855, -3.1004, -1.1562, -1.9998, -2.1681, -3.5271, -2.3060, -1.4961,\n",
      "         -4.0181, -1.7098, -3.5922, -4.7753, -3.0522, -1.3246, -1.7271, -3.5422,\n",
      "         -3.1614, -3.3220, -3.5135, -3.6665, -0.9155, -1.7357, -4.0594, -2.9186,\n",
      "         -1.1603, -2.1819, -3.3516, -4.4144, -0.4677, -5.2388, -1.5516, -1.2119,\n",
      "         -3.9872, -5.0166, -0.1670, -1.5270, -2.7707, -1.8060, -1.4727, -1.8124,\n",
      "         -2.6466, -4.5788, -2.8904, -1.6439, -1.6880, -3.1830, -2.8897, -1.1884,\n",
      "         -1.1399, -3.8984, -1.3282, -1.2092, -3.7614, -3.9980, -1.9740, -2.6266,\n",
      "         -3.2729, -2.8996, -2.4155, -3.1624, -2.4693, -5.9502, -1.9845, -4.0359,\n",
      "         -3.2980, -2.5754, -3.2987, -1.9267, -1.8626, -1.7676, -1.4626, -1.3374,\n",
      "         -3.5117, -5.2522, -0.8544, -2.4692, -4.8964, -6.1976, -4.5970, -3.1261,\n",
      "         -5.3335, -2.1477, -1.6046, -1.2895, -1.3042, -1.2331, -1.2540, -1.2601,\n",
      "         -1.3247, -1.3745, -1.4006, -1.4997, -4.9129, -4.8672, -5.1269, -5.1331,\n",
      "         -4.0207, -4.5648, -4.7809, -4.9904, -5.0584, -4.9884, -4.9089, -4.8367,\n",
      "         -4.7868, -4.7650, -4.7583, -4.7614, -4.7625, -4.7677, -4.7693, -4.7742,\n",
      "         -4.7878, -4.8109, -4.8300, -4.8493, -4.8638, -4.8738, -4.8925, -4.9165,\n",
      "         -4.9408, -4.9575, -4.9671, -4.9747, -4.9781, -4.9825, -4.9912, -4.9977,\n",
      "         -5.0035, -5.0037, -5.0029, -5.0019, -5.0038, -5.0023, -4.9945, -4.9775,\n",
      "         -4.9504, -4.9327, -4.9210, -4.9042, -4.8707, -4.8217, -4.7965, -4.7601,\n",
      "         -4.7233, -4.7002]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4722, -1.3545], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.5259, -1.4753, -1.5146, -1.3256, -1.1725, -1.5413, -1.3359, -1.5708],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1776, -0.1794, -0.1812,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1666, -0.1683, -0.1700,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2134, -0.2155, -0.2177,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1741, -0.1758, -0.1776,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2210, -0.2233, -0.2255,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2001, -0.2022, -0.2042,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4590, -1.4106, -1.4482, -1.3059, -1.1266, -1.4737, -1.2774, -1.5019],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3261, -4.2168, -1.3189,  ..., -4.8503, -4.8353, -4.8272],\n",
      "        [-2.3550, -4.2073, -1.3171,  ..., -5.0375, -5.0050, -4.9598],\n",
      "        [-2.2706, -4.2176, -1.3113,  ..., -5.0039, -5.0309, -5.0485],\n",
      "        ...,\n",
      "        [-2.3448, -4.2054, -1.3132,  ..., -5.1281, -5.0814, -5.0318],\n",
      "        [-2.3134, -4.2486, -1.3224,  ..., -4.9723, -5.0202, -5.0605],\n",
      "        [-2.3154, -4.2153, -1.3072,  ..., -4.9883, -5.0029, -5.0142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4618, -1.5259, -1.3318, -1.4165, -1.1340, -1.5021, -1.2903, -1.4403],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4734, -1.4592, -1.1794, -1.3915, -1.2955, -1.5544, -2.5734, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2389, -0.2414, -0.2438,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1648, -0.1665, -0.1681,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2410, -0.2434, -0.2459,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2104, -0.2125, -0.2146,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3736, -0.3774, -0.3812,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4587, -1.3953, -1.1561, -1.3305, -1.2387, -1.4863, -2.4975, -1.0518],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2942, -4.2397, -1.3223,  ..., -4.7919, -4.7261, -4.6675],\n",
      "        [-2.3610, -4.2033, -1.3118,  ..., -5.1966, -5.2309, -5.2396],\n",
      "        [-2.2833, -4.2564, -1.3240,  ..., -4.8783, -4.8775, -4.8811],\n",
      "        ...,\n",
      "        [-2.2915, -4.2257, -1.3159,  ..., -4.9165, -4.9301, -4.9351],\n",
      "        [-2.3066, -4.2260, -1.3293,  ..., -4.6648, -4.6331, -4.6081],\n",
      "        [-2.3663, -4.2124, -1.3327,  ..., -1.0741, -1.1749, -1.3867]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4850, -1.4646, -1.2963, -1.5271, -1.3943, -1.3450, -2.1304, -1.1108],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.0900, -1.2947, -1.4515, -1.5326, -1.5694, -1.1000, -1.2013, -1.4706],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2118, -0.2140, -0.2161,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2393, -0.2417, -0.2441,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2307, -0.2330, -0.2354,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1037, -0.1047, -0.1058,  ..., -1.0781, -1.0890, -1.1000],\n",
      "        [-0.2334, -0.2358, -0.2382,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2010, -0.2031, -0.2051,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0423, -1.2690, -1.4370, -1.5097, -1.5006, -1.0518, -1.1487, -1.4061],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3714, -4.2559, -1.3105,  ..., -4.8083, -4.7932, -4.7851],\n",
      "        [-2.3155, -4.2626, -1.3298,  ..., -4.8827, -4.8837, -4.8182],\n",
      "        [-2.3179, -4.2324, -1.3114,  ..., -4.9146, -4.8716, -4.8236],\n",
      "        ...,\n",
      "        [-2.3409, -4.2223, -1.3228,  ..., -1.1163, -1.1479, -1.5869],\n",
      "        [-2.3481, -4.2511, -1.3235,  ..., -4.9500, -4.9387, -4.9297],\n",
      "        [-2.2746, -4.2190, -1.2932,  ..., -4.8464, -4.8570, -4.8793]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1127, -1.3391, -1.4073, -1.4042, -1.4323, -1.1440, -1.1420, -1.4157],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.4835,  0.9900, -1.5442, -1.1362, -1.4363, -1.3978, -1.4599, -1.1000],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(1.7287e-08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.1675, -0.1692, -0.1709,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1705,  0.1723,  0.1740,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1744, -0.1762, -0.1779,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2290, -0.2313, -0.2336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2036, -0.2057, -0.2077,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1374, -0.1388, -0.1402,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.4185,  0.9656, -1.4766, -1.1027, -1.3734, -1.3365, -1.3959, -1.0782],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3379, -4.2042, -1.2999,  ..., -5.1188, -5.0772, -5.0313],\n",
      "        [-2.8295, -4.2964, -1.4168,  ..., -5.5480, -5.5729, -5.5560],\n",
      "        [-2.3187, -4.2093, -1.3255,  ..., -4.9172, -4.9144, -4.8961],\n",
      "        ...,\n",
      "        [-2.2792, -4.2278, -1.3016,  ..., -4.7813, -4.7804, -4.7874],\n",
      "        [-2.2705, -4.2119, -1.2926,  ..., -4.9494, -4.9747, -4.9988],\n",
      "        [-2.3462, -4.2138, -1.3153,  ..., -4.9608, -4.9578, -4.9594]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.4677, -0.0052, -1.6865, -1.1233, -1.3284, -1.4262, -1.3356, -1.3666],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-2.3550, -1.4378, -1.5215, -1.4809, -1.1666, -1.5089, -1.4083, -1.1151],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.3897, -0.3936, -0.3976,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1657, -0.1673, -0.1690,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2059, -0.2080, -0.2101,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2147, -0.2169, -0.2191,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2284, -0.2307, -0.2330,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2167, -0.2189, -0.2211,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-2.2518, -1.3748, -1.4548, -1.4160, -1.1155, -1.4428, -1.3466, -1.0662],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3038, -4.2384, -1.2791,  ..., -4.5857, -4.6325, -4.7113],\n",
      "        [-2.3285, -4.2037, -1.2997,  ..., -4.9314, -4.9142, -4.8968],\n",
      "        [-2.2806, -4.2027, -1.2947,  ..., -5.1027, -5.1180, -5.1305],\n",
      "        ...,\n",
      "        [-2.2879, -4.2165, -1.2944,  ..., -4.7432, -4.7186, -4.6974],\n",
      "        [-2.2996, -4.2410, -1.2979,  ..., -4.7536, -4.7152, -4.6869],\n",
      "        [-2.3078, -4.2311, -1.3075,  ..., -4.9108, -4.8868, -4.8764]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.6732, -1.3327, -1.4533, -1.3259, -1.1572, -1.5445, -1.3650, -1.0661],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.1685, -1.4747, -1.3749, -1.1000, -1.0733, -1.4864, -1.3517, -1.4958],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2271, -0.2294, -0.2317,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2416, -0.2440, -0.2465,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2275, -0.2298, -0.2321,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2032, -0.2052, -0.2073,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2305, -0.2328, -0.2352,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1724, -0.1741, -0.1759,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1173, -1.4101, -1.3146, -1.0518, -1.0416, -1.4213, -1.2925, -1.4303],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2900, -4.2314, -1.3005,  ..., -5.0333, -5.0284, -5.0207],\n",
      "        [-2.2452, -4.2306, -1.2768,  ..., -5.1886, -5.1655, -5.1809],\n",
      "        [-2.2634, -4.2282, -1.2862,  ..., -5.0724, -5.1270, -5.1747],\n",
      "        ...,\n",
      "        [-2.2948, -4.2082, -1.2808,  ..., -4.4995, -4.5255, -4.5616],\n",
      "        [-2.2511, -4.2320, -1.2871,  ..., -4.8703, -4.8754, -4.8946],\n",
      "        [-2.3273, -4.2040, -1.2940,  ..., -4.8215, -4.8252, -4.8229]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1870, -1.4323, -1.2175, -1.2653, -1.1949, -1.5457, -1.3503, -1.4829],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.1895, -1.1509], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2])\n",
      "tla shape\n",
      "torch.Size([2, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2311, -0.2335, -0.2358, -0.2382, -0.2406, -0.2431, -0.2455, -0.2480,\n",
      "         -0.2505, -0.2530, -0.2556, -0.2582, -0.2608, -0.2634, -0.2661, -0.2688,\n",
      "         -0.2715, -0.2742, -0.2770, -0.2798, -0.2826, -0.2855, -0.2883, -0.2913,\n",
      "         -0.2942, -0.2972, -0.3002, -0.3032, -0.3063, -0.3094, -0.3125, -0.3156,\n",
      "         -0.3188, -0.3221, -0.3253, -0.3286, -0.3319, -0.3353, -0.3387, -0.3421,\n",
      "         -0.3455, -0.3490, -0.3525, -0.3561, -0.3597, -0.3633, -0.3670, -0.3707,\n",
      "         -0.3745, -0.3782, -0.3821, -0.3859, -0.3898, -0.3938, -0.3977, -0.4017,\n",
      "         -0.4058, -0.4099, -0.4140, -0.4182, -0.4225, -0.4267, -0.4310, -0.4354,\n",
      "         -0.4398, -0.4442, -0.4487, -0.4532, -0.4578, -0.4624, -0.4671, -0.4718,\n",
      "         -0.4766, -0.4814, -0.4863, -0.4912, -0.4962, -0.5012, -0.5062, -0.5113,\n",
      "         -0.5165, -0.5217, -0.5270, -0.5323, -0.5377, -0.5431, -0.5486, -0.5542,\n",
      "         -0.5598, -0.5654, -0.5711, -0.5769, -0.5827, -0.5886, -0.5945, -0.6005,\n",
      "         -0.6066, -0.6127, -0.6189, -0.6252, -0.6315, -0.6379, -0.6443, -0.6508,\n",
      "         -0.6574, -0.6640, -0.6708, -0.6775, -0.6844, -0.6913, -0.6983, -0.7053,\n",
      "         -0.7124, -0.7196, -0.7269, -0.7343, -0.7417, -0.7492, -0.7567, -0.7644,\n",
      "         -0.7721, -0.7799, -0.7878, -0.7957, -0.8038, -0.8119, -0.8201, -0.8284,\n",
      "         -0.8367, -0.8452, -0.8537, -0.8623, -0.8711, -0.8799, -0.8887, -0.8977,\n",
      "         -0.9068, -0.9159, -0.9252, -0.9345, -0.9440, -0.9535, -0.9631, -0.9729,\n",
      "         -0.9827, -0.9926, -1.0027, -1.0128, -1.0230, -1.0333, -1.0438, -1.0543,\n",
      "         -1.0650, -1.0757, -1.0866, -1.0976, -1.1087, -1.1199, -1.1312, -1.1426,\n",
      "         -1.1541, -1.1658, -1.1776, -1.1895,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2236, -0.2259, -0.2282, -0.2305, -0.2328, -0.2352, -0.2375, -0.2399,\n",
      "         -0.2424, -0.2448, -0.2473, -0.2498, -0.2523, -0.2549, -0.2574, -0.2600,\n",
      "         -0.2627, -0.2653, -0.2680, -0.2707, -0.2734, -0.2762, -0.2790, -0.2818,\n",
      "         -0.2847, -0.2875, -0.2904, -0.2934, -0.2963, -0.2993, -0.3023, -0.3054,\n",
      "         -0.3085, -0.3116, -0.3147, -0.3179, -0.3211, -0.3244, -0.3277, -0.3310,\n",
      "         -0.3343, -0.3377, -0.3411, -0.3445, -0.3480, -0.3515, -0.3551, -0.3587,\n",
      "         -0.3623, -0.3660, -0.3697, -0.3734, -0.3772, -0.3810, -0.3848, -0.3887,\n",
      "         -0.3926, -0.3966, -0.4006, -0.4047, -0.4087, -0.4129, -0.4170, -0.4213,\n",
      "         -0.4255, -0.4298, -0.4341, -0.4385, -0.4430, -0.4474, -0.4520, -0.4565,\n",
      "         -0.4611, -0.4658, -0.4705, -0.4752, -0.4800, -0.4849, -0.4898, -0.4947,\n",
      "         -0.4997, -0.5048, -0.5099, -0.5150, -0.5202, -0.5255, -0.5308, -0.5362,\n",
      "         -0.5416, -0.5470, -0.5526, -0.5582, -0.5638, -0.5695, -0.5752, -0.5811,\n",
      "         -0.5869, -0.5929, -0.5988, -0.6049, -0.6110, -0.6172, -0.6234, -0.6297,\n",
      "         -0.6361, -0.6425, -0.6490, -0.6555, -0.6622, -0.6688, -0.6756, -0.6824,\n",
      "         -0.6893, -0.6963, -0.7033, -0.7104, -0.7176, -0.7248, -0.7322, -0.7396,\n",
      "         -0.7470, -0.7546, -0.7622, -0.7699, -0.7777, -0.7855, -0.7935, -0.8015,\n",
      "         -0.8096, -0.8177, -0.8260, -0.8344, -0.8428, -0.8513, -0.8599, -0.8686,\n",
      "         -0.8773, -0.8862, -0.8952, -0.9042, -0.9133, -0.9226, -0.9319, -0.9413,\n",
      "         -0.9508, -0.9604, -0.9701, -0.9799, -0.9898, -0.9998, -1.0099, -1.0201,\n",
      "         -1.0304, -1.0408, -1.0513, -1.0619, -1.0727, -1.0835, -1.0945, -1.1055,\n",
      "         -1.1167, -1.1280, -1.1394, -1.1509,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0')\n",
      "torch.Size([2, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.1373, -1.1004], device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.2744, -4.2230, -1.3010, -2.3813, -0.4511, -2.7696, -4.4136, -0.6008,\n",
      "         -3.4078, -2.6477, -3.5913, -0.5426, -0.7612, -3.6648, -3.2840, -2.7225,\n",
      "         -3.3842, -2.1354, -2.4858, -1.7007, -1.8362, -1.8034, -1.4073, -0.2172,\n",
      "         -2.3522, -3.7310, -3.4640, -2.8439, -3.8803,  0.0763, -2.1749, -0.9209,\n",
      "         -1.7099, -3.1671, -2.4009, -1.5786, -1.4163, -3.3807, -2.0088, -0.6651,\n",
      "         -1.2281, -1.2351, -0.8849, -5.0549, -2.0529, -2.8707, -3.6616, -3.0392,\n",
      "         -0.9574, -0.9946, -3.9210, -1.9480, -7.4855, -3.2734, -3.2665, -1.3879,\n",
      "         -2.8847, -3.3177, -3.0121, -2.1458, -3.7958, -1.6705, -6.5710, -2.9094,\n",
      "         -3.7499,  0.0445, -1.9897, -1.0036, -1.2640, -2.9218, -4.1677, -5.5844,\n",
      "         -3.2851, -0.9908, -3.9367, -4.7697, -2.9566, -4.0460, -2.6114, -3.7121,\n",
      "         -5.1154, -3.0765, -1.1168, -1.9403, -2.1499, -3.4881, -2.2796, -1.4452,\n",
      "         -3.9681, -1.6693, -3.5877, -4.7962, -3.0242, -1.2909, -1.6732, -3.5171,\n",
      "         -3.1463, -3.3069, -3.4765, -3.6120, -0.8778, -1.6797, -4.0098, -2.9287,\n",
      "         -1.0899, -2.1670, -3.3371, -4.4493, -0.4844, -5.0090, -1.6125, -4.9922,\n",
      "         -3.4030, -4.9545, -0.1745, -1.5205, -2.7236, -1.7902, -1.5267, -1.7675,\n",
      "         -2.6524, -4.6312, -2.8168, -1.5584, -1.5880, -3.1186, -2.8758, -1.1612,\n",
      "         -1.1605, -3.8909, -1.3281, -1.2100, -3.7138, -3.9789, -1.9512, -2.5811,\n",
      "         -3.2442, -2.8796, -2.3977, -3.1748, -2.6799, -2.9249, -5.8838, -1.6557,\n",
      "         -1.2185, -1.2848, -1.2423, -1.2815, -1.2789, -1.3082, -1.3208, -1.3144,\n",
      "         -1.3142, -1.2584, -4.9292, -4.8916, -5.1337, -5.3699, -4.6870, -4.7835,\n",
      "         -4.8988, -4.9204, -4.9391, -4.9470, -4.9794, -5.0420, -5.1009, -5.1100,\n",
      "         -5.0863, -5.0502, -5.0125, -4.9794, -4.9581, -4.9408, -4.9248, -4.9101,\n",
      "         -4.9005, -4.8818, -4.8613, -4.8350, -4.8187, -4.8091, -4.8157, -4.8299,\n",
      "         -4.8485, -4.8617, -4.8666, -4.8699, -4.8757, -4.8856, -4.9040, -4.9258,\n",
      "         -4.9421, -4.9595, -4.9717, -4.9830, -4.9987, -5.0139, -5.0294, -5.0426,\n",
      "         -5.0521, -5.0560, -5.0567, -5.0516, -5.0343, -5.0118, -5.0005, -4.9847,\n",
      "         -4.9749, -4.9671, -4.9639, -4.9648, -4.9499, -4.9309, -4.9289, -4.9372,\n",
      "         -4.9564, -4.9764, -5.0023, -4.9198, -4.9097, -4.8948, -4.8975, -4.9075,\n",
      "         -4.9042, -4.8967],\n",
      "        [-2.3014, -4.2339, -1.2779, -2.3792, -0.4477, -2.7585, -4.4223, -0.5949,\n",
      "         -3.4359, -2.6490, -3.5712, -0.5194, -0.7690, -3.6672, -3.2585, -2.7280,\n",
      "         -3.3794, -2.1353, -2.4652, -1.6869, -1.8404, -1.7836, -1.3809, -0.1993,\n",
      "         -2.3408, -3.7476, -3.4489, -2.8450, -3.8615,  0.0962, -2.1515, -0.9009,\n",
      "         -1.6957, -3.1458, -2.3879, -1.5613, -1.4070, -3.3746, -2.0089, -0.6490,\n",
      "         -1.2152, -1.2231, -0.8634, -5.0392, -2.0402, -2.8846, -3.6493, -3.0182,\n",
      "         -0.9314, -0.9804, -3.9263, -1.9450, -7.4997, -3.2782, -3.2552, -1.3847,\n",
      "         -2.8912, -3.3106, -3.0311, -2.1406, -3.8100, -1.6691, -6.5950, -2.9121,\n",
      "         -3.7300,  0.0661, -1.9677, -0.9876, -1.2499, -2.8972, -4.1780, -5.5643,\n",
      "         -3.2888, -0.9894, -3.9447, -4.7667, -2.9621, -4.0396, -2.6138, -3.7202,\n",
      "         -5.0961, -3.0826, -1.1102, -1.9331, -2.1325, -3.4933, -2.2622, -1.4254,\n",
      "         -3.9905, -1.6687, -3.5995, -4.7847, -3.0332, -1.2850, -1.6661, -3.5203,\n",
      "         -3.1496, -3.3165, -3.4864, -3.5951, -0.8670, -1.6737, -3.9905, -2.9361,\n",
      "         -1.0791, -2.1610, -3.3529, -4.4421, -0.4764, -4.6476, -1.5586, -1.2388,\n",
      "         -3.9924, -4.9969, -0.1085, -1.4975, -2.7291, -1.7599, -1.5074, -1.7629,\n",
      "         -2.6431, -4.5841, -2.8260, -1.5734, -1.6238, -3.1504, -2.8600, -1.1610,\n",
      "         -1.1376, -3.8668, -1.4201, -1.2078, -3.7255, -3.9792, -1.9363, -2.5778,\n",
      "         -3.2235, -2.8926, -2.3948, -3.1683, -2.6371, -2.8937, -5.9442, -1.6303,\n",
      "         -1.3239, -1.1545, -0.8833, -1.0393, -1.5915, -1.7197, -1.4788, -1.4367,\n",
      "         -1.4473, -0.9941, -4.9135, -4.8664, -5.2065, -5.3487, -4.7938, -4.8686,\n",
      "         -4.8816, -5.1502, -5.1411, -5.0400, -4.9651, -4.9457, -4.9477, -4.9458,\n",
      "         -4.9447, -4.9349, -4.9131, -4.8885, -4.8761, -4.8791, -4.8841, -4.8792,\n",
      "         -4.8704, -4.8562, -4.8494, -4.8495, -4.8689, -4.8818, -4.8919, -4.9060,\n",
      "         -4.9147, -4.9317, -4.9508, -4.9732, -4.9920, -5.0140, -5.0367, -5.0659,\n",
      "         -5.1034, -5.1516, -5.1941, -5.2266, -5.2530, -5.2688, -5.2807, -5.2923,\n",
      "         -5.3082, -5.3138, -5.3108, -5.3060, -5.3016, -5.2961, -5.2946, -5.2812,\n",
      "         -5.2427, -5.2100, -5.1901, -5.1601, -5.1292, -5.0746, -5.0612, -5.0702,\n",
      "         -5.0762, -5.0745, -5.0673, -4.9635, -4.9240, -4.9094, -4.8930, -4.8818,\n",
      "         -4.8710, -4.8659]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.2822, -1.3069], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "reward_sequences\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "turn_level_rewards\n",
      "tensor([-1.0569, -1.4684, -1.2842, -1.5478, -1.2569, -1.1452, -1.4859, -1.4763],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([8])\n",
      "tla shape\n",
      "torch.Size([8, 236])\n",
      "prune_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "non_log_prob_penalty\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "actor_loss\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "critic_loss\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "batch_token_level_rewards\n",
      "tensor([[-0.2054, -0.2075, -0.2096,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1658, -0.1675, -0.1692,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2350, -0.2373, -0.2397,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2225, -0.2248, -0.2271,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1712, -0.1729, -0.1747,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2018, -0.2039, -0.2059,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 236])\n",
      "batch_token_level_rewards mean\n",
      "tensor([-1.0106, -1.4040, -1.2651, -1.4800, -1.2018, -1.0950, -1.4208, -1.4116],\n",
      "       device='cuda:0')\n",
      "batch_mask\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
      "actor_log_probs\n",
      "tensor([[-2.3607, -4.2498, -1.2836,  ..., -4.8022, -4.7891, -4.7799],\n",
      "        [-2.3149, -4.1998, -1.2978,  ..., -5.0162, -5.0049, -4.9836],\n",
      "        [-2.2880, -4.2568, -1.2958,  ..., -4.7985, -4.8539, -4.8704],\n",
      "        ...,\n",
      "        [-2.3278, -4.2428, -1.2932,  ..., -4.9533, -4.9356, -4.9233],\n",
      "        [-2.3213, -4.2006, -1.2827,  ..., -4.8223, -4.8259, -4.8222],\n",
      "        [-2.2865, -4.2049, -1.2685,  ..., -4.4871, -4.5158, -4.5536]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "actor_log_probs_mean\n",
      "tensor([-1.1286, -1.3313, -1.3666, -1.4504, -1.3457, -1.1446, -1.4758, -1.5145],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# lr_mult = 1.0 if i < 200 else 0.1\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     lr_mult \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mdqn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_mult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_games\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     dqn_trainer\u001b[38;5;241m.\u001b[39mreset_memory(finetune\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/mamba-chat/igre_dqn_v2.py:603\u001b[0m, in \u001b[0;36mIgreDQNTrainer.update\u001b[0;34m(self, lr_mult, early_games)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_ \u001b[38;5;129;01min\u001b[39;00m batch_indices:\n\u001b[1;32m    602\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idxs[idx_]\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriority_replay_memory\u001b[38;5;241m.\u001b[39mupdate(idx, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "game_count = 1200\n",
    "\n",
    "results = []\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for i in range(game_count):\n",
    "    did_win = run()\n",
    "    results.append(int(did_win))\n",
    "    if i > 5:\n",
    "        win_rate = sum(results[-5:]) / 5\n",
    "        xs.append(i)\n",
    "        ys.append(win_rate)\n",
    "        pltsin(ax, fig, xs, ys, hdisplay)\n",
    "    if i % 16 == 0:\n",
    "        # lr_mult = 1.0 if i < 200 else 0.1\n",
    "        lr_mult = 0.1 if i < 500 else 0.01 if i < 1000 else 0.001\n",
    "        dqn_trainer.update(lr_mult, early_games = False)\n",
    "    if i % 300 == 0:\n",
    "        dqn_trainer.reset_memory(finetune=False)\n",
    "    if i % 25 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "    if i % 100 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_dqn_{i}.pt\")\n",
    "        print(f\"model saved at {i}\")\n",
    "        dqn_trainer.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_game_count = 100\n",
    "eval_results = []\n",
    "\n",
    "teacher_forcing_count = 9999999999999999999\n",
    "\n",
    "for i in range(eval_game_count):\n",
    "    did_win = run()\n",
    "    eval_results.append(int(did_win))\n",
    "    if i % 25 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "sum(eval_results) / eval_game_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are to solve the an algebraic expression.\n",
    "You have access to two commands:\n",
    "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
    "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
    "The following is the algebraic expression:\n",
    "x = 23 + 93\n",
    "What is the value of x?\n",
    "When you have an answer, use the 'finish' command to submit your answer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "player:\"\"\"\n",
    "\n",
    "get_completion(prompt)\n",
    "# prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
