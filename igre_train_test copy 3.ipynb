{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from igre_dqn_v3 import IgreDQNTrainer\n",
    "from igre_v2 import Igre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'based'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbased\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTLMHeadModel\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTLMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhazyresearch/based-360m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'based'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from based.models.gpt import GPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPTLMHeadModel.from_pretrained_hf(\"hazyresearch/based-360m\")\n",
    "\n",
    "based_model = model.transformer\n",
    "lm_head = model.lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sim4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_4_training_data_unpickled = pickle.load(open(\"sim4_data2.pkl\", \"rb\"))\n",
    "len(sim_4_training_data_unpickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_4_training_data_unpickled[1].actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Igre(based_model, lm_head, tokenizer)\n",
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_NAN = True\n",
    "def check_activation_for_nan(name):\n",
    "    def hook(model, input, output):\n",
    "        if type(output) is tuple:\n",
    "            for i, out in enumerate(output):\n",
    "                if type(out) is torch.Tensor and torch.isnan(out).any():\n",
    "                    print(f\"Found NaN in {name}[{i}]\")\n",
    "        elif type(output) is torch.Tensor:\n",
    "            if torch.isnan(output).any():\n",
    "                print(f\"Found NaN in {name}\")\n",
    "    return hook\n",
    "if DEBUG_NAN:\n",
    "    for name, module in model.named_modules():\n",
    "        module.register_forward_hook(check_activation_for_nan(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_trainer = IgreDQNTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "teacher_forcing_count = 0\n",
    "\n",
    "def get_completion(prompt):\n",
    "    global teacher_forcing_count\n",
    "    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    response_ids, z = dqn_trainer.get_response(prompt_ids)\n",
    "    metadata = {\n",
    "        \"z\": z,\n",
    "        \"response_ids\": response_ids.detach().cpu()\n",
    "    }\n",
    "    # use_teacher_forcing = random.random() < 0.5\n",
    "    # if use_teacher_forcing and teacher_forcing_count < 100:\n",
    "    #     teacher_forcing_count += 1\n",
    "    #     print(\"use teacher forcing\")\n",
    "    #     finish_X_re = re.compile(\" finish ([ABCD])\")\n",
    "    #     match = finish_X_re.search(prompt)\n",
    "    #     if match:\n",
    "    #         finish_X = match.group(0).strip()\n",
    "    #         return finish_X, metadata\n",
    "    print(\"completion response ids\")\n",
    "    print(response_ids)\n",
    "    response = tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=True)\n",
    "    # if '\"' in response or \"'\" in response:\n",
    "    #     # Remove quotes\n",
    "    #     response = response.replace('\"', \"\")\n",
    "    #     response = response.replace(\"'\", \"\")\n",
    "    #     response_ids = tokenizer.encode(response, return_tensors=\"pt\").cuda()\n",
    "    return tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=True), metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation.sim_runner import SimRunner\n",
    "from simulation.common.base_actor import AIActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transition(transition, finetune=False, finetune_state_to_prompt_fn=None):\n",
    "    assert finetune == False or (finetune == True and finetune_state_to_prompt_fn is not None)\n",
    "    def inner(prompt, response, next_state_prompt):\n",
    "        prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        response_ids = tokenizer.encode(\n",
    "            response, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).long()\n",
    "        # Add eos token to response_ids\n",
    "        response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        next_state_prompt_ids = tokenizer.encode(\n",
    "            next_state_prompt, return_tensors=\"pt\", add_special_tokens=True\n",
    "        ).long()\n",
    "\n",
    "        act_metadata = transition.action.metadata\n",
    "        z = torch.zeros(1)\n",
    "        if act_metadata is not None and \"z\" in act_metadata:\n",
    "            z = act_metadata[\"z\"]\n",
    "        if act_metadata is not None and \"response_ids\" in act_metadata:\n",
    "            response_ids = act_metadata[\"response_ids\"]\n",
    "            response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        reward = (transition.next_state.score - transition.state.score) / 100\n",
    "        reward = torch.tensor([reward]).float().detach()\n",
    "\n",
    "        next_state_is_terminal = torch.tensor([1. if transition.next_state.is_terminal else 0.])\n",
    "\n",
    "        print(f\"prompt: {prompt}\")\n",
    "        print(f\"response: {response}\")\n",
    "        dqn_trainer.add_transition(\n",
    "            prompt_ids.squeeze(0),\n",
    "            response_ids.squeeze(0),\n",
    "            z,\n",
    "            reward,\n",
    "            next_state_prompt_ids.squeeze(0),\n",
    "            next_state_is_terminal,\n",
    "            add_to_finetune_memory=finetune,\n",
    "        )\n",
    "    if finetune:\n",
    "        # Check the classname for \"player\"\n",
    "        if \"player\" in transition.actor.__class__.__name__.lower():\n",
    "            assert finetune_state_to_prompt_fn is not None\n",
    "            prompt = finetune_state_to_prompt_fn(transition.state)\n",
    "            response = transition.action.content\n",
    "            next_state_prompt = finetune_state_to_prompt_fn(transition.next_state)\n",
    "            inner(prompt, response, next_state_prompt)\n",
    "    else:\n",
    "        if isinstance(transition.actor, AIActor) and transition.actor.name == \"player\":\n",
    "            prompt = transition.action.completion_prompt\n",
    "            response = transition.action.completion_response\n",
    "\n",
    "            print(\"## DEBUG prompt\")\n",
    "            print(prompt)\n",
    "            prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            print(prompt_ids)\n",
    "\n",
    "            next_state = transition.next_state\n",
    "            next_state_prompt = transition.actor.state_to_prompt(next_state)\n",
    "            inner(prompt, response, next_state_prompt)\n",
    "\n",
    "        # prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        # response_ids = tokenizer.encode(\n",
    "        #     response, return_tensors=\"pt\", add_special_tokens=False\n",
    "        # ).long()\n",
    "        # # Add eos token to response_ids\n",
    "        # response_ids = torch.cat([response_ids, torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long)], dim=1).long()  # type: ignore\n",
    "\n",
    "        # next_state_prompt_ids = tokenizer.encode(\n",
    "        #     next_state_prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "        # ).long()\n",
    "\n",
    "        # z = transition.action.metadata[\"z\"]\n",
    "        # reward = transition.next_state.score - transition.state.score\n",
    "        # reward = torch.tensor([reward])\n",
    "        # print(f\"prompt: {prompt}\")\n",
    "        # print(f\"response: {response}\")\n",
    "        # dqn_trainer.add_transition(\n",
    "        #     prompt_ids.squeeze(0),\n",
    "        #     response_ids.squeeze(0),\n",
    "        #     z,\n",
    "        #     reward,\n",
    "        #     next_state_prompt_ids.squeeze(0),\n",
    "        #     transition.next_state.is_terminal,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_name = \"sim4\"\n",
    "sim_runner = SimRunner(\n",
    "    sim_name=sim_name,\n",
    "    seed=1,\n",
    "    model_forward_func_for_ai={\n",
    "        \"player\": get_completion,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process transitions using the player actor in the sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_actor = None\n",
    "for actor in sim_runner.sim.actors:\n",
    "    if isinstance(actor, AIActor):\n",
    "        ai_actor = actor\n",
    "        break\n",
    "ai_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_trainer.reset_memory(finetune=True)\n",
    "for x in sim_4_training_data_unpickled:\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "    process_transition(x, finetune=True, finetune_state_to_prompt_fn=ai_actor.state_to_prompt)\n",
    "len(dqn_trainer.finetune_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run():\n",
    "    sim_runner.reset(seed=1)\n",
    "    print(\"## DEBUG sim run len\")\n",
    "    print(len(sim_runner.history))\n",
    "    while not sim_runner.get_state().is_terminal:\n",
    "        transitions = sim_runner.step()\n",
    "        for x in transitions:\n",
    "            if isinstance(x.actor, AIActor) and x.actor.name == \"player\":\n",
    "                process_transition(x)\n",
    "    print(\"done\")\n",
    "    final_transitions = sim_runner.final()\n",
    "    did_win = False\n",
    "    for x in final_transitions:\n",
    "        process_transition(x)\n",
    "        if x.next_state.score > 0:\n",
    "            did_win = True\n",
    "    return did_win\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def pltsin(ax, fig, x, y, hdisplay):\n",
    "    if ax.lines:\n",
    "        for line in ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            line.set_ydata(y)\n",
    "            # update scale\n",
    "            ax.relim()\n",
    "            ax.autoscale_view()\n",
    "\n",
    "    else:\n",
    "        ax.plot(x, y)\n",
    "    hdisplay.update(fig)\n",
    "    time.sleep(1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "hdisplay = display.display(\"\", display_id=True)\n",
    "ax.set_label(\"game_count\")\n",
    "ax.set_label(\"win_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"finish x = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"finish x = 1\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "  #  remov\n",
    "  if i < 3:\n",
    "    continue\n",
    "  # Train on the existing teacher data\n",
    "  dqn_trainer.update(1.0, i == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are to solve the an algebraic expression.\n",
    "You have access to two commands:\n",
    "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
    "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
    "The following is the algebraic expression:\n",
    "x = 10 + 10\n",
    "What is the value of x?\n",
    "When you have an answer, use the 'finish' command to submit your answer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "player:\"\"\"\n",
    "\n",
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "game_count = 1200\n",
    "\n",
    "results = []\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for i in range(game_count):\n",
    "    did_win = run()\n",
    "    results.append(int(did_win))\n",
    "    if i > 5:\n",
    "        win_rate = sum(results[-5:]) / 5\n",
    "        xs.append(i)\n",
    "        ys.append(win_rate)\n",
    "        pltsin(ax, fig, xs, ys, hdisplay)\n",
    "    if i % 16 == 0:\n",
    "        # lr_mult = 1.0 if i < 200 else 0.1\n",
    "        lr_mult = 0.1 if i < 500 else 0.01 if i < 1000 else 0.001\n",
    "        dqn_trainer.update(lr_mult, early_games = False)\n",
    "    if i % 300 == 0:\n",
    "        dqn_trainer.reset_memory(finetune=False)\n",
    "    if i % 25 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "    if i % 100 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_dqn_{i}.pt\")\n",
    "        print(f\"model saved at {i}\")\n",
    "        dqn_trainer.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_game_count = 100\n",
    "eval_results = []\n",
    "\n",
    "teacher_forcing_count = 9999999999999999999\n",
    "\n",
    "for i in range(eval_game_count):\n",
    "    did_win = run()\n",
    "    eval_results.append(int(did_win))\n",
    "    if i % 25 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "sum(eval_results) / eval_game_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are to solve the an algebraic expression.\n",
    "You have access to two commands:\n",
    "- 'calc <expression>' to calculate the value of an expression. <expression> must be in the form 'a + b' where a and b are integers.\n",
    "- 'finish <solution>' to submit your answer. <solution> must be in the form '<letter> = <number>' where <letter> is the letter used in the problem and <number> is the solution.\n",
    "The following is the algebraic expression:\n",
    "x = 23 + 93\n",
    "What is the value of x?\n",
    "When you have an answer, use the 'finish' command to submit your answer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "player:\"\"\"\n",
    "\n",
    "get_completion(prompt)\n",
    "# prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
