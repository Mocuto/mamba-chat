{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model archi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from monarch_i2i import MonarchI2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mama(nn.Module):\n",
    "    \"\"\"Memory-based Retrieval + Mama Chat model\"\"\"\n",
    "\n",
    "    def __init__(self, retrieval_path=None, model_path=None):\n",
    "        super(Mama, self).__init__()\n",
    "        self.retriever = MonarchI2i()\n",
    "        if retrieval_path:\n",
    "            self.retriever.load_state_dict(\n",
    "                torch.load(retrieval_path, map_location=\"cpu\")\n",
    "            )\n",
    "        self.generator = MambaLMHeadModel.from_pretrained(\n",
    "            \"state-spaces/mamba-2.8b\", dtype=torch.bfloat16\n",
    "        )\n",
    "        self.retriever.train()\n",
    "        self.generator.train()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def retrieve(self, query_r, embedded_corpus, k=3):\n",
    "        \"\"\"Retrieve k most similar items from corpus\"\"\"\n",
    "        # Embed the query\n",
    "        query = self.retriever.model.forward(query_r)\n",
    "        # Use dot product to find the most similar\n",
    "        scores_with_idx = []\n",
    "        for i, item in enumerate(embedded_corpus):\n",
    "            emb = item[0]\n",
    "            scores_with_idx.append((self.cos(query, emb), i))\n",
    "        # Sort by score\n",
    "        scores_with_idx.sort(reverse=True, key=lambda x: x[0])\n",
    "        # Return the top k\n",
    "        return scores_with_idx[:k]\n",
    "\n",
    "    def generate(self, query_g, embedded_corpus, memory_indices, **kwargs):\n",
    "        \"\"\"Generate a response from the query and the retrieved memory\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory = [embedded_corpus[i[1]][1] for i in memory_indices]\n",
    "        # Input is memory + query\n",
    "        input_ids = torch.cat(\n",
    "            memory + [query_g], dim=1\n",
    "        )\n",
    "        # Generate the response\n",
    "        response = self.generator(input_ids)\n",
    "        return_augmented_input_ids = kwargs.get(\"return_augmented_input_ids\", False)\n",
    "        if return_augmented_input_ids:\n",
    "            return response, input_ids\n",
    "        return response\n",
    "\n",
    "    def forward(self, query_r, query_g, embedded_corpus, k=3):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory_indices = self.retrieve(query_r, embedded_corpus, k)\n",
    "        # Generate the response\n",
    "        response = self.generate(query_g, embedded_corpus, memory_indices)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama = Mama(retrieval_path=\"./monarch_768_retrieval.pt\")\n",
    "mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOROBOTS_PATH = \"./data/norobots_train.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'Please summarize the goals for s...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[{'content': 'In short, why does the article s...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[{'content': 'Give me the main idea of this po...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[{'content': 'Summarize what hi-fi audio is in...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[{'content': 'Few people would have predicted ...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              messages   category\n",
       "0    [{'content': 'Please summarize the goals for s...  Summarize\n",
       "19   [{'content': 'In short, why does the article s...  Summarize\n",
       "34   [{'content': 'Give me the main idea of this po...  Summarize\n",
       "45   [{'content': 'Summarize what hi-fi audio is in...  Summarize\n",
       "122  [{'content': 'Few people would have predicted ...  Summarize"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norobots_pq = pq.read_table(\n",
    "  NOROBOTS_PATH,\n",
    "  columns=['messages', 'category'],\n",
    ")\n",
    "norobots_pq = norobots_pq.to_pandas()\n",
    "norobots_pq = norobots_pq[norobots_pq['category'] == 'Summarize']\n",
    "norobots_pq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "norobots_all = norobots_pq['messages'].tolist()\n",
    "\n",
    "norobots_memory = norobots_all[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_corpus = [\n",
    "  \"Johnson's last name is Cook\",\n",
    "  \"Johnson is 32 years old\",\n",
    "  \"The capital of Brazil is Brasília\",\n",
    "  \"Tracy is 55 years old\",\n",
    "  \"Dougie is an elephant, not a human\",\n",
    "  \"\"\"\n",
    "  user: Summarize the following task description in four words:\n",
    "  \"Send an email to the customer about the new product\"\n",
    "  assistant: New product customer email\n",
    "  \"\"\",\n",
    "]\n",
    "for item in norobots_memory:\n",
    "    s = \"\\n\".join(f\"{x['role']}: {x['content']}\" for x in item)\n",
    "    memory_corpus.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: Summarize the main points of this article about forests.\\n\\nHere is the article you'll use:\\nDry forest\\nDry forests are found in warm climates where seasonal droughts last for several months at a time. Deciduous trees predominate these forests, and during the drought a leafless period occurs, which varies with species type. Tropical and Subtropical Dry Forests are found in southern Mexico, southeastern Africa, the Lesser Sundas, central India, Indochina, Madagascar, New Caledonia, eastern Bolivia and central Brazil, the Caribbean, valleys of the northern Andes, and along the coasts of Ecuador and Peru.\\nLatin American dry tropical forests are some of the most endangered on earth. \\nassistant: Dry forests exist in drought-ridden, warm climates with various deciduous trees. There are tropical and subtropical variations throughout southeastern Africa, South America, the Caribbean, central India, and other locations, although those in Latin America are some of the most endangered. \""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_corpus[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.load_state_dict(torch.load(\"mama_toy.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "r_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "g_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_corpus(corpus, device=\"cpu\"):\n",
    "    corpus_r_tokens = []\n",
    "    corpus_g_tokens = []\n",
    "    for item in corpus:\n",
    "        r = r_tokenizer(item, return_tensors=\"pt\", max_length=512)\n",
    "        g = g_tokenizer(f\"<|memory|>{item}{g_tokenizer.eos_token}\", return_tensors=\"pt\")\n",
    "        corpus_r_tokens.append(r)\n",
    "        corpus_g_tokens.append(g)\n",
    "    with torch.no_grad():\n",
    "        embedded_corpus = []\n",
    "        for r, g in zip(corpus_r_tokens, corpus_g_tokens):\n",
    "            r_emb = mama.retriever.model.forward(r[\"input_ids\"].to(device))\n",
    "            embedded_corpus.append((r_emb, g[\"input_ids\"].to(device)))\n",
    "    return embedded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = embed_corpus(memory_corpus, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mama_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "g_tokenizer.chat_template = mama_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 54])\n",
      "torch.Size([1, 45])\n",
      "['<|memory|>\\n  user: Summarize the following task description in four words:\\n  \"Send an email to the customer about the new product\"\\n  assistant: New product customer email\\n  <|endoftext|><|memory|>user: Can you summarize in simple words what this excerpt says? \\n\\nIn mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".[2]\\nassistant: In simple terms, an algorithm is a series of very detailed instructions for performing a calculation or processing data. <|endoftext|><|memory|>user: Shortly summarize capitalizing on natural light:\\nCapitalize on Natural Light\\nSplit bedroom/desk in apartment with two-toned walls\\n Studio Sucio\\nThis former Holiday Inn in Los Angeles was redesigned as small apartments by Studio Sucio.\\n\\n\"Capitalize on natural light and let windows be uncovered,\" says designer Leslie Barrett. \"If you must cover, employ the sparest of shades, like a roll-down mesh.\"\\n\\nIn this space, color blocking separates the ceiling from the walls and gives a foreground and background, plus a custom-designed bed with built-in storage and a desk focuses on function and saving square feet.\\nassistant: Only cover windows with light shades if they need to be covered.<|endoftext|><|system|>\\nYou are a helpful assistant.<|endoftext|>\\n<|user|>\\nRewrite the following task description in four words: Add checker in finish command for CSV, make sure it is formatted using ; instead of,<|endoftext|>\\n<|assistant|>\\n']\n",
      "Model: Add a checker to the finish command to ensure the CSV is formatted correctly.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mama.eval()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Rewrite the following task description in four words: Add checker in finish command for CSV, make sure it is formatted using ; instead of ,\"\"\"},\n",
    "    ]\n",
    "    input_ids_g = torch.LongTensor(\n",
    "        g_tokenizer.apply_chat_template(\n",
    "            messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "        )\n",
    "    )\n",
    "    input_ids_r = torch.LongTensor(\n",
    "        r_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            chat_template=\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ 'user:\\n' + message['content'] }}\\n{% elif message['role'] == 'system' %}\\n{{ 'system:\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ 'assistant:\\n'  + message['content'] }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ 'assistant:' }}\\n{% endif %}\\n{% endfor %}\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "        )\n",
    "    ).unsqueeze(0)\n",
    "    print(input_ids_g.shape)\n",
    "    topk = mama.retrieve(input_ids_r.cuda(), [(x.cuda(), y.cuda()) for x, y in Z], k=3)\n",
    "    memory_indices = topk\n",
    "    memory = [Z[i[1]][1] for i in memory_indices]\n",
    "    print(memory[0].shape)\n",
    "    # Input is memory + query\n",
    "    input_ids = torch.cat(memory + [input_ids_g.cuda()], dim=1)\n",
    "    print(g_tokenizer.batch_decode(input_ids))\n",
    "    # Generate the response\n",
    "    out = mama.generator.generate(\n",
    "        input_ids=input_ids.cuda(),\n",
    "        max_length=2000,\n",
    "        temperature=0.7,\n",
    "        top_p=0.5,\n",
    "        eos_token_id=g_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded = g_tokenizer.batch_decode(out)\n",
    "\n",
    "    print(\"Model:\", decoded[0].split(\"<|assistant|>\\n\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: Who composed the concerti 'Four Seasons'?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_corpus[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
