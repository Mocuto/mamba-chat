{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mama Script 3 Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from monarch_i2i import MonarchI2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mama(nn.Module):\n",
    "    \"\"\"Memory-based Retrieval + Mama Chat model\"\"\"\n",
    "\n",
    "    def __init__(self, retrieval_path=None, model_path=None):\n",
    "        super(Mama, self).__init__()\n",
    "        self.retriever = MonarchI2i()\n",
    "        if retrieval_path:\n",
    "            self.retriever.load_state_dict(\n",
    "                torch.load(retrieval_path, map_location=\"cpu\")\n",
    "            )\n",
    "        self.generator = MambaLMHeadModel.from_pretrained(\n",
    "            \"state-spaces/mamba-2.8b\", dtype=torch.bfloat16\n",
    "        )\n",
    "        self.retriever.train()\n",
    "        self.generator.train()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def retrieve(self, query_r, embedded_corpus, k=3):\n",
    "        \"\"\"Retrieve k most similar items from corpus\"\"\"\n",
    "        # Embed the query\n",
    "        query = self.retriever.model.forward(query_r)\n",
    "        # Use dot product to find the most similar\n",
    "        scores_with_idx = []\n",
    "        for i, item in enumerate(embedded_corpus):\n",
    "            emb = item[0]\n",
    "            scores_with_idx.append((self.cos(query, emb), i))\n",
    "        # Sort by score\n",
    "        scores_with_idx.sort(reverse=True, key=lambda x: x[0])\n",
    "        # Return the top k\n",
    "        return scores_with_idx[:k]\n",
    "\n",
    "    def generate(self, query_g, embedded_corpus, memory_indices, **kwargs):\n",
    "        \"\"\"Generate a response from the query and the retrieved memory\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory = [embedded_corpus[i[1]][1] for i in memory_indices]\n",
    "        # Input is memory + query\n",
    "        input_ids = torch.cat(\n",
    "            memory + [query_g], dim=1\n",
    "        )\n",
    "        # Generate the response\n",
    "        response = self.generator(input_ids)\n",
    "        return_augmented_input_ids = kwargs.get(\"return_augmented_input_ids\", False)\n",
    "        if return_augmented_input_ids:\n",
    "            return response, input_ids\n",
    "        return response\n",
    "\n",
    "    def forward(self, query_r, query_g, embedded_corpus, k=3):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory_indices = self.retrieve(query_r, embedded_corpus, k)\n",
    "        # Generate the response\n",
    "        response = self.generate(query_g, embedded_corpus, memory_indices)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama = Mama(retrieval_path=\"./monarch_768_retrieval.pt\")\n",
    "mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "g_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedded corpus\n",
    "def tokenize_memory_corpus(memory_corpus):\n",
    "    corpus_r_tokens = []\n",
    "    corpus_g_tokens = []\n",
    "\n",
    "    for item in memory_corpus:\n",
    "        r_tokens = r_tokenizer(item, return_tensors=\"pt\", max_length=512)\n",
    "        g_tokens = g_tokenizer(f\"<|memory|>{item}{g_tokenizer.eos_token}\", return_tensors=\"pt\", max_length=512)\n",
    "        corpus_r_tokens.append(r_tokens)\n",
    "        corpus_g_tokens.append(g_tokens)\n",
    "    return corpus_r_tokens, corpus_g_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the memory corpus\n",
    "MEMORY_PATH = \"./data/mama_toy_memory.json\"\n",
    "DATA_PATH = \"./data/mama_toy_chat.jsonl\"\n",
    "NOROBOTS_PATH = \"./data/norobots_train.parquet\"\n",
    "DISTRACTOR_PATH = \"./data/distract.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_corpus = []\n",
    "with open(MEMORY_PATH, \"r\") as f:\n",
    "    memory_corpus = json.load(f)\n",
    "len(memory_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.load_state_dict(torch.load('mama_in_progress_epoch_29.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_corpus(corpus_r_tokens, corpus_g_tokens, device=\"cpu\"):\n",
    "    with torch.no_grad():\n",
    "        embedded_corpus = []\n",
    "        for r, g in zip(corpus_r_tokens, corpus_g_tokens):\n",
    "            r_emb = mama.retriever.model.forward(r[\"input_ids\"].to(device))\n",
    "            embedded_corpus.append((r_emb, g[\"input_ids\"].to(device)))\n",
    "    return embedded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedded corpus\n",
    "def tokenize_memory_corpus(memory_corpus):\n",
    "    corpus_r_tokens = []\n",
    "    corpus_g_tokens = []\n",
    "\n",
    "    for item in memory_corpus:\n",
    "        r_tokens = r_tokenizer(item, return_tensors=\"pt\", max_length=512)\n",
    "        g_tokens = g_tokenizer(f\"<|memory|>{item}{g_tokenizer.eos_token}\", return_tensors=\"pt\", max_length=512)\n",
    "        corpus_r_tokens.append(r_tokens)\n",
    "        corpus_g_tokens.append(g_tokens)\n",
    "    return corpus_r_tokens, corpus_g_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "mr_tokens, mg_tokens = tokenize_memory_corpus(memory_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_corpus = embed_corpus(mr_tokens, mg_tokens, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"'<|system|>\\nAnswer the given question<|endoftext|>\\n<|user|>\\nHow old is Minh?<|endoftext|>\\n\"\n",
    "s_tokens = g_tokenizer.encode(s, add_special_tokens=False, return_tensors=\"pt\")\n",
    "s_tokens_r = r_tokenizer.encode(s, add_special_tokens=False, return_tensors=\"pt\")\n",
    "s_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_corpus = [(r_emb.cuda(), g_emb.cuda()) for r_emb, g_emb in embedded_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(logits=tensor([[[  5.9688,  -4.8125,   9.4375,  ...,  -4.7812,  -5.2500,  -5.5312],\n",
       "         [ 11.6875,  -0.4062,  13.3125,  ...,  -0.3945,  -0.9570,  -0.9219],\n",
       "         [  6.5312, -13.1250,   5.3125,  ..., -12.5625, -12.5625, -12.5000],\n",
       "         ...,\n",
       "         [ 27.3750,  -5.7500,   5.2812,  ...,  -6.2188,  -4.7500,  -5.4375],\n",
       "         [ 19.8750,  -5.5312,  13.0000,  ...,  -4.5625,  -4.7188,  -4.4688],\n",
       "         [ 13.4375,  -2.6875,  13.1875,  ...,  -3.2969,  -4.2812,  -3.4688]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3s = mama.forward(\n",
    "  query_r=s_tokens_r.cuda(),\n",
    "  query_g=s_tokens.cuda(),\n",
    "  embedded_corpus=embedded_corpus\n",
    ")\n",
    "out3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"|memory|>user foments dissent and increases attrition amongst your ranks<|endoftext|><|memory|>Fions are huge so if you're hungry this is the place. The biscuits are amazing with the homemade jam. Make sure to try one to share regardless what time you are there<|endoftext|><|memory|>F Vegas Biscuits and gravy · 1. Omelet House. (859). Open Now. American$$ - $$$ · 2. Mr. Mamas. (2,205). Open Now · 3. Jamm's Restaurant.<|endoftext|><use|user|>\\nYou the given question<|endoftext|>\\n<|user|>\\nWrite can is Minh?<|endoftext|>\\n<\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tokenizer.decode(out3s.logits[0].argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = mama.retrieve(\n",
    "    query_r="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
