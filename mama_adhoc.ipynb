{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model archi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from monarch_i2i import MonarchI2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mama(nn.Module):\n",
    "    \"\"\"Memory-based Retrieval + Mama Chat model\"\"\"\n",
    "\n",
    "    def __init__(self, retrieval_path=None, model_path=None):\n",
    "        super(Mama, self).__init__()\n",
    "        self.retriever = MonarchI2i()\n",
    "        if retrieval_path:\n",
    "            self.retriever.load_state_dict(\n",
    "                torch.load(retrieval_path, map_location=\"cpu\")\n",
    "            )\n",
    "        self.generator = MambaLMHeadModel.from_pretrained(\n",
    "            \"state-spaces/mamba-2.8b\", dtype=torch.bfloat16\n",
    "        )\n",
    "        self.retriever.train()\n",
    "        self.generator.train()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def retrieve(self, query_r, embedded_corpus, k=3):\n",
    "        \"\"\"Retrieve k most similar items from corpus\"\"\"\n",
    "        # Embed the query\n",
    "        query = self.retriever.model.forward(query_r)\n",
    "        # Use dot product to find the most similar\n",
    "        scores_with_idx = []\n",
    "        for i, item in enumerate(embedded_corpus):\n",
    "            emb = item[0]\n",
    "            scores_with_idx.append((self.cos(query, emb), i))\n",
    "        # Sort by score\n",
    "        scores_with_idx.sort(reverse=True, key=lambda x: x[0])\n",
    "        # Return the top k\n",
    "        return scores_with_idx[:k]\n",
    "\n",
    "    def generate(self, query_g, embedded_corpus, memory_indices, **kwargs):\n",
    "        \"\"\"Generate a response from the query and the retrieved memory\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory = [embedded_corpus[i[1]][1] for i in memory_indices]\n",
    "        # Input is memory + query\n",
    "        input_ids = torch.cat(\n",
    "            memory + [query_g], dim=1\n",
    "        )\n",
    "        # Generate the response\n",
    "        response = self.generator(input_ids)\n",
    "        return_augmented_input_ids = kwargs.get(\"return_augmented_input_ids\", False)\n",
    "        if return_augmented_input_ids:\n",
    "            return response, input_ids\n",
    "        return response\n",
    "\n",
    "    def forward(self, query_r, query_g, embedded_corpus, k=3):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Retrieve the memory\n",
    "        memory_indices = self.retrieve(query_r, embedded_corpus, k)\n",
    "        # Generate the response\n",
    "        response = self.generate(query_g, embedded_corpus, memory_indices)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Monarch Mixer for Sequence Mixing: True\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama = Mama(retrieval_path=\"./monarch_768_retrieval.pt\")\n",
    "mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOROBOTS_PATH = \"./data/norobots_train.parquet\"\n",
    "TASK_SUMMARIZATION_PATH = \"./data/instruction_summary_convo_dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'Please summarize the goals for s...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[{'content': 'In short, why does the article s...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[{'content': 'Give me the main idea of this po...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[{'content': 'Summarize what hi-fi audio is in...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[{'content': 'Few people would have predicted ...</td>\n",
       "      <td>Summarize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              messages   category\n",
       "0    [{'content': 'Please summarize the goals for s...  Summarize\n",
       "19   [{'content': 'In short, why does the article s...  Summarize\n",
       "34   [{'content': 'Give me the main idea of this po...  Summarize\n",
       "45   [{'content': 'Summarize what hi-fi audio is in...  Summarize\n",
       "122  [{'content': 'Few people would have predicted ...  Summarize"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norobots_pq = pq.read_table(\n",
    "  NOROBOTS_PATH, \n",
    "  columns=['messages', 'category'],\n",
    ")\n",
    "norobots_pq = norobots_pq.to_pandas()\n",
    "norobots_pq = norobots_pq[norobots_pq['category'] == 'Summarize']\n",
    "norobots_pq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norobots_all = norobots_pq['messages'].tolist()\n",
    "\n",
    "norobots_memory = norobots_all[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Let's load up the instruction summarization data\n",
    "task_summarization_data = []\n",
    "with open(TASK_SUMMARIZATION_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        task_summarization_data.append(json.loads(line))\n",
    "len(task_summarization_data)\n",
    "\n",
    "# %%\n",
    "# Let's sample some to add to the memory\n",
    "task_summarization_memory = task_summarization_data[:30]\n",
    "task_summarization_data = task_summarization_data[30:]\n",
    "len(task_summarization_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Please summarize the following instructions in five words or less: Using the following resources:\\r\\nAPI Documentation and Resources:\\r\\n•\\tGoogle Docs API Quickstart: https://developers.google.com/docs/api/quickstart/python\\r\\n•\\tAdditional resource for Google Docs: https://stackoverflow.com/questions/14726409/using-python-how-can-i-read-plain-text-from-a-google-doc\\r\\n\\r\\nWrite the python functions: \\r\\nGoogle Docs API Python Functions:\\r\\n•\\tread_google_doc: This function fetches data from a specified Google Doc.\\r\\nAPI Documentation and Resources:\\r\\n•\\tGoogle Docs API Quickstart: https://developers.google.com/docs/api/quickstart/python\\r\\n•\\tAdditional resource for Google Docs: https://stackoverflow.com/questions/14726409/using-python-how-can-i-read-plain-text-from-a-google-doc\\r\\n\\r\\nSend the code for these functions in an email to tobi@donada.ai using the tobi@donada.com email address. Remember, you can write code so be confident in yourself as you approach this task.'},\n",
       " {'role': 'assistant',\n",
       "  'content': ' \"Fetch data from Google Docs using Python API.\"'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_summarization_memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system: You are a helpful AI assistant.\\nuser: The following are instructions for a task. Please summarize them in five words or less:\\nSend a confirmation email to the pilot customers identified in the previous step. Use the following template:\\n\\nHello [FIRST_NAME],\\n\\nThis is Tobi\\'s AI Receptionist. I am reaching out to confirm that you are still available for our demo presentation tomorrow. Please feel free to reach out to Tobi at tobi@donada.com if you need to reschedule or have any questions.\\n\\nThanks,\\n\\n-T\\n\\nThe task is finished when the email has been sent to all identified customers.\\nassistant:  \"Send demo confirmation email to pilot customers.\"'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_summarization_memory_ = [\n",
    "    \"\\n\".join([x['role'] + \": \" + x['content'] for x in conv])\n",
    "    for conv in task_summarization_memory \n",
    "]\n",
    "task_summarization_memory_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_corpus = [\n",
    "  \"Johnson's last name is Cook\",\n",
    "  \"Johnson is 32 years old\",\n",
    "  \"The capital of Brazil is Brasília\",\n",
    "  \"Tracy is 55 years old\",\n",
    "  \"Dougie is an elephant, not a human\",\n",
    "  \"\"\"\n",
    "  user: Summarize the following task description in four words:\n",
    "  \"Send an email to the customer about the new product\"\n",
    "  assistant: New product customer email\n",
    "  \"\"\",\n",
    "]\n",
    "# for item in norobots_memory:\n",
    "#     s = \"\\n\".join(f\"{x['role']}: {x['content']}\" for x in item)\n",
    "#     memory_corpus.append(s)\n",
    "memory_corpus = memory_corpus + task_summarization_memory_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system: You are a helpful AI assistant.\\nuser: The following are instructions for a task. Please summarize them in five words or less:\\nSend a confirmation email to the pilot customers identified in the previous step. Use the following template:\\n\\nHello [FIRST_NAME],\\n\\nThis is Tobi\\'s AI Receptionist. I am reaching out to confirm that you are still available for our demo presentation tomorrow. Please feel free to reach out to Tobi at tobi@donada.com if you need to reschedule or have any questions.\\n\\nThanks,\\n\\n-T\\n\\nThe task is finished when the email has been sent to all identified customers.\\nassistant:  \"Send demo confirmation email to pilot customers.\"'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_corpus[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.load_state_dict(torch.load(\"mama_in_progress_epoch_29.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "r_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "g_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_corpus(corpus, device=\"cpu\"):\n",
    "    corpus_r_tokens = []\n",
    "    corpus_g_tokens = []\n",
    "    for item in corpus:\n",
    "        r = r_tokenizer(item, return_tensors=\"pt\", max_length=512)\n",
    "        g = g_tokenizer(f\"<|memory|>{item}{g_tokenizer.eos_token}\", return_tensors=\"pt\")\n",
    "        corpus_r_tokens.append(r)\n",
    "        corpus_g_tokens.append(g)\n",
    "    with torch.no_grad():\n",
    "        embedded_corpus = []\n",
    "        for r, g in zip(corpus_r_tokens, corpus_g_tokens):\n",
    "            r_emb = mama.retriever.model.forward(r[\"input_ids\"].to(device))\n",
    "            embedded_corpus.append((r_emb, g[\"input_ids\"].to(device)))\n",
    "    return embedded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "Z = embed_corpus(memory_corpus, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mama_template = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "g_tokenizer.chat_template = mama_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mama(\n",
       "  (retriever): MonarchI2i(\n",
       "    (model): BasicModel(\n",
       "      (model): HuggingFaceModel(\n",
       "        (model): BertForMaskedLM(\n",
       "          (bert): BertModel(\n",
       "            (embeddings): BertEmbeddings(\n",
       "              (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "              (token_type_embeddings): Embedding(2, 768)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (encoder): BertEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-11): 12 x BertLayer(\n",
       "                  (attention): MonarchMixerSequenceMixing(\n",
       "                    (filter_fn): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (filter_fn2): HyenaFilter(\n",
       "                      (dropout): Dropout(p=0.2, inplace=False)\n",
       "                      (pos_emb): PositionalEmbedding()\n",
       "                      (implicit_filter): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (implicit_filter_rev): Sequential(\n",
       "                        (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "                        (1): Sin()\n",
       "                        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (3): Sin()\n",
       "                        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "                        (5): Sin()\n",
       "                        (6): Linear(in_features=128, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (modulation): ExponentialModulation()\n",
       "                    )\n",
       "                    (in_linear): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                    (out_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (act): Identity()\n",
       "                    (drop): Dropout(p=0.0, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (short_filter): Conv1d(2304, 2304, kernel_size=(3,), stride=(1,), padding=(2,), groups=2304)\n",
       "                  )\n",
       "                  (mlp): BertGatedLinearUnitMLP(\n",
       "                    (gated_layers): BlockdiagLinear()\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (wo): BlockdiagLinear()\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (cls): BertOnlyMLMHead(\n",
       "            (predictions): BertLMPredictionHead(\n",
       "              (transform): BertPredictionHeadTransform(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (transform_act_fn): GELUActivation()\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (decoder): Linear(in_features=768, out_features=30528, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dot_product): DotProduct()\n",
       "  )\n",
       "  (generator): MambaLMHeadModel(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(50280, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-63): 64 x Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
       "            (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
       "            (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
       "  )\n",
       "  (cos): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mama.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "asst_tokens = g_tokenizer.encode(\"<|assistant|>\\n\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "torch.Size([1, 98])\n",
      "[\"<|memory|>system: You are a helpful AI assistant.\\nuser: Write a concise header for a task with the following instructions: For each pilot customer, compile a list of features they have requested from the 'feature_requests' table. Then, send an email to tobi@donada.ai asking for an update on these feature requests. This task is finished when the email has been sent.\\nassistant:  Compile list, send email, task complete.<|endoftext|><|memory|>system: You are a helpful AI assistant.\\nuser: Write a super short (less than 5 words) summary of the following instructions: Search the following websites for updates related to:\\n1. CoC Leadership Board Notes\\n2. Policies\\n3. Affordable Housing Being Built\\n- https://everyonehome.org/about/leadership-board/\\n- https://homelessness.acgov.org/\\n- https://www.oaklandca.gov/departments/department-of-housing-and-community-development\\nThe task is finished when all relevant updates from the fourth week of each month have been collected.\\nassistant:  Search for CoC Leadership Board Notes, Policies and Affordable Housing updates on specified websites. Collect relevant updates from 4th week of each month until task complete.<|endoftext|><|memory|>system: You are a helpful AI assistant.\\nuser: Write a concise header for a task with the following instructions: Search the following websites for updates related to:\\n1. CoC Leadership Board Notes\\n2. Policies\\n3. Affordable Housing Being Built\\n- https://everyonehome.org/about/leadership-board/\\n- https://homelessness.acgov.org/\\n- https://www.oaklandca.gov/departments/department-of-housing-and-community-development\\nThe task is finished when all relevant updates from the fourth week of each month have been collected.\\nassistant:  Search for CoC Leadership Board Notes, Policies and Affordable Housing updates on specified websites. Collect relevant updates from 4th week of each month until task complete.<|endoftext|><|system|>\\nYou are a helpful assistant.<|endoftext|>\\n<|user|>\\nPlease give a super short summary of the following task instructions: Research online tutorials on RHLF, send this in an email to tobi@donada.ai using the tobi@donada.com email address<|endoftext|>\\n<|assistant|>\\n\"]\n",
      "['<|memory|>system: You are a helpful AI assistant.\\nuser: Write a concise header for a task with the following instructions: For each pilot customer, compile a list of features they have requested from the \\'feature_requests\\' table. Then, send an email to tobi@donada.ai asking for an update on these feature requests. This task is finished when the email has been sent.\\nassistant:  Compile list, send email, task complete.<|endoftext|><|memory|>system: You are a helpful AI assistant.\\nuser: Write a super short (less than 5 words) summary of the following instructions: Search the following websites for updates related to:\\n1. CoC Leadership Board Notes\\n2. Policies\\n3. Affordable Housing Being Built\\n- https://everyonehome.org/about/leadership-board/\\n- https://homelessness.acgov.org/\\n- https://www.oaklandca.gov/departments/department-of-housing-and-community-development\\nThe task is finished when all relevant updates from the fourth week of each month have been collected.\\nassistant:  Search for CoC Leadership Board Notes, Policies and Affordable Housing updates on specified websites. Collect relevant updates from 4th week of each month until task complete.<|endoftext|><|memory|>system: You are a helpful AI assistant.\\nuser: Write a concise header for a task with the following instructions: Search the following websites for updates related to:\\n1. CoC Leadership Board Notes\\n2. Policies\\n3. Affordable Housing Being Built\\n- https://everyonehome.org/about/leadership-board/\\n- https://homelessness.acgov.org/\\n- https://www.oaklandca.gov/departments/department-of-housing-and-community-development\\nThe task is finished when all relevant updates from the fourth week of each month have been collected.\\nassistant:  Search for CoC Leadership Board Notes, Policies and Affordable Housing updates on specified websites. Collect relevant updates from 4th week of each month until task complete.<|endoftext|><|system|>\\nYou are a helpful assistant.<|endoftext|>\\n<|user|>\\nPlease give a super short summary of the following task instructions: Research online tutorials on RHLF, send this in an email to tobi@donada.ai using the tobi@donada.com email address<|endoftext|>\\n<|assistant|>\\n \"Research online tutorials on RHLF, send email to tobi@donada.ai using tobi@donada.com email address.\"<|endoftext|>']\n",
      "--------------\n",
      "Model:  \"Research online tutorials on RHLF, send email to tobi@donada.ai using tobi@donada.com email address.\"<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mama.eval()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Please give a super short summary of the following task instructions: Research online tutorials on RHLF, send this in an email to tobi@donada.ai using the tobi@donada.com email address\"\"\"},\n",
    "    ]\n",
    "    input_ids_g = torch.LongTensor(\n",
    "        g_tokenizer.apply_chat_template(\n",
    "            messages, return_tensors=\"pt\", add_generation_prompt=False\n",
    "        )\n",
    "    )\n",
    "    input_ids_r = torch.LongTensor(\n",
    "        r_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            chat_template=\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ 'user:\\n' + message['content'] }}\\n{% elif message['role'] == 'system' %}\\n{{ 'system:\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ 'assistant:\\n'  + message['content'] }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ 'assistant:' }}\\n{% endif %}\\n{% endfor %}\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "        )\n",
    "    ).unsqueeze(0)\n",
    "    print(input_ids_g.shape)\n",
    "    topk = mama.retrieve(input_ids_r.cuda(), [(x.cuda(), y.cuda()) for x, y in Z], k=3)\n",
    "    memory_indices = topk\n",
    "    memory = [Z[i[1]][1].cuda() for i in memory_indices]\n",
    "    print(memory[0].shape)\n",
    "    # Input is memory + query\n",
    "    input_ids = torch.cat(memory + [input_ids_g.cuda()] + [asst_tokens.to(\"cuda\")], dim=1)\n",
    "    print(g_tokenizer.batch_decode(input_ids))\n",
    "    # Generate the response\n",
    "    out = mama.generator.generate(\n",
    "        input_ids=input_ids.cuda(),\n",
    "        max_length=2000,\n",
    "        temperature=0.9,\n",
    "        top_p=0.7,\n",
    "        eos_token_id=g_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded = g_tokenizer.batch_decode(out)\n",
    "    print(decoded)\n",
    "    print(\"--------------\")\n",
    "    print(\"Model:\", decoded[0].split(\"<|assistant|>\\n\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: Who composed the concerti 'Four Seasons'?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_corpus[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
