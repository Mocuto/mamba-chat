{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MixerModel, create_block, _init_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGRE\n",
    "<ins>I</ins>nter<ins>g</ins>alactic <ins>R</ins>easoning <ins>E</ins>ngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        d_model: int,\n",
    "        n_vocab: int,\n",
    "    ):\n",
    "        super(iEncoder, self).__init__()\n",
    "        n_layer = 1\n",
    "        self.d_model = d_model\n",
    "        self.mixer = MixerModel(\n",
    "            d_model=d_model,\n",
    "            n_layer=n_layer,\n",
    "            vocab_size=n_vocab + 2,\n",
    "        )\n",
    "\n",
    "        initializer_cfg = None\n",
    "\n",
    "        self.apply(\n",
    "            partial(\n",
    "                _init_weights,\n",
    "                n_layer=n_layer,\n",
    "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x, output_pooled_state=False):\n",
    "        x = self.mixer(x)\n",
    "        if output_pooled_state:\n",
    "            s = x.mean(dim=1)\n",
    "            return x, s\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iOptionGenProbablyWrong(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "      super(iOptionGenProbablyWrong, self).__init__()\n",
    "      c_channels = 5\n",
    "      self.c_embedding = nn.Embedding(c_channels, d_model)\n",
    "      self.fc_mu = nn.Linear(d_model * 2, d_model)\n",
    "      self.fc_logvar = nn.Linear(d_model * 2, d_model)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"c is a tensor of shape (batch_size)\"\"\"\n",
    "        c = self.c_embedding(c)\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:\n",
    "    \"\"\"Inverse softplus function.\n",
    "\n",
    "    Args:\n",
    "        bias (float or tensor): the value to be softplus-inverted.\n",
    "    \"\"\"\n",
    "    is_tensor = True\n",
    "    if not isinstance(bias, torch.Tensor):\n",
    "        is_tensor = False\n",
    "        bias = torch.tensor(bias)\n",
    "    out = bias.expm1().clamp_min(1e-6).log()\n",
    "    if not is_tensor and out.numel() == 1:\n",
    "        return out.item()\n",
    "    return out\n",
    "\n",
    "class biased_softplus(nn.Module):\n",
    "    \"\"\"A biased softplus module.\n",
    "\n",
    "    The bias indicates the value that is to be returned when a zero-tensor is\n",
    "    passed through the transform.\n",
    "\n",
    "    Args:\n",
    "        bias (scalar): 'bias' of the softplus transform. If bias=1.0, then a _bias shift will be computed such that\n",
    "            softplus(0.0 + _bias) = bias.\n",
    "        min_val (scalar): minimum value of the transform.\n",
    "            default: 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bias: float, min_val: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "        self.bias = inv_softplus(bias - min_val)\n",
    "        self.min_val = min_val\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.softplus(x + self.bias) + self.min_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mappings(key: str) -> Callable:\n",
    "    \"\"\"Given an input string, returns a surjective function f(x): R -> R^+.\n",
    "\n",
    "    Args:\n",
    "        key (str): one of \"softplus\", \"exp\", \"relu\", \"expln\",\n",
    "            or \"biased_softplus\". If the key beggins with \"biased_softplus\",\n",
    "            then it needs to take the following form:\n",
    "            ```\"biased_softplus_{bias}\"``` where ```bias``` can be converted to a floating point number that will be used to bias the softplus function.\n",
    "            Alternatively, the ```\"biased_softplus_{bias}_{min_val}\"``` syntax can be used. In that case, the additional ```min_val``` term is a floating point\n",
    "            number that will be used to encode the minimum value of the softplus transform.\n",
    "            In practice, the equation used is softplus(x + bias) + min_val, where bias and min_val are values computed such that the conditions above are met.\n",
    "\n",
    "    Returns:\n",
    "         a Callable\n",
    "\n",
    "    \"\"\"\n",
    "    _mappings: dict[str, Callable] = {\n",
    "        \"softplus\": torch.nn.functional.softplus,\n",
    "        \"exp\": torch.exp,\n",
    "        \"relu\": torch.relu,\n",
    "        \"biased_softplus\": biased_softplus(1.0),\n",
    "    }\n",
    "    if key in _mappings:\n",
    "        return _mappings[key]\n",
    "    elif key.startswith(\"biased_softplus\"):\n",
    "        stripped_key = key.split(\"_\")\n",
    "        if len(stripped_key) == 3:\n",
    "            return biased_softplus(float(stripped_key[-1]))\n",
    "        elif len(stripped_key) == 4:\n",
    "            return biased_softplus(\n",
    "                float(stripped_key[-2]), min_val=float(stripped_key[-1])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid number of args in  {key}\")\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown mapping {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalParamExtractor(nn.Module):\n",
    "    \"\"\"A non-parametric nn.Module that splits its input into loc and scale parameters.\n",
    "\n",
    "    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.\n",
    "\n",
    "    Args:\n",
    "        scale_mapping (str, optional): positive mapping function to be used with the std.\n",
    "            default = \"biased_softplus_1.0\" (i.e. softplus map with bias such that fn(0.0) = 1.0)\n",
    "            choices: \"softplus\", \"exp\", \"relu\", \"biased_softplus_1\";\n",
    "        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> from tensordict.nn.distributions import NormalParamExtractor\n",
    "        >>> from torch import nn\n",
    "        >>> module = nn.Linear(3, 4)\n",
    "        >>> normal_params = NormalParamExtractor()\n",
    "        >>> tensor = torch.randn(3)\n",
    "        >>> loc, scale = normal_params(module(tensor))\n",
    "        >>> print(loc.shape, scale.shape)\n",
    "        torch.Size([2]) torch.Size([2])\n",
    "        >>> assert (scale > 0).all()\n",
    "        >>> # with modules that return more than one tensor\n",
    "        >>> module = nn.LSTM(3, 4)\n",
    "        >>> tensor = torch.randn(4, 2, 3)\n",
    "        >>> loc, scale, others = normal_params(*module(tensor))\n",
    "        >>> print(loc.shape, scale.shape)\n",
    "        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])\n",
    "        >>> assert (scale > 0).all()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_mapping: str = \"biased_softplus_1.0\",\n",
    "        scale_lb = 1e-4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.scale_mapping = scale_mapping\n",
    "        self.scale_lb = scale_lb\n",
    "\n",
    "    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "        tensor, *others = tensors\n",
    "        loc, scale = tensor.chunk(2, -1)\n",
    "        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)\n",
    "        return (loc, scale, *others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iOptionGen(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        super(iOptionGen, self).__init__()\n",
    "        c_channels = 5\n",
    "        self.c_embedding = nn.Embedding(c_channels, d_model)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "          nn.Linear(d_model * 2, d_model * 2),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(d_model * 2, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, s, c):\n",
    "        c = self.c_embedding(c)\n",
    "        z = torch.cat([s, c], dim=1)\n",
    "        z = s + self.net(z)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_vocab: int,\n",
    "    ):\n",
    "        super(iDecoder, self).__init__()\n",
    "        n_layer = 1\n",
    "        ssm_cfg = None\n",
    "        norm_epsilon = 1e-5\n",
    "        rms_norm = False\n",
    "        residual_in_fp32 = False\n",
    "        fused_add_norm = False\n",
    "        device = None\n",
    "        dtype = None\n",
    "        initializer_cfg = None\n",
    "        factory_kwargs = { \"device\": device, \"dtype\": dtype}\n",
    "        self.layers = nn.ModuleList([\n",
    "            create_block(\n",
    "                d_model,\n",
    "                ssm_cfg=ssm_cfg,\n",
    "                norm_epsilon=norm_epsilon,\n",
    "                rms_norm=rms_norm,\n",
    "                residual_in_fp32=residual_in_fp32,\n",
    "                fused_add_norm=fused_add_norm,\n",
    "                layer_idx=i,\n",
    "                **factory_kwargs,\n",
    "            )\n",
    "            for i in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.norm_f = nn.LayerNorm(d_model, eps=norm_epsilon)\n",
    "\n",
    "        self.lm_head = nn.Linear(d_model, n_vocab + 2, bias=False)\n",
    "\n",
    "        self.apply(\n",
    "            partial(\n",
    "                _init_weights,\n",
    "                n_layer=n_layer,\n",
    "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x, num_last_tokens=0):\n",
    "        hidden_states = x\n",
    "        residual = None\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(\n",
    "                hidden_states, residual, inference_params=None\n",
    "            )\n",
    "\n",
    "        residual = (hidden_states + residual) if residual is not None else hidden_states\n",
    "        hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
    "\n",
    "        if num_last_tokens > 0:\n",
    "            hidden_states = hidden_states[:, -num_last_tokens, :]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iActor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_vocab: int,\n",
    "    ):\n",
    "        super(iActor, self).__init__()\n",
    "        self.decoder = iDecoder(d_model, n_vocab)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                z=None,\n",
    "                num_last_tokens=0,\n",
    "        ):\n",
    "        x = x + z\n",
    "        lm_logits = self.decoder(x, num_last_tokens=num_last_tokens)\n",
    "        return lm_logits\n",
    "\n",
    "class iCritic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        super(iCritic, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.fc(s)\n",
    "\n",
    "class iOptionAppraiserFast(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        super(iOptionAppraiserFast, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, s, z):\n",
    "        x = s + z\n",
    "        return self.fc(x)\n",
    "\n",
    "class iSys1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_vocab: int,\n",
    "    ):\n",
    "        super(iSys1, self).__init__()\n",
    "        self.option_gen = iOptionGen(d_model)\n",
    "        self.actor = iActor(d_model, n_vocab)\n",
    "        self.critic = iCritic(d_model)\n",
    "        self.appraiserfast = iOptionAppraiserFast(d_model)\n",
    "        #TODO: Add appraiserfast and appraiserdeep\n",
    "\n",
    "    def generate_option(self, s, c):\n",
    "        z = self.option_gen(s, c)\n",
    "        return z\n",
    "\n",
    "    def appraise_option(self, s, z):\n",
    "        return self.appraiserfast(s, z)\n",
    "\n",
    "    def decode_option(self, x, z, num_last_tokens=0):\n",
    "        return self.actor(x, z, num_last_tokens=num_last_tokens)\n",
    "\n",
    "    def forward(self, x,\n",
    "                c=None,\n",
    "                z=None,\n",
    "                mu=None,\n",
    "                logvar=None,\n",
    "        ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iSys2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        super(iSys2, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(9, d_model)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.pos_embedding(torch.arange(x.size(1)).to(x.device)).unsqueeze(0)\n",
    "        x = self.transformer_encoder(x, mask)\n",
    "        return x, self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "class InferenceParams:\n",
    "    \"\"\"Inference parameters that are passed to the main model in order\n",
    "    to efficienly calculate and store the context during inference.\"\"\"\n",
    "\n",
    "    max_seqlen: int\n",
    "    max_batch_size: int\n",
    "    seqlen_offset: int = 0\n",
    "    batch_size_offset: int = 0\n",
    "    key_value_memory_dict: dict = field(default_factory=dict)\n",
    "    lengths_per_sample: Optional[torch.Tensor] = None\n",
    "\n",
    "    def reset(self, max_seqlen, max_batch_size):\n",
    "        self.max_seqlen = max_seqlen\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.seqlen_offset = 0\n",
    "        if self.lengths_per_sample is not None:\n",
    "            self.lengths_per_sample.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_logits_for_top_p_filtering(logits, top_p):\n",
    "    \"\"\"Set the logits for none top-p values to -inf. Done in-place.\"\"\"\n",
    "    if top_p <= 0.0 or top_p >= 1.0:\n",
    "        return\n",
    "    # First sort and calculate cumulative sum of probabilities.\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n",
    "    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n",
    "    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n",
    "    # scatter sorted tensors to original indexing\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "        1, sorted_indices, sorted_indices_to_remove\n",
    "    )\n",
    "    logits.masked_fill_(indices_to_remove, float(\"-inf\"))\n",
    "\n",
    "\n",
    "def sample(logits, top_k=1, top_p=0.0, temperature=1.0):\n",
    "    \"\"\"Sample from top-k logits.\n",
    "    Arguments:\n",
    "        logits: Tensor of shape (batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    if top_k == 1:  # Short-circuit for greedy decoding\n",
    "        return logits.argmax(dim=-1)\n",
    "    else:\n",
    "        if top_p > 0.0:\n",
    "            assert top_p <= 1.0, \"top-p should be in (0, 1].\"\n",
    "        if top_k > 0:\n",
    "            top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "            logits_top, indices = torch.topk(logits, top_k, dim=-1)\n",
    "            if temperature != 1.0:\n",
    "                logits_top /= temperature\n",
    "            modify_logits_for_top_p_filtering(logits_top, top_p)\n",
    "            return indices[\n",
    "                torch.arange(indices.shape[0], device=indices.device),\n",
    "                torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(dim=-1),\n",
    "            ]\n",
    "        else:\n",
    "            # Clone so that when we modify for top_p we don't change the original logits\n",
    "            logits_top = logits / temperature if temperature != 1.0 else logits.clone()\n",
    "            modify_logits_for_top_p_filtering(logits_top, top_p)\n",
    "            return torch.multinomial(torch.softmax(logits_top, dim=-1), num_samples=1).squeeze(\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "\n",
    "class Igre(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_vocab: int,\n",
    "    ):\n",
    "        super(Igre, self).__init__()\n",
    "        self.encoder = iEncoder(d_model, n_vocab)\n",
    "        self.sys1 = iSys1(d_model, n_vocab)\n",
    "        self.sys2 = iSys2(d_model)\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.sys1.actor.decoder.lm_head.weight = self.encoder.mixer.embedding.weight\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, x, output_pooled_state=False):\n",
    "        return self.encoder(x, output_pooled_state=output_pooled_state)\n",
    "\n",
    "    def _gen1_decode(self, input_ids, z):\n",
    "        batch_size, seqlen_og = input_ids.shape\n",
    "        inference_params = InferenceParams(max_seqlen=2, max_batch_size=batch_size)\n",
    "        scores, seqs = [], [input_ids]\n",
    "\n",
    "        def get_logits(input_ids, z):\n",
    "            return self.sys1_forward_with_option(input_ids, z, num_last_tokens=1)\n",
    "\n",
    "        def sample_tokens(logits, inference_params):\n",
    "            token = sample(logits)\n",
    "            return token.unsqueeze(1)\n",
    "\n",
    "        def should_stop(current_token, inference_params):\n",
    "            max_length = 4\n",
    "            if inference_params.seqlen_offset == 0:\n",
    "                return False\n",
    "            # if eos_token_id is not None and (current_token == eos_token_id).all():\n",
    "            #     return True\n",
    "            if inference_params.seqlen_offset >= max_length - 1:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        while not should_stop(seqs[-1], inference_params):\n",
    "            logits = get_logits(seqs[-1], z)\n",
    "            print(\"logits\")\n",
    "            print(logits.shape)\n",
    "            scores.append(logits)\n",
    "            inference_params.seqlen_offset += seqs[-1].shape[1]\n",
    "            sampled_tokens = sample_tokens(logits, inference_params)\n",
    "            print(\"sampled_tokens\")\n",
    "            print(sampled_tokens)\n",
    "            print(sampled_tokens.shape)\n",
    "            seqs.append(sampled_tokens)\n",
    "        return torch.cat(seqs, dim=1), scores\n",
    "\n",
    "    def sys1_forward_with_option(self, input_ids, z, num_last_tokens=0):\n",
    "        print(input_ids)\n",
    "        x = self.encode(input_ids)\n",
    "        logits = self.sys1.decode_option(x, z, num_last_tokens=num_last_tokens)\n",
    "        return logits\n",
    "\n",
    "    def sys1_logits_and_critic_value_with_option(self, input_ids, z):\n",
    "        _, s = self.encode(input_ids, output_pooled_state=True)\n",
    "        logits = self.sys1.decode_option(s, z)\n",
    "        value = self.sys1.critic(s)\n",
    "        return logits, value\n",
    "\n",
    "    def gen1(self, input_ids, c):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len) input id sequence\n",
    "        c: (batch_size) option class\n",
    "        \"\"\"\n",
    "        # First we get the hidden state representation of the input sequence\n",
    "        x, s = self.encode(input_ids, output_pooled_state=True)\n",
    "        # Now we need to produce the option embedding\n",
    "        z = self.sys1.generate_option(s, c)\n",
    "        # Let's get the output sequence now\n",
    "        seqs, scores = self._gen1_decode(input_ids, z)\n",
    "        return seqs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mocuto/anaconda3/envs/m2/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = Igre(512, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2]), torch.Size([1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([1,0], dtype=torch.long)\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "c = torch.tensor([0], dtype=torch.long)\n",
    "input_ids.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Igre(\n",
       "  (encoder): iEncoder(\n",
       "    (mixer): MixerModel(\n",
       "      (embedding): Embedding(7, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (sys1): iSys1(\n",
       "    (option_gen): iOptionGen(\n",
       "      (c_embedding): Embedding(5, 512)\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (actor): iActor(\n",
       "      (decoder): iDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): Block(\n",
       "            (mixer): Mamba(\n",
       "              (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "              (act): SiLU()\n",
       "              (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "              (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            )\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (norm_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (lm_head): Linear(in_features=512, out_features=7, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (critic): iCritic(\n",
       "      (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (appraiserfast): iOptionAppraiserFast(\n",
       "      (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sys2): iSys2(\n",
       "    (pos_embedding): Embedding(9, 512)\n",
       "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0]], device='cuda:0')\n",
      "logits\n",
      "torch.Size([1, 7])\n",
      "sampled_tokens\n",
      "tensor([[1]], device='cuda:0')\n",
      "torch.Size([1, 1])\n",
      "tensor([[1]], device='cuda:0')\n",
      "logits\n",
      "torch.Size([1, 7])\n",
      "sampled_tokens\n",
      "tensor([[1]], device='cuda:0')\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "blah = model.gen1(input_ids.cuda().long(), c.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 1, 1]], device='cuda:0'),\n",
       " [tensor([[ 0.0896,  0.7436, -0.8696, -0.5133, -0.3299,  0.1710, -0.4116]],\n",
       "         device='cuda:0', grad_fn=<MmBackward0>),\n",
       "  tensor([[ 0.2647,  0.6973, -0.9764,  0.1195,  0.1003,  0.2609,  0.6685]],\n",
       "         device='cuda:0', grad_fn=<MmBackward0>)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
